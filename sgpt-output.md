# Knowledge File for Example Project - ~auto-sgp-refactoring-1.0.1.md
## Source Folder: S:\OneDrive\@Dev\!GPT\ScriptGPT\library\Refactoring\Source
## Output Folder: output
### URL: [https://example.com](https://example.com)

#### VARIABLES ####
NAME: Example Project
FILE: ~auto-sgp-refactoring-1.0.1.md
TYPE: Text/Markdown
VERSION: 1.0.0
DATE: 2024-09-24
AUTHOR: ScriptGPT
GOAL: Track project file parsing activities and output refactoring details.
FOCUS: Refactoring, Analysis, Documentation
TAGS: Project, Refactoring, Analysis

#### COMMANDS ####
- [ ] Initialize Project
- [ ] Run Parsing
- [ ] Generate Report
- [ ] Export Data


#### INSTRUCTIONS ####
1. **Run the initial analysis on the source folder.**
2. **Process the files and extract data summaries.**
3. **Generate a final report in markdown format.**


#### ZEN ####
- Simplicity is the ultimate sophistication.
- Code should be written for humans first, then for machines.
- Continuous improvement leads to mastery.


#### ARTICLES ####
##### Articles on Project #####
- Introduction to Refactoring
- Best Practices for Code Analysis


##### Articles on Analysis #####
- Understanding Code Complexity
- Improving Software Performance


---

## Parsed Files Summary

**File**: S:\OneDrive\@Dev\!GPT\ScriptGPT\library\Refactoring\Source\MooijKetemaKlusenerSchuts2020.pdf
- Time Taken: 13.31s
- Data Extracted: {'text': ['ResearchGate\n\nSee discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/340412930\n\nReducing Code Complexity through Code Refactoring and Model-Based\nRejuvenation\n\nConference Paper - February 2020\n\nDOI: 10.1109/SANER48275,2020.9054823\n\nCITATIONS READS\n6 213\n\n4 authors, including:\n\nSteven Klusener Mathijs Schuts\n\nKlusener Consultancy TNO\n\n8 PUBLICATIONS 22 CITATIONS 27 PUBLICATIONS 176 CITATIONS\nSEE PROFILE SEE PROFILE\n\nAll content following this page was uploaded by Mathijs Schuts on 27 April 2022.\n\nThe user has requested enhancement of the downloaded file.\n', 'Reducing Code Complexity through\nCode Refactoring and Model-Based Rejuvenation\n\nArjan J. Mooij, Jeroen Ketema, Steven Klusener\n\nESI (TNO), Eindhoven, The Netherlands\n\nAbstract—Over time, software tends to grow more complex,\nhampering understandability and further development. To reduce\naccidental complexity, model-based rejuvenation techniques have\nbeen proposed. These techniques combine reverse engineering\n(extracting models) with forward engineering (generating code).\nUnfortunately, model extraction can be error-prone, and vali-\ndation can often only be performed at a late stage by testing\nthe generated code. We intend to mitigate the aforementioned\nchallenges, making model-based rejuvenation more controlled.\n\nWe describe an exploratory case study that aims to rejuvenate\nan industrial embedded software component implementing a\nnested state machine. We combine two techniques. First, we\ndevelop and apply a series of small, automated, case-specific\ncode refactorings that ensure the code (a) uses well-known\nprogramming idioms, and (b) easily maps onto the type of\nmodel we intend to extract. Second, we perform model-based\nrejuvenation focusing on the high-level structure of the code.\n\nThe above combination of techniques gives ample opportunity\nfor early validation, in the form of code reviews and testing,\nas each refactoring is performed directly on the existing code.\nMoreover, aligning the code with the type of model we intend to\nextract significantly simplifies the extraction, making the process\nless error-prone. Hence, we consider code refactoring to be a\nuseful stepping stone towards model-based rejuvenation.\n\nI. INTRODUCTION\n\nEmbedded software is typically reused in multiple product\ngenerations, with changes being made for each generation.\nUnfortunately, as observed by Lehman [1], “as an evolving\nprogram is continually changed, its complexity ...\nunless work is done to maintain or reduce it.’ In other words,\nsoftware modernization is required. However, this turns out\nto be challenging. Industrial practitioners [2] indicate hurdles\nsuch as limited knowledge of the software to be modernized,\nwhich, e.g., makes it hard to identify business logic.\n\nMultiple techniques have been proposed to address the\nchallenges related to software modernization, such as code\nrefactoring [3], [4] and model-based rejuvenation [5]-[8], with\nthe latter combining reverse engineering (extracting abstract\nmodels) with forward engineering (generating new code). In\nspite of this, challenges remain. Pizka [9] observes that “the\nimpact of refactoring is limited if the code base has gone\nastray for a longer period of time.” The risk of introducing\nbugs is also widely recognized [10], and validating correctness\ncan often only be done at a late stage by testing of the changed\nor newly generated code [3]-[5], [7], [8].\n\nIn this paper, we propose a mitigation strategy for the\nchallenges sketched above. The strategy (see Fig. 1) com-\n\nincreases\n\nMathijs Schuts\nPhilips, Best, The Netherlands\n\nbines code refactoring with model-based rejuvenation (Extract-\nTransform-Generate). Specifically, to ensure that the code\n\ne uses well-known programming idioms, and\n\ne easily maps onto the type of model we intend to extract,\nwe first refactor the code in a number of small, case-specific\nsteps. Once done, we apply model-based rejuvenation.\n\nTransform\nModel\nExtact nae Extract Generates Generate\n(askew) 2-7 (aligned) (askew) 9 (aligned)\n= Refactor Refactor ys,\nCode +, Code +, Code % Code\n\nFigure 1. Code refactoring and model-based rejuvenation. The solid arrows\nrepresent our proposed strategy, which combines refactoring and rejuvenation.\nThe dashed arrows represent a more traditional rejuvenation approach that\nlacks explicit refactoring and transformation steps.\n\nAlthough performing a number of small refactoring steps up\nfront may sound counter-intuitive, as we eventually generate\nnew code, the benefits of this approach are four-fold:\n\n1) The refactoring steps give ample opportunity for early\n\nvalidation, as changes can be easily reviewed and tested.\n\n2) Ensuring that the code easily maps onto the type of\nmodel we intend to extract simplifies model extraction.\n\n3) As low-level code fragments have been tested and follow\nan idiomatic style after refactoring, we can re-use them\nas part of the rejuvenated code, allowing us to abstract\nfrom these fragments and focus on the high-level code\nstructure during model extraction.\n\n4) Aligning the refactored code and the extracted model\nmeans that the extracted model has clearly defined\nsemantics in terms of the code.\n\nResearch Method and Contributions: We conducted the\nreported research as an exploratory case study that aimed to\nmodernize an industrial software component (see Sect. II).\nOur main contribution is the formulation of a strategy making\nmodel-based rejuvenation more controlled. We report on the\nstrategy in Sects. III and IV in the context of our case study.\n\nIl. INDUSTRIAL CASE STUDY\n\nOur case study concerns a Controller Area Network (CAN)\nadapter from a large embedded system. The adapter, written in\nC++, was hand coded and implements a nested state machine,\nwith states and substates, that handles incoming messages and\n', "StateEnum StateInit::ReceiveResp(Resp+ pCanMsg) {\nCanModuleStateEnum state = GetState();\nm_iReceivedMsg++;\nif (m_iReceivedMsg == MODULE_STATUS) {\n\nVersionRequest *pNewCanMsg = NULL;\nif (m_bPost) {\n\nm_bPost = ! (pCanMsg->GetPostError ());\n\nt\n9 if (pCanMsg->GetRuntimeError()) {\n10 AddDefectiveControl (pCanMsg->GetControlID());\nli state = FATAL;\n12 } else {\n13 pNewCanMsg = new VersionRequest;\n14 pNewCanMsg->Set ID (m_byID) ;\n15 SendMsg (pNewCanMsg) ;\n\n}\n17. } else {\n\n18 m_iReceivedMsg--;\n19}\n\n20 return (state);\n\n21 }\n\nFigure 2. Coding style of the original code\n\nperforms some data processing. The goal is to modernize\nthe adapter to facilitate the implementation of envisioned\narchitectural changes in future product generations.\n\nThe state machine consists of 14 states implemented using\nthe state pattern [11], with two states inheriting from a single\nabstract, partial state. Each state uses up to 13 variables to\nstore state-specific data. The substates, of which there are 69,\nare distributed over eight states, with the maximum number of\nsubstates of a state being 13. The substates are implemented\nusing enumerated types and their C++ integer representation.\nThere are 27 incoming message types.\n\nThe adapter has evolved over a long period and functions\nreliably, but is considered difficult to understand, maintain, and\nextend. Inspection of the code indicated the use of a number\nof unconventional programming idioms:\n\ne the integer variables representing substates are often\nchanged multiple times during the handling of a single\nmessage and generally hold values that are off-by-one (an\nincrement operation occurs just before every use);\n\ne large non-encapsulated code fragments are used to im-\nplement elementary behavior such as sending messages.\n\nFigure 2 illustrates the coding style. The code handles incom-\ning messages of type Resp in state StateInit and returns\nthe next state. The integer variable m_iReceivedMsg rep-\nresents a substate. Lines 5-16 encode the behavior in substate\nMODULE_STATUS for message type Resp.\n\nIntended Model and Extraction Approach: As the adapter\nimplements a state machine, we would like to obtain a state\nmachine model as part of our rejuvenation effort. To this end,\nwe can either apply automata learning techniques or static code\nanalysis techniques. As the adapter performs some amount of\ndata processing, which automata learning techniques do not\nhandle well, we opted for static code analysis techniques.\n\nIII. IMPROVING PROGRAMMING IDIOMS\n\nAs highlighted above, the CAN adapter uses some un-\nconventional programming idioms, hindering understandabil-\nity. To improve understandability, we began by applying a\nnumber of refactorings that substituted the idioms by more\n\n1 StateEnum $class::$method($$args) {\n\n2 aif (m_iReceivedMsg == $substate) {\n\n3 // REPLACE: S$boolVar = SboolVar && $boolExp;\n4 if (Sboolvar) {\n\n5 Sboolvar = $boolExp;\n\n6 t\n\n7 43\n\n8}\n\nFigure 3. An example refactoring specified in our refactoring language\n\nconventional ones. The applied refactorings were selected\nby hand, and were mostly case specific (e.g., because no\nstandard refactorings exist that can turn an off-by-one substate\nrepresentation into a non-off-by-one representation).\n\nA. Refactoring Technique\n\nAfter manually selecting appropriate refactorings, we want\nto apply them automatically, as this allows for repeatability\nover, e.g., multiple development branches. Moreover, to not\ninterfere with active development, we do not want annotate\nthe source code that is to be refactored to indicate where\nrefactorings should be applied.\n\n1) Refactoring Language: To satisfy the above constraints,\nwe specify our refactorings in a separate language. We do\nnot aim for the language to be fully generic; the language\nshould simply suffice for our industrial case. As the software\ndevelopers we work with are already familiar with C++, the\nlanguage is defined as an extension of C++. This allows the\ndevelopers to review proposed refactorings without having to\nlearn a completely new language.\n\nThe refactoring language consists of several elements that\nexpress which refactoring operations to apply, to which code\nfragments in which contexts, and in which order and how\noften. Figure 3 showcases some of these elements. The refac-\ntoring operation is specified via an annotation (line 3) that\noccurs immediately above the code pattern to which it should\nbe applied (lines 4-6). The context in which the operation\nmay be applied surrounds the operation and pattern (lines 1-\n2 and 7-8). As a convention [8], an identifier starting with a\nsingle $ denotes a placeholder for a single syntactical element\n(expression, function argument, ...), and an identifier starting\nwith $$ denotes a list of placeholders. Although not shown,\nwe use annotations similar to those for refactoring operations\nto order refactorings and express how often they should be\napplied.\n\n2) Refactoring Operations: A refactoring operation either\nspecifies how a certain code fragment should be changed, or\ninstructs the refactoring engine to extract certain values for\nlater use. We can also specify side-conditions, e.g., to indicate\nwhether an operation should be applied when the code being\nrefactored has side-effects.\n\nCode-changing refactoring operations come in two varieties:\n\n« replacement of concrete syntax patterns (see Fig. 3), and\n\n¢ direct manipulation of the abstract syntax tree (AST).\nIn both cases we build on the C++ parser that is part of the\nEclipse C/C++ Development Tools (CDT)!. Building on this\n\n'http://www.eclipse.org/cdt/\n", '1 StateEnum StateInit::ReceiveResp(Resp* pCanMsg) {\n2  CanModuleStateEnum state = GetState();\n\n3 if (InSubState(MODULE_STATUS)) {\n\n4 m_bPost = m_bPost && !pCanMsg->GetPostError ();\n5 if (pCanMsg->GetRuntimeError()) {\n\n6 AddDefectiveControl (pCanMsg->GetControlID());\n7 state = FATAL;\n\n8\n\n} else {\n9 SendMsg (new VersionRequest (m_byID));\n10 ChangeSubSt ate (VERSION_REQUESTED) ;\nli }\n\n12. }\n\n13 return state;\n\n14 }\n\nFigure 4. Coding style after refactoring\n\nparser side-steps the need to write our own, which is a non-\ntrivial task.\n\nInitially, we focused only on replacement of concrete syntax\npatterns, which software developers find intuitive. However,\nwe observed that some operations are more conveniently\nexpressed at the AST-level, such as:\n\ne inlining methods, to avoid having to specify complete\nmethod bodies;\n\n¢ extracting methods, to avoid having to specify method\nbodies twice (once for creating methods, and once for\nidentifying locations where calls should be introduced);\n\n« removing unused variables, to avoid having to specify\nwhat “unused” means using concrete code patterns;\n\n« combining series of assignments to the same variable,\nto avoid having to explicitly specify the multitude of\npossible concrete assignment patterns;\n\n« moving statements backwards and forwards by swapping\nthem with independent predecessor and successor state-\nments, again to avoid the multitude of possible patterns.\n\nB. Industrial Case Study\n\nAlthough all states and substates of the CAN adapter all\nhave different functionality, their original coding patterns were\nall very similar, and hence they could be refactored in similar\nways. We applied the following refactoring steps in order,\nwhere steps 3-5 depend on steps 1-2:\n\n1) removing dead code, and declaring local variables as late\nas possible in the closest encompassing scope;\n\n2) ensuring that updates to (sub)states occur as late as pos-\nsible and are not mixed with the sending of messages;\n\n3) replacing updates to substates via integer operations\nwith direct assignments of the members of the relevant\nenumerated types, and ensuring these are not off-by-one;\n\n4) encapsulating all operations required for message cre-\nation in message class constructors;\n\n5) making logging homogeneous, and introducing auxiliary\nmethods encapsulating data processing.\n\nThe result of applying the refactorings to the code of Fig. 2\ncan be found in Fig. 4. The first refactoring is obviously\ngeneric, while the others are case specific. All refactorings\naim to improve the understandability of the code.\n\nThe individual refactorings were easily validated (by means\nof code reviews and existing test suites), and integrating the\n\nresults into the code base went smoothly. The refactorings\ncould also easily be adapted to other development branches,\nand the developers of the adapter considered the refactored\ncode to be much easier to understand.\n\nC. What We Learned\n\nWe learned the following while refactoring the adapter:\n\n¢ specifying case-specific refactorings is both about the\noperations and the context in which they are applied;\n\ne specifying refactorings is sometimes done best at the\nconcrete syntax-level, and sometimes at the AST-level;\n\ne case-specific refactorings enable code changes that are\nonly valid for the specific case;\n\ne incrementally applying small, case-specific refactorings\nfacilitates early validation.\n\nIV. REDESIGNING THE HIGH-LEVEL STRUCTURE\n\nThe refactoring steps from the previous section addressed\nthe main issues with the CAN adapter identified in Sect. II.\nHowever, the steps did not improve insight into the overall\nbehavior of the implemented state machine. To improve in-\nsight, we next visualized the state machine by extracting its\nhigh-level structure (again using CDT’s C++ parser), while\nignoring low-level details. The visualization provided some\nnew insights and could be kept up-to-date by regenerating it,\nbut its size was substantial given the number of states and\nsubstates, which still made it difficult to fully comprehend the\nstate machine.\n\nWe next investigated whether we could improve the high-\nlevel structure of the code to reduce the need for a separate\nvisualization. The key observation we made was that the code\nwas structured from the top down (i.e., viewed from class\ndefinitions, via method definitions, to control statements) as\n\nstate + message type — substate > logic\nwhereas the visualization was structured as\n\nstate + substate > message type — logic.\nAlthough the top-down structure of the code reduces code\nduplication when incoming messages are handled similarly\nacross multiple substates, the top-down structure of the visu-\nalization is more useful when trying to understand the code’s\nbehavior in terms of the order of operations from entering a\ncertain substate, via processing of various incoming messages,\nto exiting the substate.\n\nTo transform the top-down code structure to match the one\nused in the visualization, we first created a model by extracting\nthe high-level structure of the code. Thereafter, we transformed\nthe model to give it the appropriate top-down structure, and\nwe generated new code by combining the transformed model\nwith low-level code fragments that we had retained.\n\nA. Rejuvenation Technique\n\nBuilding again on CDT’s C++ parser, we parse the relevant\nsource files without expanding any #include directives. We\nthen extract a high-level model that is close to the structure\nof the original source code, using pattern matching on both\nconcrete syntax patterns and AST patterns. We retain low-level\n', 'code details by saving relevant code fragments along with the\nhigh-level model, as proposed by [8].\n\nTo validate the extraction and give the extracted model\nsemantics, we develop a code generator in Xtend? that is able\nto regenerate the original code (apart from formatting) from\nthe high-level model and the retained low-level fragments.\nThereafter, we transform the model to obtain the desired high-\nlevel structure, and use Xtend to generate new code from\nthe transformed model and the retained low-level fragments.\nLastly, we use CDT’s code formatting capabilities to format\nthe new code (to nicely integrate the retained fragments).\n\nB. Industrial Case Study\n\nOnce we started to extract a state machine model from the\nCAN adapter code, we realized that although we did improve\nthe structure of the code while refactoring, extraction was not\nas straightforward as it could be. Therefore, before proceeding,\nwe applied a number of additional refactorings:\n\ne introducing explicit state assignments in simple cases\n\nwhere the state does not change (for homogeneity);\n\n¢ replacing if-statements with guards containing a disjunct\n\nwith an InSubState call by a chain of if-then-else\nstatements (to isolate InSubState);\n\n¢ replacing if-statements with guards containing a conjunct\n\nwith an InSubState call by nested if-statements (again\nto isolate InSubState);\n\n¢ moving calls to the InSubState method out of helper\n\nfunctions, by inlining fragments of these functions.\nThe above refactorings were geared towards making the state\nmachine, and hence the high-level structure, more explicit. We\ndid not perform these steps earlier, as they somewhat increased\nthe number of lines of code.\n\nOnce we applied the above refactorings, model extraction\nwas straightforward, with the inheritance from partial states\nbeing the only source of complexity. After extraction we\ntransformed the model to match the top-down structure of our\nvisualization, and generated new code.\n\nFigure 5 presents the result of applying the above steps to\nthe code of Fig. 4. The depicted method handles all messages\nthat can be received in substate MODULE_STATUS of state\nStateInit. Of course, as the steps changed the high-\nlevel structure, their impact is difficult to gauge from code\nfragments only. However, the adapter’s developers considered\nthe code much better to understand even without a separate\nvisualization, although code size increased slightly.\n\nC. What We Learned\n\nWe learned the following while rejuvenating the adapter:\n\ne aligning the original code with the type of model we in-\ntend to extract simplifies the extraction, and also enables\nearly validation through easy regeneration of the code;\n\n¢ retaining low-level code fragments and linking these to\nthe extracted model allows us to focus on the high-level\nbehavior in the model;\n\n?https://www.eclipse.org/xtend/\n\n1 StateEnum StateInit::ModuleStatus (Msg+ pMsg) {\n\n2  CanModuleStateEnum state = GetState();\n\n3 if (dynamic_cast<Resp*>(pMsg) != NULL) {\n\n4 Resp* pCanMsg = dynamic_cast<Resp*>(pMsg) ;\n\n5 m_bPost = m_bPost && !pCanMsg->GetPostError ();\n6 if (pCanMsg->GetRuntimeError()) {\n\n7 AddDefect iveCont rol (pCanMsg->GetCont rolID());\n8 state = FATAL;\n\n9 } else {\n\n10 SendMsg (new VersionRequest (m_byImageID) );\n\n11 ChangeSubState (VERSION_REQUESTED) ;\n\n12 }\n\n13}\n\n14 return state;\n\n15 }\n\nFigure 5. Coding style after refactoring and rejuvenation\n\ne code refactoring can act as a useful stepping stone to-\nwards model-based rejuvenation;\n\ne changing the high-level code structure can reduce the\nneed for separate visualizations.\n\nV. THREATS TO VALIDITY\n\nThreats to internal validity come from the way in which we\ncarried out our case study. Our study focused on qualitative\naspects and ignored quantitative aspects. The time to create\nthe case-specific refactoring steps was not considered.\n\nThreats to construct validity come from the way in which\nwe evaluated our case study. We did not attempt to transform\nthe considered adapter using different approaches.\n\nThreats to external validity come from the way in which\nour results will generalize to other software components. Our\ncase study only considered a single adapter. We believe that\nthe presented approach could have helped in case studies such\nas [8], but more research is needed to determine its generality.\n\nVI. RELATED WORK\n\nWe discuss various avenues of related work.\n\nA. Model-Based Methods for Software Modernization\n\nThe literature review from [12] compares in detail fifteen\ndifferent model-driven reverse engineering approaches, and\nobserves that the approaches and applications are diverse.\n\nIndustrial case studies on model-based rejuvenation are\npresented in [5], [8]. The study from [5] employs models\nbased on generic concepts such as data structures, algorithms,\nand GUI elements. Any source element that does not easily\nfit the model is reported to the user. The study from [8]\nemploys domain-specific models, and inspired us to retain low-\nlevel code fragments during rejuvenation. Contrary to us, the\nauthors of [8] do not first attempt to align the code with the\ntype of model they intend to extract.\n\nAn industrial case that combines model extraction and\ncode refactoring is presented in [13]. Architectural models\nare extracted to give insight into code structure, after which\ndesired model changes are specified. Next, case-specific code\nrefactorings are derived, but no rejuvenation is performed.\n', 'B. Code Refactoring Tools\n\nMost refactorings are performed in batches [14], and\ndeveloper-specified refactorings are sometimes seen as the\nHoly Grail of refactoring [15]. Unfortunately, tooling for\ndeveloper-specified refactorings is currently lacking [10].\n\nFrameworks for specifying refactorings generally only allow\nrefactorings to be specified at the AST-level. Examples of\nsuch frameworks are Clang’s C++ refactoring engine*, the\nJava refactoring engine presented in [16], and the MoDisco\nframework [17], which has been used to perform large-scale\nJava refactorings.\n\nSpecifying C/C++ refactorings using concrete syntax pat-\nterns is supported by a limited number of tools. Coccinelle [18]\nenables developers to define C refactorings, and Rascal [19]\nsupports C/C++ via its ClaiR* module [20]. DMS [21] and\nTXL [22], which focus on program transformations including\nrestructuring, also support concrete C/C++ syntax patterns, and\nhave been used in commercial applications.\n\nC. Extracting State Machine Models from Code\n\nTo enhance insight in software, [23] describes a method\nfor extracting visual representations of state machines imple-\nmented in C. The method is based on matching specific im-\nplementation patterns. Nested state machines and conditional\nstate transitions are left as future work, although the authors do\ngive examples that exhibit a similar kind of top-down structure\nas the state machine from our case study (see Sect. IV).\n\nAn industrial case study on extracting flat state machine\nmodels from C code is described in [24]. The approach taken\nin the study is similar to ours, namely, recognizing occurrences\nof specific implementation patterns that are used consistently\nin the considered code base. Semi-automated techniques for\nextracting state machines from C code that does not follow\nspecific implementation patterns are presented in [25] in the\ncontext of embedded control software.\n\nVII. CONCLUSIONS AND FUTURE WORK\n\nAs we have shown, code refactoring can be used to make\nmodel-based rejuvenation a more controlled software modern-\nization technique. Code refactoring allows for early validation,\nand can be used to simplify the extraction process.\n\nIn future work we would like to develop more sophisti-\ncated techniques for specifying, applying, and validating case-\nspecific refactoring operations, to allow for easier refactoring\nof code. We would also like to establish whether refactoring\nafter rejuvenation could be beneficial.\n\nACKNOWLEDGMENTS\n\nThis research was carried out as part of the Vivace program\nunder the responsibility of ESI (TNO) with Royal Philips as\ncarrying industrial partner. The Vivace program is supported\nby the Netherlands Organisation for Applied Scientific Re-\nsearch TNO.\n\n3https://clang.Ilvm.org/docs/RefactoringEngine.html\n4https://github.com/ewi-swat/clair\n\nWe would like to thank Peter Blom and Roel Kolman of\nPhilips for their technical support and their help integrating\nthe modernized code.\n\nREFERENCES\n\n1] M. M. Lehman, “Programs, life cycles, and laws of software evolution,”\nProceedings of the IEEE, vol. 68, no. 9, pp. 1060-1076, 1980.\n\n2] R. Khadka, B. V. Batlajery, A. Saeidi, S. Jansen, and J. Hage, “How do\nprofessionals perceive legacy systems and software modernization?” in\nICSE’14, 2014, pp. 36-47.\n\n3] M. Fowler, Refactoring: Improving the Design of Existing Code.\nAddison-Wesley Professional, 1999.\n\n4] M. C. Feathers, Working Effectively with Legacy Code.\n2004.\n\n5] F. Fleurey, E. Breton, B. Baudry, A. Nicolas, and J.-M. Jézéquel,\n“Model-driven engineering for software migration in a large industrial\ncontext,” in MoDELS’07, 2007, pp. 482-497.\n\n6] A. J. Mooij, G. Eggen, J. Hooman, and H. van Wezep, “Cost-\neffective industrial software rejuvenation using domain-specific models,”\nin ICMT’I5, 2015, pp. 66-81.\n\n7) A. J. Mooij, M. M. Joy, G. Eggen, P. Janson, and A. Radulescu, “In-\ndustrial software rejuvenation using open-source parsers,” in JCMT’16,\n2016, pp. 157-172.\n\n8] S. Klusener, A. J. Mooij, J. Ketema, and H. van Wezep, “Reducing code\nduplication by identifying fresh domain abstractions,” in JCSME’18,\n2018, pp. 569-578.\n\n9] M. Pizka, “Straightening spaghetti-code with refactoring?” in SERP’04,\n2004, pp. 846-852.\n\n10] M. Kim, T. Zimmermann, and N. Nagappan, “An empirical study of\nrefactoring challenges and benefits at Microsoft,” JEEE Trans. Software\nEng., vol. 40, no. 7, pp. 633-649, 2014.\n\n11] E. Gamma, R. Helm, R. Johnson, and J. Vlissides, Design Patterns:\nElements of Reusable Object-Oriented Software. Addison-Wesley,\n1995.\n\n12] C. Raibulet, F. A. Fontana, and M. Zanoni, “Model-driven reverse\nengineering approaches: A systematic literature review,” IEEE Access,\nvol. 5, pp. 14516-14542, 2017.\n\n13] D. Dams, A. J. Mooij, P. Kramer, A. Radulescu, and J. Vanhara, “Model-\nbased software restructuring: Lessons from cleaning up COM interfaces\nin industrial legacy code,” in SANER’18, 2018, pp. 552-556.\n\n14] E. R. Murphy-Hill, C. Parnin, and A. P. Black, “How we refactor, and\nhow we know it,” JEEE Trans. Software Eng., vol. 38, no. 1, pp. 5-18,\n2012.\n\n15] R. M. Fuhrer, M. Keller, and A. Kiezun, “Advanced refactoring in the\nEclipse JDT: past, present, and future,” in WRT’07, 2007, pp. 30-31.\n16] M. Schiafer and O. de Moor, “Specifying and implementing refactor-\nings,” in OOPSLA’10, 2010, pp. 286-301.\n\n17] H. Bruneliére, J. Cabot, G. Dupé, and F. Madiot, “MoDisco: A model\ndriven reverse engineering framework,” Information & Software Tech-\nnology, vol. 56, no. 8, pp. 1012-1032, 2014.\n\n18] Y. Padioleau, J. L. Lawall, and G. Muller, “Understanding collateral\nevolution in Linux device drivers,” in EuroSys’06, 2006, pp. 59-71.\n\n19] P. Klint, T. van der Storm, and J. J. Vinju, “RASCAL: A domain specific\nlanguage for source code analysis and manipulation,” in SCAM’09, 2009,\npp. 168-177.\n\n20] R. Aarssen, J. J. Vinju, and T. van der Storm, “Concrete syntax with\nblack box parsers,” The Art, Science, and Engineering of Programming,\nvol. 3, no. 3, 2019.\n\n21) I. D. Baxter, C. W. Pidgeon, and M. Mehlich, “DMS®: Program\ntransformations for practical scalable software evolution,” in ICSE’04,\n2004, pp. 625-634.\n\n22] J. R. Cordy, T. R. Dean, A. J. Malton, and K. A. Schneider, “Source\ntransformation in software engineering using the TXL transformation\nsystem,” Information & Software Technology, vol. 44, no. 13, pp. 827—\n837, 2002.\n\n23] S.S. Somé and T. Lethbridge, “Enhancing program comprehension with\nrecovered state models,” in JWPC’02, 2002, pp. 85-93.\n\n24] M.G. J. van den Brand, A. Serebrenik, and D. van Zeeland, “Extraction\nof state machines of legacy C code with Cpp2XMI,” in BENEVOL’08,\n2008, pp. 28-30.\n\n25] W. Said, J. Quante, and R. Koschke, “On state machine mining from\nembedded control software,” in ICSME’18, 2018, pp. 138-148.\n\nPrentice Hall,\n\n']}


**File**: S:\OneDrive\@Dev\!GPT\ScriptGPT\library\Refactoring\Source\Analysis_of_Code_Refactoring_Impact_on_Software_Qu.pdf
- Time Taken: 13.48s
- Data Extracted: {'text': ['MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\nAnalysis of Code Refactoring Impact on Software Quality\n\nAmandeep Kaur‘ and Manpreet Kaur*\n\n‘Computer Science and Engg. Department, Punjab Technical University, Jalandhar, India, amansidhu1092@gmail.com\n2Computer Science and Engg. Department, Punjab Technical University, Jalandhar, India, manpreet.kaur@bbsbec.ac.in\n\nAbstract. Code refactoring is a “Technique used for restructuring an existing source code, improving its internal\nstructure without changing its external behaviour”. It is the process of changing a source code in such a way that it\ndoes not alter the external behaviour of the code yet improves its internal structure. It is a way to clean up code that\nminimizes the chances of introducing bugs. Refactoring is a change made to the internal structure of a software\ncomponent to make it easier to understand and cheaper to modify, without changing the observable behaviour of that\nsoftware component. Bad smells indicate that there is something wrong in the code that have to refactor. There are\ndifferent tools that are available to identify and remove these bad smells. It is a technique that change our source code\nin a more readable and maintainable form by removing the bad smells from the code. Refactoring is used to improve\nthe quality of software by reducing the complexity. In this paper bad smells are found and perform the refactoring\nbased on these bad smell and then find the complexity of program and compare with initial complexity. This paper\nshows that when refactoring is performed the complexity of software decrease and easily understandable.\n\n1 Introduction\n\n1.1 Refactoring\n\nRefactoring is a change made to the internal structure of a\nsoftware component to make it easier to understand and\ncheaper to modify, without changing the observable\nbehaviour of that software component [1]. This definition\nis attractive because it not only defines what refactoring\nis, but also why refactoring are performed. Refactoring\nare performed to improve the functional factorings of a\nsoftware system in order to increase understand ability\nand modifiability.\n\nRefactoring is the process of changing a software system\nin such a way that it does not alter the external behaviour\nof the code yet improves its internal structure. It is a way\nto clean up code that minimizes the chances of\nintroducing bugs.\n\nCode refactoring is a “disciplined technique for\nrestructuring an existing body of code, altering its\ninternal structure without changing its external\nbehaviour”, undertaken in order to improve some of the\nnon-functional attributes of the software. Typically, this\nis done by applying a series of “Refactoring”, each of\nwhich is a (usually) tiny change in a computer program’s\nsource code that does not modify its conformance to\nfunctional requirements. Advantages include improved\ncode readability and reduced complexity to improve the\nmaintainability of the source code, as well as a more\n\nAmandeep Kaur: amansidhu1092@gmail.com\n\nexpressive internal architecture or object model to\nimprove extensibility.\n\nRefactoring is the process of taking existing code and\nchanging it in some way to make it more readable and\nperhaps less complex. The key thing to note, is that when\nthe code is changed, is does not affect the underlying\nfunctionality of the code itself. So the changes you’re\nmaking are literally just to make the code easier.\n\nManual refactoring are often error-prone and time-\nconsuming [2]. For instance, renaming a method requires\nchecking that the method’s new name is not yet in use as\nwell as updating all invocations. Besides being obviously\ntime-consuming this operation is also error-prone because\npolymorphism may cause a forgotten update to compile\ncorrectly but change the software’s behaviour\ninadvertently. This requires the maintainer to manually\nlocate the changed functionality and update the omitted\ninvocation. As a result, these refactoring are often not\nperformed and the software’s structure deteriorates as a\nresult of functional changes.\n\nAutomated refactoring tools can reduce these problems.\nIf performed by a tool that can guarantee that the\nrefactoring it performs are behaviour-preserving, the\nerror-prone aspect is mitigated. Furthermore, performing\nthe required analysis automatically can drastically reduce\nthe time required to perform refactoring. Automated\nrefactoring is made possible through the use of\npreconditions that if satisfied guarantee that a refactoring\nis behaviour-preserving [3]. Also, composing larger\n\n© The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons Attribution\n\nLicense 4.0 (http://creativecommons.org/licenses/by/4.0/).\n', 'MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\nrefactoring from smaller ones means that a relatively\nsmall set of automated refactoring can be used to perform\na large set of refactoring.\n\nWhen performed by a tool, refactoring consists of at least\ntwo steps [2]. The first is analysis, where the program to\nbe refactored is analyzed in order to determine whether\nthe desired refactoring’s preconditions are satisfied. If\nthis is the case, the second step is executed, the actual\ntransformation of the program source code. Both steps\nmust take both the syntax and semantics of the\nprogramming language the tool supports into\nconsideration, making it a considerable effort to\nimplement a refactoring tool from scratch. This may\nexplain why refactoring tools are often integrated with\nother development tools such as IDEs, since these\ntypically expose a large part of this required functionality\nwhich can be reused by the refactoring tool. No matter\nhow complex a refactoring tool is, in theory it only needs\nto be implemented once for each programming language\nwhere automated refactoring functionality is desired and\nthen evolved along with the language it supports. Given\nthe large developer communities and companies backing\nthe most popular programming languages, several\nrefactoring tools are readily available.\n\n1.2 Bad smell\n\nBad smells indicate that there is something wrong in the\ncode that we have to refactor. Bad smells are design\nflaws in the code. There are many tools that are available\nto identify the bad smells and remove these bad smells by\nusing refactoring tools and by using refactoring\ntechnique. Refactoring is a technique that restrict our\nsource code in a more readable and maintainable form by\nremoving the bad smells from the code. Refactoring does\nnot change the external behaviour of software[7].\n\nBad smells are potential problems that can be remove\nthrough refactoring. There are various kind of bad smells\nthat make our source code difficult to understand and\nmodify. Bad in a code is not any problem but may lead to\nany mistake in future. We can detect and refactor this bad\nsmells through various tools. Jdeodorant are such kind of\ntools that are used to detect the bad smells [7]. Bad smells\nare of following types:\n\n* LARGE CLASS- means a class that is too large. Size\nof the large class is too much large. This type of class is\ndifficult to understand and it is too much hard to\nunderstand that which functioning is performed by this\nclass.\n\n* LONG METHOD- is a method that is too long. Long\nmethods are same as large classes. Long methods also\nleads to confusion for the new developer and these are\nalso very much difficult to understand.\n\n* DUPLICATE CODE- is a code that is repeated at too\nmany places in a same source code. The problem arises\nwhen we try to update this code. We have updated the\nduplicate code at all the places in which this code is\nplaced. If we forget to update the code on single place, it\nmay create problem. Hence duplicate code is difficult to\nmaintain.\n\n* FEATURE ENVY - is a bad smell that violates the\nprinciple of its class in a source code. From many\ndiscussion we found that this a bad smell that is not\ninterested to use its own source class but interested to use\nany another source class.\n\nAll these bad smells can be clean up by using the\nrefactoring.\n\n1.3 Refactoring Techniques\n\nRefactoring Technique is used to remove the bad smells\nfrom code and make code clean. Refactoring is a set of\ntechniques, procedures and steps to keep your source\ncode as clean as possible. Clean code, on the other hand,\ndescribes how well-written code should look in ideal\nconditions. In a pragmatic sense, all refactoring\nrepresents simple steps toward clean code. There are\nsome basic techniques that are used for refactoring[7]-\n\n* EXTRACT METHOD:- Extract method means\nextract a method that appear at many places in the source\ncode and place it in a different method.\n\n* INLINE METHOD:-Its working is totally opposite to\nthe extract method. A method of body is as clear as its\nname. Put the body of the method into the body of its\ncaller. Then remove the method.\n\n* MOVE METHOD:-Move method means moving the\nmethod from one class to another when a class has too\nmany functions to do.\n\n+ INLINE CLASS:-This method is applicable to those\nclasses which are not doing too much work. With the\nhelp of inline class we can move the functionality of this\nclass to another class and remove the class.\n\n* RENAME METHOD:-Rename method simply\nmeans renames any method. We can rename the method\naccording to the functionality of the method. It makes our\ncode easier to understand.\n\n* REPLACE ARRAY WITH OBJECT:-If we have\nmany or different elements. We can replace this array\nwith an object, in this object each element have specific\nfield.\n\nRefactoring technique is used to make code clean . After\nfinding the bad smell in code then we apply the respected\nrefactoring to remove the bad smell in code. After\nperforming the refactoring the code complexity is\ndecrease and it is easily understandable by anyone.\n\n1.4 Refactoring Tool\n\nECLIPSE:- Eclipse is an IDE(Integrated Development\nEnvironment). The Eclipse platform which provides the\nfoundation for the Eclipse IDE is composed of plug-ins\nand is designed to be extensible using additional plug-ins.\nDeveloped using Java, the Eclipse platform can be used\nto develop rich client applications, integrated\ndevelopment environments and other tools. Eclipse can\nbe used as an IDE for any programming language for\nwhich a plug-in is available. In it there is a workspace\n', 'MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\nand an extensible plug-in for customizing the\nenvironment. Written in java language it is used to\ndevelop application. Eclipse may also helpful to create\nthe application in C, C++, PHP and COBOL and many\nmore languages. Eclipse also provide an option to\nrefactor the source code. There are many plug-in for\neclipse that can used to detect the bad smells in the code.\nTo refactor the bad smells it provides various refactoring\ntechniques.\n\nIn order to increase the availability of refactoring tools\nfor DSLs it is worthwhile to investigate whether\nrefactoring tool implementation can be simplified in\norder to be able to develop refactoring tools (for DSLs)\nfaster or even generate them. To this end, it is useful to\nexamine the implementation of existing refactoring tools\nin order to gain insight into the complexities of their\ndevelopment. Eclipse is a suitable candidate for such an\nexamination, since it is a very widely used IDE and it\nsupports a large set of refactoring out-of-the-box,\nespecially in the JDT, the Java Development Tooling\nplug-in[18].\n\nPlugins-\n\nJDeodorant is an Eclipse plugin that are used to detect the\nbad smells. JDeodorant was simplistic in both design and\nusage. It support 4 type of bad smells-God class, Long\nmethod, Type checking, Feature envy. JDeodorant that\nautomatically identifies the Feature Envy, God Class,\nLong Method and Switch Statement (in its Type\nChecking variant) code smells in Java programs. This\nplugin is used to find the bad smells in the software.\n\nEMF Refactor are such kind of tool that are used to\nperform the refactoring on code. It support various type\nof refactoring like Rename Method/Class, Extract\nMethod/Class, Inline Method , Move ,Replace etc. This\nplugin is used to remove the bad smells in the software.\n\nEclipse Metrics Plugin are such kind of tool that are used\nto calculate the various type of metrics in the program. It\ncalculate Lines of code, No. of attributes, No. of Classes,\nNo. of Methods, Cohesion, Coupling, McCabe\nCyclomatic Complexity, Weighted methods per class.\nThis plugin is used to find the complexity of software to\ndetect the quality of software.\n\n2 Literature Survey\n\nFowler et al., [1] say that, Refactoring is basically the\nobject-oriented variant of restructuring: “the process of\nchanging a software system in such a way that it does not\nalter the external behavior of the code, yet improves its\ninternal structure”. They outlines four different occasions\nwhen a programmer should refactor. First, when code is\nduplicated for the second time, a programmer should\nfactor out the duplication. Second, they should refactor\nwhen functionality needs to be added, but the existing\ncode is hard to understand or the addition is not easy to\nmake because of the existing design. Third, they should\nrefactor when a bug needs to be fixed and refactoring the\ncode will help make the code clearer and expose the bug.\n\nFinally, the should refactor when programmers are doing\na code review and refactoring will immediately produce\ncode that everyone understands.\n\nMartin Fowler[1] discusses in his paper that how\nrefactoring improve the existing design. He introduce\ndeeply about refactoring in his paper. This paper also\ndiscuss more about the bad smells. How to detect these\nsmells and tells that which refactoring technique is\napplicable to remove this bad smell. Martin also\nintroduce that a code that have bad smells are hard to\nmaintain and hard to modify. A bad smell is an indication\nof some problem in the code, which requires refactoring\nto deal with. Many tools are available here for detection\nand refactoring of these code smells. These tools vary\ngreatly in detection methodologies and acquire different\ncompetencies. In this work, we studied different code\nsmell detection tools minutely and try to comprehend our\nanalysis by forming a comparative of their features and\nworking scenario. We also extracted some suggestions on\nthe bases of variations found in the results of both\ndetection tools. This helps us to select the tool to refactor\nthe code.\n\nSandeep Kaur[7], says that, Bad smells indicate that there\nis something wrong in the code that we have to refactor.\nBad smells are design flaws in the code. There are many\ntools that are available to identify the bad smells and\nremove these bad smells by using refactoring tools and\nby using refactoring technique. Refactoring is a technique\nthat restrict our source code in a more readable and\nmaintainable form by removing the bad smells from the\ncode. Refactoring does not change the external behaviour\nof software. In this paper we discussed about tools and\ntechniques to refactor the source code.\n\nRoberts [2] says that, Refactoring consists of at least two\nsteps, The first is analysis, where the program to be\nrefactored is analyzed in order to determine whether the\ndesired refactoring’s preconditions are satisfied. If this is\nthe case, the second step is executed, the actual\ntransformation of the program source code. Both steps\nmust take both the syntax and semantics of the\nprogramming language the tool supports into\nconsideration, making it a considerable effort to\nimplement a refactoring tool from scratch. This may\nexplain why refactoring tools are often integrated with\nother development tools such as IDEs, since these\ntypically expose a large part of this required functionality\nwhich can be reused by the refactoring tool.\n\nMealy and Strooper [4] say that, Refactoring is a method\nwhich used to rearrange and modify the existing code in a\nway that the intentional behaviour of code stays the same.\nRefactoring allows to simplify and improve both\nperformance and readability of your code. One of the key\nissue in software refactoring is source code that should be\nrefactored. It should be implemented by the object\noriented programs. Kent beck and Martin fowler calls\nthem Bad smells, indicating that some part of the source\ncode are terrible. Sometimes in other words bad smells\nare assigned as the duplicate code. Duplicate code is a\ncomputer program sequence of source code that occurs\n', 'MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\nmore than once. Improving the design that often includes\nremoving duplicate code .\n\nRoberts D. Brant [2] says that refactoring is provided as a\nprogram transformation that has a precondition and a\npost-condition that a program must satisfy for the\nrefactoring to be easily applied. Each program is thought\nto have a specification for it and that specification is\nsatisfied (or unsatisfied) by a test suite. A refactoring is\ntherefore behaviour preserving if it satisfies the original\ntest suite. If a new component is added to the program,\nthe program must satisfy the original test suite plus any\nadditional tests. When satisfying the original test suite,\none must recognize that this is the conceptual original test\nsuite that is satisfied.\n\nKumar and Chanaky[20] say that A, Code and design\nsmells are the indicators of potential problems in code.\nThey may obstruct the development of a system by\ncreating difficulties for developers to fulfil the changes.\nDetecting and resolving code smells, is time-consuming\nprocess. Many number of code smells have been\nidentified and the sequences through which the detection\nand resolution of code smells are operated rarely because\ndevelopers do not know how to rectify the sequences of\ncode smells. Refactoring tools are used to facilitate\nsoftware refactoring and helps the developers to\nrestructure the code. Refactoring tools are passive and\nused for code smell detection. Few refactoring tools\nmight result in poor software quality and delayed\nrefactoring may lead to higher refactoring cost. A\nRefactoring Framework is proposed which instantly\ndetects the code smells and changes in the source code\nare analyzed by running a monitor at the background. The\nproposed framework is evaluated on different non trivial\nopen source applications and the evaluation results\nsuggest that the refactoring framework would help to\navoid code smells and average life span of resolved\nsmells can be reduced.\n\nElish [22] says that, Refactoring to patterns allows\nsoftware designers to safely move their designs towards\nspecific design patterns by applying multiple low-level\nrefactorings. There are many different refactoring to\npattern techniques, each with a particular purpose and a\nvarying effect on software quality attributes. Thus far,\nsoftware designers do not have a clear means to choose\nrefactoring to pattern techniques to improve certain\nquality attributes. This paper takes the first step towards a\nclassification of refactoring to pattern techniques based\non their measurable effect on software quality attributes.\nThis classification helps software designers in selecting\nthe appropriate refactoring to pattern techniques that will\nimprove the quality of their design based on their design\nobjectives. It also enables them to predict the quality drift\ncaused by using specific refactoring to pattern techniques.\n\n3 Problem Formulation\n\nProducing software is a very complex process that takes a\nconsiderable time to evolve. Poorly designed software\nsystems are difficult to understand and maintain.\nSoftware maintenance can take up to 50% of the overall\n\ndevelopment costs of producing software. One of the\nmain attributes to these high costs is poorly designed\ncode, which makes it difficult for developers to\nunderstand the system even before considering\nimplementing new code. In the context of software\nengineering process, Software Refactoring has a direct\ninfluence on reducing the cost of software maintenance\nthrough changing the internal structure of the code,\nwithout changing it external behaviour. Refactoring is a\ntechnique for restructuring an existing body of code.\n\n. Code is not easily maintainable and difficult to\nunderstand.\n\n. Design of program is more complex and difficult to\nfind bugs in the program.\n\n. Code review is more time consuming.\n\nTo remove these problems in our java program we\nperform refactoring using Eclipse Tool. After performing\nrefactoring, the internal structure of program is modified\nand its external behaviour remains same.\n\nModifications and Improvements\n. After refactoring, code is easy to understand.\n\n. Refactored code is easily maintainable and reduce\nthe code maintenance cost.\n\n. Refactored code is of better quality and reduce the\nchance of introducing bugs.\n\nObjectives\n\n. To find the code smells.\n\n. To remove bad smells in code.\n\n. To improve the quality of code.\n\n. To improve code readability and reduced\ncomplexity.\n\n. To make software easy to understand.\n\n. To reduce the maintenance cost.\n\n4 Research Methodology\n\nThis section gives description of step involved for\nRefactoring-\n\n1. Run the program to check its external behaviour\nand ensure that it is unchanged after refactoring.\n\n2. Before applying any single refactoring, calculate the\ncomplexity of program.\n\n3. Identify where the code should be refactored-\nDetermine which refactoring should be applied to the\nidentified places.\n', 'MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\n4. Make a small change- a single refactoring without\nchanging the outer behaviour of the code.\n\n5. Test Refactored code if everything work’s, move on\nto the next refactoring.\n\n6. If fails rollback the last smaller change and repeat\nthe refactoring in a different way.\n\n7. After applying all the refactoring technique,\nCalculate the complexity to determine the impact of\nrefactoring on Quality.\n\n5 Experiment\n\nThe experiment is done by using the eclipse tool and\njdeodrant plugin as bad smell detection tool and eclipse\nmetrics plugin as to calculate the complexity of source\ncode. We take an source code of library software which is\nwritten in Java.\n\nBad Jdeodoran | Refactoring technique that\nsmells t can be applied\n\nFeature | Yes Move method\n\nenvy\n\nType Yes Replace\n\ncheckin replace type code with state\ng\n\nLarge Yes Move Method\n\nclass\n\nLong Yes Extract Method\n\nMethod\n\nComparison between Before and After Refactoring\n\nBefore refactoring, we can find two types of bad smells\nin our project and the initial complexity is calculate.\nWhen bad smells find in our project, then we remove\nthese bad smells by applying the appropriate techniques\nof refactoring. After applying all type of refactoring then\nagain we calculate the complexity and compare it with\ninitial complexity and check the difference between these\ntwo. Complexity can also be measure in the form of\nMcCabe Cyclomatic Complexity, Weighted Methods per\n\nclass(WMC), Lack of Cohesion of Methods(LCOM),\nDepth of Inheritance Tree(DIT). A low number for DIT\nand WMC implies less complexity and a high number for\nDIT and WMC implies higher complexity with a higher\nprobability of errors in the code. If LCOM value is small\nthen there is more cohesion between the methods and if\nthe value is large then there is lack of cohesion between\nthe methods. The complexity of our project is reduced\nafter apply the refactoring technique. We can say that\nrefactoring improve the internal structure of our project\nby decreasing the complexity and improve the quality of\nour project.\n\nMeasures Before After\nRefacto | Refact\nring oring\n\nLong Method yes No\nBad Feature Envy No No\nSmells | Long Class Yes No\nType Checking No No\nMcCabe 3.7 2.701()\nCompl | Cyclomatic )\ne Complexity\n-xity -\nWeighted 13.45 12.25(|\nMethods Per )\nwnc | Class(WMC)\nLCO Lack of Cohesion | 0.43 0.397()\nM of )\nMethods(LCOM)\nDIT Depth of | 3.864 3.25(|)\nInheritance Tree\n\n6 Conclusion\n\nRefactoring is an important and easy activity to refactor\nthe source code in a well manner. Refactoring makes a\ncode easier to understand and improve the quality of\ncode. We take a source code of library software which is\nwritten in Java. We can detect bad smells by JDeodorant\nand find the complexity by Metrics plugin. Bad smells\nmake our source code more difficult to manage. By\napplying refactoring using Eclipse, we can refactor these\nbad smells and make our source code essay to\nunderstand. After applying the refactoring, calculate the\ncomplexity of project and compare it with initial\ncomplexity and then check the result. The complexity of\nour project is reduced after apply the refactoring and\nimprove the quality of our project. Also we can say that\nrefactoring reduces the maintenance cost because the\ncomplexity of software is decrease and it is easily\nunderstandable.\n\nReferences\n\n1. Martin Fowler, Kent, John Brant, William Opdyke,\nDon Roberts, D. B. “Refactoring: Improving the\nDesign of Existing Code”, Addison-Wesley, New\nYork, (1999).\n', 'MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\n10.\n\n11.\n\n12.\n\n13,\n\n14.\n\n15.\n\n16.\n\n17.\n\n18.\n\nRoberts, D. B “Practical Analysis for Refactoring”,\nPhD thesis, Department of Computer Science ,\nUniversity of Illinois at Urbana-Champaign, (1999).\nOpdyke, W. F , “Refactoring Object-Oriented\nFrameworks”, PhD thesis, University of Illinois at\nUrbana-Champaign,(1992).\n\nE.Mealy and P.Strooper,’Evaluating — software\nRefactoring Tool support”, Proceeding of Australian\nSoftware Engineering Conference, pp. 331-340,\n(2006).\n\nE.Mealy, D.Carrington, P.Strooper, and P.Wyeth,\n“Improving usability of software refactoring tools”,\nProceeding of Australian Software Engineering\nConference , pp.307-318, (Apr.2007).\n\nTom Mens and Tom Touwe, “A survey of software\nrefactoring’ IEEE Transactions on _ software\nEngineering ,vol.30, no.2, pp. 126-139, (Feb 2004).\nSandeep kaur , “ Review on Identification and\nRefactoring of Bad Smells using Eclipse”,\nInternational Journal For Technological Research In\nEngineering (IJTRE) Volume 2, (March-2015).\n\nR. Fanta and V. Rajlich, “Reengineering object-\noriented code,” in Proceeding of International\nConference on Software Maintenance, pp. 238-246,\n1998, IEEE Computer Society.\nEclipse Modeling Framework\nhttp://www.eclipse.org/emf/\n\nEMF Refactor -\nhttp://www.eclipse.org/emf/refactor/.\n\nJDeodorant -\nhttps://marketplace.eclipse.org/content/jdeodorant\nEMF Metrics Plugin -\nhttp://sourceforge.net/projects/metrics/\n\nMika V. Mantylé, Jari Vanhanen, and Casper\nLassenius,” A taxonomy and an initial empirical\nstudy of bad smells in code”, In Proceedings of\nInternational Conference on Software Maintenance\n(ICSM 2003), IEEE Computer Society pages 381—\n384, Amsterdam, The Netherlands, (September\n2003).\n\nNikolaos Tsantalis and Alexander Chatzigeorgiou,\n“Identification of move method refactoring\nopportunities”, IEEE Transactions on Software\nEngineering, 35(3):347—367, (2009).\n\nNikolaos Tsantalis, “Identification Of Move Method\nRefactoring Opportunities”, IEEE Transactions On\nSoftware Engineering, Vol. 35, No. 3, (May/June\n2009).\n\nS.H. Kannangara, “An Empirical Evaluation Of\nImpact Of Refactoring On Internal And External\nMeasures Of Code Quality”, International Journal Of\nSoftware Engineering & Applications (Ijsea), Vol.6,\nNo.1, (January 2015).\n\nJ. van den Bos, “Refactoring (in) Eclipse”, Master\nSoftware Engineering, Universiteit van Amsterdam,\nMaster’s thesis , (August 2008).\n\nMesfin Abebe and Cheol-Jung Yoo, “ Trends,\nOpportunities and Challenges of Software\nRefactoring: A Systematic Literature Review”,\nInternational Journal of Software Engineering and Its\nApplications Vol.8, No.6 ,pp.299-318,( 2014).\n\n(EMF) -\n\n19.\n\n20.\n\n21.\n\n22.\n\nEmerson Murphy-Hill and Andrew P. Black,\n“Refactoring Tools: Fitness for Purpose”,\nDepartment of Computer Science, Portland State\nUniversity Portland, Oregon, (May 7, 2008).\n\nD. Raj Kumar, G.M. Chanakya, “Refactoring\nFramework for Instance Code Smell Detection”,\nInternational Journal of Advanced Research in\nComputer Engineering & Technology (IJARCET)\nVolume 3 Issue 9, (September 2014).\n\nEmerson Murphy-Hill, Chris Parnin, And Andrew P.\nBlack, “How We Refactor, And How We Know It”,\nIEEE Transactions On Software Engineering, Vol.\n38, No. 1, (January/February 2012).\n\nKarim O. Elish, “Using Software Quality Attributes\nto Classify Refactoring to Patterns”, Journal Of\nSoftware, Vol. 7, No. 2, (February 2012).\n']}


**File**: S:\OneDrive\@Dev\!GPT\ScriptGPT\library\Refactoring\Source\Refactoring-vs-Refuctoring-Advancing-the-state-of-AI-automated-code-improvements.pdf
- Time Taken: 13.81s
- Data Extracted: {'text': ['9 January 2024\n\nRefactoring vs\nRefuctoring:\n\nAdvancing the state of Al-\n\nautomated code improvements\nBy Adam Tornhill, Markus Borg, PhD & Enys Mones, P\n\nSummary\n\nThis report is the conclusion of a benchmark study of the mos\nLanguage Models (LLMs) and their ability to generate code fo!\ntasks. We aim to illustrate the current standards and limitati\nshow new methodologies with higher confidence results.\n\nC) CodeScene\n', "Introduction\n\nThe remarkable advances in Al promised a coding revolution, spawning tools to help us\nwrite code faster. Yet the true gains elude us. The crux? The majority of a developer's time\nisn't writing but understanding and maintaining existing code.\n\nThis whitepaper explores this new frontier by investigating Al support for improving existing\ncode. We do that via two important contributions:\n\n« First, we benchmark the performance of the most popular Large-Language Models\n(LLM) on refactoring tasks for improving real-world code. We find that existing Al\nsolutions only deliver functionally correct refactorings in 37% of the cases.\n\n« Second, as a response to the poor performance of LLMs, we introduce a novel\ninnovation for fact-checking the Al output and augmenting the proposed refactorings\nwith a confidence level. By rejecting incorrect solutions, 98% of the remaining Al-\ngenerated refactorings improve the code while retaining the original behavior.\n\nThis level of precision exceeds that of even human experts, highlighting the utility of fact-\nchecked Al. By applying this innovation, software organizations get a viable way forward for\nautomating improvements to existing code, including auto-mitigations of technical debt.\n\n2. Improved performance:\nFact-checked Al refactoring\n\n98%\n\n1. Al refactoring performance\n\nHigh confidence 70% Adding a fact-checking validation layer allows\n\nyou to refactor without breaking the code.\n\n100%\nLLMs are more likely than not to break existing «x\ncode during a refactoring attempt.\n70%\n\n2%\n\n63% ;\n- Vad Validate\n40% 7 w? refactorings\n37% Correct refactorin Incorrect refactoring\nwhich preserves the which introduces bugs\nLow confidence behavior of the code\n\nCorrect refactorin Incorrect refactoring Rejected refactoring proposals\nwhich preserves the which introduces bugs\n\nbehavior of the code TT\n\nFigure 1: State-of-the-art generative Al breaks the code in 63% of all refactoring attempts (left). Fact-checking the\nAl allows us to reject the majority of all broken refactoring attempts.\n\nCodeScene 1\n", "Benchmark: Al\nperformance on code\nrefactoring\n\nAs exciting as the Al revolution is, we are far\nfrom realizing the claimed productivity\nbreakthrough. At least for non-trivial coding\ntasks. (See our Forbes article for why Al-\nassisted coding is still in its infancy).\n\nSpecifically, two main barriers remain to be\nconquered before Al can truly disrupt the\nway we work with source code:\n\n1. Optimize for software maintenance\nwhich accounts for more than 90% of a\nsoftware product's life cycle costs.\nSpecifically, 70% of developers’ time is\nspent on program understanding,\nmeaning that any improvements that\nmake the existing code easier to grasp\nwill have a high return on investment.\n\nWriting/Editing code\n5%\n\n2.Improve Al precision to the level of a\nhuman expert. The disappointing 37%\ncorrectness score of today’s Al solutions\nsimply isn’t good enough for refactoring\nproduction code. Rather, the hit-or-miss\nsuccess ratio adds to the problem by\nincreasing developers’ cognitive load as\nwe have to scrutinize all Al refactorings\nwith great care to sort out the good\nfrom the bad. Reviewing code _ is\narguably a harder task than writing it.\n\nThese two factors indicate that innovation\nin tooling to support improving existing\ncode - without breaking it - is a more\nimportant direction than focusing on\noptimizing the less significant code-writing\nprocess. Let’s discuss why.\n\nAre we refactoring or just\nbreaking code?\n\nRefactoring is defined as improving the\ndesign of existing code without changing its\nbehavior. It’s a simple definition, but with\nsome important implications:\n\nOther activities\n(e.g. meetings,\nnavigating code)\n\n25%\n\n« It’s not a refactoring unless we improve\nthe design. “Improve” has been largely\nsubjective. To automate refactoring, we\nneed a gold standard to make\nimprovements objectively measurable.\n\n« It’s not a refactoring if we fail to\npreserve the behavior of the original\ncode, e.g. we introduce a bug. To\n\nUnderstanding automate refactoring, we need\n\n70% confidence that the machine adheres to\nthis assumption.\n\nFigure 2: The majority of a developer's time is spent trying\n\nto understand the existing system (data from Minelli, et. Unless these two conditions are met, a code\n\nal., 2015) 1 change is simply not a refactoring. For the\npurpose of this article, we will use the term\nrefuctoring to refer to the process of\nchanging existing code while - involuntarily\n\n- altering the program’s behavior.\n\n1. https://ieeexplore.ieee.org/abstract/document/7181430\n\nCodeScene 2\n", 'Benchmarking criteria: a gold standard for code\nimprovements\n\nThis study uses the Code Health metric as a proxy for code quality. Code Health is the only\ncode-level metric with a proven business impact in terms of development velocity and post-\nrelease defects. (See the Code Red paper for details).\n\nThe Code Health metric is a particularly good fit when refactoring code as the measure is\nbased on factors known to make code harder to understand and riskier to maintain in terms\nof defect introduction:\n\nCode health categories:\n\nModule/Class level smells tomate\n\nSource code — (e.g. Low Cohesion, God Classes) —™,\nFunction level smells Score, aggregate,\n\ne.g. conv pasted lic, God Functions, and categorize\n> rimitive Obsession,\nNL Implementation smells __ 7”\nee (e.g. deep nested logic, complex\n\nconditionals)\n\nFigure 3: Code Health is a language-neutral, aggregated code quality metric based on a combination of 25 code smells.\n\nThe detailed Code Health scores used in this study go from 10.0 (healthy code) all the way\ndown to 1.0 (a maintenance nightmare / spaghetti code / high technical debt). As such, the\nCode Health metric offers an objective assessment of any code changes: did the Code\nHealth improve - a refactoring that makes the code easier to understand - or was the code\nmerely changed without getting objectively better?\n\nAl performance on code refactoring\n\nTo evaluate how well current Al platforms perform, we collected more than 100,000 real-\nworld code smells, and pointed state-of-the-art Al models at these targets to refactor the\ncode. We used the CodeScene tool to identify Code Health issues in codebases. We then\nevaluated the correctness of the attempted refactoring by running the code’s automated\ntests, as well as making sure the code quality improved by re-assessing the Code Health.\n\nFor this benchmarking study, we focused on code in JavaScript and TypeScript. LLM\nperformance varies across programming languages, so we chose to start with two popular\nand well-supported languages. We also centered the refactorings on four common Code\nHealth issues: Complex Conditionals, Deep Nested Logic, Bumpy Road, and Complex\nMethod. (See the docs for descriptions of these code smells).\n\nCodeScene\n', "Using this data, we measure the ratio of refactoring vs refuctoring for a series of popular Al\nmodels:\n\nValid code? rode ren\nP . Valid refactoring?\nAl model (check the (did the code ;\nsyntax of the (do the tests still pass after the\nrefactored change by the Al changed the code?)\nAl mitigate the 9 .\ncode smell?)\n\nPaLM 2 code\n\nPaLM 2t\nphind-codellama-34B-v2*\n\nTable 1: Benchmarking of refactoring correctness for a series of popular Large Language Models. *A fine-tuned model\n\nbased on CodeLlama 34B.\n\nAs the preceding table shows, using an out-of-the-box Al is very much a hit-or-miss situation.\nIn fact, with the best-performing model only giving a 37% probability of success, it’s more\nlikely that the attempted refactoring will break your code than not.\n\nNotes on GPT4\nPerformance\n\nDuring our research, we also\nmade some benchmarks using\nGPT4. These tentative studies\nindicate that GPT4 seems to\nperform marginally better.\nHowever, those potential\n\nimprovements are offset by GPT4\nbeing significantly slower and an\norder of magnitude more\nexpensive. Without a drastic gain,\nGPT4-based refactoring doesn’t\nseem to be a viable alternative\neither.\n\nIs fragile code an acceptable\nnew normal?\n\nOur research findings align with a 2023 study\nwhich found that popular Al-assistants like\nCopilot and CodeWhisperer only deliver\nfunctionally correct code in 31% - 65% of the\ncases. Generating new code is arguably a\nsimpler task than refactoring complex code,\nwhich explains the higher-end of those\nnumbers. However, those research findings\nare within the same ballpark; the performance\nof those Al-assistants reflects the\nperformance of the underlying LLM.\n\nIt's safe to assume that a human developer\nshipping code which breaks 60-80% of the\ntime would be asked to look for new\nchallenges. Promptly.\n\nCodeScene 4\n", 'How does an Al err?\n\nStrictly speaking, an Al based on LLMs doesn’t have a concept of “correctness”;\nsome tokens (e.g. output) are more probable than others, but the Al itself doesn’t\nexecute the code, nor does it “prove” its response in any meaningful way.\n\nThis spells trouble as the nature of programming is a much more constrained\nenvironment than a conversation between humans. For example, when\ngenerating a natural text, choosing one synonym over another usually doesn’t\nmake or break a text. (e.g. consider “gear” and “equipment”). Not so in code - a\nsingle character can alter the behavior of any program.\n\nDuring our research we inspected a lot of Al-generated code. Some common\npatterns are that the Al:\n\n- Drops entire branches, e.g. throwing away an if-block. Note that these bugs\ncan easily become a cybersecurity threat in case important input validation\ngets removed.\n\n- Inverts boolean logic, e.g. a && b becomes !(a && b).\n\n« In JavaScript - which is notorious for its complex rules for the this-keyword —\nthe Al often mistreats the function-level this by extracting the literal\nexpression to a new function, a classic refuctoring pitfall.\n\nAll of these failures are subtle and not obvious to the human eye during a code\ninspection.\n\ndeep nesting constructs: the Al introduced a nasty and subtle bug:\n\nOriginal code with complex, o> Al refactored code. Looks much cleaner, but\n\nexport function maxDepth(ary) { export function maxDepth(ary) {\nvar stack = (J; var stack = [];\nvar max_depth = @; var max_depth = @;\nfor (var i = @; i < ary.length; i++) {\nstack. push(arylil); for (var i = @; i < ary.length; i++) { Bug: max_depth is\nstack. push(ary[il); sed by value...\nif (stack. length > 1) { p y passed by value...\n\nvar top = stack[stack. length - 1] uadarestack(atack)s\nzs 4\n\nvar bottom_idx = -1; updateMaxDepth(stack, max_depth) ; Vi\nfor (var j = @; j < stack.length ~ 1; j++) {\n\ni Fase ecel 7 ..-and, consequently, the function will\n\ni return max_depth; always return 0 (zero)..\n—————————\n\nif (bottom_idx >= 0) { -\nstack = stack.slice(bottom_idx + 1); _—___ function updateMaxDepth(stack, max_depth) {\n; , ee if (stack. length > max_depth) {\nif (stack. length > maxdepth) ( —— fax_depth = stack. length; because this will only be a\nmax_depth = stack. length; 7 } local modification!\n}\n}\nfunction updateStack(stack) {\nif (stack.length > 1) {\nvar top = stack[stack. length - 1];\nvar bottom_idx = findBottomIndex(stack, top[@]);\nif (bottom_idx >= @) {\n\nreturn max_depth;\n\nFigure 4: Example on an Al-introduced, subtle bug that takes a great deal of effort to track down.\n\n', "These findings also indicate that we should be more cautious about how and when we apply\nAl-assisted coding. What if we could add a safety net around the Al? That would let us reap\nthe benefits of automation by outsourcing the mechanics to an Al while still giving us - as\nsoftware people - some guarantees that we are refactoring, not refuctoring. Welcome to the\nfuture — there’s a light on the horizon.\n\nTools for inspiration...but use with care!\n\nThe Al-assistance of today can still be helpful, despite their frequent errors. In particular, an\nAl-assistant like Copilot or CodeWhisperer can be useful as the starting point for new code,\nserving as an inspiration and a coach. However, the burden is still on you to verify that the\ncode is correct and — just as important - that it’s code you and your team can maintain going\nforward.\n\nInto the future: improving automated refactoring\nby fact-checking the Al\n\nGiven the low correctness of the stochastic Al models, it becomes strikingly clear that we\ncannot use out-of-the-box Al models or tools that merely wrap an LLM API. Instead, a more\npromising approach is to use generative Al to come up with a pool of potential solutions and\nthen add a fact-checking layer around the Al. That way, we get the benefits of automation\nwhile retaining a certain level of guarantee that a proposed solution is a refactoring rather\nthan a refuctoring.\n\nCodeScene’s research team took on this challenge by creating a layered fact-checking\nmodel:\n\nAl-refactored code to be\nfact-checked.\n\ncode?\n\n(x) © No Confidence\n\n———>> | Reject the “solution”, never\n\nshow to users.\n\nCode Health review:\nimproved code quality?\n\nSemantic comparison: g -» High Confidence ae fraction\nn r? —_> :\nTragedies The code is likely to be a valid\nrefactoring.\n| © Mid Confidence\n\nMinimal refactoring: small g 80% Precision\nand focused code change? —_ Likely to be OK, but requires a\nreview before the refactoring is\napplied.\n\n(x) ‘© Low Confidence\n\nUseful as an inspiration, but\nshouldn't be applied.\n\nFigure 5: A schematic overview of the layered model for fact-\n\nchecking Al-refactored code. CodeScene\n", "Benchmarking: improving Al correctness with a fact-\nchecking model\n\nTo evaluate the fact-checking model, we re-ran the benchmarking study described in Table 1\n\nabove. This makes it possible to compare the correctness gained from the fact-checking\nmodel:\n\nCorrect code smell refactorings\n\nDeep\nComplex Nested Complex\n\nConditional . Method\nLogic\n\nAl with CodeScene’s fact-\nchecking\n\nTable 2: Benchmarking data showing how the confidence in refactoring can be improved to 98%. GPT-3.5 performance\nadded for comparison.\n\nTable 2 shows that the layered fact-\nchecking model is a massive\nimprovement over GPT-3.5 - and any\nother commercially available LLM - with\nrespect to correctness. An LLM without\nfact-checking will always give you an\nanswer, be it correct or not. CodeScene’s\nfact-checking model is able to validate\nthe proposed code changes, and reject\n98% of the incorrect refactorings.\n\nBefore we discuss the disruptive\npotential that this level of Al performance\nenables, we need to look behind the\nmodel to understand how the Al fact-\nchecking is possible at all.\n\nData as the secret sauce\n\nThe main challenge in the fact-checking\nis to ensure semantic equivalence\nbetween the original code and the\nrefactored code. This is a largely\nunsolved research problem\n\nin academia where the problem is studied\nin the area of Automatic Program Repair\n(APR).\n\nPotential solutions like formal methods\nand code similarity metrics haven't been\nable to reliably verify semantic\ncorrectness between a given piece of\ncode and its fixed/refactored counterpart.\nSo how did the CodeScene team pull this\noff? There are two unfair advantages at\nour disposal plus one critical constraint\nthat we chose:\n\nFirst, when building the fact-checking\nmodel we had access to our data lake\nconsisting of +100,000 real-world\nJavaScript refactoring samples with a\nknown ground truth (i.e. semantically\nequivalent or not). This made it possible\nfor our algorithms to observe and learn\npatterns in successfully refactored code.\n\nCodeScene\n", "Second - and in fact a basis and pre-requisite for #1 - the data lake was built up using the\nautomated code review capabilities of the CodeScene tool. This automated code review is\ndeterministic and driven by the Code Health metric. This step is crucial as it directly\ninfluences the quality of the data; poor sample quality, and it won't be possible to achieve\nthese levels of accuracy.\n\nThird, it’s important to point out that we didn’t attempt to solve semantic equivalence in\ngeneral. Doing so would be futile at best. Instead, we limited the fact-checking to the set of\ncode smells identified via the Code Health metric. That way, we could be more specific in our\nAl prompts as well as constraining the fact-checking model to a finite number of structural\nchanges that can be learned by our in-house models.\n\nSummary\n\nThis benchmarking study shows that Al is nowhere near replacing humans in a coding\ncontext; today’s Al is simply too error-prone, and far from a point where it is able to securely\nmodify existing code. However, by introducing a novel fact-checking model for the Al output,\nwe can elevate generative Al to a point where it is genuinely useful as several complex code\nsmells can be mitigated safely. This allows us to optimize for understanding - the dominant\nand most human-intensive aspect —- not just the narrow task of writing new code.\n\nPerhaps the most intriguing possibility is the progress in technical debt mitigation made\npossible via this innovation. Every business manager is aware of technical debt, but few\nprioritize it - and even fewer manage it actively. Traditionally, there’s been a hard trade-off\nbetween improving existing code vs. adding the next big feature. Predictably, improvements\nget the back seat despite hard numbers showing how a healthy codebase is a competitive\nadvantage. Now that the process can be automated to a large degree, companies can finally\nstart to reap these benefits without having to put feature development on hold.\n\nDuring our research, we also couldn't help to reflect on the fact that these benchmarks on Al-\nassisted programming re-emphasize solid engineering practices like unit testing, code quality\ngates, and continuous code reviews. Those practices were always important, but perhaps\neven more so in the age of Al where humans need to understand and verify machine-\ngenerated code.\n\nIn our study, we too used commonly available Al models which we augmented with specific\ndomain data to improve their refactoring performance. Yet, the key to our breakthrough\nwasn’t Al augmentation or magic prompt engineering, but rather the ability to provide a\nconfidence indication for each refactoring with respect to its semantic equivalence to the\noriginal code. Knowing the confidence of a proposed refactoring is a time saver.\n\nCodeScene\n\n8\n", 'Try the Automated Refactoring on your own code\n\nThe fact-checking innovation described in this whitepaper will be available to the general\npublic via CodeScene. Sign-up for the beta testing waitlist at https://codescene.com/ai.\n\n25 VectorLayerjs 9+ JS indexjs 8 CodeScene AI Refactor x\n\n}\n\n} else\n\nThe automated code review highlights a\n\n>\n(e) Refactoring recommendation\n\nThe proposed change is “High Confidence”,\n\nLt} code smell, Complex Conditional ——=- and refactors the code by extracting a\n\nreturn targe variable to clarify the intent of the code\n* ! Proposed change\nfunction propagationHandler: (qi, options) { 4\n\nreturn function( ’ 4 function propagationHandler (qi, options) {\n\nreturn function( ’ ¢\nit const = options. stopPropagation &&\nloptions. stopPropagation && tqi- options. stopPropagation &&\n\n|onnust@ ds options. stopPropagation 66 Gi. options. parent;\n\nqi.options. parent I\nMt if ( 4\nreturn qi.options.parent. $query (options); - return qi.options. parent. $query (options) ;\n\nfunction findAverageatSite(site) {\nconst nea = fetchMeasurenents();\n\nAbout the authors\n\nAdam Tornhill is the founder and CTO of CodeScene. Adam is a programmer who combines\ndegrees in engineering and psychology. He’s also the author of the best-selling Your Code as\na Crime Scene as well as multiple other technical books.\n\nMarkus Borg, PhD, is a senior researcher at the intersection of software engineering and\napplied Al. He is a principal researcher at CodeScene and an adjunct associate professor at\nLund University, Sweden.\n\nEnys Mones, PhD, is the Lead Data Scientist at CodeScene who also enjoys doing basic\n\nresearch. A theoretical physicist by training, his focus is applying mathematical models to\nunderstand human-computer interaction.\n\n(eo) CodeScene\n\nNext generation code analysis\n\nwww.codescene.com\n\n']}


**File**: S:\OneDrive\@Dev\!GPT\ScriptGPT\library\Refactoring\Source\Refactoring Practices in the Context of Modern Code Review.pdf
- Time Taken: 25.19s
- Data Extracted: {'text': ["2102.05201v1 [cs.SE] 10 Feb 2021\n\narXiv\n\nRefactoring Practices in the Context of Modern Code Review:\nAn Industrial Case Study at Xerox\n\nEman Abdullah AlOmar*, Hussein AlRubaye', Mohamed Wiem Mkaouer*, Ali Ouni?, Marouane Kessentini!\n“Rochester Institute of Technology, Rochester, NY, USA\ntXerox Corporation, Rochester, NY, USA\nSETS Montreal, University of Quebec, Montreal, QC, Canada\nSUniversity of Michigan, Dearborn, MI, USA\neman.alomar@maiL.rit.edu, hussein.alrubaye@xerox.com, mwmvse @rit.edu, ali.ouni@etsmtl.ca, marouane@umich.edu\n\nAbstract—Modern code review is a common and essential\npractice employed in both industrial and open-source projects\nto improve software quality, share knowledge, and ensure con-\nformance with coding standards. During code review, developers\nmay inspect and discuss various changes including refactoring\nactivities before merging code changes in the code base. To date,\ncode review has been extensively studied to explore its general\nchallenges, best practices and outcomes, and _ socio-technical\naspects. However, little is known about how refactoring activities\nare being reviewed, perceived, and practiced.\n\nThis study aims to reveal insights into how reviewers develop\na decision about accepting or rejecting a submitted refactoring\nrequest, and what makes such review challenging. We present an\nindustrial case study with 24 professional developers at Xerox.\nParticularly, we study the motivations, documentation practices,\nchallenges, verification, and implications of refactoring activities\nduring code review.\n\nOur study delivers several important findings. Our results\nreport the lack of a proper procedure to follow by developers\nwhen documenting their refactorings for review. Our survey\nwith reviewers has also revealed several difficulties related to\nunderstanding the refactoring intent and implications on the\nfunctional and non-functional aspects of the software. In light of\nour findings, we recommended a procedure to properly document\nrefactoring activities, as part of our survey feedback.\n\nIndex Terms—Refactoring, Code Review, Software Quality\n\nI. INTRODUCTION\n\nThe role of refactoring has been growing in practice beyond\nsimply improving the internal structure of the code without\naltering its external behavior to become a widespread\nconcept for the agile methodologies, and a de-facto practice to\nreduce technical debt [2]. In parallel, contemporary software\nprojects adopt code review, a well-established practice for\nmaintaining software quality and sharing knowledge about\nthe project By}. (4). Code review is the process of manually\ninspecting new code changes to verify their adherence to\nstandards and its freedom from faults Bi. Modern code review\nhas emerged as a lightweight, asynchronous, and tool-based\nprocess with reliance on a documentation of the inspection\nprocess, in the form of a discussion between the code change\nauthor and the reviewer(s)\n\nRefactoring, just like any code change, has to be reviewed,\nbefore being merged into the code base. However, little is\nknown about how developers perceive and practice refactoring\nduring the code review process, especially that refactoring, by\n\ndefinition, is not intended to alter to the system’s behavior, but\nto improve its structure, so its review may differ from other\ncode changes. Yet, there is not much research investigating\nhow developers review code refactoring. The research on\nrefactoring has been focused on its automation by identifying\nrefactoring opportunities in the source code, and recommend-\ning the adequate refactoring operations to perform (6)-I8}.\nMoreover, the research on code reviews has been focused on\nautomating it by recommending the most appropriate reviewer\nfor a given code change (3). However, despite the critical role\nof refactoring and code review, the innate relationship between\nthem is still largely unexplored in practice.\n\nThe goal of this paper is to understand how developers\nreview code refactoring, i.e., what criteria developers rely on\nto develop a decision about accepting or rejecting a submitted\nrefactoring change, and what makes this process challenging.\nThis paper seeks to gain practical insights from the existing\nrelationship between refactoring and code review through the\ninvestigation of five main research questions:\n\nRQ1. What motivates developers to apply refactorings in the\ncontext of modern code review?\n\nRQ2. How do developers document their refactorings for code\nreview?\n\nRQ3. What challenges do reviewers face when reviewing\nrefactoring changes?\n\nRQ4. What mechanisms are used by developers and reviewers\nto ensure the correctness after refactoring?\n\nRQS5. How do developers and reviewers assess and perceive\nthe impact of refactoring on the source code quality?\n\nTo address these research questions, we surveyed 24 pro-\nfessional software developers, from the research and develop-\nment team, at Xerox. Our survey questions were designed to\ngather the necessary information that can answer the above-\nmentioned research questions and insights into the review\npractices of refactoring activities in an industrial setting.\nMoreover, we perform a pilot study by comparing between\ncode reviews related to refactoring, and the remaining code\nreviews, in terms of time to resolution and number of ex-\nchanged responses. Our findings indicate that refactoring-\nspecific code reviews take longer to be resolved and typically\n", 'triggers more discussions between developers and reviewers\nto reach a consensus. The survey with reviewers, has revealed\nmany challenges they are facing when they review refactored\ncode. We report them as part of our survey results, and we\nprovide some guidelines for developers to follow in order to\nfacilitate the review of their refactorings.\n\nIl. RELATED WORK\nA. Surveys & Case Studies on Refactoring\n\nPrior works have conducted literature surveys on refactoring\nfrom different aspects. The focus of these surveys ranges\nbetween investigating the impact of refactoring on software\nquality (13). to comparing refactoring tools [9], and exploring\nrefactoring challenges and _ practices a (14), (15).\nThese studies are depicted in Table\n\nMurphy-Hill & Black i) surveyed 112 Agile Open North-\nwest conference attendees and found that refactoring tools are\nunderused by professional programmers. In an explanatory\nsurvey involving 33 developers, Arcoverde et al. [1\nhow developers react to the presence of design defects in\nthe code. Their primary finding indicates that design defects\ntend to live longer due to the fact that developers avoid\nperforming refactoring to prevent unexpected consequences.\nYamashita & Moonen performed an empirical study in\ncommercial software to evaluate the severity of code smells\nand the usefulness of code smell-related tooling. The authors\nfound that 32% of the interviewed developers are unaware\nof code smells, and refactoring tools should provide better\nsupport for refactoring suggestions. Kim et al. surveyed\n328 professional software engineers at Microsoft to investigate\nwhen and how they do refactoring. When surveyed, the de-\nvelopers cited the main benefits of refactoring to be: improved\nreadability (43%), improved maintainability (30%), improved\nextensibility (27%) and fewer bugs (27%). When asked what\nprovokes them to refactor, the main reason provided was poor\nreadability (22%). Only one code smell, i.e., code duplication,\nwas reported (13%). Szoke et al. conducted 5 large-scale\nindustrial case studies on the application of refactoring while\nfixing coding issues; they have shown that developers tend\nto apply refactorings manually at the expense of a large time\noverhead. Sharma et al. surveyed 39 software architects\nasking about the problems they faced during refactoring tasks\nand the limitations of existing refactoring tools. Their main\nfindings are: (1) fear of breaking code restricts developers\nto adopt refactoring techniques; and (2) refactoring tools\nneed to provide better support for refactoring suggestions.\nNewman et al. conducted a survey of 50 developers\nto understand their familiarity with transformation languages\nfor refactoring. They found that there is a need to increase\ndeveloper confidence in refactoring and transformation tools.\n\nB. Refactoring Awareness & Code Review\n\nResearch on modern code review topics has been of import-\nance to practitioners and researchers. A considerable effort is\nspent by the research community in studying traditional and\nmodern code review practices and challenges. This literature\n\nhas been includes case studies (e.g., ), user studies\n]), and surveys (e.g., By}. . However, most of the\nudies focus on studying the effectiveness of modern\ncode review in general, as opposed to our work that focuses on\nunderstanding developers’ perception of code review involving\nrefactoring. In this section, we are only interested in research\nrelated to refactoring-aware code review.\n\nIn a study performed at Microsoft, Bacchelli and Bird\nobserved, and surveyed developers to understand the chal-\nlenges faced during code review. They pointed out purposes for\ncode review (e.g., improving team awareness and transferring\nknowledge among teams) along with the actual outcomes\n(e.g., creating awareness and gaining code understanding). In\na similar context, MacLeod et al. interviewed several\nteams at Microsoft and conducted a survey to investigate the\nhuman and social factors that influence developers’ experi-\nences with code review. Both studies found the following\ngeneral code reviewing challenges: (1) finding defects, (2)\nimproving the code, and (3) increasing knowledge transfer.\nGe et al. developed a refactoring-aware code review tool,\ncalled ReviewFactor, that automatically detects refactoring\nedits and separates refactoring from non-refactoring changes\nwith the focus on five refactoring types. The tool was inten-\nded to support developers’ review process by distinguishing\nbetween refactoring and non-refactoring changes, but it does\nnot provide any insights on the quality of the performed\nrefactoring. Inspired by the work of [16], Alves et al.\nproposed a static analysis tool, called RefDistiller, that helps\ndevelopers inspect manual refactoring edits. The tool compares\ntwo program versions to detect refactoring anomalies’ type\nand location. It supports six refactoring operations, detects\nincomplete refactorings, and provides inspection for manual\nrefactorings.\n\nTo summarize, existing studies mainly focus on proposing\nand evaluating refactoring tools that can be useful to support\nmodern code review, but the perception of refactoring in\ncode review remains largely unexplored. To the best of our\nknowledge, no prior studies have conducted case studies in\nan industrial setting to explore the following five dimensions:\n(1) developers motivations to refactor their code, (2) how\ndevelopers document their refactoring for code review, (3)\nthe challenges faced by reviewers when reviewing refactoring\nchanges, (4) the mechanisms used by reviewers to ensure the\ncorrectness after refactoring, and (5) developers and reviewers\nassessment of refactoring impact on the source code’s quality.\nPrevious studies, however, discussed code review motivations\nand challenges in general . To gain more in-depth\nunderstanding of the above-mentioned five dimensions, in this\npaper, we surveyed several developers at Xerox.\n\nIII. STUDY DESIGN\nA. Research Questions\nRQI1. What motivates developers to apply refactorings\n\nin the context of modern code review? Several motivations\nbehind refactoring have been reported in the literature (i).\n', "Table (1) Related work in industrial case study & survey on refactoring.\n\nStudy Year — Research Method Focus Single/Multi Company Subject/Object Selection Criteria # Participants\nMurphy-Hill & 2008 Survey Refactoring tools Yes/No programmers 112\nArcoverde et al 2011 Survey Longevity of code smells No/Yes belongs to development team 33\n‘Yamashita & M 2013 Survey Developer perception of code smells NolYes developers 35\n\nm eta 2014 Survey & Interview Refactoring challenges & benefits YesINo has change messages including “retactor™ 328\n\nwithin last 2 years\n\nSzoke etal. [13 2014 __Case Study & Survey Impact of refactoring on quality Nolves developers a\n‘Sharma et al 2015 Survey Challenges & solutions for refactoring adoption YesINo 3\nNewman et al 2018 Survey Developer familiarity of transformation NolYes development” in job title & not students 50\n\nlanguages for refactoring\n\nor faculty members\n\nCreate Review\nRequest\n\nview Request\nInder Review\n\nRevisions Requested\n\nReviewer Assigned\n\nRevisions Completed\n\n<>\n\nChanges Approved\n\nFigure (1)\n\nReview process overview.\n\n(}. Our first research question seeks to understand\n\nwhat motivations drive code review involving refactoring in\nvarious development contexts to augment our understanding\nof refactorings in theory versus in practice.\n\nRQ2. How do developers document their refactorings\nfor code review? Since there is no consensus on how to\nformally document refactoring activities (22) , we aim in\nthis research question to explore what information developers\nhave explicitly provided, and what keywords developers have\nused when documenting refactoring changes for a review.\nThis question aims to capture the taxonomy used and observe\nwhether it is currently helpful in providing enough insights for\nreviewers to be able to adequately assess the proposed changes\nto the software design.\n\nRQ3. What challenges do reviewers face when reviewing\nrefactoring changes? We investigate the challenges associated\nwith refactoring, as well as the bad refactoring practices that\ndevelopers catch when reviewing refactoring changes. This\nsheds light on how developers should mitigate some of these\nchallenges.\n\nRQ4. What mechanisms are used by developers and\nreviewers to ensure code correctness after refactoring?\nWe pose this research question to study current approaches\nfor testing behavior preservation of refactoring, and to get\nan overview of what different criteria are addressed by these\napproaches.\n\nRQS5. How do developers and reviewers assess and per-\nceive the impact of refactoring on the source code quality?\nFinally, in our last research question, we are interested in\nunderstanding how refactoring connects current research and\npractice. This helps exploring if the implications or outcomes\nof refactoring-aware code review match what outlined in the\nprevious research questions.\n\nB. Research Context and Setting\n\nHost Company and Object of Analysis. To answer the\nabove-mentioned research questions, we conducted our survey\n\nwith developers from the research and development division,\nat Xerox Research Center Webster (XRCW), currently Xerox’s\nlargest research center. The research and development di-\nvision is responsible for implementing and maintaining the\nsoftware that is currently being shipped with Xerox Printers,\n(i.e, ConnectKey interface technology|'p. The software is\ndirectly connected to the hardware and performs various\noperations going from basic scanning and printing to more\ncomplex commands such as exchanging with cloud services.\nThe software is constructed using object-oriented, object-based\nand markup languages. Despite being a legacy, around 20\nyears old, lengthy and complex software, the developers in\ncharge have been successfully evolving it to meet business\nrequirements and provide secure and reliable functionality to\nend users. This reflects the maturity of the engineering process\nwithin the research and development division, which raised\nour interest to understand how they perform code review in\ngeneral, and how they review refactoring in particular.\n\nCode Review Process at Xerox. The research and devel-\nopment division uses a collaborative code review framework\nallowing developers to directly tag submitted code changes\nand request its assignment to a reviewer. Similar to existing\nmodern code review platforms, e.g., Gerrit“| a code change\nauthor opens a code Review Request (ReR) containing a title, a\ndetailed description of the code change being submitted, writ-\nten in natural language, along with the current code changes\nannotated. Once an ReR is submitted, it appears in the requests\nbacklog, open for reviewers to choose. If an ReR remains\nopen for more than 72 hours, a team leader would handle its\nassignment to reviewers. Once reviewers are assigned to the\nReR, they inspect the proposed changes and comment on the\nReR’s thread, to start a discussion with the author, just like\na forum or a live chat. This way, the authors and reviewers\ncan discuss the submitted changes, and reviewers can request\nrevisions to the code being reviewed. Following up discussions\nand revisions, a review decision is made to either accept (i.e.,\nship it!) or decline, and so the proposed code changes are\neither “Merged” to production or “Abandoned”. An activity\ndiagram, modeling a simplified bird’s view of the code review\nprocess, is shown in Figure [I]\n\nC. Pilot Study and Motivation\n\nRationale. As we were analyzing the review process, to\nprepare our survey, we had access to the code review plat-\nform, containing the team’s history of processed ReRs for\n\n", 'Table (II) Summary of survey questions (the full list is available in |\n\nCategory Question\nBackground (1) How many years have you worked in the software industry?\n\n(2) How many years have you worked on refactoring?\n\n(3) How many years have you worked on code review?\nMotivation (@) As a code change author, in which situation(s) you typically refactor the code?\n\nDocumentation\n\n(S) As a code change author, what information do you explicitly provide when documenting your refactoring activity?\n\n(6) As a code change author, what phrases (keywords) have you used when documenting refactoring changes for a review?\n\nChallenge (7) As a code reviewer, what challenges have you face when reviewing refactoring changes?\n(8) As a code reviewer, what are the bad refactoring practices you typically catch when reviewing refactoring changes?\n\nVerification (9) As a code change author/code reviewer, what mechanism(s) do you use to ensure the correctness after the application of refactoring?\n\nImplication (10) As a code reviewer, what implication(s) do you typically experience as software evolves through refactoring?\n\n(11) How strongly do you agree with each of the following statements?\n\n« Ihave guidelines on how to document refactoring acti\n« Ihave guidelines on how to review refactoring act\n\nies,\nhile performing code review.\n\n« Reviewing refactoring activities slow down the review process.\n«Reviewing refactoring typically takes longer to reach a consensus.\n\nTable (III) Participant professional development experience\nin years.\n\nCode Review Ex-\nperience (%)\n\nIndustrial\nExperience (%)\n\nYears of Experi-\nence\n\nRefactoring Ex-\nperience (%)\n\n1-5 9 (37.5%) 15 (62.5%) 14 (58.33%)\n6-10 5 (20.83%) 4 (16.66%)\n11-15 4 (16.66%) 1 (4.16%) 2 (8.33%)\n\n16+ 6 (25%) 5 (20.83%) 4 (16.66%)\n\nthe ConnectKey software system. After reviewing various\nReRs, we noticed the existence of a number of refactoring-\nspecific ReRs, i.e., requests to specifically review a refactored\ncode. The existence of such refactoring ReRs raised our\ncuriosity to further study in deeper whether these ReRs are\nmore difficult to resolve than other non-refactoring ReRs. We\nhypothesize that refactoring ReRs, take longer time and trigger\nmore discussions between developers and reviewers before\nreaching a decision and closing the ReR. If such hypothesis\nholds, then it further justifies the need for a more detailed\nsurvey targeting these refactoring ReRs.\n\nExtraction of Review Requests Metadata. We aim to\nidentify all recent refactoring ReRs. Similarly to Kim et al.\nwe start with scanning the ReRs repository to distin-\nguish ReRs whose title or description contains the keyword\n“refactor*”. We only considered recent reviews, which were\ncreated between January 2019 and December 2019. We chose\nto analyze recent ReRs to maximize the chance of developers,\nwho authored or reviewed them, as still within the company.\nWe manually analyze the extracted set to verify that each\nselected ReR is indeed about requesting the review of a\nproposed refactoring. This extraction and filtering process\nresulted in identifying 161 refactoring ReR. To perform the\ncomparison, we need to sample 161 non-refactoring ReR from\nthe remaining ones in the review framework. To ensure the\nrepresentativeness of the sample, we use the stratified random\nsampling by choosing ReRs which were (1) created between\nJanuary 2019 and December 2019; (2) created by the same\nset of authors of the refactoring ReRs; and (3) created to\nupdate the same subsystem(s) that were also updated by the\nrefactoring ReRs.\n\nWe then compared both groups based on two factors: (1) re-\nview duration (time from starting the review until a decision of\nclose/merge is made), and (2) number of exchanged responses\n(i.e., review comments) between the author and reviewer(s).\nFigure |2| reports the boxplots depicting the distribution of\neach group values, clustered by two above-mentioned factors.\nTo test the significance of the difference between the groups\nvalues, we use the Mann-Whitney U test, a non-parametric\ntest that checks continuous or ordinal data for a significant\ndifference between two independent groups. Our hypothesis\nis formulated to test whether the values of the refactoring\nReRs group is significantly higher than the values of the\nnon-refactoring ReRs group. The difference is considered\nstatistically significant if the p-value is less than 0.05.\n\nPilot Study Results. According to Figure |2} refactoring\ncode reviews take longer to be completed than the non-\nrefactoring code reviews, as the difference was found to be\nstatistically significant (i.e., p< 0.05). Similarly, refactoring\ncode reviews were found to significantly trigger longer dis-\ncussion between the code author and the reviewers before\nreaching a consensus (i.e., p< 0.05). This motivates us to\nbetter understand the challenges reviewers face when review-\ning refactoring. We designed our survey to ask developers\nof this team about the kind of problems that triggers them\nto refactor, and to close the loop, we asked reviewers about\nwhat they foresee when they are assigned a refactoring code\nreview, along with the issues they typically face for that type\nof assignment. The next subsection details our survey design.\n\nD. Research Method\n\nTo answer our research questions, we follow a mixture\nqualitative and quantitative survey questions, as demonstrated\nin Creswell’s design . The quantitative analysis was per-\nformed by the analysis of ReRs metadata, and the comparison\nbetween refactoring ReRs and non-refactoring ReRs, in terms\nof time to completion and number of exchanged responses.\nDevelopers survey constitutes the qualitative aspect that we\nare going to detail in the next section.\n\nSurvey Design. For our survey design, we followed the\nguidelines proposed by Kitchenham and Pfleeger\n\n', 'Rolactoring Review\n\nNon.efactoring Reviow\n\n(a) Review duration\n\nRetactoring Review Non refactoring Review\n\n(b) Number of exchanged responses\n\nFigure (2) Boxplots of (a) review duration and (b) number\nof exchanged responses, for refactoring and non-refactoring\ncode review.\n\nincrease the participation rate, we made our survey anonym-\nous. The survey consisted of 11 questions that are divide:\ninto 2 parts. The first part of the survey includes demo-\ngraphics questions about the participants. In the second part,\nwe asked about the (1) motivations behind refactoring, (2)\ndocumentation of refactoring changes, (3) challenges face:\nwhen reviewing refactoring, (4) verification of refactoring\nchanges, and (5) implications of refactoring on code quality.\nAs suggested by Kitchenham and Pfleeger (27). we constructe\nthe survey to use a 5-point ordered response scale (“Likert\nscale”) question on the general refactoring-related code review,\n2 open-ended questions on the refactoring documentation an\nchallenges, and 5 multiple choice questions on the refactor-\ning motivations, documentation, mechanisms and implications\nwith an optional “Other” category, allowing the respondents\nto share thoughts not mentioned in the list. Table [Il] contains\na summary of the survey questions; the full list is available\nin (25). In order to increase the accuracy of our survey, we\nfollowed the guidelines of Smith et al. (28). and we targeted\ndevelopers who have previously been exposed to refactoring\nin the considered project. So instead of broadcasting the\nsurvey to the entire development body, we only intend to\ncontact developers who have previously authored or reviewed\na refactoring code change. We performed this subject selection\ncriteria to ensure developers’ familiarity with the concept of\nrefactoring so that they can be more prepared to answer the\nquestions. This process resulted in emailing 38 target subjects\nwho are currently active developers and regularly perform\ncode reviews. Participation in the survey was voluntary. In\ntotal, 24 developers participated in the survey (yielding a\nresponse rate of 63%, which is considered high for software\nengineering research (28). The industrial experience of the\n\nrespondents ranged from 1 to 35 years, their refactoring\nexperience ranged from 1 to 30 years, and their experience\nin code review ranged from 1 to 25 years. On average, the\nparticipants had 10.7 years of experience in industry, 7.5 years\nof experience in refactoring, and 6.97 years of experience in\ncode review. Table summarizes developers’ experience in\nindustry, refactoring and code review.\n\nIV. RESULTS & DISCUSSIONS\n\nA. RQI. What motivates developers to apply refactorings in\nthe context of modern code review?\n\nFigure |3| shows developers’ intentions when they refactor\ntheir code. The Code Smell and BugFix categories had the\nhighest number of responses, with a response ratio of 23.7%\nand 22.4%, respectively. The category Functional was the\nthird popular category for refactoring-related commits with\n21.1%, followed by the Internal Quality Attribute and External\nQuality Attribute, which had a ratio of 17.1% and 14.5%,\nrespectively. However, we observe that all motivations do not\nsignificantly vary as all of them are in the interval 14.5% to\n23.7% with no dominant category, as can be seen in Figure [3]\nOnly one participant selected the “other” option stating that,\n“When i feel it’s painful to fulfill my current task without\nrefactoring”.\n\nIf we refer to the Fowler’s refactoring book (i). refactoring\nis mainly solicited to enforce best design practices, or to cope\nwith design defects. With bad programming practices, i.e.,\ncode smells, earning 24% of developer responses, these results\ndo not deviate from the Fowler’s refactoring guide. However,\neven though the code smell resolution category is prominent,\nthe observation that we can draw is that motivations driving\nrefactoring vary from structural design improvement to feature\nadditions and bug fixes, i.e., developers interleave refactoring\nwith other development tasks. This observation is aligned with\nthe state-of-the-art studies by Kim et al. , Silva et al.\n{19}, and AlOmar et al. [21]. The sum of the design-related\ncategories, namely code smell, internal, and external quality\nattributes represent the majority with 55.3%. These categories\nencapsulate all developers’ design-improvement changes that\nrange from low level refactoring changes such as renaming\nelements to increase naming quality in the refactored design,\nand decomposing methods to improve the readability of the\ncode, up to higher level refactoring changes such as re-\nmodularizing packages by moving classes, reducing class-level\ncoupling, increasing cohesion by moving methods, etc.\n\nSummary: According to the survey, coping with poor\ndesign and coding style is the main driver for de-\n\nvelopers to apply refactoring in their code changes.\nYet, functional changes and bug fixing activities often\ntrigger developers to refactor their code as well.\n\nB. RQ2. How do developers document their refactorings for\ncode review?\n\nWhen we asked developers, “what information do you expli-\ncitly provide when documenting your refactoring activity?”, 21\n', 'External QA 14.5%\n\nOther 1.3%\n\nFigure (3) Developers’ refactoring motivations for code re-\nview.\n\nout of the 24 developers (91.3%) indicated that they explicitly\nmention the motivation behind the application of refactoring\nsuch as ‘improving readability’ and ‘eliminate code smell’.\nMoreover, only 8 out of the 24 developers (34.8%) indicated\ntheir refactoring strategy by stating explicitly the type of\nrefactoring operation they perform in their submitted code\nchange description, such as ‘move class’. We observe that\ndevelopers are eager to explain the rationale of their refact-\noring more than the actual refactoring operations performed.\nDue to the nature of inspection, developers need to develop a\n“case” to justify the need for refactoring, in order to convince\nthe reviewers. Therefore, the majority of participants (91.3%)\nfocus on reporting the motivation rather than the operation.\nMoreover, the identification of the operations can be deducted\nby the reviewers when they inspect the code before and after\nits refactoring. Finally, only a few respondents (6 participants)\nresponded that they thoroughly document their refactoring by\nreporting both the motivation and operation. Moreover, when\nwe asked, “what typical keywords you use when documenting\nrefactoring changes for a review?”, the developers answers\ncontain various refactoring phrases. Table [IV|enumerates these\npatterns (keywords in bold indicate that the keyword was\nmentioned by more than one developer).\n\nis quite revealing in several ways. First, we observe\nthat developers state the motivation behind refactoring, and\nthat some of these patterns are not restricted only to fixing\ncode smells, as in the original definition of refactoring in\nFowler’s book (ij. Second, developers tend to use a variety of\ntextual patterns to document their refactoring activities, such as\n‘refactor’, ‘clean up’, and ‘best practice’. These patterns can\nbe (1) generic to describe the act of refactoring without giving\nany details; or (2) specific to give more insights on how mainly\nprovide a generic description/motivation of the refactoring\nactivity such as ’improving readability’. A common trend\namongst developers is that they either report a problem to\nindicate that refactoring action is needed (e.g., ‘duplicate’,\n‘bugs’, ‘bad code’, etc.), or they state the improvement to the\ncode after the application of refactoring (e.g., ‘best practice’,\n‘ease of use’, ‘improving code quality’, etc.). By looking at\nthe refactoring discussion (see Figure 22). we realized that\ndevelopers do ask for more details to understand the performed\n\nTable (IV) List of refactoring keywords reported by the\n\nparticipants.\n\nPatterns\n\n(1) allow easier integration with\n(2) bad code\n(3) bad management\n\n(16) fix\n(17) improving code quality\n(18) loose coupling\n\n(31) remove legacy code\n(32) replace hard coded\n(33) reorganiz*\n\n(4) best practice (19) moderniz* (34) restructur*\n(5) break out (20) modif* (35) rewrit*\n\n(6) bugs (21) modulariz* (36) risks\n\n(7) cleanup (22) not documented (37) simply\n\n(8) cohesion (23) open close (38) single responsibility\n\n(9) comment (24) optimiz* (39) single level of abstraction\n(10) complexity (25) performance per function\n\n(11) consistency (26) readability (40) splitting logic\n\n(12) decouple (27) redundaney (Al) strategy pattern\n\n(13) duplicate (28) refactor* (42) stress test results\n\n(14) ease of use\n(15) extract class\n\n(29) regression\n(30) remov*\n\n(43) testing\n(44) uncomment\n\nrefactoring activities.\n\nSummary: Developers rarely report specific refactor-\ning operations as part of their documentation. Instead,\nthey use general keywords to indicate the motivation\nbehind their refactorings. Nevertheless, several pat-\n\nterns are solicited by developers to describe their re-\nfactorings. With the lack of refactoring documentation\nguidelines, reviewers are forced to ask for more details\nin order to recognize the need for refactoring.\n\nC. RQ3. What challenges do reviewers face when reviewing\nrefactoring changes?\n\nAs shown in Figure [4] we report the main challenges faced\nby reviewers when inspecting a refactoring review request.\nThe majority of the developers (17 respondents (70.8%))\ncommunicated that they were concerned about avoiding the\nintroduction of regression in system’s functionality. Interest-\ningly, refactoring by default, ensures the preservation of the\nsystem’s behavior through a set of pre and post conditions,\nyet, reviewers main focus was to validate the behavior of\nthe refactored code. In this context, a recent study have\nshown that developers do not rely on built-in refactoring\nin their Integrated Development Environments (IDEs) and\nthey perform refactoring manually [19], e.g., when moving\na method from one class to another, instead of activating\nthe ‘move method’ from the refactoring menu, developers\nprefer to cut and paste the method declaration into its new\nlocation, and manually update any corresponding memberships\nand dependencies. Such process is error prone, and therefore,\nreviewers tend to treat refactoring like any other code change\nand inspect the functional aspect of any refactored code.\n\nIn Figure 14 developers (58.3%) revealed the need to\ninvestigate the impact of refactoring on software quality.\nSuch investigation is not trivial, as it has been the focus of\na plethora of previous studies (e.g., 29), finding that not\nall refactoring operations have beneficial impact on software\nquality, and so developers need to be careful as various design\nand coding defects may require different types of refactorings.\nIn this context, we identified, in our previous study which\n\n', 'structural metrics (coupling, complexity, etc.) are aligned\nwith the developer’s perception of quality optimization when\ndevelopers explicitly mention in their commit messages that\nthey refactor to improve these quality attributes. Interestingly,\nwe observed that, not all structural metrics capture developers\nintentions of improving quality, which indicated the existence\nof a gap between what developers consider to be a design\nimprovement, and their measurements in the source code.\nWhen asked about their quality verification process, developers\nuse, as part of their internal process, the Quality Gate of\nSonarQube. While SonarQube is a popular, widely adopted\nquality framework, it suffers, like any other static analysis\ntools, from the high false positiveness of its findings, when\nit is not properly tuned.\n\nA moderate subset of 11 developers (45.8%) were con-\ncerned about having inadequate documentation about refact-\noring, whereas 10 developers (41.7%) were concerned about\nunderstanding the motivations for refactoring changes. 9 de-\nvelopers (37.5%) found that reviewing refactoring changes in a\ntimely manner is difficult, whereas 6 of them (25%) found that\nthe challenge is centered around understanding how refactor-\ning changes were implemented. In addition to these challenges,\ntwo participants stated, “The quality of code readability (being\nable to understand what the code author intended to do with\nthe logic/algorithm even without documentation”, and “Style\nchanges or personal preference that the author holds and feels\nstrongly about”.\n\nTo get a more qualitative sense, we also study bad refactor-\ning practices that reviewers catch when reviewing refactoring\nchanges. We analyzed the survey responses to this open ques-\ntion to create a comprehensive high-level list of bad refactoring\npractices that are being caught by reviewers. These practices\nare centered around five main topics: (1) interleaving refact-\noring with multiple other development-related tasks, (2) lack\nof refactoring documentation, (3) avoiding refactoring negative\nside effects on software quality, (4) inadequate testing, and (5)\nlack of design knowledge. In the rest of this subsection, we\nprovide more in-depth analysis of these refactoring practices.\n\nChallenge #1: Interleaving refactoring with multiple other\ndevelopment-related tasks. One participant indicated that,\n“Refactoring changes are intermixed with bug fix changes”\nand another mentioned “Refactoring after adding to many\nfeatures’, indicating that these practices are not desirable when\nperforming or reviewing refactoring changes. This suggests\nthat interleaving refactoring with bug fixes and new features\ncould be a challenge from a reviewer’s point of view. Even\nthough we did not ask a specific question concerning interleav-\ning refactorings with other development-related context, three\nparticipants acknowledged that mixing refactoring with any\nother activity is a potential problem. This can be explained by\nthe fact that behavior preservation cannot be guaranteed and\nit may introduce new bugs.\n\nChallenge #2: Lack of refactoring documentation. In con-\ntrast with how developers document bug fixes and functional\nchanges, the documentation of refactoring seems to be vague\n\nand unstructured. If we refer to our findings in our previ-\nous research question, developers lack guidelines on how to\ndescribe their refactoring activities, and they refer to their\npersonal interpretation to justify their decisions. To mitigate\nthis ambiguity, there is a need for proper methodology that\narticulates how developers should document refactoring code\nchanges. Reviewers did explicitly share their concerns during\nthe survey:\n“1. Lack of documentation, 2. Inconsistent variable nam-\ning, 3. Unorganized code, 4. No explanation why changes\nwere made [...]”; “[...],no guideline, different guidelines\nused in the project, bad code practices’; “[...] Not enough\ncomments”\n\nChallenge #3: Avoiding refactoring negative side effects on\nsoftware quality. The majority of the participants commented\nthat wrongly naming code elements and duplicate code are the\ncommon bad refactoring practices that they typically catch. It\nhas been proven by previous studies that a developer may\naccidentally introduce a design anti-pattern while trying to\nfix another (e.g., [30}). One mentioned example was how a\nlong method (large in lines of code, and has more than one\nfunctionality) can be fixed by splitting the method into two,\nusing the extract method refactoring operation. However, if the\nsplit does not create two cohesive methods (i.e., segregation\nof concerns), then the results could be two tightly coupled\nmethods, which one method can envy the other method’s\nattributes (i.e., feature envy anti-pattern). Thus, it is part of the\ncode review to verify the impact of refactoring on the software\ndesign from different perspectives (e.g., code smell removal,\nadherence to object-oriented design practices such as SOLID\nand GRASP, etc.). We report samples of the participants’\ncomments below to illustrate this challenge:\n“Poorly named methods, poorly named variables, lack of\nbasic Object Oriented Design principles and concepts,\nincreased complexity, increased coupling.” “duplication,\nlow-cohesion”, “Code refactoring does not follow the\ncoding standards set by the project. [...]”; “Tight coup-\nling, Lack of tests, convoluted logic, inconsistent variable\nnames, outdated comments”\nChallenge #4: Inadequate testing. By default, refactoring is\nsupposed to preserve the behavior of the software. Ideally,\nusing the existing unit tests to verify that the behavior is\nmaintained should be sufficient. However, since refactoring\ncan also be interleaved with other tasks, then there might be a\nchange in the software’s behavior, and so, unit tests, may not\ncapture such changes if they were not revalidated to reflect\nthe newly introduced functionality. This can be a concern\nif developers are unaware of such non behavior preserving\nchanges, and so, deprecated unit tests will not guarantee the\nrefactoring correctness. The following reviewers’ comments\nillustrate this challenge:\n“1) Not testing refactor code changes on all potential\nimpacted areas 2) Not adding newly named functions to\nold test suites [...]”; “[...] partial testing process”; “[...]\n', '99, 6\n\nNo follow-up testing”; “[...] No regression testing”; “Tight\n\ncoupling, Lack of tests [...]”\nChallenge #5: Lack of design knowledge. Developers typ-\nically refactor classes and methods that they recently and\nfrequently change. So, the more they change the same code\nelements, the more confident they become about their design\ndecisions. However, not all team members have access to all\nsoftware codebase, and so they do not draw the full picture\nof the software design, which makes their decision adequate\nlocally, but not necessarily at the global level. Moreover,\ndevelopers only reason on the actual screenshot of the current\ndesign, and there is no systematic way for them to recognize\nits evolution by, for instance, accessing previously performed\nrefactorings. This may also narrow their decision making, and\nthey may end up reverting some previous refactorings. These\nconcerns along others were also raised by participants, for\ninstance, one participant stated:\n\n“Lack of knowledge about existing design patterns in code\n\n(strategy, builder, etc.) and their context along with lack\n\nof knowledge about SOLID principles (especially open\n\nclose and dependency inversion). I’ve seen people claim\n\nthat the code cannot be tested but in reality the problem\n\nis in the way they’ve structured their code.”\nIt is clear that the code review plays also a major role in\nknowledge transfer between junior and senior developers, and\nin educating software practitioners about writing clean code\nthat meet quality standards.\n\nSummary: Challenges of reviewing refactored code\ninherits challenges of reviewing traditional code\nchanges, as refactoring can also be mixed with func-\ntional changes. Reviewers also report the lack of\nrefactoring documentation, and inspect any negative\n\nside effects of refactorings on design quality The\ninadequate testing of such changes hinder the safety\nof the performed refactoring. Finally, the lack of de-\nveloper’s exposure to whole system design can reduce\nthe visibility of their refactoring decision making.\n\nD. RQ4. What mechanisms are used by developers and re-\nviewers to ensure code correctness after refactoring?\n\nDevelopers reported mechanisms to verify the application\nof refactoring (see Figure 5p. 23 of the participants (95.8%)\nrefer to testing the refactored code; 17 (70.8%) reported\ndoing manual validation; 11 (45.8%) brought up ensuring the\nimprovement of software quality metrics; 9 (37.5%) mentioned\nusing visualization techniques; and 9 (37.5%) selected running\nstatic checkers and linters. Besides performing testing, two\nparticipants mentioned in the “other” option: “Automated Test\nCoverage”, and “Existing Unit tests’.\n\nWe observe that reviewers treat refactoring like any tra-\nditional code change, and they unit-test it for correctness.\nThis eventually minimizes the introduction of faults. However,\nwhen developers assume refactoring is preserving the behavior,\n\nwhile it is not, then they may not have updated their unit\ntests, and so their execution later by reviewers can become\nunpredictable, i.¢., some test cases may or may not fail because\nof their deprecation. Furthermore, some refactoring operations,\nsuch as ’extract method’, do create new code elements that\nare not covered by unit tests. So reviewers need to enforce\ndevelopers to write test cases for any newly introduced code.\n\nReviewers also refer to the quality gate to inspect if they\nrefactoring did not introduce any design debt or anti-patterns\nin the system. Yet, the manual inspection of the code is still the\ntules, some reviewers refer to visualizing the code before and\nafter refactoring to verify the completeness of the refactoring.\n\nSummary: Since reviewers unit test refactoring, just\nlike any other code change, developers need to add or\n\nupdate unit tests to the newly introduced or refactored\ncode. Furthermore, reviewers are manually inspecting\nthe refactored code to guarantee its correctness.\n\nE. RQS5. How do developers and reviewers assess and perceive\nthe impact of refactoring on the source code quality?\n\nAs can be seen from Figure [6] all participants (24, 100%)\nreplied that the code becomes more readable and understand-\nable. Intuitively, the main purpose of refactoring, is to ease\nthe maintenance and evolution of software. So reviewers,\nimplicitly consider refactoring to be an opportunity to clean\nthe code and make it adhere to the team’s coding conventions\nand style. Also, 12 (50%) indicated that it becomes easier to\npass Sonar Qube’s Quality Gate. So, it is expected that the\nrefactored code does not increase the quality deficit index, if\nnot decreasing it. Finally, 11 (45.8%) stated their expectation\nthat refactored, through better renames, and more modular\nobjects, should reduce the code’s proneness to bugs.\n\nSummary: Besides using Quality Gates and static\ncheckers to assess the impact of refactoring on the\n\nsoftware design, reviewers rate the success of refact-\noring to the extent to which the refactored code has\nimproved in terms of readability and understandability.\n\nV. RECOMMENDATIONS\nA. Recommendations for Practitioners\n\nIt is heartening for us to realize that developers refactor\ntheir code and perform reviews for the refactored code. Our\nmain observation, from developers’ responses, is how the\nreview process for refactoring is being hindered by the lack\nof documentation. Therefore, as part of our survey report to\nthe company, we designed a procedure for documenting any\nrefactoring ReR, respecting three dimensions that we refer to\nas the three Zs, namely, Intent, Instruction, and Impact. We\ndetail each one of these dimensions as follows:\n\nIntent. According to our survey results, (cf, Figure [3) it\nis intuitive that reviewers need to understand the purpose of\nthe intended refactoring as part of evaluating its relevance.\n\n', 'Avoiding the introduction of 70.8\nregression in system functionalities 5\nUnderstanding the impact of (a 58.3\nrefactoring on quality 5\nUnderstanding the motivation\nbehind refactoring 45.8\nInadequate documentation\nabout factoring | 41.7\nReviewing refactorings in\ntimely nner A 37.5\nUnderstanding how refactoring (mes\nchanges were implemented\na\n\n0 20 40 60 80 100\n\nFigure (4) Challenges faced by developers when reviewing\nrefactoring.\n\nTesting by running the old\nversion and the new versions\nand make sure they still\ngive the same result\n\nManual validation / experience\n\nEnsuring the improverment\nsoftware quality metrics\n\nVisualization of refactored\ncode\n\nRunning static checkers and\nlinters\n\n0 20 40 60 80 100\n\nFigure (5) Mechanisms used to ensure the correctness after\nthe application of refactoring.\n\nCode becomes more\nreadable and understandable\n\n100\n\nIt becomes easier to 50\npass quality gate\n\nCode becomes less prone\nto bugs and errors\n\n45.8\n0 20 40 60 80 100\n\nFigure (6) Implications experienced as software evolves\nthrough refactoring.\n\nTherefore, when preparing the request for review, developers\nneed to start with explicitly stating the motivation of the\nrefactoring. This will provide the context of the proposed\nchanges, for the reviewers, so they can quickly identify how\nthey can comprehend it. According to our initial investigations,\nexamples of refactoring intents, reported in Table |IV| include\nenforcing best practices, removing legacy code, improving\nreadability, optimizing for performance, code clean up, and\nsplitting logic.\n\nInstruction. Our second research question shows how rarely\ndevelopers report refactoring operations as part of their docu-\nmentation. Developers need to clearly report all the refactor-\ning operations they have performed, in order to allow their\nreproducibility by the reviewers. Each instruction needs to\nstate the type of the refactoring (move, extract, rename, etc.)\nalong with the code element being refactored (i.e., package,\nclass, method, etc.), and the results of the refactoring (the\nnew location of a method, the newly extracted class, the new\nname of an identifier, etc.). If developers have applied batch or\ncomposite refactorings, they need to be broken down for the\n\nreviewers. Also, in case of multiple refactorings applied, they\nneed to be reported in their execution chronological order.\n\nImpact. We observe from Figures[4] and[6] that practitioners\ncare about understanding the impact of the applied refactoring.\nThus, the third dimension of the documentation is the need to\ndescribe how developers ensure that they have correctly imple-\nmented their refactoring and how they verified the achievement\nof their intent. For instance, if this refactoring was part of a\nbug fix, developers need to reference the patch. If developers\nhave added or updated the selected unit tests, they need to\nattach them as part of review request. Also, it is critical to self-\nassess the proposed changes using Quality Gate, to report all\nthe variations in the structural measurements and metrics (e.g.,\ncoupling, complexity, cohesion, etc.), and provide necessary\nexplanation in case the proposed changes do not optimize the\nquality deficit index.\n\nUpon its acceptance for trial at Xerox, a set of developers\nhave adopted the Js procedure when submitting any refactoring\nrelated code change. These developers were initially given\nsupport for adopting it by us rewriting samples of their previ-\nous code review requests, using our template. We will closely\nmonitor its adoption, and perform any necessary tweaking. We\nalso plan on following up on whether this practice was able\nto be beneficial for reviewers by (1) empirically validating\nwhether refactoring ReRs, using our template, take less time\nto be reviewed, in comparison with other refactoring ReRs;\nand (2) rescheduling another follow up interview with the\ndevelopers have been using it.\n\nB. Recommendations for Research and Education\n\nProgram Comprehension. Refactoring for readability was\npointed out by the majority of participants. In contrast with\nstructural metrics, being automatically generated by the Qual-\nity Gate, reviewers are currently relying on their own in-\nterpretation to assess the readability improvement, and such\nevaluation can be subjective and time-consuming. There is\na need for a refactoring-aware code readability metrics that\nspecifically evaluate the code elements that were impacted\nby the refactoring. Such metrics help in contextualizing the\nmeasurement to fulfill the developer’s intention.\n\nTeaching Documentation Best Practices. Prospective soft-\nware engineers are mainly taught how to model, develop and\nmaintain software. With the growth of software communities,\nand their organizational and socio-technical issues, it is im-\nportant to also teach the next generation of software engineers\nthe best practices of refactoring documentation. So far, these\nskills can only be acquired by experience or training.\n\nVI. THREATS TO VALIDITY\n\nConstruct & Internal Validity. Concerning the complete-\nness and correctness of our interpretation of open responses\nwithin the survey, we did not extensively discuss all responses\nbecause some of them are open to various interpretations,\nand we need further follow up surveys to clarify them.\nConcerning the selection criteria of the participants, we tar-\ngeted participants whose code review description included the\nkeyword “refactor*”. Since the validity of our study requires\n', 'familiarity with the concept of refactoring, we assume that\nparticipants who used this keyword know the meaning and\nthe value of refactoring. Another potential threat relates to\nthe communication channel to identify the motivation driving\ncode review involving refactoring. We examined threaded\ndiscussions and some situations may not have been easily\nobservable. For example, determining whether the reviewer\nconfusion was primarily caused by the refactoring and not\nby another phenomenon is not practically easy to assess\nthrough discussions. Interviewing developers would be a good\ndirection to consider in the future to capture such motivations.\nExternal Validity. Concerning the representativeness of\nthe results, we designed our study with the goal of better\nunderstanding developer perception of code review involving\nrefactoring actions within a specific company. Further research\nin this regard is needed. As with every case study, the results\nmay not generalize to other contexts and other companies. But\nextending this survey with the open-source communities is part\nof our future investigation to challenge our current findings.\n\nVII. CONCLUSION\n\nUnderstanding the practice of refactoring code review is\nof paramount importance to the research community and\nindustry. In this work, we aim to understand the motivations,\ndocumentation, challenges, mechanisms and implications of\nrefactoring-aware code review by carrying out an industrial\ncase study of 24 software engineers at Xerox. In summary,\nwe found that: (1) refactoring is completed for a wide variety\nof reasons, going beyond its traditional definition, such as\nreducing the software’s proneness to bugs, (2) refactoring-\nrelated patterns mainly demonstrate developer perception of\nrefactoring, but practitioners sometimes provide information\nabout refactoring operations performed in the source code, (3)\nparticipants considered avoiding the introduction of regression\nin system functionality as the main challenge during their re-\nview, (4) although participants do use different static checkers,\ntesting is the main driver for developers to ensure correctness\nafter the application of refactoring, and (5) readability and\nunderstandability improvement is the primary implications of\nrefactoring on software evolution.\n\nVIII. ACKNOWLEDGEMENTS\n\nWe would like to thank the Software Development Man-\nager Wendy Abbott for approving the survey and all Xerox\ndevelopers who volunteered their time to participate in this\nresearch.\n\nREFERENCES\n\n[I] M. Fowler, K. Beck, J. Brant, W. Opdyke, and d. Roberts, Refactoring: Improving\nthe Design of Existing Code. Boston, MA, USA: Addison-Wesley Longman\nPublishing Co., Inc., 1999.\n\n[2] W. Cunningham, “The wycash portfolio management system,” ACM SIGPLAN\nOOPS Messenger, vol. 4, no. 2, pp. 29-30, 1992.\n\n[3] A. Bacchelli and C. Bird, “Expectations, outcomes, and challenges of modern code\nreview,” in International conference on software engineering, pp. 712-721, 2013.\n\n[4] C. Sadowski, E. Séderberg, L. Church, M. Sipko, and A. Bacchelli, “Modern\ncode review: a case study at google,” in International Conference on Software\nEngineering: Software Engineering in Practice, pp. 181-190, 2018.\n\n[5]\n\n[6\n\n(7)\n\n[8\n\n9\n\n[10]\n\noy\n\n[12]\n\n[13]\n\n[14]\n\n15]\n\n[16]\n\n07]\n\n[18]\n\n[19]\n\n[20]\n\nPi\n\n[22]\n\n[23]\n\n[24]\n\n[25\n\n[26]\n\n227]\n\n[28]\n\n[29]\n\n[30]\n\nA. Bosu, J. C. Carver, C. Bird, J. Orbeck, and C. Chockley, “Process aspects\nand social dynamics of contemporary code review: Insights from open source\ndevelopment and industrial practice at microsoft,” IEEE Transactions on Software\nEngineering, vol. 43, no. 1, pp. 56-75, 2016.\n\nN. Tsantalis, T. Chaikalis, and A. Chatzigeorgiou, “Jdeodorant: Identification and\nremoval of type-checking bad smells,” in 2008 12th European Conference on\nSoftware Maintenance and Reengineering, pp. 329-331, IEEE, 2008.\n\nW. Mkaouer, M. Kessentini, A. Shaout, P. Koligheu, S. Bechikh, K. Deb, and\nA. Ouni, “Many-objective software remodularization using nsga-iii;” ACM Transac-\ntions on Software Engineering and Methodology (TOSEM), vol. 24, no. 3, pp. 1-45,\n2015.\n\nA. Ouni, M. Kessentini, H. Sahraoui, K. Inoue, and K. Deb, “Multi-criteria code\nrefactoring using search-based software engineering: An industrial case study,”\nACM Transactions on Software Engineering and Methodology (TOSEM), vol. 25,\nno. 3, p. 23, 2016.\n\nE, Murphy-Hill and A. P. Black, “Refactoring tools: Fitness for purpose,” IEEE\nsoftware, vol. 25, no. 5, pp. 38-44, 2008.\n\nR. Arcoverde, A. Garcia, and E. Figueiredo, “Understanding the longevity of code\nsmells: preliminary results of an explanatory survey,” in Proceedings of the 4th\nWorkshop on Refactoring Tools, pp. 33-36, ACM, 2011.\n\nA. Yamashita and L. Moonen, “Do developers care about code smells? an\nexploratory survey,” in Working Conference on Reverse Engineering (WCRE),\npp. 242-251, 2013.\n\nM. Kim, T. Zimmermann, and N. Nagappan, “An empirical study of refactor-\ningchallenges and benefits at microsoft,” IEEE Transactions on Software Engin-\neering, vol. 40, no. 7, pp. 633-649, 2014.\n\nG. Szdke, C. Nagy, R. Ferenc, and T. Gyiméthy, “A case study of refactoring large-\nscale industrial systems to efficiently improve source code quality,” in International\nConference on Computational Science and Its Applications, pp. 524-540, Springer,\n2014.\n\nT. Sharma, G. Suryanarayana, and G. Samarthyam, “Challenges to and solutions\nfor refactoring adoption: An industrial perspective,” IEEE Software, vol. 32, no. 6,\npp. 44-51, 2015.\n\nC. D. Newman, M. W. Mkaouer, M. L. Collard, and J. I. Maletic, “A study on\ndeveloper perception of transformation languages for refactoring,” in International\nWorkshop on Refactoring, pp. 34-41, 2018.\n\nX. Ge, S. Sarkar, J. Witschey, and E. Murphy-Hill, “Refactoring-aware code\nreview,” in IEEE Symposium on Visual Languages and Human-Centrie Computing\n(VL/HCC), pp. 71-79, 2017.\n\nE. L. Alves, M. Song, T. Massoni, P. D. Machado, and M. Kim, “Refactoring\ninspection support for manual refactoring edits,” JEEE Transactions on Software\nEngineering, vol. 44, no. 4, pp. 365-383, 2017.\n\nL. MacLeod, M. Greiler, M.-A. Storey, C. Bird, and J. Czerwonka, “Code reviewing\nin the trenches: Challenges and best practices,” IEEE Software, vol. 35, no. 4,\npp. 34-42, 2017.\n\nD. Silva, N. Tsantalis, and M. T. Valente, “Why we refactor? confessions of\ngithub contributors,” in Proceedings of the 2016 24th ACM SIGSOFT International\nSymposium on Foundations of Software Engineering, FSE 2016, (New York, NY,\nUSA), pp. 858-870, ACM, 2016.\n\nE, Murphy-Hill, C. Parnin, and A. P. Black, “How we refactor, and how we know\nit?’ IEEE Transactions on Software Engineering, vol. 38, pp. 5-18, Jan 2012.\n\nE, A. AlOmar, A. Peruma, M. W. Mkaouer, C. Newman, A. Ouni, and M. Kes-\nsentini, “How we refactor and how we document it? on the use of supervised\nmachine learning algorithms to classify refactoring documentation,” Expert Systems\nwith Applications, p. 114176, 2020.\n\nE. A. AlOmar, M. W. Mkaouer, and A. Ouni, “Can refactoring be self-affirmed?\nan exploratory study on how developers document their refactoring activities in\ncommit messages,” in 2019 IEEE/ACM 3rd International Workshop on Refactoring\n(1WoR), pp. 51-58, IEEE, 2019.\n\nE, A. AlOmar, M. W. Mkaouer, A. Ouni, and M. Kessentini, “On the impact of\nrefactoring on the relationship between quality attributes and design metrics,” in\n2019 ACMAEEE International Symposium on Empirical Software Engineering and\nMeasurement (ESEM), pp. 1-11, IEEE, 2019.\n\nE. A. AlOmar, M. W. Mkaouer, and A. Ouni, “Toward the automatic classification\nof self-affirmed refactoring,” Journal of Systems and Software, vol. 171, p. 110821,\n2020.\n\nAlOmar., https://smilevo.github.io/self-affirmed-refactoring/, 2020 (last accessed\nOctober 16, 2020).\n\nJ. W. Creswell, “Research design: Quantitative, qualitative and mixed methods,”\n2009.\n\nB. A. Kitchenham and S. L. Pfleger, “Personal opinion surveys,” in Guide to\nadvanced empirical software engineering, pp. 63-92, Springer, 2008.\n\nE, Smith, R. Loftin, E. Murphy-Hill, C. Bird, and T. Zimmermann, “Improving\ndeveloper participation rates in surveys,” in 2013 6th International Workshop on\nCooperative and Human Aspects of Software Engineering (CHASE), pp. 89-92,\nIEEE, 2013.\n\nG. Bavota, A. De Lucia, M. Di Penta, R. Oliveto, and F. Palomba, “An experimental\ninvestigation on the innate relationship between quality and refactoring,” Journal\nof Systems and Software, vol. 107, pp. 1-14, 2015.\n\nF, Palomba, G. Bavota, M. Di Penta, F. Fasano, R. Oliveto, and A. De Lucia,\n“On the diffuseness and the impact on maintainability of code smells: a large scale\nempirical investigation,” Empirical Software Engineering, vol. 23, no. 3, pp. 1188-\n1221, 2018.\n\n']}


**File**: S:\OneDrive\@Dev\!GPT\ScriptGPT\library\Refactoring\Source\Software Testing and Code Refactoring.pdf
- Time Taken: 17.90s
- Data Extracted: {'text': ['Software Testing and Code Refactoring: A Survey\nwith Practitioners\n\nDanilo Leandro Lima\nAccenture\nRecife, PE, Brazil\ndaniloleandro@gmail.com\n\nSildemir S. da Silva\nCESAR School\nRecife, PE, Brazil\nsss@cesar.school\n\nAbstract—Nowadays, software testing professionals are com-\nmonly required to develop coding skills to work on test automa-\ntion. One essential skill required from those who code is the\nability to implement code refactoring, a valued quality aspect\nof software development; however, software developers usually\nencounter obstacles in successfully applying this practice. In this\nscenario, the present study aims to explore how software testing\nprofessionals (e.g., software testers, test engineers, test analysts,\nand software QAs) deal with code refactoring to understand\nthe benefits and limitations of this practice in the context of\nsoftware testing. We followed the guidelines to conduct surveys\nin software engineering and applied three sampling techniques,\nnamely convenience sampling, purposive sampling, and snow-\nballing sampling, to collect data from testing professionals. We\nreceived answers from 80 individuals reporting their experience\nrefactoring the code of automated tests. We concluded that in the\ncontext of software testing, refactoring offers several benefits,\nsuch as supporting the maintenance of automated tests and\nimproving the performance of the testing team. However, prac-\ntitioners might encounter barriers in effectively implementing\nthis practice, in particular, the lack of interest from managers\nand leaders. Our study raises discussions on the importance of\nhaving testing professionals implement refactoring in the code of\nautomated tests, allowing them to improve their coding abilities.\n\nIndex Terms—software testing, test automation, code refactor-\ning, test engineers.\n\nI. INTRODUCTION\n\nSoftware quality is defined as the degree to which the\nsoftware meets the expectation of clients and users and the\nextent to which it adheres to both specifications of products\nand processes [1]; this means that software quality includes not\nonly tasks related to the software evaluation (e.g., testing), but\nalso how the whole software development life-cycle adheres\nto product standards, processes, and procedures (e.g., reviews,\ncode quality, automation, among others) [2].\n\nSoftware quality is an umbrella, i.e., an overall evaluation\nprocess composed of multiple attributes that can attest that\nthe development of a system was successful [3]. Testing is the\nmost common quality assurance strategy applied in software\n\nRonnie de Souza Santos\nCape Breton University & CESAR School\nSydney, NS, Canada\nronnie_desouza@cbu.ca\n\nCesar Franga\nCESAR School\nRecife, PE, Brazil\nfranssa.cesar.school\n\nGuilherme Pires Garcia\nAgape2IT\nRecife, PE, Brazil\nguilhermeg@agap2.pt\n\nLuiz Fernando Capretz\nWestern University\nLondon, ON, Canada\nIcapretz@uwo.ca\n\ndevelopment. Software testing involves appraising the system\nfunctionalities to determine that they behave as expected, e.g.,\nsearching for failures [4]. Software testing can be performed\non several levels (e.g., unit, component, integration, system),\nusing different approaches (e.g., white box, black box, or grey\nbox), and essentially performed in two ways, either manually\nor using automation [5].\n\nManual testing requires a great amount of human interac-\ntion, as professionals seek to identify failures by interacting\ndirectly with the system and observing its behavior. On the\nother hand, automated testing relies on building and executing\ncode to simulate human interaction and observe the system\nbehavior to identify failures [6]. Currently, automated testing\nis prioritized in many projects as this technique is presumed to\nbe faster, less repetitive, and provide higher software coverage\n(e.g., more tests can be performed at the same time) [7]-[9].\nHowever, relying on automated tests can be costly as they\nrequire regular maintenance and frequent updating [10].\n\nCode maintenance is a crucial software quality factor;\ntherefore, automated testing demands from software testing\nprofessionals a whole set of technical expertise in coding and\nversioning, in particular, towards refactoring testing code [10],\n[11]. Refactoring is the process of changing the code without\nchanging the external behavior of the software. Software\ndevelopers widely apply this practice to improve the quality\nof the source code [12], [13]. However, professionals can\nencounter difficulties implementing refactoring in their code\n[14].\n\nAs refactoring is a practice related to code maintenance,\nit is only natural that testing professionals who work with\nautomation must use this practice in their work, which raises\nthe following research question:\n\nResearch question: How do software testing profession-\nals deal with code refactoring when they are working with\ntest automation?\n\nFrom this introduction, our study is organized as follows.\n', 'In Section II, we discuss existing studies about refactoring. In\nSection III, we describe how we conducted the survey, while\nSection IV presents our results. In Section V, we discuss the\nimplications of our study. Finally, Section VI summarizes our\ncontributions.\n\nIl. BACKGROUND\n\nRefactoring promotes improvements in the software by ap-\nplying changes to its internal components (e.g., the code) while\nmaintaining its observable behavior, i.e., this is a quality strat-\negy that aims to improve code readability, understandability,\nand maintenance and provide professionals with opportunities\nto implement re-engineering [12], [13], [15].\n\nUsually, refactoring is a practice associated with the work\nof developers/programmers, i.e., those who deal more directly\nwith the source code [16]. Developers know that refactoring\nimproves their code; however, several barriers make it difficult\nto do so in many software projects. These barriers include\na lack of resources, tight deadlines, complex changes in\nthe code, high costs, lack of technical knowledge, lack of\nmanagement, and unavailability of appropriate tools [14].\n\nCurrently, with the dynamic software environment result-\ning from agile development, software testing professionals\nare required to acquire more programming skills. Therefore,\nknowing how to code has become one of the primary abilities\nrequested in job advertisements for testing professionals [17].\nSoftware companies require testing professionals to develop a\ncertain level of programming capabilities to successfully build\na career in software testing [18], which means that knowing\nhow to refactor code is a necessary technical skill.\n\nSoftware testing professionals implement code, especially\nwhen working with test automation. Knowing how to automate\ntests is one of the most valued technical skills in the software\nindustry [17], [19], in particular, because of the benefits of test\nautomation, which include the improvement of product quality,\nhigh test coverage, reduced testing time, reliability, reusability,\nreduced human effort, cost reduction, and increased fault\ndetection [20].\n\nHowever, there are challenges associated with testing au-\ntomation that sometimes generates problems for testing pro-\nfessionals, such as high costs, unavailability of appropriate\ntools, inadequate testing structure, insufficient programming\nknowledge, and constant need for code maintenance [21].\nMany of these challenges are similar to or closely related to\nthe barriers that programmers face when refactoring code [14].\n\nIII. METHOD\n\nIn this study, we conducted a cross-sectional survey [22] to\nexplore software testing professionals’ experience with refac-\ntoring the code of automated tests. Our goal is to understand\nhow testing professionals perceive the general benefits and\nlimitations of code refactoring, which are usually associated\nwith the work of software developers (e.g., programmers). To\nachieve this goal, we surveyed practitioners following three\nguidelines for conducting surveys in software engineering\n[23]-[25]. Figure 1 presents an overview of our study, and\nour methodology steps are presented below.\n\nA, Using the Literature to Investigate the Industry Practice\n\nMany studies explore the influence of code refactoring\non the general quality of software [12], [13], [15]. Usually,\nthe difficulties of conducting code refactoring are discussed\nconsidering the work of developers [14]. However, nowadays,\nwe observe in the software industry an increase in the need for\ntesting professionals to improve their programming skills [17],\n[19], which includes dealing with code refactoring. Therefore,\nwe designed this survey using the findings published in the\nliterature to explore how testing professionals are experiencing\ncode refactoring when working with test automation tasks in\nthe industry.\n\nB. Instruments\n\nWe built our questionnaire based on several published evi-\ndence in the literature about how software developers deal with\ncode refactoring [14]-[18], [21]. In particular, we followed\nthe findings discussed by [14] to explore how the general\nchallenges of code refactoring could apply to software testing.\nTherefore, our questionnaire is mainly based on the results\npublished in the literature that investigated how developers\nwork with code refactoring, including:\n\n- Testing professionals’ knowledge about test automation\nand code refactoring: We asked professionals whether\nthey have experience working with test automation\nor not since this is a valued technical skill in the\nsoftware industry nowadays, as discussed in [17], [18].\nIn particular, the perception of code refactoring would\nbe different depending on the experience of participants\nwith programming.\n\n- Benefits and challenges of refactoring: we asked\nparticipants what are the benefits and challenges of code\nrefactoring based on what was reported in previous\nstudies [14]-[16], [21]. These questions were built to\nexplore the types of refactoring performed in the code\nof automated tests, the benefits of this practice, and\nthe challenges associated with refactoring testing code\n\nregularly.\n\n- Practitioners profile: we asked demographic questions\nabout the participants’ backgrounds to understand our\nsample profile. We asked participants to inform their\ngender, the location of the company where they work,\ntheir education level, their experience level (i.e., years\nof experience working with software development), and\nwhether they have testing certifications.\n\n- Volunteer participation: we started the questionnaire by\ninforming practitioners about the goal of our study. In\naddition, we made sure that they knew their participation\nwas voluntary and anonymous. Therefore, in order to\nanswer the questionnaire, the participants needed to\nagree to participate under these conditions.\n', 'INDUSTRY PROBLEM\n\nTesting professionals\nare expected to develop\nand improve\nprogramming skills to\nwork with automation.\n\nPREVIOUS RESEARCH\n\nResearchers investigated\ncode refactoring for the\nperspective of software\n\ndevelopers, e.g.,\nprogrammers.\n\nINDUSTRY STUDY\n\nHow do software testing\nprofessionals deal with\ncode refactoring when\n\nthey are working with\ntest automation?\n\nPRESENT PAPER\n\nPerspective of testing\nprofessionals on code\nrefactoring and\nrecommendations to\npractitioners.\n\nMOTIVATION\n\nLITERATURE\n\nSURVEY IMPLICATIONS\n\nFig. 1. Study Overview\n\nDuring the process of designing the survey questionnaire,\nto increase construct validity and delimit the scope of our\ninvestigation, we defined test automation as the automation of\ntest cases performed by software testing professionals at the\nsystem level or for regression testing. Therefore, unit testing\nand integration testing were not explored in this study.\n\nOnce all questions were defined, a pilot questionnaire was\nvalidated by three senior testing professionals who suggested\nchanges in the wording and terms used in questions 7, 10,\n11, and 12 and proposed modifying the sequence of some\nquestions. The participants who validated the pilot question-\nnaire agreed that presenting the questionnaire with mostly\nclosed-ended questions is a good strategy for exploring the\ntopic since refactoring is not consistently common among\ntesting professionals in the industry. Therefore, having options\nto be selected would increase the willingness of participants\nto answer the survey, i.e., extensive open-ended questions or\ninterviews might have increased the difficulty of obtaining\ndata.\n\nThe final questionnaire was composed of 12 questions.\nNone of the questions were optional (e.g., participants needed\nto answer every question). We included an option for the\nparticipants to provide other answers different from those\nlisted in the questionnaire, even though, as stated above, our\nprimary goal is to explore in the industry setting a topic that\nis being mainly explored in the academic context or with\ndifferent practitioners (e.g., developers). Table I presents the\nfinal questionnaire.\n\nC. Data Collection\n\nWe followed the recommendations for treating software pro-\nfessionals as a hidden population [26], which is a strategy that\n\ncan mitigate sampling bias in software engineering surveys, as\nwell as support reaching more individuals of a population.\n\nTreating software professionals as hidden populations\nmakes sense in this study because a hidden population cannot\nbe easily defined or enumerated based on existing knowl-\nedge [26]. In this case, enumerating all software testing\nprofessionals despite their experience with code refactoring is\nimpractical. Recently, the number of studies treating software\nengineers as a hidden population in the literature is increasing\n[27]{29].\n\nConsidering this, we applied three techniques to spread our\nquestionnaire and collect data from software testing profes-\nsionals. All three techniques were non-probability sampling,\nthat is, sampling methods that do not employ randomness. The\ntechniques were applied as follows:\n\n+ Convenience sampling relied on selecting participants\nbased on availability [26]. Following this technique,\nwe used our extensive network of software testing\nprofessionals to share the questionnaire and ask them to\nanswer it, depending on their availability.\n\n- Purposive sampling relied on selecting participants\nfrom a specific site and inviting them to participate in\nthe study [26]. Following this technique, we contacted\nthe QA manager of a large software company, who\nforwarded our questionnaire to over 187 software\ntesting professionals in his organization. In addition, we\nadvertised the questionnaire in online software testing\ncommunities.\n\n- Snowballing sampling relied on identifying some indi-\n', 'TABLE I\nSURVEY QUESTIONNAIRE\n\n1. We invite you to participate in the survey: Refactoring code of automated tests.\nThis survey is COMPLETELY ANONYMOUS; no information provided can be\nlinked back to you. Answering the questionnaire will take up to 5 minutes. Do\nyou agree to participate?\n\n( Yes\n\n2. Which gender do you identify with?\n() Female\n\n() Male\n\n() Non-binary\n\n() Prefer not to answer\n\n3. What is your Country?\n\n4, What is your highest educational level?\n() High-School\n\n() Bachelor’s degree\n\n() Post-baccalaureate\n\n() Master’s degree\n\n() PhD.\n\n5. How long have you been working with software testing?\n6. Do you have any testing certification?\n\n7. What is your job level?\n() Trainee\n\n() Junior\n\n() Mid-level\n\n() Senior\n\n() Principal\n\n8. Do you know what code refactoring is?\n() Yes\n() No\n\n9. Have you ever refactored any test automation code?\n() Yes\n() No\n\n10. What refactoring techniques do you know or apply?\n\n11. What are the benefits of code refactoring in test automation?\n() Increase of automation reusability\n\n() Improvement of code readability\n\n() Improvement of team productivity\n\n() Improvement of test automation performance\n\n() Removal of duplicated code\n\n() No observable benefit\n\n() Other benefit:\n\n12. What are the challenges of code refactoring in test automation?\n() Changing test code that is working\n\n() Complexity of changes\n\n() Lack of knowledge on refactoring\n\n() Lack of priority\n\n() No interest from managers/leaders\n\n() No significant gain\n\n() No time available (time-consuming)\n\n() Lack of proper tools\n\n() Other challenge:\n\nviduals from the population and asking them to identify\nother individuals that could participate in the study [26].\nFollowing this technique, we asked participants from\nthe convenience sample to forward our questionnaire to\ncolleagues and co-workers.\n\nD. Data Analysis\n\nThe information collected in this study is mainly quan-\ntitative. Therefore, we applied descriptive statistics [30] to\nanalyze the data collected from participants and summarize\nthe information emerging from our data set.\n\nDescriptive statistics allowed us to present the distribution\nand the frequency of participants’ answers regarding their\nexperience refactoring the code of their automated tests and\nthe difficulties faced in completing this task. Since this is a\nwork in progress, more qualitative data will be collected in\nthe upcoming stages of the research.\n\nQualitative data is not the focus of the survey since we are\nexploring, in the industrial setting, the benefits and limitations\nof code refactoring reported in the literature. In particular,\nwe focus on the discussions presented in [11], [14], [16].\nIn addition, even though we have provided participants with\nan open-ended option to discuss other aspects related to\nthe theme, the vast majority of our sample stuck with the\noptions and the additional data collected (e.g., other benefits\nor challenges in addition to the ones listed in the questionnaire)\nwere not statistically representative to be incorporated to the\nresults. Instead, we are keeping them in mind for a future\ninvestigation of the theme using qualitative approaches.\n\nE. Ethics\n\nNo personal information about the participants was col-\nlected in this study (e.g., name, e-mail, or employer) to main-\ntain participants’ anonymity. As mentioned above, we included\nthe beginning of the questionnaire with general information\nabout the study and the research team and asked participants\nto agree (by checking a yes box) to use their data for scientific\npurposes.\n\nIV. FINDINGS\n\nWe received 82 answered questionnaires. However, we\nconsidered only 80 of them valid, as two participants informed\nhaving no experience with software testing; therefore, not\nbeing part of our targeted population. Below, we present\nthe results obtained from our descriptive analysis. Table II\nsummarizes the profile of professionals that participated in\nthis survey.\n\nA, Demographics\n\nIn general, our sample is composed of experienced software\ntesting professionals, as 48% of the participants have more\nthan five years of experience working in testing activities, and\n46% are working in Senior or Principal positions.\n\nRegarding education, 35% of the participants have a post-\nbaccalaureate certificate in software testing, which shows that\nover 86% of participants have a high level of training in testing\npractices.\n', 'Further, 26% of our sample are non-male individuals (i.e.,\nwomen and individuals that preferred not to respond about\ntheir gender). Although this is a relatively low rate, previous\nstudies have discussed the lack of diversity in the software\nindustry, which explains why most of our sample is composed\nof men.\n\nFinally, we have participants working from companies in\neight countries. However, almost 59% of our sample is com-\nposed of professionals that work for companies in Brazil. Two\nfacts can explain this:\n\n- Two data collection techniques started with Brazilian\n\nprofessionals: convenience and purposive samplings.\n\n- The post-pandemic scenario and the possibility of remote\nwork increased the number of professionals that live in\none location and work in another.\n\nEven though most of our sample is from one country,\nwe understand that several participants work on projects that\ninvolve international clients, which increases their experience\nwith practices and processes of software development used\nworldwide.\n\nTABLE II\nDEMOGRAPHICS\n\nParticipants Profile\n\nGender Male 59\nFemale 20\nPrefer not to answer 1\nEducational Level High-School 4\nBachelor’s degree 41\nPost-baccalaureate 28\nMaster’s degree 7\nJob Level Trainee 2\nJunior 13\nMidlevel 28\nSenior 29\nPrincipal 8\nExperience 0-1 Years 5\n2-4 Years 13\n3-5 Years 23\n5+ Years 39\nTesting Certification Yes 60\nNo 20\nLocation Argentina 4\nBrazil 33\nCanada 3\nGermany 1\nIreland 5\nMexico 1\nNetherlands 1\nus 12\n\nB. Experience with Refactoring Automation Code\n\nWe asked participants about their experience with test\nautomation and refactoring, and as a result, 75% (60/80)\nof our sample reported that they are currently working on\ntest automation activities. This is the same percentage of\nprofessionals who performed refactoring in their tests, now,\nor in previous projects. This means that 25% of our sample\n(20/80) have never refactored any code.\n\nFurther analyzing our data, from the amount of software\ntesting professionals that never refactored any code, 3% (6/20)\nreported not knowing how to perform the refactoring. How-\never, two of them are currently working with test automation.\nMost of these professionals are currently at the beginning of\ntheir careers (e.g., Junior professionals), which can explain the\nlack of knowledge about automation and refactoring.\n\nWe also asked participants what type of refactoring they\nused in their automated tests. We compared their answers with\nthe techniques discussed in the literature (e.g., [11], [14], [16])\nto build a list of refactoring types. Our results demonstrate that\nthe most used types of refactoring used in testing are:\n\n- Changing the structure of methods: to make sure that\nthe automated tests are effectively used by the team, the\ntesting methods are modified to improve readability and\nunderstandability.\n\n- Removing useless code: to keep up with the project\nestimations, the automation code is frequently modified\nby simply commenting on the code, thus, requiring\nfuture refactoring to remove dead code.\n\n- Renaming variables: to improve code maintainability,\nreadability, and understandability, automation testing\nrefactoring includes renaming variables to more\ndescriptive and concise names.\n\n- Adding test assertions: to guarantee test coverage,\nrefactoring the testing code with new assertions is\nfrequently necessary, enabling the verification of new\nconditions.\n\n- Other necessary changes: to guarantee that the\nautomation continues running with no problems, other\nrefactorings might be necessary from time to time, e.g.,\nupdating libraries and fixing dependencies related to\ntools and system versions.\n\nTable II] summarizes these results and presents the percent-\nage of answers in our sample.\n\nTABLE III\nTYPES OF REFACTORING IN TESTING CODE\n\n% of Participants\n\n76%\n75%\n\nChanging the structure of methods\nRemoving useless code\n\nRenaming variables 50%\nAdding test assertions 49%\nOther necessary changes 5%\n\nRefactoring Types\n\nC. Benefits of Code Refactoring in Test Automation\n\nBased on the general benefits of code refactoring reported in\nthe literature (e.g., [11], [14], [16]) and usually discussed from\nthe perspective of software developers (e.g., programmers and\nsoftware engineers), we asked software testing professionals\nhow these benefits are perceived in software testing.\n', 'As presented in Table IV, our results demonstrate that\nthe general benefits of code refactoring usually observed by\nsoftware developers also apply to refactoring tasks in test\nautomation, as the testing professionals mostly agreed with\nwhat is reported in the literature.\n\nIn addition, 35% of our sample (28/80) reported that refac-\ntoring improves automation performance, i.e., refactoring the\ncode of automated tests could support these professionals in\nimproving and maintaining the tests, therefore saving time.\nMoreover, 26% (21/80) reported that refactoring also increases\nthe performance of the testing team, i.e., once the tests are easy\nto maintain, the professionals would have more time available\nto focus on other quality activities.\n\nTABLE IV\nBENEFITS OF REFACTORING IN TESTING CODE\n\n% of Participants\n\nSupport automation code maintenance 75%\nImprovement of code readability 60%\nIncrease of automation reusability 56%\n\nBenefits Removal of duplicated code 41%\nImprovement of test automation performance 35%\nImprovement of team performance 26%\nNo observable benefit 4%\n\nD. Barriers to Code Refactoring in Test Automation\n\nWe provided participants with a list of difficulties in con-\nducting refactoring that was previously reported in the litera-\nture (e.g., [14]) and asked them to select those that are more\nchallenging when refactoring the code of automated tests.\nAs summarized in Table V, there are at least eight barriers\nthat keep software testing professionals from consistently\nperforming refactoring in their work.\n\nOur result demonstrated that most barriers that challenge\ntesting professionals also challenge software developers. How-\never, software testers struggle with the lack of practical knowl-\nedge of refactoring techniques. Testing professionals might not\nhave enough opportunities to deal with refactoring due to how\ntheir work is organized and how test automation activities are\ndesigned. However, as 75% of our sample have performed\ncode refactoring before (see Section IV-B), this means that this\nlack of knowledge might refer to complex refactoring changes\nin the code, i.e., not easy to implement, as pointed out by 20%\nof the sample.\n\nAlmost 80% of our participants reported that refactoring\nthe code of their automated tests would consume too much\ntime; therefore, they need to focus on other activities instead\nof refactoring. Moreover, 61% of our sample reported a\nlack of interest from software project managers and team\nleaders in prioritizing this type of activity in the project.\nThis result indicates that the main barrier faced by software\ntesting professionals is related to how the project activities\nare planned. Project estimations (e.g., sprint planning) do not\nusually include code refactoring in testing activities, and this\nmight affect how refactoring is perceived in software testing.\n\nTABLE V\nCHALLENGES OF REFACTORING IN TESTING CODE\n\n% of Participants\n\n79%\n59%\n\nTime-consuming\nNo interest from managers/leaders\n\nLack of knowledge on refactoring 51%\n\nChallenges Complexity of changes 20%\nChanging test code that is working 19%\nTools unavailability 16%\nNo significant gain 15%\nLack of priority %\n\nV. DISCUSSION\n\nWe start our discussions by comparing our results with\nthe previous literature. Following this, we discuss threats to\nvalidity. Finally, we present our plans for future research.\n\nA, Enfolding the Literature\n\nIn the industry, software testing professionals encounter\nsimilar difficulties that developers face when dealing with\ncode refactoring, including tight deadlines, complex changes,\nhigh costs, lack of specific knowledge, and unavailability\nof appropriate tools [14]. However, they have an additional\nchallenge to overcome, namely, the lack of interest from\nmanagers and leaders in refactoring the code of automated\ntests.\n\nAlthough the ability to automate tests is frequently required\nin job positions and particularly valued in the software industry\nnowadays [17], testing professionals still need opportunities\nto practice their programming skills. This would require that\nsoftware projects include time, resources, tools, and attention\nto the importance of refactoring for software testing. Software\nmanagers should be aware of this barrier and plan project\nactivities accordingly.\n\nThis study has implications for research, as there is a\nlack of studies in the literature that discuss code refactoring\nof automated tests in the industrial context, e.g., using the\nexperience of professionals to guide the study. We chose to use\nthis perspective, i.e., analyzing how the evidence published in\nthe literature applies to industrial practice as an opportunity to\nwork closely with those effectively experiencing the problem\ndaily. In particular, previous studies have demonstrated the\nexistence of a gap between what academia and practice are\ninterested in software testing [31]. In this study, we tried to\nbring both worlds together.\n\nIn addition, we believe that our findings relate to the discus-\nsions raised in recent studies about how software engineering\nstudents are not considering careers in software testing because\nthey believe there is a lack of programming activities and\nchallenges in the job [32]. Our results demonstrate that, differ-\nent from this general belief, software testing involves several\nchallenging activities since refactoring the code of automated\ntests is a dynamic task performed by testing professionals.\nTherefore, investing time and resources in refactoring and\nother coding-related activities in software testing might help\nincrease the attractiveness of this career.\n', 'Our results have implications for industry practice as well.\nTesting is an essential activity in software development, and\ntesting cycles can consume a lot of time and resources\nin a software project [33]. Therefore, test automation and\nrefactoring are essential practices in software development\nnowadays. In this sense, our results highlight various benefits\nof code refactoring to software quality. In particular, because\none of the main costs associated with test automation is\nmaintenance, which includes updating the automated tests (i.¢.,\nrefactoring) [21]. However, to successfully apply this practice,\nprofessionals need to understand the barriers presented in this\nstudy and identify strategies to overcome them.\n\nB. Recommendations\n\nBased on our results, some recommendations can be useful\nfor practitioners and software projects that want to use code\nrefactoring associated with test automation to increase quality:\n\n- Managers and leaders need to plan activities (e.g.,\nsprints) estimating time for refactoring not just for\ndevelopers but for testing professionals as well.\n\n- Testing professionals need to discuss with their teams\nthe importance of refactoring for test automation, in\nparticular, the relevance of this practice in improving\ntesting performance.\n\n- Developers must be supportive in helping testing\nprofessionals to deal with complex refactoring.\n\n- Researchers must focus on the development of tools to\nsupport automation in the context of software testing,\ne.g., automated test code.\n\nIn addition, we understand that our results are a starting\npoint for discussions on improving software testing education\nwith training opportunities on testing automation and refactor-\ning. However, since we did not investigate this topic from the\nperspective of students, we can only hypothesize this.\n\nC. Threats to Validity\n\nSince our study targeted software testing professionals, to\navoid surveying individuals from a different sample [23], we\nfocused only on professionals who specialized in testing and\nquality activities on their teams (e.g., software testers, test\nengineers, test analysts, QAs). We advertised our question-\nnaire directly to these professionals using convenience and\npurposive sampling and asked participants to only forward\nthe questionnaire to professionals working in similar roles\n(snowballing sampling).\n\nTo address construct validity [23], [24], our message in-\nvitation informed participants that the survey was focused\non test automation in the context of system-level testing and\nregression testing, i.e., unit and integration testing was out of\nthe scope of the research.\n\nFor criterion validity and to avoid ambiguity or questions\nthat confuse the participants [23], we validated our pilot ques-\ntionnaire with three senior testing professionals who supported\nus in this study; however, they were not included in the sample.\n\nIn addition, our survey has a limitation related to how the\ndata was collected. Our questionnaire was mostly based on the\nbenefits and limitations of code refactoring faced by software\ndevelopers and reported in the literature. In particular, [11],\n\n[14], [16] were used to build the list of options provided\n\nto participants to select in the survey. Following this, the\n\nliterature limited our results. We acknowledge that other\nstudies might have been published discussing other general\nbenefits and limitations of refactoring; however, we did not\nfollow a systematic review process to identify the papers in\nthe literature. Therefore, we plan to improve our understanding\nof code refactoring in software testing by collecting qualitative\ndata from practitioners in the future.\n\nFinally, our survey has a cultural limitation, as our partic-\nipants are mostly from one country. We plan to solve this\nlimitation by replicating the survey in other regions to in-\ncrease the participation of professionals from other companies.\nHowever, we believe that our current results can be used\nto inform practitioners about code refactoring in software\ntesting, as 75% of our sample has testing certifications that are\nrecognized worldwide (ID), 1.e., they are familiar with practices\nand processes that are applied in software companies from\nmany countries.\n\nD. Future Work\n\nSeveral interesting aspects of the work in software testing\ncan be explored in the industrial context, in particular, consid-\nering agile software development and the impact of software\nquality on software projects, and we expect to investigate them\nin our future studies.\n\nSpecifically, the next steps of this research on supporting\ntest automation will be focused on the following:\n\n- Replicating the survey to increase the sample, targeting\ntesting professionals from other locales to allow\ngeneralization of results.\n\n- Conducting focus groups with testing professionals to\nimprove our discussions on the benefits and limitations\nof refactoring in test automation.\n\n- Exploring tools that can support testing professionals in\ntheir refactoring activities;\n\n- Understanding the impacts of the post-pandemic work\narrangements (e.g., hybrid work) on quality activities,\nincluding test automation and refactoring processes.\n\nWe understand that these studies will contribute significantly\nto the improvement of software quality activities in the indus-\n\ntry.\n', 'VI. CONCLUSION\n\nIn this study, we investigated the perspective of software\ntesting professionals on test automation and code refactoring.\nBased on the experience of 80 individuals, we concluded\nthat refactoring offers several benefits, from supporting the\nmaintenance of automated tests to improving the performance\nof the testing team.\n\nHowever, practitioners might encounter many difficulties in\neffectively implementing this practice. Some difficulties are\nsimilar to the ones faced by programmers. In contrast, other\nchallenges are observed only in the testing context, such as the\nlack of attention from managers and leaders toward improving\nthe test automation processes.\n\nOur study has implications for research and practice. We\nexpect that our findings can motivate researchers to develop\ntools to support testing professionals in coding activities, such\nas refactoring. As for industrial practice, we are calling the\nattention of software managers and leaders to the importance\nof supporting testing professionals in learning, exploring, and\napplying refactoring in their work to improve the quality of\nsoftware products.\n\nREFERENCES\n\nJ. Harischandra and S. Hettiarachchi, “Organizational factors that affect\nthe software quality a case study at the engineering division of a selected\nsoftware development organization in sri lanka,” in 2020 IEEE 7th\nInternational Conference on Industrial Engineering and Applications\n(ICIEA). IEEE, 2020, pp. 984-988.\n\n2] I. Ozkaya, “Can we really achieve software quality?” JEEE Software,\nvol. 38, no. 3, pp. 3-6, 2021.\n\n3] Z. Dong, “An approach to multiple attribute decision making with\nintuitionistic fuzzy information and its application to software quality\nevaluation,” in JOP Conference Series: Materials Science and Engineer-\ning, vol. 740, no. 1. IOP Publishing, 2020, p. 012202.\n\n4] A. K. Arumugam, “Software testing techniques & new trends,” Jnterna-\ntional Journal of Engineering Research & Technology (IJERT), vol. 8,\nno. 12, 2019.\n\n5] M.A. Umar, “Comprehensive study of software testing: Categories, lev-\nels, techniques, and types,” Jnternational Journal of Advance Research,\nIdeas and Innovations in Technology, vol. 5, no. 6, pp. 32-40, 2019.\n\n6] K. R. Halani, R. Saxena ef al., “Critical analysis of manual versus\nautomation testing,” in 202/ International Conference on Computational\nPerformance Evaluation (ComPE). YEEE, 2021, pp. 132-135.\n\n7] G. Grano, T. V. Titov, S. Panichella, and H. C. Gall, “How high\nwill it be? using machine learning models to predict branch coverage\nin automated testing,” in 20/8 IEEE workshop on machine learning\ntechniques for software quality evaluation (MaLTeSQuE). IEEE, 2018,\npp. 19-24.\n\n8] I. Rana, P. Goswami, and H. Maheshwari, “A review of tools and\ntechniques used in software testing,” Int. J. Emerg. Technol. Innovative\nRes, vol. 6, no. 4, pp. 262-266, 2019.\n\n9] P. Kong, L. Li, J. Gao, K. Liu, T. F. Bissyande’, and J. Klein, “Auto-\nmated testing of android apps: A systematic literature review,” JEEE\nTransactions on Reliability, vol. 68, no. 1, pp. 45—66, 2018.\n\nM. Sa’nchez-Gordo’n, L. Rijal, and R. Colomo-Palacios, “Beyond tech-\nnical skills in software testing: Automated versus manual testing,”\nin Proceedings of the IEEE/ACM 42nd International Conference on\nSofiware Engineering Workshops, 2020, pp. 161-164.\n\nE. Soares, M. Ribeiro, R. Gheyi, G. Amaral, and A. Santos, “Refactoring\ntest smells with junit 5: Why should developers keep up-to-date?” JEEE\nTransactions on Software Engineering, vol. 49, no. 3, pp. 1152-1170,\n2022.\n\nM. De Stefano, M. S. Gambardella, F. Pecorelli, F. Palomba, and\nA. De Lucia, “casper: A plug-in for automated code smell detection\nand refactoring,” in Proceedings of the International Conference on\nAdvanced Visual Interfaces, 2020, pp. 1-3.\n\n[10\n\n{ll\n\n[l2\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32]\n\n[33]\n\nM. Wyrich and J. Bogner, “Towards an autonomous bot for automatic\nsource code refactoring,” in 2019 IEEE/ACM Ist international workshop\non bots in software engineering (BotSE). YEEE, 2019, pp. 24-28.\n\nE. Tempero, T. Gorschek, and L. Angelis, “Barriers to refactoring,”\nCommunications of the ACM, vol. 60, no. 10, pp. 54-61, 2017.\n\nG. Lacerda, F. Petrillo, M. Pimenta, and Y. G. Gue’he’neuc, “Code\nsmells and refactoring: A tertiary systematic review of challenges and\nobservations,” Journal of Systems and Software, vol. 167, p. 110610,\n2020.\n\nG. H. Pinto and F. Kamei, “What programmers say about refactoring\ntools? an empirical investigation of stack overflow,” in Proceedings of\nthe 2013 ACM workshop on Workshop on refactoring tools, 2013, pp.\n33-36.\n\nM. Cerioli, M. Leotta, and F. Ricca, “What 5 million job advertisements\ntell us about testing: a preliminary empirical investigation,” in Proceed-\nings of the 35th Annual ACM Symposium on Applied Computing, 2020,\npp. 1586-1594.\n\nM. Kassab, P. Laplante, J. Defranco, V. V. G. Neto, and G. Destefanis,\n“Exploring the profiles of software testing jobs in the united states,”\nIEEE Access, vol. 9, pp. 68 905-68 916, 2021.\n\nR. Florea and V. Stray, “A global view on the hard skills and testing tools\nin software testing,” in 2019 ACM/IEEE 14th International Conference\non Global Software Engineering (ICGSE). YEEE, 2019, pp. 143-151.\nD. M. Rafi, K. R. K. Moses, K. Petersen, and M. V. Ma’ntyla’, “Benefits\nand limitations of automated software testing: Systematic literature\nreview and practitioner survey,” in 2012 7th International Workshop on\nAutomation of Software Test (AST). YEEE, 2012, pp. 36-42.\n\nB. Oliinyk and V. Oleksiuk, “Automation in software testing, can\nwe automate anything we want,” in Proceedings of the 2nd Student\nWorkshop on Computer Science & Software Engineering (CS&SE@ SW\n2019), Kryvyi Rih, Ukraine, 2019, pp. 224-234.\n\nS. Easterbrook, J. Singer, M.-A. Storey, and D. Damian, “Selecting em-\npirical methods for software engineering research,” Guide to advanced\nempirical software engineering, pp. 285-311, 2008.\n\nJ. Linaker, S. M. Sulaman, M. Ho’st, and R. M. de Mello, “Guidelines\nfor conducting surveys in software engineering v. 1.1,” Lund University,\nvol. 50, 2015.\n\nS. L. Pfleeger and B. A. Kitchenham, “Principles of survey research: part\n1: turning lemons into lemonade,” ACM SIGSOFT Software Engineering\nNotes, vol. 26, no. 6, pp. 16-18, 2001.\n\nP. Ralph, N. b. Ali, S. Baltes, D. Bianculli, J. Diaz, Y. Dittrich, N. Ernst,\nM. Felderer, R. Feldt, A. Filieri et a/., “Empirical standards for software\nengineering research,” arXiv preprint arXiv:2010.03525, 2020.\n\nS. Baltes and P. Ralph, “Sampling in software engineering research: A\ncritical review and guidelines,” Empirical Software Engineering, vol. 27,\nno. 4, pp. 1-31, 2022.\n\nI. Manotas, C. Bird, R. Zhang, D. Shepherd, C. Jaspan, C. Sadowski,\nL. Pollock, and J. Clause, “An empirical study of practitioners’ per-\nspectives on green software engineering,” in Proceedings of the 38th\ninternational conference on software engineering, 2016, pp. 237-248.\nR. de Souza Santos, B. Stuart-Verner, and C. Magalha™es, “What do\ntransgender software professionals say about a career in the software\nindustry?” JEEE Software, 2023.\n\nF. Zampetti, G. Fucci, A. Serebrenik, and M. Di Penta, “Self-admitted\ntechnical debt practices: a comparison between industry and open-\nsource,” Empirical Software Engineering, vol. 26, pp. 1-32, 2021.\n\nD. George and P. Mallery, “Descriptive statistics,” in JBM SPSS Statistics\n25 Step by Step. Routledge, 2018, pp. 126-134.\n\nR. E. Santos, A. Bener, M. T. Baldassarre, C. V. Magalha™es, J. S.\nCorreia-Neto, and F. Q. da Silva, “Mind the gap: are practitioners and\nresearchers in software testing speaking the same language?” in 20/9\nIEEE/ACM Joint 7th International Workshop on Conducting Empirical\nStudies in Industry (CESI) and 6th International Workshop on Software\nEngineering Research and Industrial Practice (SER&IP). YEEE, 2019,\npp. 10-17.\n\nR. E. Souza, R. E. de Souza Santos, L. F. Capretz, M. A. de Sousa,\nand C. V. de Magalha’es, “Roadblocks to attracting students to software\ntesting careers: Comparisons of replicated studies,” in Quality of Infor-\nmation and Communications Technology: 15th International Conference,\nQUATIC 2022, Talavera de la Reina, Spain, September 12-14, 2022,\nProceedings. Springer, 2022, pp. 127-139.\n\nR. Ramler, S. Biffl, and P. Gru“nbacher, Value-based management of\nsoftware testing. Springer, 2006.\n']}


**File**: S:\OneDrive\@Dev\!GPT\ScriptGPT\library\Refactoring\Source\30 Years of Software Refactoring Research.pdf
- Time Taken: 57.41s
- Data Extracted: {'text': ['IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020 1\n\n30 Years of Software Refactoring Research:\nA Systematic Literature Review\n\nChaima Abid, Vahid Alizadeh,Marouane Kessentini, Thiago do Nascimento Ferreira and Danny Dig\n\nAbstract—Due to the growing complexity of software systems, there has been a dramatic increase and industry demand for tools and\ntechniques on software refactoring in the last ten years, defined traditionally as a set of program transformations intended to improve\nthe system design while preserving the behavior. Refactoring studies are expanded beyond code-level restructuring to be applied at\n\ndifferent levels (architecture, model, requirements, etc.), adopted in many domains beyond the object-oriented paradigm (cloud\ncomputing, mobile, web, etc.), used in industrial settings and considered objectives beyond improving the design to include other\nnon-functional requirements (e.g., improve performance, security, etc.). Thus, challenges to be addressed by refactoring work are,\nnowadays, beyond code transformation to include, but not limited to, scheduling the opportune time to carry refactoring,\nrecommendations of specific refactoring activities, detection of refactoring opportunities, and testing the correctness of applied\nrefactorings. Therefore, the refactoring research efforts are fragmented over several research communities, various domains, and\nobjectives. To structure the field and existing research results, this paper provides a systematic literature review and analyzes the\nresults of 3183 research papers on refactoring covering the last three decades to offer the most scalable and comprehensive literature\nreview of existing refactoring research studies. Based on this survey, we created a taxonomy to classify the existing research, identified\nresearch trends, and highlighted gaps in the literature and avenues for further research.\n\nIndex Terms—Refactoring, systematic literature review, program transformation, software quality.\n\n1. INTRODUCTION\n\nFor decades, code refactoring has been applied in infor-\nmal ways before it was introduced and properly defined in\nacademic work. The first known use of the term Refactoring\nin the published literature was in an article written by\nWilliam Opdyke and Ralph Johnson in September 1990\n[1]. William Griswold’s Ph.D. dissertation [2], published\nin 1991, is also one of the first major academic works\non refactoring functional and procedural programs. The\nauthor defined a set of automatable transformations and\ndescribed their impact on the code structure. One year later,\nWilliam Opdyke also published his Ph.D. dissertation [3]\non the Refactoring of object-oriented programs. In 1999,\nMartin Fowler published the first book about refactoring\nthat has as title Improving the Design of Existing Code [4].\nThis book popularised the practice of code refactoring, set\nits fundamentals, and had a high impact on the world of\nsoftware development. Martin Fowler defined Refactoring\nin his book as a sequence of small changes - called refac-\ntoring operations - made to the internal structure of the\ncode without altering its external behavior. The goal of these\nrefactoring operations is to improve the code readability and\nreusability as well as reduce its complexity and maintenance\ncosts in the long run. Since then, a lot has changed in the\nsoftware development world, but one thing has remained\nthe same: The need for Refactoring.\n\ne = Chaima Abid, Vahid Alizadeh, Marouane Kessentini, and Thiago do\nNascimento Ferreira are with the department of Computer and Informa-\ntion Science, University of Michigan, Dearborn, MI, USA.\n\nDanny Dig is with the Computer Science department, University of\nColorado, Boulder, CO, USA.\nE-mail: marouane@umich.edu\n\nManuscript received on June 2020.\n\nNearly 30 years later, Refactoring has become a crucial\npart of software development practice, especially with the\never-changing landscape of IT and user requirements. It is a\ncore element of agile methodologies, and most professional\nIDEs include refactoring tools. Recent studies show that\nrestructuring software systems may reduce developers’ time\nby over 60% [5]. Others demonstrate how Refactoring can\nhelp detect, fix, and reduce software bugs [6]. Companies\nare becoming more and more aware of the importance of\nRefactoring, and they encourage their developers to contin-\nuously refactor their code to set a clean foundation for future\nupdates.\n\nIt might be difficult for a developer to be justified to\nspend time on improving a piece of code to have the same\nfunctionality. However, it can be seen as an investment\nfor future developments. Specifically, Refactoring is a\ncrucial task on software with longer lifespans with multiple\ndevelopers need to read and understand the codes.\nRefactoring can improve both the quality of software and\nthe productivity of its developers. Increasing the quality of\nthe software is due to decreasing its complexity at design\nand source code level caused by refactoring, which is proved\nby many studies [7], [8]. The long-term effect of Refactoring\nis improving developers’ productivity by increasing two\ncrucial factors, understandability and maintainability of the\ncodes, especially when a new developer joins an existing\nproject. It is shown that Refactoring can help to detect, fix,\nand reduce software bugs and leading to software projects\nwhich are less likely to expose bug in development process\n[6]. Another study claims that there are some specific kinds\nof refactoring methods that are very probable to induce bug\nfixes [9].\n\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n1.1 Problem Description and Motivation\n\nRefactoring is among the fastest-growing software engineer-\ning research areas, if not the fastest. Figure 1 shows the\ndistribution of publications related to refactoring across the\nglobe. Figure 2 reflects the number of publications in the\ntop 10 most active countries in the field of Refactoring. The\nUnited States tops the list of countries with a total of 714\npublications followed by Germany and Canada with a total\nof 317 and 248 publications, respectively. During the past\n4 years, the number of published refactoring studies has\nincreased with an average of 37% in all top 10 countries.\nThis demonstrates a noticeable increase in interest/need in\nRefactoring.\n\nOver 5584 authors from all over the world contributed to\nthe field of Refactoring. We highlight the most active authors\nin Figure 3 and 4, based on both the number of publications\nand citations in the area. Many scholars started research in\nthe refactoring filed prior to 2000. Others are relatively new\nto the field and started their contributions after year 2010.\nAll top 10 authors in the field have a constantly increasing\nnumber of publications over the past 20 years. Marouane\nKessentini heads the list with a total of 43 publications\n(51% of them were published during the past five years)\n‘ollowed by Steve Counsell and Danny Dig with a total of\n39 and 36 publications, respectively. Marouane kessentini\nublished an average of more than 4 articles per year while\nall other authors published an average between 1.5 and\n2.75 publications per year. Figure 5 is a histogram showing\nhow many publications were issued each year starting from\n990. The number of published journal articles, conference\napers, and books has increased dramatically during the last\ndecade, reaching a pick of 265 publications in 2016. During\njust the last four years (2016-2019), over 1026 papers were\nublished in the field, with an average of 256 papers each\nyear.\n\nRecently, several researchers and practitioners have\nadopted the use of refactoring operations at higher degrees\nof abstraction than source code level (e.g., databases, Uni-\nied Modeling Language (UML) models, Object Constraint\nLanguage (OCL) rules, etc.). As a result, they often had\nto redefine the principles and guidelines of refactoring\naccording to the requirements and specifications of their\ndomains. For instance, in User Interface Refactoring, de-\nvelopers make changes to the UI to retain its semantics\nand consistency for all users. These refactorings include,\nbut not limited to, Align entry field, Apply common button\nsize, Apply font, Indicate format, and Increase color contrast.\nIn Database Refactoring, developers improve the database\nschema by applying changes such as Rename column, Split\ntable, Move method, Replace LOB with table, and Introduce\ncolumn constraint. Henceforth, the refactoring operations are\ncalled restructuring operations when applied to artifacts\nother than the ones related to object-oriented programming.\nAlthough the different refactoring communities (e.g., soft-\nware maintenance and evolution, model-driven engineer-\ning, formal methods, search-based software engineering,\netc.) are interdependent in many ways, they remain dis-\nconnected, which may create inconsistencies. For example,\nwhen model-level Refactoring does not match the code-\nlevel practice, it can lead to incoherence and technical issues\n\n2\n\nduring development. The detachment is visible not only\nbetween different refactoring domains but also between\npractitioners and researchers. The distance between them\nprimarily originates from the lack of insights into both\nworlds’ recent findings and needs. For instance, developers\ntend to use the refactoring features provided by IDEs due\nto their accessibility and popularity. Most of the time, they\nare uninformed of the benefits that can be derived from\nadopting state-of-the-art advances in academia. All these\nchallenges call for a need to identify, critically appraise, and\nsummarize the existing work published across the different\ndomains. Existing systematic literature reviews examine\nfindings in very specific refactoring areas such as identifying\nthe impact of refactoring on quality metrics [10] or code\nsmells [11]. To the best of our knowledge, no work collects\nand synthesizes existing research, tools, and recent advances\nmade in the refactoring community. This paper is the most\ncomprehensive synthesis of theories and principles of refac-\ntoring intended to help researchers and practitioners make\nquick advances and avoid reinventing or re-implementing\nresearch infrastructure from scratch, wasting time and re-\nsources. We also build a refactoring infrastructure that will\nconnect researchers with practitioners in industry and pro-\nvide a bridge between different refactoring communities in\norder to advance the field of refactoring research.\n\n1.2 Contributions\n\nThe Refactoring area is growing very rapidly, and many\nadvances, challenges, and trends have lately emerged. The\nprimary purpose of this study is to implement a systematic\nliterature review (SLR) for the field of refactoring as a whole.\nThis SLR follows a defined protocol to increase the study’s\nvalidity and rationality so that the output can be high\nin quality and evidence-based. We used various electronic\ndatabases and a large number of articles to comprise all\nthe possible candidate studies and cover more works than\nexisting SLRs.\n\nThis SLR contributes to the existing literature in the\nfollowing ways:\n\ne We identify a set of 3183 studies related to refactor-\ning published until May 2020, fulfilling the quality\nassessment criteria. These studies can be used by\nthe research and industry communities as a reliable\nbasis and help them conduct further research on\nRefactoring.\n\nWe present a comprehensive qualitative and quanti-\ntative synthesis reflecting the state-of-the-art in refac-\ntoring with data extracted from those 3183 high-rigor\nstudies. Our synthesis covers the following themes:\nartifacts, refactoring tools, different approaches, and\nperformance evaluation in refactoring research.\n\nWe provide guidelines and recommendations based\non our findings to support further research in the\narea.\n\nWe implement a platform that includes the following\ncomponents: (1) A searchable repository of refactor-\ning publications based on our proposed taxonomy;\n(2) A searchable repository of authors who con-\ntributed to the refactoring community; (3) Analysis\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n?\nns © o\nEi)\n10\n7 GB a\n199 «8 4\n756% ‘\nm | 480\nSe ee 99 306\n4 G<V\\ 28\n7 B wo ae\n9 @ %, 237\n4\ni)\n4 1 Biz\n2 30\nt 52 46\n7\n5\n6\n— I\n\n1000 1500 20.\n\nFig. 1. Distribution of refactoring publications around the world.\n\nUnited states\n\nGermany\n\ncanada\n\nbrazil\n\nchina\n\nUnited Kingdom\n\naly\n\nindia\n\nJapan\n\nFrance\n\n8\n2\n\nEd\n\n2%\n\n24%\n\n41%\n\n22%\n\nmE Before 2016\nwm Between 2016 - 2020,\n\n100\n\n300 00 300 700 ‘800\n‘Total number of documents, with percentage of documents\n\nPublished in the last years 2016 - 2020\n\nFig. 2. Number of publications in the top 10 most active countries in the refactoring field\n\nand visualization of the refactoring trends and tech-\nniques based on the collected papers. The proposed\ninfrastructure will allow researchers and practition-\ners to easily report refactoring publications and up-\nload information about active authors in the field of\nRefactoring. It will also bridge the different commu-\nnities to advance the field of refactoring research and\nprovide opportunities to educate the next refactoring\n\ngeneration.\n\n1.3. Related Surveys\n\nMens et al. [12] provided an overview of existing research\nin the field of software refactoring. They compared and\ndiscussed different approaches based on different criteria\nsuch as refactoring activities, techniques and formalisms,\ntypes of software artifacts that are being refactored, and\nthe effect of refactoring on the software process. Elish et al.\n[13] proposed a classification of refactoring methods based\non their measurable effect on software quality attributes.\nThe investigated software quality attributes are adaptability,\ncompleteness, maintainability, understandability, reusabil-\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nKessentini M.\nDig D.\nCounsell S.\nInoue K.\nBavota G.\n\nLiu H.\n\nRoy C.K.\nMurphy-Hill E.\nOliveto R.\n\nDe Lucia A.\n\n51%\n23%\n19%\n38%\n41%\n\n41%\n\n37%\n4%\n31%\n\nGam Before 2016\n\n9\n32% Gam ~Between 2016 - 2020\n\nT\n\n10\n\no\n\nT\n20\n\nT\n\n30\n\nT\n\n40\n\nTotal number of documents, with percentage of documents\npublished in the last years 2016 - 2020\n\nFig. 3. Top 10 Authors with the highest number of publications and citations in the field of refactoring\n\nity, and testability. Du Bois et al. [14] provided an overview\nof the field of software restructuring and Refactoring. They\nsummarized Refactoring’s current applications and tool\nsupport and discussed the techniques used to implement\nrefactorings, refactoring scalability, dependencies between\nrefactorings, and application of refactorings at higher levels\nof abstraction. Mens et al. [15] identified emerging trends in\nrefactoring research (e.g., refactoring activities, techniques,\ntools, processes, etc.), and enumerates a list of open ques-\ntions, from a practical and theoretical point of views. Misb-\nhauddin et al. [16] provide a systematic overview of existing\nresearch in the field of model Refactoring. Al Dallal et al.\n[17] presented a systematic literature review of existing\nstudies, published through the end of 2013, identifying op-\nportunities for code refactoring activities. In another of their\nwork [10], they presented a systematic literature review that\nsummarizes the impact of refactoring on several internal\nand external quality attributes. Singh et al. [11] published a\nsystematic literature review of refactoring concerning code\nsmells. However, the review of Refactoring is done in a gen-\neral manner, and the identification of code smells and anti-\npatterns is performed in-depth. Abebe et al. [18] conducted\na study to reveal the trends, opportunities, and challenges\nof software refactor researches using a systematic literature\nreview. Baqais et al. [19] performed a systematic literature\nreview of papers that suggest, propose, or implement an\nautomated refactoring process.\n\nThe different studies mentioned above are mainly about\nidentifying the studies related to very specific or specialized\ntopics. In this paper, we are trying to be as comprehensive\nas possible by collecting, categorizing, and summarizing all\nthe papers related to refactoring in general that conform to\nour quality standards.\n\n1.4 Organization\n\nThe rest of the paper is organized as follows: First, Section 2\noutlines the research method and the underlying protocol\nfor the systematic literature review. Section 3 describes\nthe proposed refactoring infrastructure. The results of this\nsystematic review are reported in Sections 4. Finally, Section\n5 presents the conclusions.\n\n2 RESEARCH METHODOLOGY\n\nOur literature review follows the guidelines established by\nKitchenham and Charters [20], which decompose a sys-\ntematic literature review in software engineering into three\nstages: planning, conducting, and reporting the review. We\nhave also taken inspiration from recent systematic litera-\nture reviews in the fields of empirical software engineering\n[10] and search-based software engineering [21]. All the\nsteps of our research are well documented, and all the\nrelated data are available online for further validation and\nexploration []. This section details the performed research\nsteps and the protocol of the literature review. First, section\n2.1 describes the research questions underlying our survey.\nSecond, section 2.2 details the literature search step. Next,\nsection 2.3 highlights the inclusion and exclusion criteria.\nThe data preprocessing step and our proposed taxonomy\nare described in sections 2.4 and 2.5, respectively. The qual-\nity assessment criteria are defined in section 2.6. Finally,\nSection 2.7 discusses threats to the validity of our study.\n\n2.1 Research Questions\n\nThe following research questions have been derived based\non the objectives described in the introduction, which form\nthe basis for the literature review:\n', "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n45\n-@ Kessentini mM. res\n: 5\nao | “ae Dig D. go @\n4H Counsell s. S44\n25. ] ~@ Inoue k. 3\nz hk Bavota G. Ss\n5 Liu H. q\n8 304 '\n8 Roy C.K. 34\noS —@ Murphy-till E. 8\ng 257 3 Oliveto R. oC e ~\nS\nE > De Lucia A. o A Vv\n2 204 524 Ce\nvo a\n2 2 O\nS 2\n® 454\n215 5 oO\n—E i\n3 3\n3 104 814\nvo\ne\n54 3\n=\n<\nte) r r r T T T r O+ T r T r\n1992 1996 2000 2004 2008 2012 2016 2020 40% 60% 80% 100% 120%\n\nPublication year\n\nFig. 4. Evolution of the Top 10 Authors during the past 10 years\n\nFig. 5. Trend of publications in the field of refactoring during the last\nthree decades.\n\ne RQ1: What is the refactoring life-cycle?\n\ne RQ2: What are the types of artifacts that are being\nrefactored at each step of the refactoring life-cycle?\n\ne RQ3: Why do software practitioners and researchers\nperform refactoring?\n\ne« RQ4: What are the different approaches used by\nsoftware practitioners and researchers to perform\nrefactoring?\n\ne RQ5: What types of datasets are used by software\npractitioners and researchers to validate the refactor-\ning?\n\n2.2 Literature Search Strategy\n\nAll the papers have been queried from a wide range of scien-\ntific literature sources to make our search as comprehensive\nas possible:\n\nPercentage of documents\npublished in the last years 2011 - 2020\n\ne Digital libraries: ACM Library, IEEE Xplore, Science-\nDirect, SpringerLink.\n\ne Citation databases: Web of Science (formerly ISI Web\nof Knowledge), Scopus.\n\ne Citation search engines: DBLP, Google Scholar.\n\nWe first defined a list of terms covering the variety of both\napplication domains and refactoring techniques. For that,\nwe checked the title, keywords, and abstract of the relevant\npapers that were already known to us. Synonyms and\nkeywords were derived from this list. These keywords were\ncombined using logical operators ANDs and ORs to create\nsearch terms. Before starting collecting the primary studies\n(PS), we tested the search terms’ effectiveness on all the\ndata sources. Then, we refined the queries to avoid getting\nirrelevant papers. The string adjustments were agreed on\nby all authors. The final list of search strings are shown\nin Table 1. These search strings were modified to suit the\nspecific requirements of different electronic databases. We\nconducted our search on May 31st, 2020, and identified\nstudies published up until that date. The search was done\nfirst by the corresponding author and then verified by the\nrest of the authors. In our systematic review, we followed a\nmulti-stage model to minimize the probability of missing\nrelevant publications as much as possible. The different\nstages are shown in figure 6 along with the total returned\npublications at each stage. The first stage consists of execut-\ning the search queries on the databases mentioned above;\na total of 6158 references were found. Then, we removed\nthe duplicates, which reduced the list of candidate papers\nto 3882. Then, we performed a manual examination of\ntitles and abstracts to discard irrelevant publications based\non the inclusion and exclusion criteria. We also looked at\nthe body of the paper whenever necessary. This decreased\nthe list of candidate papers to 3161 publications. Next, we\n\n", "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nSelect Scientific Literature Sources\n\nDefine Search Strings\n\nStage 1\n\nExecuting the Search Queries\n(6158 publications)\n\nRemove Duplicates\n(3882 publications)\n\nStage 2 |\n\n-721\n\nManual Examination of Titles and Abstracts\n\nStage 3 (3161 publications)\n\n+3\n\nConsult Web Profiles of Relevant Authors\n\nStage 4 (3164 publications)\n\n+14\n\nCheck Cross-references of Relevant Publications\n\nStage 5 (3178 publications)\n\n+5\n\nContact Authors =\n\nStage 6 (3183 publications)\n\nFig. 6. SLR steps\n\nused the resulting set as input for the snowballing process,\nrecommended by Wohlin [22], to identify additional studies.\nWe consulted web profiles of relevant authors and their\nnetworks. We also checked cross-references until no further\npapers were detected. As a result, 17 new references were\nadded. After that, we contacted the corresponding authors\nof the identified publications to inquire about any missing\nrelevant studies. This led to adding 5 studies.\n\n2.3\n\nTo filter out the irrelevant articles among those selected in\nStage 2 and determine the Primary studies, we considered\nthe following inclusion and exclusion criteria.\n\nInclusion and Exclusion Criteria\n\n2.3.1\n\nAll of the following criteria must be satisfied in the selected\nprimary studies:\n\nInclusion criteria\n\n1) The article must have been published in a peer\nreviewed journal or conference proceeding between\nthe years 1990 and 2020. The main reason for im-\nposing a constraint over the start year is because\nthe first known use of the term “refactoring” in\nthe published literature was in a September, 1990\narticle by William Opdyke and Ralph Johnson [1].\nWe included papers up till May 31st 2020.\n\n2) The article must be related to computer science and\nengineering and propose techniques, methods and\ntools for refactoring.\n\n3) The paper must be written in English.\n\n-2276\n\n6\n\n4) Incase a conference paper has a journal extension,\nwe would include both the conference and journal\npublications.\n\n5) The paper must pass the quality assessment criteria\nthat are elaborated in Section 2.6.\n\n2.3.2 Exclusion criteria\n\nPapers satisfying any of the exclusion criteria were dis-\ncarded, as follows:\n\n1) Studies that are not related to the computer science\nfield.\n\n2) Studies that investigated the impact of general\nmaintenance on code quality. In this case, the main-\ntenance tasks were potentially performed due to\nseveral reasons and not limited to refactoring, and\ntherefore, we cannot judge whether the impact was\ndue to refactoring or to other maintenance tasks\nsuch as corrective or adaptive maintenance.\n\n3) Grey Literature\n\n2.4 Data Preprocessing\n\nA pre-processing technique was applied to improve reliabil-\nity and precision, as detailed in the following sub sections.\n\n2.4.1 Simplifying Author's name\n\nIn general, scientific and bibliographic databases such as\nWeb of Science (WoS) and Scopus have the following incon-\nsistencies in authors names:\n\ne Most journals abbreviate the author’s first name to\nan initial and a dot.\n\ne Most journals use the author name’s special accents.\n\ne WoS uses a comma between the author’s last name\nand first name initial, but Scopus does not.\n\nThese name-related inconsistencies mean that sciento-\nmetrics scripts cannot find all of the similar author’s names.\nFor that reason, ScientoPy script applies the following steps\nto simplify author’s name fields:\n\ne Remove dots and coma from author’s name.\ne Remove special accents from author’s name\n\n2.4.2 Fixing inconsistent country names\n\nSome authors use different naming to refer to the same\ncountry (such as USA and United States). For that reason,\nsome country names were replaced based on Table 3.\n\n2.5 Study Classification\n\nAccording to the research questions listed in Section 2.1, we\nclassified the PSs into five dimensions: (1) refactoring life-\ncycle (related to RQ1), (2) artifacts affected by refactoring\n(related to RQ2), (3) refactoring objectives (related to RQ3),\n(4) refactoring techniques (related to RQ4) and (5) refactor-\ning evaluation (related to RQ5). The determination of the\nattributes of each dimension was performed incrementally.\nThat is, for each dimension, we started with an empty\nset of attributes. The authors of this study screened the\nfull texts of the PSs one by one, analyzed each reported\nstudy based on the considered dimension, and determined\n", 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nTABLE 1\nfinal list of search strings\n\nsearch strings\n\n(software OR system OR code OR service OR diagram OR database OR architecture OR Model OR\n\nGUI OR user interface OR UI OR design OR artifac!\n\nt OR developer OR computer OR programming\n\nOR object-oriented OR implement OR mobile app OR cloud OR document ) AND (refactor OR\n\nrefactoring)\n\nTABLE 2\nPS quality assessment questions [17]\n\nQuestion\n\nAre the applied identification techniques for refactoring opportunities clearly described? |\nDesi Are the refactoring activities considered clearly stated and defined?\n\nesign Was the sample size justified?\nP. J\n\nAre the evaluation measures fully defined?\nConduct Are the data collection methods adequately described?\n\nAre the results of applying the identification techniques evaluated?\n\nAre the data sets adequately described? (size, programming languages, source)\n\nAre the study participants or observational units adequately described?\nAnalysis Are the statistical methods described?\n\nAre the statistical methods justified?\n\nIs the purpose of the analysis clear?\n\nAre the scoring systems (performance evaluation) described?\n\nAre all study questions answered?\n\nAre negative findings presented?\nConclusion | Are the results compared with previous reports?\n\nDo the results add to the literature?\n\nAre validity threats discussed?\n\nTABLE 3\nList of countries and their replacements\n\nCountry Replacement\nRepublic of China China\n\nUSA United States\nEngland, Scotland and Wales England\n\nU Arab Emirates\nRussia\n\nViet Nam\n\nTrinid & Tobago\n\nUnited Arab Emirates\nRussian Federation\nVietnam\n\nTrinidad and Tobago\n\nthe attributes of that dimension as considered by each PS.\nTable 4 outlines the keywords extracted for each category.\nIt should be pointed out that, most of the time, we remove\nall of the affixes (i.e., suffixes, prefixes, etc.) attached to a\nword in order to keep its lexical base, also known as root\nor stem or its dictionary form or lemma. For instance, the\nword document allows us to detect the words documentation\nand documenting. Also, we did not include bi-grams and\ntri-grams that can be detected using one uni-gram. For\nexample, Class Diagram, Object Diagram, Sequence Diagram,\nand Use Case Diagram can all be detected using the word\nDiagram alone.\n\nThe screening of the PSs resulted in determining six\nstages for the refactoring life-cycle (e.g., detection, pri-\noritization, recommendation, testing, documentation, and\nprediction). We also classified the papers according to the\nlevel of automation of the proposed technique (e.g., auto-\nmatic, manual, semi-automatic). The results are described\nin section 4.1. For the second dimension, we identified five\nartifacts on which the impact of refactoring is studied by\nat least one of the PSs. These artifacts are code, architec-\nture, model, GUI, and database. The classification of PSs\nbased on these artifacts is discussed in detail in Section\n4.2. We subdivided the third dimension into five categories\n\n(e.g., External quality, internal quality, performance, migra-\ntion, and security) to reflect the refactoring objective and\nsix categories (e.g., Object-oriented design, Aspect-oriented\ndesign, Model-driven engineering, Documentation, Mobile\ndevelopment, and Cloud computing) to describe the refac-\ntoring paradigms. The classification of PSs based on these\ncategories is discussed in detail in Section 4.3. We divided\nthe fourth dimension into four categories (e.g., data mining,\nsearch-based algorithms, formal methods, and fuzzy logic)\nto reveal the refactoring techniques adopted in the studies\nand into twelve categories (e.g., Java, C, C#, Python, Cobol,\nPHP, Scala, Smalltalk, Ruby, Javascript, MATLAB, and CSS)\nto show the most common programming languages used in\nour PSs. The details of this categorization are reported in\nsection 4.4. Finally, for the fifth dimension, we divide the\nPSs into two categories: open-source and industrial. The\nopen-source category includes studies that validate their\napproaches using open source systems. In contrast, the\nindustrial category consists of the studies that validate their\nwork on systems of their industrial collaborators. These\nfindings are outlined in Section 4.5.\n\n2.6 Study Quality Assessment\n\nTo ensure a level of quality of papers, we only included\nvenues that are known for publishing high-quality software\nengineering research in general with an h-index of at least\n10, as has been done by [23] . Each of the papers that\nwere published before 2019 has to be cited at least once.\nThe quality of each primary study was assessed based on\na quality checklist defined by Kitchenham and Charters\n[20]. This step aims to extract the primary studies with\ninformation suitable for analysis and answering the defined\nresearch questions. The quality checklist, (described in table\n2) were defined by Galster et al. [23]. They are developed\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nTABLE 4\nList of keywords used to detect the different categories\n\nCategory\n\nKeywords\n\nRefactoring life-cycle (RQT)\n\nDetection\nPrioritization\nRecommendation\nTesting\nDocumentation\nPrediction\n\ndetect, opportunity, smell, antipattern, design defect\n\nschedul, sequence, priorit\n\nrecommend, correction, correcting, fixing, suggest\n\ntest, regression testing, test case, unit test\n\ndocument\n\npredict, future release, next release, development history, refactoring history\n\nLevel of automation (RQT)\n\nManual manual\n\nSemi-automatic semi-automat, semi-manual\n\nAutomatic automat\n\nArtifact (RQ2)\n\nCode code, java, object orient, smell, antipattern, anti-pattern, object-orient\nModel design, model, UML, diagram, Unified Modeling Language\nArchitecture architecture, hotspot, hierarchy\n\nGUI gui, user interface, UI\n\nDatabase relational, schema, database, Structured Query Language, SQL\n\nParadigm (RQ3)\n\nObject-oriented design\n\nAspect-oriented design\nModel-driven engineering\nDocumentation\n\nMobile development\nCould computing\n\nobject orient, object-orient, 00, java, c, ++, python, C sharp, c#, css, Python, R, PHP, JavaScript, Ruby,\nPerl, Object Pascal, Objective-C, Dart, Swift, Scala, Kotlin, Common Lisp, MATLAB, Smalltalk\n\naspect\n\nmodel transform, uml, reverse engineering, diagram, Unified Modeling Language\n\ndocument\n\nandroid, mobile, IOS, phone, smartphone, cellphones\n\nweb service, wsdl, restful, cloud, Apache Hadoop, Docker, Middleware, Software-as-a-Service,\nSaaS, XaaS, Anything-as-a-Service, Platform-as-a-Service, PaaS, Infrastructure-as-a-Service, laaS, AWS,\nAmazon EC2, Amazon Simple Storage Service, S3\n\nRefactoring Objectives (RQ\n\nInternal Quality maintainability, cyclomatic, depth of inheritance, coupling, quality, Flexibility, Portability, Re-usability,\nReadability, Testability, Understandability\nPerformance performance, parallel, Response Time, Error Rates, Request Rate, availability\n\nExternal quality\n\nanalysability, changeability, time behaviour, resource, Correctness, Usability, Efficiency, Reliability,\nIntegrity, Adaptability, Accuracy, Robustness\n\nMigration migrat\nSecurity secure, safety, Attack surface, virus, hack, vulnerability, vulnerable, spam\nProgramming languages (RQ4)\n\nJava java\n\nCc c, C++\n\nC# c sharp, c#\nPython python\ncss css\n\nPHP php\nCobol cobol\nScala scala\nJavascript javascript\nRuby ruby\nSmalltalk smalltalk\nMATLAB matlab\n\nAdopted methods (RQ4)\n\nSearch-based algorithms\n\nData mining\n\nFormal methods\nFuzzy logic\n\nsearch, search-base, sbse, genetic, fitness, simulated annealing, tabu search, search space, Hill climbing,\nMulti-objective evolutionary algorithms, multi objective optimization, multi-objective programming,\nvector optimization, multi-criteria optimization, multi-attribute optimization, Pareto optimization,\nEvolutionary Multi-objective Optimization, EMO, Single-Objective Optimization, Many-Objective\nOptimization, multi objective\n\nartificial intelligence, ai , machine learning, naive bayes, decision tree, SVM, support vector machine,\nCluster, Classification, classify, Association, Neural networks, deep learning, random forest, regression,\nreinforcement learning, learning\n\nmodel check, formal method, B-Method, RAISE, Z notation, SPARK Ada\n\nfuzzy\n\nEvaluation method (RQ5)\n\nOpen source\nIndustrial\n\nopen source, Open-source\nproprietary, industrial, industry, collaborator, collaboration\n\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nby considering bias and validity problems that can occur\nat different stages, including the study design, conduct,\nanalysis, and conclusion. Each question is answered by a\n“Yes”, “Partially”, or “No”, which correspond to a score of\n1, 0.5, or 0, respectively. If a question does not apply to a\nstudy, we do not evaluate the study for that question. The\nquality assessment checklist was independently applied to\nall 3882 studies by two of the authors. All disagreements\non the quality assessment results were discussed, and a\nconsensus was reached eventually. Few cases where agree-\nment could not be reached were sent to the third author for\nfurther investigation. 154 studies did not meet the quality\nassessment criteria.\n\n2.7 Threats to Validity\n\nSeveral limitations may affect the generalizability and the\ninterpretations of our results. The first is the possibility of\npaper selection bias. To ensure that the studies were se-\nlected in an unbiased manner, we followed the well-defined\nresearch protocol and guidelines reported by Kitchenham\nand Charters [20] instead of proposing nonstandard quality\nfactors. Also, the final decision on the articles with selection\ndisagreements was performed based on consensus meet-\nings. The Primary studies were assessed by one researcher\nand checked by the other, a technique applied in similar\nstudies [21]. The second threat consists of missing a rele-\nvant study. To overcome this threat, we employed several\nstrategies that we mentioned in Section 2.2. Few related\nstudies were detected after performing the automatic search,\nwhich indicates that the constructed search strings and the\nmentioned utilized libraries were comprehensive enough to\nidentify most of the relevant articles. Another critical issue\nis whether our taxonomy is complete and robust sufficient\nto analyze and classify the primary studies. To overcome\nthis problem, we used an iterative content analysis method\nby going through the papers one by one and continuously\nexpand the taxonomy for every new encountered concept.\nFurthermore, to gather sufficient keywords to detect the\ndifferent categories, we followed the same iterative process,\nand we added synonyms based on the authors’ expertise\nin the field of refactoring. Another threat is related to the\ntagging of the papers according to our taxonomy. To miti-\ngate this problem, we asked 27 graduate students to check\nthe correctness of the classification results by reading the\nabstract, the title, and keywords. They also check the body\nof the paper whenever necessary.\n\n3 REFACTORING INFRASTRUCTURE\n\nWe implemented a large scale platform [24] that collects,\nmanages, and analyzes refactoring related papers to help\nresearchers and practitioners share, report, and discover\nthe latest advancements in software refactoring research. It\nincludes the following components:\n\n1) Asearchable repository of refactoring publications\nbased on our proposed taxonomy. Figure 9 shows a\nscreenshot of the publications’ tab of the refactoring\nrepository website. The papers can be searched by\nauthor, title, or year of publication. Each paper has\n\ntags that describe its content based on our taxonomy\n\n9\n\ndescribed in section 2.5. The papers can also be\nfiltered using those tags and sorted alphabetically\nor chronologically according to the title and year\nof publication, respectively. The user can export the\npublications’ dataset to many formats, including\npdf, excel, and CSV. He can also easily report a new\npublication by entering its link.\n\nA searchable repository of authors who con-\ntributed to the refactoring community. Figure\n8 shows a screenshot of the authors’ tab of the\nrefactoring repository website. The authors can be\nsearched and sorted alphabetically by name, affil-\niation, or country. They can also be sorted based\non the total number of refactoring publications.\nThe user can also consult the Google Scholar and\nScopus profiles of the authors if available. Finally,\nthe user can easily report a new author by entering\ntheir information and their profile. Furthermore, we\ndefined the refactoring h-index, which shows how\nmany papers about refactoring published by the\nauthor have been cited proportionately. A refac-\ntoring h-index of X means that the author has X\npapers about refactoring that have been cited at\nleast X times. Authors can also be sorted according\nto the refactoring h-index and the total number of\ncitations (see figure 11). Besides, we created a co-\nauthor network and corresponding visualizations\n(see figure 12) to get a snapshot view of the breadth\nand depth of an individual’s collaborations in the\nfield of refactoring research. Finally, we generated a\nhistogram (see figure 7) that shows the number of\npublications issued by the top institutions active in\nthe refactoring research by considering the authors’\naffiliations.\n\nAnalysis and visualization of the refactoring\ntrends and techniques based on the collected pa-\npers. Figure 10 shows a screenshot of the refactoring\nrepository dashboard. It contains histograms and\npie charts that show the distribution and percent-\nages of the categories defined in our taxonomy. It\nalso includes maps that reflect the spread of refac-\ntoring activity across the world.\n\n2)\n\n3)\n\nThe proposed infrastructure will enable researchers to\nperform a fair comparison between their new refactoring\napproaches and state-of-the-art tools; enable researchers to\nuse refactoring data of large software systems; facilitate in-\nteractions between researchers from currently disconnected\ndomains/communities of refactoring (model-driven engi-\nneering, service computing, parallelism and performance\noptimization, software quality, testing, etc.); enable practi-\ntioners and researchers to quickly identify relevant existing\nresearch papers and tools for their problems based on the\nproposed taxonomy and classification; create benchmarks\nagainst which various refactoring approaches can be evalu-\nated; enable effective interactions between practitioners and\nrefactoring researchers to identify relevant problems faced\nby the software industry.\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nTop Institutions\n\nFig. 7. Top institutions active in the refactoring field\n\nAuthors of Refactoring Publications\n\nFig. 8. A screenshot of the authors tab of the refactoring repository\nWebsite\n\n4 RESULTS\n\nIn this section, we aim to answer the research questions. To\nprovide an overview of the current state of the art in refac-\ntoring and guide the reader to a specific set of approaches,\ntools, and recent advances that are of interest, we classified\nthe 3183 reviewed papers based on the taxonomy described\nin Section 2.5. Table 5 contains representative references for\nthe categories created for each RQ. We only provided 10\nreferences per category because we cannot possibly report\nin this paper the categorization of all the studies since\nwe are dealing with a total of 3183 papers. The results\nof the classification of all the papers are provided in our\nwebsite [24]. For some taxonomy categories, papers may\nhave multiple values and thus be listed several times. As\na result, percentages in the tables may sum up to more than\n100 percent. Also, not all the papers were classified in all\ndimensions. Consequently, percentages in one dimension\nmay not sum up to 100 percent. The rest of this section\npresents the observations and insights that can be derived\nfrom the visualization of the categories.\n\n4.1 Refactoring life-cycle\n\nGoing through the primary studies, we have been able to\nestablish a refactoring life-cycle that is composed of six\n\nRefactoring Publications\n\n@ siPopers Target » Hocyeio ~ — @ Languages ~ @ Objectives » vohution» — @) Folds ~\n\ncord 18 nove\n\nFig. 9. A screenshot of the publications tab of the refactoring repository\nWebsite\n\nPublications per Voor esearch Categorias\n\nwill\n\nPublications per Country\n\n‘Authors per Country\n\nnm\n\nFig. 10. A screenshot of the Dashboard of the refactoring repository\nwebsite\n\nstages:\n\ne Refactoring detection: Identifying refactoring op-\nportunities is an important stage that precedes the\nactual refactoring process. It can be done by man-\nually inspecting and analyzing an artifact of a sys-\ntem to identify refactoring opportunities. However,\nthis technique is time-consuming and costly. Re-\nsearchers in this area typically propose fully or semi-\nautomated techniques to identify refactoring oppor-\ntunities. These techniques may be applicable to dif-\nferent artifacts and should be evaluated empirically.\nRefactoring prioritization: The number of refactor-\ning opportunities usually exceeds the amount of\nproblems that the developer can deal with, par-\nticularly when the effort available for performing\nrefactorings is limited. Moreover, not all refactoring\nopportunities are equally relevant to the goals of the\nsystem or its health. In this stage, the refactorings op-\nerations are prioritized using different criteria (e.g.,\nmaximizing the refactoring of classes with a large\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nTABLE 5\nRepresentative references for all categories\n\n1\n\nCategory Percentage | Papers\n\nRefactoring life-cycle (RQ1)\n\nDetection 28.65% S1], [82], [S3], [$4], [S5], [S6], [S7], [S8], [S9], [S10\n\nPrioritization 9.43% $11], [S12], [S13], [S14], [S15], [S16], [S17], [S18], [S19], [S20]\nRecommendation 16.18% $3], [S11], [S12], [S21], [S22], [S23], [S24], [S25], [S26], [S27]\nTesting 18.44% $4], [S6], [S7], [S8], [S13], [S28], [S29], [S30], [S31], [S32]\nDocumention 5.22% $33], [S34], [S35], [S36], [S37], [S38], [S39], [S40], [S41], [S42], [S43]\n\nPrediction 4.818% $44], [$45], [$46], [$47], [$48], [S49], [S50], [$51], [$52], [$53]\n\nLevel of automation (RQ1)\n\nAutomatic 30.95% $54], [S55], [S56], [S57], [S58], [S59], [S60], [S61], [S62], [S63]\nSemi-automatic 1.95% $64], [S65], [S66], [S67], [S68], [S69], [S70], [S71], [S72], [S73], [S74], [S75]\nManual 8.67% $69], [S76], [S77], [$78], [S79], [S80], [S81], [$82], [$83], [S84]\n\nArtifact (RQ2)\n\nCode 72.89% S1], [S2], [$3], [S11], [S65], [S85], [S86], [S87], [S88], [S89]\n\nModel 59.25% $1], [S3], [S28], [S29], [S65], [S87], [S89], [S90], [$91], [S92]\n\nArchitecture 17.25% $28], [S91], [S93], [S94], [S95], [S96], [S97], [S98], [S99], [S100]\n\nGUI 2.58% S6], [S8], [S28], [S87], [$89], [$90], [S101], [$102], [$103], [$104]\nDatabase 4.12% $27], [$36], [S65], [S100], [$105], [$106], [$107], [S108], [S109], [S110]\nParadigm (RQ3)\n\nObject-oriented design 34.09% S1], [S8], [S30], [S85], [S87], [S88], [S101], [S111], [S112], [S113]\nAspect-oriented 10.87% $88], [S96], [$101], [$102], [$103], [S114], [S115], [S116], [$117], [$118]\nModel-driven engineering | 7.35% $3], [S15], [S32], [S58], [S65], [S119], [S120], [S121], [S122], [S123]\nMobile apps development | 3.55% $23], [S87], [S87], [S95], [S99], [S112], [S124], [S125], [S126], [S127]\nCould computing 4.15% $128], [S129], [S130], [S131], [S132], [S133], [S134], [S135], [S136], [S137]\nRefactoring Objective (RQ3)\n\nInternal Quality 41.63% $3], [S12], [S21], [S29], [S30], [S89], [S90], [S94], [S138], [S139]\nPerformance 15.93% S10], [S12], [S28], [$86], [$88], [S91], [S92], [$96], [$115], [$119]\nExternal quality 22.68% $87], [S91], [S92], [$95], [S102], [$140], [$141], [$142], [$143], [$144]\nMigration 3.61% S95], [$100], [S113], [$145], [S146], [$147], [$148], [$149], [$150], [$151]\n\nSecurity 3.11% $113], [S152], [S153], [S154], [S155], [S156], [S157], [S158], [S159], [S160\nProgramming language (RQ4)\n\nJava 17.15% ST], [S8], [S10], [S30], [S85], [S87], [S88], [S112], [S113], [S140]\n\nC 4.65% $59], [S96], [$104], [$105], [$111], [$146], [S161], [$162], [$163], [$164]\non 0.66% $61], [$165], [S166], [S167], [S168], [$169], [S170], [$171], [$172], [$173]\nPython 0.53% $174], [S175], [S176], [S177], [S178], [S179], [S180], [S181], [S182], [S183\ncss 0.5% $147], [S184], [S185], [S186], [S187], [S188], [S189], [S190], [S191], [S192\nPHP 0.35% $169], [S193], [S194], [S195], [S196], [S197], [S198], [S199], [S200], [S201\nCobol 0.31% 12], [S202], [S203], [$205], [$206], [$207], [$208], [$209]\n\nMATLAB 0.28% $210], [S211], [S212], [S213], [S214], [S215], [S216], [S217\n\nSmalltalk 0.79% 25], [$219], [S220], [S221], [S222], [S223], [$224], [$225], [$226], [$227]\nRuby 0.22% $169], [S181], [S228], [S229], [S230], [S231\n\nJavascript 0.72% $112], [S232], [S233], [S234], [S235], [S236], [S237], [S238], [S239], [S240], [S241]\nScala 4.02% $33], [S55], [S86], [S126], [S242], [S243], [S244], [S245], [S246], [S247]\n\nAdopted Method (RQ4)\n\nSearch-based algorithms 25.76% $12], [S248], [S249],\n\n[5250], [S251], [$252], [S253], [S254], [S255], [5256]\nData mining 15.49% S2], [S82], [S107], [S185], [S257], [$258], [S259], [S260], [S261], [S262]\n\nFormal methods 2.92% $42], [S199], [S263], [S264], [S265], [S266], [$267], [$268], [$269]\nFuzzy logic 0.28% $257], [S270], [S271], [S272], [S273], [S273], [S274\n\nEvaluation method (RQ5)\n\nOpen source 16.31% S1], [S7], [S12], [S30], [S32], [S88], [S112], [S139], [S248], [S275]\nIndustrial 10.4% $9], [S12], [$16], [S115], [$120], [$147], [S276], [S277], [S278], [S279]\n\nTop Authors\n\nFig. 11. A screenshot of the refactoring repository dashboard that shows\nthe authors, their h-index and total number of publications and citations\n\nnumber of anti-patterns or with the previous history\n\nof bugs, etc.) according to the needs of developers.\nRefactoring recommendation: Several refactoring\nrecommendation tools have been proposed that dy-\nnamically adapt and suggest refactorings to develop-\ners. The output is sequences of refactorings that de-\nvelopers can apply to improve the quality of systems\nby fixing, for example, code smells or optimizing\nsecurity metrics.\n\nRefactoring testing: After choosing the refactorings\nto be applied, tests need to be done to ensure the cor-\nrectness of artifacts transformations and avoid future\nbugs. This is done by checking the satisfaction of the\npre-and post-conditions of the refactoring operations\nand the preservation of the system behavior.\nRefactoring documentation: After applying and test-\ning the refactorings, we need to document the refac-\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nAuthors Network Graph\n\nFig. 12. A screenshot of the authors network graph from the refactoring\nrepository website\n\ntorings, their locations, why they have been applied,\nand the quality improvements.\n\n¢ Prediction: It is interesting for developers to know\nwhich locations are likely to demand refactoring in\nfuture releases of their software products. This will\nhelp them focus on the relevant artifacts that will\nundergo changes in the future, prepare them for fur-\nther improvements and extensions of functionality,\nand optimize the management of limited resources\nand time. Predicting locations of future refactoring\ncan be done using the development history.\n\nFigure 13 illustrates the percentage of the papers related\nto each stage of the refactoring life-cycle. 33.08% of the\npapers deal with testing. Researchers have invested heavily\nin testing to ensure the reliability of refactoring because\nchanging the structure of code can easily introduce bugs\nin the program and lead to challenging debugging sessions.\nA plenty of effort is made towards the automation of the\ntesting process to facilitate the adoption of refactoring [S54],\n[S55], [S56]. Detecting refactoring opportunities is also a\ntopic of interest to researchers. Several approaches have\nbeen proposed to detect refactoring opportunities including\nbut not limited to techniques that depend on quality metrics\n(e.g., cohesion, coupling, lines of code, etc.), code smells\n(e.g., feature envy, Blob class, etc.), Clustering (similarities\nbetween one method and other methods, distances between\nthe methods and attributes, etc.), Graphs (e.g., represent\nthe dependencies among classes, relations between methods\nand attributes, etc.), and Dynamic analysis (e.g., analyzing\nmethod traces, etc.). Refactoring documentation is an under-\nexplored area of research. Only 5.22% of the collected pa-\npers dived into refactoring documentation. Many studies\nexamined the automation of the different refactoring stages\nto reduce the refactoring effort and, therefore, increase its\nadaption. Figure 14 shows the count of publications dealing\nwith manual, semi-automatic, and automated refactoring.\nIn fact, 30.95% of the papers deal with the automation\nof refactoring. Only 1.95% and 8.67% of the papers used\nmanual and semi-automatic refactoring, respectively.\n\n12\n\n4.2 Artifacts affected by refactoring\n\nAs we mentioned before, refactoring is not limited to soft-\nware code. In fact, it can be applied to any type of soft-\nware artifacts (e.g., software architectures, database schema,\nmodels, user interfaces, and code). Figure 15 shows the per-\ncentage of refactoring publications per artifact. The evidence\nfrom this histogram shows that the most popular refactoring\nartifact is code (72.89%). Model refactoring has also received\nconsiderable attention, with a percentage of 59.25%. Graph-\nical user interfaces (GUIs) and Database refactoring have\nreceived the least attention of all with a fraction of only\n4.12% and 2.58%, respectively. This might be due to the fact\nthat database refactoring is conceptually more difficult than\ncode refactoring; code refactorings only need to maintain\nbehavioral semantics while database refactorings also must\nmaintain informational semantics. Also, GUI refactoring is\nvery demanding, requiring the adoption of user interfaces\narchitectural patterns from the early software design stages.\nFuture research should explore database and user interface\nrefactoring further as they are an indispensable part of\ntoday’s software.\n\n4.3 Refactoring objectives\n\nFive paradigms have been identified from analyzing the\nprimary studies: object-oriented designs, cloud computing,\nmobile apps, model-driven, and aspect-oriented. Object-\noriented programming has gained popularity because it\nmatches the way people actually think in the real world,\nstructuring their code into meaningful objects with relation-\nships that are obvious and intuitive. The increased popular-\nity of the object-oriented paradigm has also increased the\ninterest in object-oriented refactoring. This can be observed\nin figure 16 where more than 34% of the studies related to\nrefactoring focus on object-oriented designs. Less than 5% of\nthe papers investigated refactoring for cloud computing and\nmobile app development. For the refactoring objectives clas-\nsification of the taxonomy, five subcategories are considered:\nexternal quality (e.g. correctness, usability, efficiency, relia-\nbility, etc.) , internal quality (e. g. maintainability, flexibility,\nportability, re-usability, readability etc.) , performance (e.g.\nresponse time, error rate, request rate, memory use, etc.),\nmigration (e.g. Dispersion in the Class Hierarchy, number\nof referenced variables, number of assigned variables etc. ),\nsecurity (e.g. time needed to resolve vulnerabilities, Number\nof viruses and spams blocked, Number of port probes,\nnumber of patches applied, Cost per defect, Attack surface\netc.). Figure 17 is illustrating the reasons why people refactor\ntheir systems. Improving the internal quality takes up the\nlargest portion (41.63%) followed by refactoring to improve\nthe external quality (22.68%). Although security is a major\nconcern for almost all systems, only 3.11% of the papers\ninvestigated refactorings for security reasons.\n\n4.4 Refactoring techniques\n\nObject-oriented programming languages have common\ntraits/properties that facilitate the development of widely\nautomated source code analysis and transformation tools.\nMany studies [25] have given sufficient proof that a refac-\ntoring tool can be built for almost any object-oriented lan-\nguage (Python, PHP, Java, and C++). Support for multiple\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n13\n\n800\n\nQ\n3\n3S\n\n400\n\nCount of studies\n\n200\n\n0%)\n: -\n\n: Detection Prioritization Recommendation\n\nTesting Documentation Prediction\n\nRefactoring life-cycle\n\nFig. 13. Histogram illustrating the percentage of refactoring publications per refactoring life-cycle\n\nCount of studies\n\n9\n\nSemi-automatic\nRefactoring technique\n\nAutomatic\n\nFig. 14. Histogram illustrating the percentage of publications dealing with\nmanual, semi-automatic and automated refactoring\n\nlanguages in a refactoring tool is mentioned by [26]. Java\nis probably the most commercially important recent object-\noriented language with an infrastructure that is designed\nto support analysis. It has generic parsing, tree building,\nprettyprinting, tree manipulation, source-to-source rewrit-\ning, attribute grammar evaluations, control, and data flow\nanalysis. This explains the fact that 17.15% of refactoring\nstudies (see figure 18) provided refactoring techniques and\ntools that support Java. At the same time, most of the other\nprogramming languages have a fraction of less than 1%.\nWe classified the refactoring techniques into four main cat-\negories: data mining (e.g., Clustering, Classification, Deci-\nsion trees, Association, Neural networks, etc.), search-based\nmethods (e.g., Genetic algorithms, Hill climbing, Simulated\nannealing, Multi-objective evolutionary algorithms, etc.),\nformal methods (B-Method, the specification languages\nused in automated theorem proving, RAISE, the Z notation,\n\nCount of studies\n\nDatabase cul ‘Architecture ‘Model Code\nRefactoring Artifact\n\nFig. 15. histogram illustrating the count of refactoring publications per\nartifact\n\nSPARK Ada, etc.), and fuzzy logic. More than 25% of the\npapers use Search-based techniques to address refactoring\nproblems (see figure 19). This can be explained by the\nfact that search-based approaches have been proven to be\nefficient at finding solutions for complex and labor-intensive\ntasks. With the growing complexity of software systems,\nthere’s an infinite amount of improvement/changes you can\nmake to any piece of artifact. Exact algorithms are hard to\nuse to solve the refactoring problem within an instance-\ndependent, finite run-time. That’s why finding optimal\nrefactoring solutions are sacrificed for the sake of getting\nperfect solutions in polynomial time using heuristic meth-\nods like search-based algorithms. Data mining techniques\nhave also received significant attention (17.59%) as they are\nknown to be efficient at discovering new information, such\nas unknown patterns or hidden relationships, from huge\ndatabases like, for our case, large code repositories.\n\n4.5 Refactoring evaluation\n\nOpen-source software systems are becoming increasingly\nimportant these days. 61.1% of the studies (see figure 20)\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n1000\n\n800\n\n600\n\nCount of studies\n\n400\n\n200\n\nCould computing Mobile development\n\nModel-driven engineering Aspect-oriented design\nRefactoring Paradigm\n\n14\n\nObject-oriented design\n\nFig. 16. Histogram illustrating the count of refactoring publications per paradigm\n\n1200\n\n1000\n\n800\n\n600\n\nCount of studies\n\n400\n\n200\n\nSecurity Migration\n\nExternal quality\n\nPerformance\n\nInternal Quality\n\nRefactoring Objective\n\nFig. 17. Histogram illustrating the count of publications per refactoring objective\n\nused open-source systems to validate their work compared\nto 38.9% of studies that validated their work on industrial\nprojects. This result is expected because of the availability\nand accessibility of open source systems. However, open-\nsource software is often developed with a different man-\nagement style than the industrial ones. Thus, refactoring\ntechniques and tools must be validated and checked for\nquality and reliability using industrial systems. More indus-\ntrial collaborations are needed to bridge the gap between\nacademic research and the industry’s research needs, and\ntherefore, produce groundbreaking research and innovation\nthat solves complex real-world problems.\n\n5 CONCLUSION\n\nIn this paper, we have conducted a systematic literature\nreview on refactoring accompanied by meta-analysis to an-\nswer the defined research questions. After a comprehensive\n\nsearch that follows a systematic series of steps and assessing\nthe quality of the studies, 3183 publications were identified.\nBased on these selected papers, we derived a taxonomy\nfocused on five key aspects of Refactoring: refactoring life-\ncycle, artifacts affected by refactoring, refactoring objectives,\nrefactoring techniques, and refactoring evaluation. Using\nthis classification scheme, we analyzed the primary studies\nand presented the results in a way that enables researchers\nto relate their work to the current body of knowledge and\nidentify future research directions. We also implemented a\nrepository that helps researchers/practitioners collect and\nreport papers about Refactoring. It also provides visualiza-\ntion charts and graphs that highlight the analysis results of\nour selected studies. This infrastructure will bridge the gap\namong the different refactoring communities and allow for\nmore effortless knowledge transfer. To conclude, we believe\nthat the results of our systematic review will help advance\nthe refactoring research area. Since we expect this research\n', "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n15\n\nCount of studies\n\nscala javascript Ruby ‘Smalltalk MATLAB Cobol\n\nProgramming Language\n\nFig. 18. histogram illustrating the count of refactoring publications per programming language\n\n800\n\n700\n\n600\n\n500\n\n400\n\nCount of studies\n\n300\n\n200\n\n100\n\n9\n\nFuzzy logic Formalmethods Datamining _ Search-based\nRefactoring Method\n\nFig. 19. histogram illustrating the count of refactoring publications per\nfield\n\narea to continue to grow in the future, we hope that our\nrepository and taxonomy will become useful in organizing,\ndeveloping and judging new approaches.\n\nREFERENCES\n\n1] W. E Opdyke, “Refactoring: An aid in designing application frame-\nworks and evolving object-oriented systems,” in Proc. SOOPPA’90:\nSymposium on Object-Oriented Programming Emphasizing Practical\nApplications, 1990.\n\n2] W. G. Griswold, “Program restructuring as an aid to software\nmaintenance.” 1992.\n\n3] W. FE Opdyke, “Refactoring object-oriented frameworks,” 1992.\n\n4] M. Fowler, K. Beck, J. Brant, W. Opdyke, and D. Roberts, “Refactor-\ning: Improving the Design of Existing Code,” Xtemp01, pp. 1-337,\n1999.\n\n5] C. A. C. Coello, G. B. Lamont, D. A. Van Veldhuizen et al., Evolu-\ntionary algorithms for solving multi-objective problems. Springer, 2007,\nvol. 5.\n\n6] W. Ma, L. Chen, Y. Zhou, and B. Xu, “Do We Have\na Chance to Fix Bugs When Refactoring Code Smells?”\n2016 International Conference on Software Analysis, Testing and\nEvolution (SATE), pp. 24-29, 2016. [Online]. Available: http:\n/ /ieeexplore.ieee.org /document/7780189/\n\n7] K. Stroggylos and D. Spinellis, “Refactoring—Does It Improve Soft-\nware Quality?” Fifth International Workshop on Software Quality\n(WoSQ'07: ICSE Workshops 2007), pp. 3-8, 2007.\n\nIndustrial\n\nOpen source\n\nFig. 20. Pie chart illustrating the percentage of publications in which the\n\nauthors used industrial and/or open source systems in the validation\n\nstep\n\n[8] A. Kaur and M. Kaur, “Analysis of Code Refactoring Impact on\nSoftware Quality,” MATEC Web of Conferences, vol. 57, p. 02012,\nmay 2016. [Online]. Available: http://www.matec-conferences.\norg / 10.1051 /matecconf /20165702012\nG. Bavota, B. De Carluccio, A. De Lucia, M. Di Penta, R. Oliveto,\nand O. Strollo, “When does a refactoring induce bugs? An empirical\nstudy,” Proceedings - 2012 IEEE 12th International Working Conference\non Source Code Analysis and Manipulation, SCAM 2012, pp. 104-113,\n2012.\n[10] J. Al Dallal and A. Abdin, “Empirical evaluation of the impact of\nobject-oriented code refactoring on quality attributes: A systematic\nliterature review,” IEEE Transactions on Software Engineering, vol. 44,\nno. 1, pp. 44-69, 2017.\n[11] S. Singh and S. Kaur, “A systematic literature review: Refactoring\nfor disclosing code smells in object oriented software,” Ain Shams\nEngineering Journal, vol. 9, no. 4, pp. 2129-2151, 2018.\n[12] T. Mens and T. Tourwé, “A survey of software refactoring,” IEEE\nTransactions on software engineering, vol. 30, no. 2, pp. 126-139, 2004.\n[13] K. O. Elish and M. Alshayeb, “A classification of refactoring\nmethods based on software quality attributes,” Arabian Journal for\nScience and Engineering, vol. 36, no. 7, pp. 1253-1267, 2011.\n[14] B. Du Bois, P. Van Gorp, A. Amsel, N. Van Eetvelde, H. Stenten,\nS. Demeyer, and T. Mens, “A discussion of refactoring in research\nand practice,” Reporte Técnico. Universidad de Antwerpen, Bélgica,\n2004.\n[15] T. Mens, A. Van Deursen et al., “Refactoring: Emerging trends\nand open problems,” in Proceedings First International Workshop on\n\n[9\n\n", 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n16\n\n17]\n\n18\n\n19\n\n20.\n\n21\n\n22\n\n23,\n\n24\n25,\n\n26\n\nREFactoring: Achievements, Challenges, Effects (REFACE). University\nof Waterloo, 2003.\n\nM. Misbhauddin and M. Alshayeb, “Uml model refactoring: a\nsystematic literature review,” Empirical Software Engineering, vol. 20,\nno. 1, pp. 206-251, 2015.\n\nJ. Al Dallal, “Identifying refactoring opportunities in object-\noriented code: A systematic literature review,” Information and\nsoftware Technology, vol. 58, pp. 231-249, 2015.\n\nM. Abebe and C.-J. Yoo, “Trends, opportunities and challenges of\nsoftware refactoring: A systematic literature review,” International\nJournal of Software Engineering and Its Applications, vol. 8, no. 6, pp.\n299-318, 2014.\n\nA. A. B. Bagais and M. Alshayeb, “Automatic software refactoring:\na systematic literature review,” Software Quality Journal, pp. 1-44,\n2019.\n\nB. Kitchenham and S. Charters, “Guidelines for performing sys-\ntematic literature reviews in software engineering,” 2007.\n\nA. Ramirez, J. R. Romero, and C. L. Simons, “A systematic review\nof interaction in search-based software engineering,” IEEE Transac-\ntions on Software Engineering, vol. 45, no. 8, pp. 760-781, 2018.\n\nC. Wohlin, “Guidelines for snowballing in systematic literature\nstudies and a replication in software engineering,” in Proceedings\nof the 18th international conference on evaluation and assessment in\nsoftware engineering, 2014, pp. 1-10.\n\nM. Galster, D. Weyns, D. Tofan, B. Michalik, and P. Avgeriou,\n“Variability in software systems—a systematic literature review,”\nIEEE Transactions on Software Engineering, vol. 40, no. 3, pp. 282-\n306, 2013.\n\n(2020) Slr website. URL: https: //slr.iselab.us/.\n\nS. Tichelaar, S. Ducasse, S. Demeyer, and O. Nierstrasz, “A meta-\nmodel for language-independent refactoring,” in Proceedings Inter-\nnational Symposium on Principles of Software Evolution. IEEE, 2000,\npp. 154-164.\n\nM. O. Cinnéide and P. Nixon, “A methodology for the automated\nintroduction of design patterns,” in Proceedings IEEE International\nConference on Software Maintenance-1999 (ICSM’99).’Software Main-\ntenance for Business Change’(Cat. No. 99CB36360). IEEE, 1999, pp.\n463-472.\n\nPRIMARY SOURCES\n\nS2\n\nS3.\n\nS4\n\nSS.\n\nS6\n\nS7]\n\nS8\n\nSo\n\nS1] H. Sajnani, V. Saini, and C. V. Lopes, “A comparative study of\n\nbug patterns in java cloned and non-cloned code,” in 2014 IEEE\n14th International Working Conference on Source Code Analysis and\nManipulation. IEEE, 2014, pp. 21-30.\n\nJ. Ghofrani, M. Mohseni, and A. Bozorgmehr, “A conceptual\nframework for clone detection using machine learning,” in 2017\nIEEE 4th International Conference on Knowledge-Based Engineering\nand Innovation (KBEI). IEEE, 2017, pp. 0810-0817.\n\nI. Verebi, “A model-based approach to software refactoring,” in\n2015 IEEE International Conference on Software Maintenance and\nEvolution (ICSME). IEEE, 2015, pp. 606-609.\n\nM. Tufano, F. Palomba, G. Bavota, M. Di Penta, R. Oliveto,\nA. De Lucia, and D. Poshyvanyk, “An empirical investigation\ninto the nature of test smells,” in Proceedings of the 31st IEEE/ACM\nInternational Conference on Automated Software Engineering, 2016,\npp. 4-15.\n\nB. Zhang, G. Huang, Z. Zheng, J. Ren, and C. Hu, “Approach to\nmine the modularity of software network based on the most vital\nnodes,” IEEE Access, vol. 6, pp. 32543-32553, 2018.\n\nG. Balogh, T. Gergely, A. Beszédes, and T. Gyiméthy, “Are my\nunit tests in the right package?” in 2016 IEEE 16th International\nWorking Conference on Source Code Analysis and Manipulation\n(SCAM). IEEE, 2016, pp. 137-146.\n\nN. Tsantalis, D. Mazinanian, and G. P. Krishnan, “Assessing the\nrefactorability of software clones,” IEEE Transactions on Software\nEngineering, vol. 41, no. 11, pp. 1055-1090, 2015.\n\nG. Soares, R. Gheyi, and T. Massoni, “Automated behavioral\ntesting of refactoring engines,” IEEE Transactions on Software\nEngineering, vol. 39, no. 2, pp. 147-162, 2012.\n\nJ. Zhang, S. Han, D. Hao, L. Zhang, and D. Zhang, “Automated\nrefactoring of nested-if formulae in spreadsheets,” in Proceedings\nof the 2018 26th ACM Joint Meeting on European Software Engi-\nneering Conference and Symposium on the Foundations of Software\nEngineering, 2018, pp. 833-838.\n\nS10\n\nS11\n\n$12\n\nS13.\n\n$14\n\nS15\n\nS16\n\n‘S17\n\nS18\n\n$19\n\n$20\n\n$21\n\n$22\n\n$23.\n\nS24\n\nS25\n\nS26\n\nS27\n\n$28\n\n$29\n\n16\n\nY. Kataoka, M. D. Ernst, W. G. Griswold, and D. Notkin, “Au-\ntomated support for program refactoring using invariants,” in\nProceedings IEEE International Conference on Software Maintenance.\nICSM 2001. IEEE, 2001, pp. 736-743.\n\nM. Mondal, C. K. Roy, and K. A. Schneider, “A comparative study\non the bug-proneness of different types of code clones,” in 2015\nIEEE International conference on software maintenance and evolution\n(ICSME). IEEE, 2015, pp. 91-100.\n\nV. Alizadeh, M. Kessentini, W. Mkaouer, M. Ocinneide, A. Ouni,\nand Y. Cai, “An interactive and dynamic search-based approach\nto software refactoring recommendations,” IEEE Transactions on\nSoftware Engineering, 2018.\n\nW. Snipes, B. Robinson, and E. Murphy-Hill, “Code hot spot: A\ntool for extraction and analysis of code change history,” in 2011\n27th IEEE International Conference on Software Maintenance (ICSM).\nIEEE, 2011, pp. 392-401.\n\nH. Liu, Q. Liu, Z. Niu, and Y. Liu, “Dynamic and automatic\nfeedback-based threshold adaptation for code smell detection,”\nIEEE Transactions on Software Engineering, vol. 42, no. 6, pp. 544—\n558, 2015.\n\nV. Cosentino, S. Duenas, A. Zerouali, G. Robles, and J. M.\nGonzalez-Barahona, “Graal: The quest for source code knowl-\nedge.”\n\nS. Charalampidou, A. Ampatzoglou, A. Chatzigeorgiou, A. Gko-\nrtzis, and P. Avgeriou, “Identifying extract method refactoring\nopportunities based on functional relevance,” IEEE Transactions\non Software Engineering, vol. 43, no. 10, pp. 954-974, 2016.\n\nA. Rani and J. K. Chhabra, “Prioritization of smelly classes: A two\nphase approach (reducing refactoring efforts),” in 2017 3rd Inter-\nnational Conference on Computational Intelligence & Communication\nTechnology (CICT). IEEE, 2017, pp. 1-6.\n\nP. Rachow, “Refactoring decision support for developers and ar-\nchitects based on architectural impact,” in 2019 IEEE International\nConference on Software Architecture Companion (ICSA-C). IEEE,\n2019, pp. 262-266.\n\nH. Liu, Z. Ma, W. Shao, and Z. Niu, “Schedule of bad smell detec-\ntion and resolution: A new way to save effort,” IEEE transactions\non Software Engineering, vol. 38, no. 1, pp. 220-235, 2011.\n\nJ. Kim, D. Batory, and D. Dig, “Scripting parametric refactorings\nin java to retrofit design patterns,” in 2015 IEEE International\nConference on Software Maintenance and Evolution (ICSME). IEEE,\n2015, pp. 211-220.\n\nM. A. Parande and G. Koru, “A longitudinal analysis of the\ndependency concentration in smaller modules for open-source\nsoftware products,” in 2010 IEEE International Conference on Soft-\nware Maintenance. IEEE, 2010, pp. 1-5.\n\nH. Liu, Z. Xu, and Y. Zou, “Deep learning based feature envy\ndetection,” in Proceedings of the 33rd ACM/IEEE International\nConference on Automated Software Engineering, 2018, pp. 385-396.\nR. Morales, R. Saborido, F. Khomh, F. Chicano, and G. Anto-\nniol, “Earmo: An energy-aware refactoring approach for mobile\napps,” IEEE Transactions on Software Engineering, vol. 44, no. 12,\npp. 1176-1206, 2017.\n\nH. Liu, L. Yang, Z. Niu, Z. Ma, and W. Shao, “Facilitating software\nrefactoring with appropriate resolution order of bad smells,”\nin Proceedings of the 7th joint meeting of the European software\nengineering conference and the ACM SIGSOFT symposium on The\nfoundations of software engineering, 2009, pp. 265-268.\n\nH. Liu, Q. Liu, Y. Liu, and Z. Wang, “Identifying renaming op-\nportunities by expanding conducted rename refactorings,” IEEE\nTransactions on Software Engineering, vol. 41, no. 9, pp. 887-900,\n2015.\n\nB. Lin, S. Scalabrino, A. Mocci, R. Oliveto, G. Bavota, and\nM. Lanza, “Investigating the use of code analysis and nlp to\npromote a consistent usage of identifiers,” in 2017 IEEE 17th\nInternational Working Conference on Source Code Analysis and Ma-\nnipulation (SCAM). IEEE, 2017, pp. 81-90.\n\nG. Bavota, R. Oliveto, M. Gethers, D. Poshyvanyk, and A. De Lu-\ncia, “Methodbook: Recommending move method refactorings via\nrelational topic models,” IEEE Transactions on Software Engineer-\ning, vol. 40, no. 7, pp. 671-694, 2013.\n\nC. Hinds-Charles, J. Adames, Y. Yang, Y. Shen, and Y. Wang,\n“A longitude analysis on bitcoin issue repository,” in 2018 Ist\nIEEE International Conference on Hot Information-Centric Network-\ning (HotICN). YEEE, 2018, pp. 212-217.\n\nT. D. Oyetoyan, D. S. Cruzes, and C. Thurmann-Nielsen, “A\ndecision support system to refactor class cycles,” in 2015 IEEE\n', "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nS30.\n\n$31\n\n$32.\n\n$33,\n\nS34\n\nS35.\n\nS36\n\nS37\n\nS38.\n\nS39\n\nS40\n\nS41\n\n$42\n\nS43,\n\nS44\n\nS45.\n\nS46\n\nS47\n\nS48\n\nS49\n\nS50\n\nInternational Conference on Software Maintenance and Evolution\n(ICSME). IEEE, 2015, pp. 231-240.\n\nN. Rachatasumrit and M. Kim, “An empirical investigation into\nthe impact of refactoring on regression testing,” in 2012 28th Ieee\nInternational Conference on Software Maintenance (Icsm). IEEE,\n2012, pp. 357-366.\n\nM. Mirzaaghaei, F. Pastore, and M. Pezze, “Automatically repair-\ning test cases for evolving method declarations,” in 2010 IEEE\nInternational Conference on Software Maintenance. IEEE, 2010, pp.\n15.\n\nB. Van Rompaey, B. Du Bois, and S. Demeyer, “Characterizing\nthe relative significance of a test smell,” in 2006 22nd IEEE\nInternational Conference on Software Maintenance. IEEE, 2006, pp.\n391-400.\n\nA. Sherwany, N. Zaza, and N. Nystrom, “A refactoring library for\nscala compiler extensions,” in International Conference on Compiler\nConstruction. Springer, 2015, pp. 31-48.\n\nS. Paydar and M. Kahani, “A semantic web based approach\nfor design pattern detection from source code,” in 2012 2nd\nInternational eConference on Computer and Knowledge Engineering\n(ICCKE). IEEE, 2012, pp. 289-294.\n\nT. Haendler, “A card game for learning software-refactoring\nprinciples,” 2019.\n\nC. Kastner, S. Apel, and D. Batory, “A case study implementing\nfeatures using aspectj,” in 11th International Software Product Line\nConference (SPLC 2007). IEEE, 2007, pp. 223-232.\n\nT. Viana, “A catalog of bad smells in design-by-contract method-\nologies with java modeling language,” Journal of Computing Sci-\nence and Engineering, vol. 7, no. 4, pp. 251-262, 2013.\n\nD. Foetsch and E. Pulvermueller, “A concept and implementation.\nof higher-level xml transformation languages,” Knowledge-Based\nSystems, vol. 22, no. 3, pp. 186-194, 2009.\n\nJ. Reutelshoefer, J. Baumeister, and F. Puppe, “A data structure for\nthe refactoring of multimodal knowledge,” in Proceedings of the\n5th Workshop on Knowledge Engineering and Software Engineering,\n2009, pp. 33-45.\n\nS. Mouchawrab, L. C. Briand, and Y. Labiche, “A measurement\nframework for object-oriented software testability,” Information\nand software technology, vol. 47, no. 15, pp. 979-997, 2005.\n\nI. Cassol and G. Arévalo, “A methodology to infer and refactor\nan object-oriented model from c applications,” Software: Practice\nand Experience, vol. 48, no. 3, pp. 550-577, 2018.\n\nG. De Ruvo and A. Santone, “A novel methodology based on\nformal methods for analysis and verification of wikis,” in 2014\nIEEE 23rd International WETICE Conference. YEEE, 2014, pp. 411-\n416.\n\nS. Rebai, O. B. Sghaier, V. Alizadeh, M. Kessentini, and M. Chater,\n“Interactive refactoring documentation bot,” in 2019 19th Interna-\ntional Working Conference on Source Code Analysis and Manipulation\n(SCAM). IEEE, 2019, pp. 152-162.\n\nJ. Krinke, “Mining execution relations for crosscutting concerns,”\nIET software, vol. 2, no. 2, pp. 65-78, 2008.\n\nD. Bowes, D. Randall, and T. Hall, “The inconsistent measure-\nment of message chains,” in 2013 4th International Workshop on\nEmerging Trends in Software Metrics (WETSoM). IEEE, 2013, pp.\n62-68.\n\nJ. Liu, “Feature interactions and software derivatives.” Journal of\nObject Technology, vol. 4, no. 3, pp. 13-19, 2004.\n\nA. Swidan, F. Hermans, and R. Koesoemowidjojo, “Improving\nthe performance of a large scale spreadsheet: a case study,”\nin 2016 IEEE 23rd International Conference on Software Analysis,\nEvolution, and Reengineering (SANER), vol. 1. _ IEEE, 2016, pp.\n673-677.\n\nH. Li, S. Thompson, and T. Arts, “Extracting properties from test\ncases by refactoring,” in 2011 IEEE Fourth International Conference\non Software Testing, Verification and Validation Workshops. IEEE,\n2011, pp. 472-473.\n\nS. Ducasse, O. Nierstrasz, N. Scharli, R. Wuyts, and A. P. Black,\n“Traits: A mechanism for fine-grained reuse,” ACM Transactions\non Programming Languages and Systems (TOPLAS), vol. 28, no. 2,\npp. 331-388, 2006.\n\nR. Ramos, J. Castro, J. Aratijo, F. Alencar, and R. Penteado, “Di-\nvide and conquer refactoring: dealing with the large, scattering or\ntangling use case model,” in Proceedings of the 8th Latin American\nConference on Pattern Languages of Programs, 2010, pp. 1-11.\n\nS51\n\nS52\n\nS53.\n\nS54\n\nS55,\n\nS56\n\n‘S57\n\nS58\n\nS59\n\nS60\n\nS61\n\nS62\n\nS63.\n\nS64\n\nS65,\n\nS66\n\nS67\n\nS68\n\nS69\n\nS70\n\nS71\n\n17\n\nE. Murphy-Hill, A. P. Black, D. Dig, and C. Parnin, “Gathering\nrefactoring data: a comparison of four methods,” in Proceedings\nof the 2nd Workshop on Refactoring Tools, 2008, pp. 1-5.\n\nA. Dereziriska, “A structure-driven process of automated refac-\ntoring to design patterns,” in International Conference on Informa-\ntion Systems Architecture and Technology. Springer, 2017, pp. 39-\n48.\n\nE. Selim, Y. Ghanam, C. Burns, T. Seyed, and F. Maurer, “A test-\ndriven approach for extracting libraries of reusable components\nfrom existing applications,” in International Conference on Agile\nSoftware Development. Springer, 2011, pp. 238-252.\n\nY. Zhang, S. Dong, X. Zhang, H. Liu, and D. Zhang, “Automated.\nrefactoring for stampedlock,” IEEE Access, vol. 7, pp. 104900-\n104.911, 2019.\n\nH. Xue, S. Sun, G. Venkataramani, and T. Lan, “Machine learning-\nbased analysis of program binaries: A comprehensive study,”\nIEEE Access, vol. 7, pp. 65 889-65 912, 2019.\n\nY. Zhang, S. Shao, H. Liu, J. Qiu, D. Zhang, and G. Zhang,\n“Refactoring java programs for customizable locks based on\nbytecode transformation,” IEEE Access, vol. 7, pp. 66 292-66 303,\n2019.\n\nM. FE Dolz, D. D. R. Astorga, J. Fernandez, J. D. Garcia, and J. Car-\nretero, “Towards automatic parallelization of stream processing\napplications,” IEEE Access, vol. 6, pp. 39 944-39 961, 2018.\n\nB. K. Sidhu, K. Singh, and N. Sharma, “A catalogue of model\nsmells and refactoring operations for object-oriented software,”\nin 2018 Second International Conference on Inventive Communication\nand Computational Technologies (ICICCT). IEEE, 2018, pp. 313-319.\nF. Medeiros, M. Ribeiro, R. Gheyi, and B. F. dos Santos Neto, “A\ncatalogue of refactorings to remove incomplete annotations.” J.\nUCS, vol. 20, no. 5, pp. 746-771, 2014.\n\nP. Ma, Y. Bian, and X. Su, “A clustering method for pruning false\npositive of clonde code detection,” in Proceedings 2013 Interna-\ntional Conference on Mechatronic Sciences, Electric Engineering and\nComputer (MEC). IEEE, 2013, pp. 1917-1920.\n\nG.-S. Cojocar and A.-M. Guran, “A comparative analysis of mon-\nitoring concerns implementation in object oriented systems,” in\n2018 IEEE 12th International Symposium on Applied Computational\nIntelligence and Informatics (SACI). IEEE, 2018, pp. 000355-\n000 360.\n\nS. Negara, N. Chen, M. Vakilian, R. E. Johnson, and D. Dig, “A\ncomparative study of manual and automated refactorings,” in\nEuropean Conference on Object-Oriented Programming. Springer,\n2013, pp. 552-576.\n\nT. Chen and C. He, “A comparison of approaches to legacy\nsystem crosscutting concerns mining,” in 2013 International Con-\nference on Computer Sciences and Applications. IEEE, 2013, pp.\n813-816.\n\nA. Martini, E. Sikander, and N. Madlani, “A semi-automated\nframework for the identification and estimation of architectural\ntechnical debt: A comparative case-study on the modularization\nof a software component,” Information and Software Technology,\nvol. 93, pp. 264-279, 2018.\n\nM. T. Valente, V. Borges, and L. Passos, “A semi-automatic ap-\nproach for extracting software product lines,” IEEE Transactions\non Software Engineering, vol. 38, no. 4, pp. 737-754, 2011.\n\nK. Garcés, J. M. Vara, F. Jouault, and E. Marcos, “Adapting trans-\nformations to metamodel changes via external transformation\ncomposition,” Software & Systems Modeling, vol. 13, no. 2, pp.\n789-806, 2014.\n\nS. A. Vidal, C. Marcos, and J. A. Diaz-Pace, “An approach to\nprioritize code smells for refactoring,” Automated Software Engi-\nneering, vol. 23, no. 3, pp. 501-532, 2016.\n\nC. Brown, H. Li, and S. Thompson, “An expression processor:\na case study in refactoring haskell programs,” in International\nSymposium on Trends in Functional Programming. Springer, 2010,\npp. 31-49.\n\nM. Marin, A. van Deursen, L. Moonen, and R. van der Rijst,\n“An integrated crosscutting concern migration strategy and its\nsemi-automated application to jhotdraw,” Automated Software\nEngineering, vol. 16, no. 2, pp. 323-356, 2009.\n\nA. O'Riordan, “Aspect-oriented reengineering of an object-\noriented library in a short iteration agile process,” Informatica,\nvol. 35, no. 4, 2011.\n\nK. Fujiwara, K. Fushida, N. Yoshida, and H. lida, “Assessing\nrefactoring instances and the maintainability benefits of them\nfrom version archives,” in International Conference on Product\n", "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nS72\n\nS73.\n\nS74\n\nS75.\n\nS76\n\nS77\n\nS78\n\nS79\n\nS80.\n\nS81\n\nS82.\n\nS83.\n\nS84\n\nS85.\n\nS86\n\nS87\n\nS88.\n\nS89\n\nS90\n\nFocused Software Process Improvement.\n323.\n\nB. Alkhazi, T. Ruas, M. Kessentini, M. Wimmer, and W. I.\nGrosky, “Automated refactoring of atl model transformations:\na search-based approach,” in Proceedings of the ACM/IEEE 19th\nInternational Conference on Model Driven Engineering Languages and\nSystems, 2016, pp. 295-304.\n\nM. Tanhaei, J. Habibi, and S.-H. Mirian-Hosseinabadi, “Au-\ntomating feature model refactoring: A model transformation\napproach,” Information and Software Technology, vol. 80, pp. 138—\n157, 2016.\n\nV. Alizadeh, H. Fehri, and M. Kessentini, “Less is more: From\nmulti-objective to mono-objective refactoring via developer’s\nknowledge extraction,” in 2019 19th International Working Con-\nference on Source Code Analysis and Manipulation (SCAM). IEEE,\n2019, pp. 181-192.\n\nV. Alizadeh, M. A. Ouali, M. Kessentini, and M. Chater, “Refbot:\nintelligent software refactoring bot,” in 2019 34th IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE).\nTEEE, 2019, pp. 823-834.\n\nZ. Mushtaq, G. Rasool, and B. Shehzad, “Multilingual source\ncode analysis: A systematic literature review,” IEEE Access, vol. 5,\npp. 11 307-11 336, 2017.\n\nF. Schmidt, S. G. MacDonell, and A. M. Connor, “An auto-\nmatic architecture reconstruction and refactoring framework,” in\nSoftware Engineering Research, Management and Applications 2011.\nSpringer, 2012, pp. 95-111.\n\nG. Cong, H. Wen, I.-h. Chung, D. Klepacki, H. Murata, and\nY. Negishi, “An efficient framework for multi-dimensional tuning\nof high performance computing applications,” in 2012 IEEE 26th\nInternational Parallel and Distributed Processing Symposium. IEEE,\n2012, pp. 1376-1387.\n\nJ. Park, M. Kim, and D.-H. Bae, “An empirical study of sup-\nplementary patches in open source projects,” Empirical Software\nEngineering, vol. 22, no. 1, pp. 436-473, 2017.\n\nT. L. Nguyen, A. Fish, and M. Song, “An empirical study on\nsimilar changes in evolving software,” in 2018 IEEE International\nConference on Electro/Information Technology (EIT). IEEE, 2018, pp.\n0560-0563.\n\nM. Bruntink, A. Van Deursen, T. Tourwe, and R. van Engelen,\n“An evaluation of clone detection techniques for crosscutting\nconcerns,” in 20th IEEE International Conference on Software Main-\ntenance, 2004. Proceedings. EEE, 2004, pp. 200-209.\n\nY. Kosker, B. Turhan, and A. Bener, “An expert system for\ndetermining candidate software classes for refactoring,” Expert\nSystems with Applications, vol. 36, no. 6, pp. 10 000-10 003, 2009.\nB. L. Sousa, M. A. Bigonha, and K. A. Ferreira, “An exploratory\nstudy on cooccurrence of design patterns and bad smells using\nsoftware metrics,” Software: Practice and Experience, vol. 49, no. 7,\npp. 1079-1113, 2019.\n\nO. Mehani, G. Jourjon, T. Rakotoarivelo, and M. Ott, “An in-\nstrumentation framework for the critical task of measurement\ncollection in the future internet,” Computer Networks, vol. 63, pp.\n68-83, 2014.\n\nM. Schafer, A. Thies, F. Steimann, and F. Tip, “A comprehensive\napproach to naming and accessibility in refactoring java pro-\ngrams,” IEEE Transactions on Software Engineering, vol. 38, no. 6,\npp. 1233-1257, 2012.\n\nD. Dig, “A practical tutorial on refactoring for parallelism,” in\n2010 IEEE International Conference on Software Maintenance. IEEE,\n2010, pp. 1-2.\n\nX. Li and J. P. Gallagher, “A source-level energy optimization\nframework for mobile applications,” in 2016 IEEE 16th Interna-\ntional Working Conference on Source Code Analysis and Manipulation\n(SCAM). IEEE, 2016, pp. 31-40.\n\nR. Khatchadourian, Y. Tang, M. Bagherzadeh, and S. Ahmed,\n“Tengineering paper] a tool for optimizing java 8 stream software\nvia automated refactoring,” in 2018 IEEE 18th International Work-\ning Conference on Source Code Analysis and Manipulation (SCAM).\nIEEE, 2018, pp. 34-39.\n\nZ. Xing and E. Stroulia, “Api-evolution support with diff-\ncatchup,” IEEE Transactions on Software Engineering, vol. 33,\nno. 12, pp. 818-836, 2007.\n\nR. Gheyi, T. Massoni, and P. Borba, “A rigorous approach for\nproving model refactorings,” in Proceedings of the 20th IEEE/ACM\ninternational Conference on Automated software engineering, 2005,\npp. 372-375.\n\nSpringer, 2013, pp. 313—\n\n18\n$91] B. Cyganek, “Adding parallelism to the hybrid image processing\nlibrary in multi-threading and multi-core systems,” in 2011 IEEE\n2nd International Conference on Networked Embedded Systems for\nEnterprise Applications. YEEE, 2011, pp. 1-8.\n\nR. Hardt and E. V. Munson, “An empirical evaluation of ant build\n\nmaintenance using formiga,” in 2015 IEEE International Conference\n\non Software Maintenance and Evolution (ICSME). IEEE, 2015, pp.\n\n201-210.\n\nR. Kolb, D. Muthig, T. Patzke, and K. Yamauchi, “A case study\n\nin refactoring a legacy component for reuse in a product line,”\n\nin 21st IEEE International Conference on Software Maintenance\n\n(ICSM’05). IEEE, 2005, pp. 369-378.\n\nD. Strein, R. Lincke, J. Lundberg, and W. Léwe, “An extensible\n\nmeta-model for program analysis,” IEEE Transactions on Software\n\nEngineering, vol. 33, no. 9, pp. 592-607, 2007.\n\nY.-W. Kwon, “Automated s/w reengineering for fault-tolerant\n\nand energy-efficient distributed execution,” in 2013 IEEE Inter-\n\nnational Conference on Software Maintenance. IEEE, 2013, pp. 582—\n\n585.\n\nB. Adams, H. Tromp, K. De Schutter, and W. De Meuter, “De-\n\nsign recovery and maintenance of build systems,” in 2007 IEEE\n\nInternational Conference on Software Maintenance. YEEE, 2007, pp.\n\n114-123.\n\nR. Bahsoon and W. Emmerich, “Evaluating architectural stability\n\nwith real options theory,” in 20th IEEE International Conference on\n\nSoftware Maintenance, 2004. Proceedings. IEEE, 2004, pp. 443-447.\n\nJ. O'neal, K. Weide, and A. Dubey, “Experience report: refactoring\n\nthe mesh interface in flash, a multiphysics software,” in 2018\n\nIEEE 14th International Conference on e-Science (e-Science). IEEE,\n\n2018, pp. 1-6.\n\nM. A. Khan and H. Tembine, “Meta-learning for realizing self-x\n\nmanagement of future networks,” IEEE Access, vol. 5, pp. 19 072—\n\n19083, 2017.\n\n$100] A. Cleve, “Program analysis and transformation for data-\nintensive system evolution,” in 2010 IEEE International Conference\non Software Maintenance. IEEE, 2010, pp. 1-6.\n\n$101] D. Binkley, M. Ceccato, M. Harman, F. Ricca, and P. Tonella,\n“Automated refactoring of object oriented code into aspects,”\nin 21st IEEE International Conference on Software Maintenance\n(ICSM’05). IEEE, 2005, pp. 27-36.\n\n$102] F. Castor Filho, A. Garcia, and C. M. F. Rubira, “Extracting\nerror handling to aspects: A cookbook,” in 2007 IEEE International\nConference on Software Maintenance. IEEE, 2007, pp. 134-143.\n\n$103] M. Bajammal, D. Mazinanian, and A. Mesbah, “Generating\nreusable web components from mockups,” in Proceedings of the\n33rd ACM/IEEE International Conference on Automated Software\nEngineering, 2018, pp. 601-611.\n\n$104] N. A. Kraft, E. B. Duffy, and B. A. Malloy, “Grammar recovery\nfrom parse trees and metrics-guided grammar refactoring,” IEEE\nTransactions on Software Engineering, vol. 35, no. 6, pp. 780-794,\n2009.\n\n$105] D. Spinellis, “Global analysis and transformations in prepro-\ncessed languages,” IEEE Transactions on Software Engineering,\nvol. 29, no. 11, pp. 1019-1030, 2003.\n\n$106] C. Noguera, A. Kellens, C. De Roover, and V. Jonckers, “Refac-\ntoring in the presence of annotations,” in 2012 28th IEEE Interna-\ntional Conference on Software Maintenance (ICSM). IEEE, 2012, pp.\n337-346.\n\n$107] S. Rongrong, Z. Liping, and Z. Fengrong, “A method for iden-\ntifying and recommending reconstructed clones,” in Proceedings\nof the 2019 3rd International Conference on Management Engineering,\nSoftware Engineering and Service Sciences, 2019, pp. 39-44.\n\n$108] Y. Khan and M. El-Attar, “A model transformation approach\ntowards refactoring use case models based on antipatterns,”\nin 21st International Conference on Software Engineering and Data\nEngineering (SEDE’12), Los Angeles, California, USA, 2012, pp. 49—\n54,\n\nS92\n\nS93.\n\nS94\n\nS95,\n\nS96\n\n‘S97\n\nS98\n\nS99\n\n$109] K. Grolinger and M. A. Capretz, “A unit test approach for\ndatabase schema evolution,” Information and Software Technology,\nvol. 53, no. 2, pp. 159-170, 2011.\n\n$110] O. Febbraro, K. Reale, and F. Ricca, “Aspide: Integrated develop-\nment environment for answer set programming,” in International\nConference on Logic Programming and Nonmonotonic Reasoning.\nSpringer, 2011, pp. 317-330.\n\n$111] A. Garrido and R. Johnson, “Analyzing multiple configurations\nof ac program,” in 21st IEEE International Conference on Software\nMaintenance (ICSM’05). IEEE, 2005, pp. 379-388.\n", 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n$112\n\n$113\n\n$114\n\n$115\n\nS116\n\n$117]\n\n$118\n\n$119\n\n$120\n\n$121\n\n$122\n\n$123\n\n$124\n\n$125\n\nS126\n\n$127]\n\n$128\n\n$129\n\n$130\n\nS131\n\nA. Paltoglou, V. E. Zafeiris, E. A. Giakoumakis, and N. Diaman-\ntidis, “Automated refactoring of client-side javascript code to es6\nmodules,” in 2018 IEEE 25th International Conference on Software\nAnalysis, Evolution and Reengineering (SANER). IEEE, 2018, pp.\n402-412.\n\nR. Khatchadourian, J. Sawin, and A. Rountev, “Automated\nrefactoring of legacy java software to enumerated types,” in 2007\nIEEE International Conference on Software Maintenance. IEEE, 2007,\npp. 224-233.\n\nM. Marin, L. Moonen, and A. van Deursen, “A classification\nof crosscutting concerns,” in 21st IEEE International Conference on\nSoftware Maintenance (ICSM’05). IEEE, 2005, pp. 673-676.\n\nM. Mortensen, S. Ghosh, and J. Bieman, “Aspect-oriented refac-\ntoring of legacy applications: An evaluation,” IEEE Transactions\non Software Engineering, vol. 38, no. 1, pp. 118-140, 2010.\n\nM. Mondal, C. K. Roy, and K. A. Schneider, “Automatic identi-\nfication of important clones for refactoring and tracking,” in 2014\nIEEE 14th International Working Conference on Source Code Analysis\nand Manipulation. IEEE, 2014, pp. 11-20.\n\nA. Kellens, K. De Schutter, T. D’Hondt, V. Jonckers, and\nH. Doggen, “Experiences in modularizing business rules into\naspects,” in 2008 IEEE International Conference on Software Mainte-\nnance. IEEE, 2008, pp. 448-451.\n\nR. Stoiber, S. Fricker, M. Jehle, and M. Glinz, “Feature unweav-\ning: Refactoring software requirements specifications into soft-\nware product lines,” in 2010 18th IEEE International Requirements\nEngineering Conference. IEEE, 2010, pp. 403-404.\n\nG. Zhao and J. Huang, “Deepsim: deep learning code functional\nsimilarity,” in Proceedings of the 2018 26th ACM Joint Meeting on\nEuropean Software Engineering Conference and Symposium on the\nFoundations of Software Engineering, 2018, pp. 141-151.\n\nS. Demeyer, S. Ducasse, and O. Nierstrasz, “Object-oriented\nreengineering: patterns and techniques,” in 21st IEEE Interna-\ntional Conference on Software Maintenance (ICSM’05). IEEE, 2005,\npp. 723-724.\n\nP. Hegedus, “Revealing the effect of coding practices on soft-\nware maintainability,” in 2013 IEEE International Conference on\nSoftware Maintenance. IEEE, 2013, pp. 578-581.\n\nT. Feng, J. Zhang, H. Wang, and X. Wang, “Software design\nimprovement through anti-patterns identification,” in 20th IEEE\nInternational Conference on Software Maintenance, 2004. Proceedings.\nTEEE, 2004, p. 524.\n\nS. Meng and L. S. Barbosa, “A coalgebraic semantic framework\nfor reasoning about uml sequence diagrams,” in 2008 The Eighth\nInternational Conference on Quality Software. IEEE, 2008, pp. 17—\n26.\n\nP. Mayer and A. Schroeder, “Cross-language code analysis and\nrefactoring,” in 2012 IEEE 12th International Working Conference on\nSource Code Analysis and Manipulation. IEEE, 2012, pp. 94-103.\n\nR. Moser, P. Abrahamsson, W. Pedrycz, A. Sillitti, and G. Succi,\n“A case study on the impact of refactoring on quality and\nproductivity in an agile team,” in IFIP Central and East European\nConference on Software Engineering Techniques. Springer, 2007, pp.\n252-266.\n\nA. L. Candido, F. A. Trinta, L. S. Rocha, P. A. Rego, N. C.\nMendonga, and V. C. Garcia, “A microservice based architecture\nto support offloading in mobile cloud computing,” in Proceedings\nof the XIII Brazilian Symposium on Software Components, Architec-\ntures, and Reuse, 2019, pp. 93-102.\n\nA. Peruma, “A preliminary study of android refactorings,” in\n2019 IEEE/ACM 6th International Conference on Mobile Software\nEngineering and Systems (MOBILESoft). IEEE, 2019, pp. 148-149.\n\nM. Mascheroni and E. Irrazabal, “A design pattern approach\nfor restful tests: A case study,” in IEEE 12th Colombian Computing\nCongress, 2018.\n\nD. Kermek, T. Jakupié, and N. Vréek, “A model of heterogeneous\ndistributed system for foreign exchange portfolio analysis,” Jour-\nnal of Information and Organizational Sciences, vol. 30, no. 1, pp.\n83-92, 2006.\n\nG. Rodriguez, A. Teyseyre, A. Soria, and L. Berdun, “A visu-\nalization tool to detect refactoring opportunities in soa applica-\ntions,” in 2017 XLII Latin American Computer Conference (CLEI).\nIEEE, 2017, pp. 1-10.\n\nH. Li, S. Thompson, P. Lamela Seijas, and M. A. Francisco,\n“Automating property-based testing of evolving web services,”\nin Proceedings of the ACM SIGPLAN 2014 Workshop on Partial\nEvaluation and Program Manipulation, 2014, pp. 169-180.\n\n19\n\n$132] D. Athanasopoulos, A. V. Zarras, G. Miskos, V. Issarny, and\nP. Vassiliadis, “Cohesion-driven decomposition of service inter-\nfaces without access to source code,” IEEE Transactions on Services\nComputing, vol. 8, no. 4, pp. 550-562, 2014.\n\n$133] M. Kessentini and H. Wang, “Detecting refactorings among\nmultiple web service releases: A heuristic-based approach,” in\n2017 IEEE International Conference on Web Services (ICWS). IEEE,\n2017, pp. 365-372.\n\n$134] F. Wei, C. Ouyang, and A. Barros, “Discovering behavioural\ninterfaces for overloaded web services,” in 2015 IEEE World\nCongress on Services. IEEE, 2015, pp. 286-293.\n\n$135] K. Fekete, A. Pelle, and K. Csorba, “Energy efficient code opti-\nmization in mobile environment,” in 2014 IEEE 36th International\nTelecommunications Energy Conference (INTELEC). IEEE, 2014, pp.\n16.\n\n$136] W. B. Langdon, “Genetic improvement of programs,” in 2014\n16th International Symposium on Symbolic and Numeric Algorithms\nfor Scientific Computing. IEEE, 2014, pp. 14-19.\n\n$137] H. Wang, A. Ouni, M. Kessentini, B. Maxim, and W. I. Grosky,\n“Identification of web service refactoring opportunities as a\nmulti-objective problem,” in 2016 IEEE International Conference\non Web Services (ICWS). IEEE, 2016, pp. 586-593.\n\n$138] M. Kim, T. Zimmermann, and N. Nagappan, “An empirical\nstudy of refactoringchallenges and benefits at microsoft,” IEEE\nTransactions on Software Engineering, vol. 40, no. 7, pp. 633-649,\n2014.\n\n$139] S. M. Olbrich, D. S. Cruzes, and D. I. Sjoberg, “Are all code\nsmells harmful? a study of god classes and brain classes in the\nevolution of three open source systems,” in 2010 IEEE Interna-\ntional Conference on Software Maintenance. IEEE, 2010, pp. 1-10.\n\n$140] P. S. Kochhar, F. Thung, and D. Lo, “Automatic fine-grained\nissue report reclassification,” in 2014 19th International Conference\non Engineering of Complex Computer Systems. IEEE, 2014, pp. 126—\n135.\n\n$141] G. Bastide, A. Seriai, and M. Oussalah, “Dynamic adaptation\nof software component structures,” in 2006 IEEE International\nConference on Information Reuse & Integration. IEEE, 2006, pp.\n404409.\n\n$142] G. Zhang, L. Shen, X. Peng, Z. Xing, and W. Zhao, “Incremental\nand iterative reengineering towards software product line: An\nindustrial case study,” in 2011 27th IEEE International Conference\non Software Maintenance (ICSM). IEEE, 2011, pp. 418-427.\n\n$143] G. Bavota, R. Oliveto, A. De Lucia, G. Antoniol, and Y.-G.\nGueheneuc, “Playing with refactoring: Identifying extract class\nopportunities through game theory,” in 2010 IEEE International\nConference on Software Maintenance. IEEE, 2010, pp. 1-5.\n\n$144] P. S. Kochhar, Y. Tian, and D. Lo, “Potential biases in bug\nlocalization: Do they matter?” in Proceedings of the 29th ACM/IEEE\ninternational conference on Automated software engineering, 2014, pp.\n803-814.\n\n$145] R. Khatchadourian and H. Masuhara, “Defaultification refactor-\ning: A tool for automatically converting java methods to default,”\nin 2017 32nd IEEE/ACM International Conference on Automated\nSoftware Engineering (ASE). IEEE, 2017, pp. 984-989.\n\n$146] H. K. Wright, D. Jasper, M. Klimek, C. Carruth, and Z. Wan,\n“Large-scale automated refactoring using clangmr,” in 2013 IEEE\nInternational Conference on Software Maintenance. IEEE, 2013, pp.\n548-551.\n\n$147] D. Mazinanian and N. Tsantalis, “Migrating cascading style\nsheets to preprocessors by introducing mixins,” in Proceedings of\nthe 31st IEEE/ACM International Conference on Automated Software\nEngineering, 2016, pp. 672-683.\n\n$148] P. Tonella and M. Ceccato, “Migrating interface implementa-\ntions to aspects,” in 20th IEEE International Conference on Software\nMaintenance, 2004. Proceedings. IEEE, 2004, pp. 220-229.\n\n$149] M. Ceccato, “Migrating object oriented code to aspect oriented\nprogramming,” 2006.\n\n$150] D. Majumdar, “Migration from procedural programming to\naspect oriented paradigm,” in 2009 IEEE/ACM International Con-\nference on Automated Software Engineering. EEE, 2009, pp. 712—\n715.\n\n$151] C. Marcos, S. Vidal, E. Abait, M. Arroqui, and S. Sampaoli,\n“Refactoring of a beef-cattle farm simulator,” IEEE Latin America\nTransactions, vol. 9, no. 7, pp. 1099-1104, 2011.\n\n$152] R. Khatchadourian and B. Muskalla, “Enumeration refactoring:\na tool for automatically converting java constants to enumerated\n', "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nS153.\n\n$154\n\nS155\n\nS156\n\n$157]\n\nS158\n\nS159\n\nS160\n\nS161\n\nS162\n\nS163.\n\nS164\n\nS165\n\nS166\n\n$167]\n\nS168\n\nS169\n\nS170\n\nS171\n\n$172\n\ntypes,” in Proceedings of the IEEE/ACM international conference on\nAutomated software engineering, 2010, pp. 181-182.\n\nE. L. Alves, M. Song, T. Massoni, P. D. Machado, and M. Kim,\n“Refactoring inspection support for manual refactoring edits,”\nIEEE Transactions on Software Engineering, vol. 44, no. 4, pp. 365—\n383, 2017.\n\nY. Yu, J. Jurjens, and J. Mylopoulos, “Traceability for the main-\ntenance of secure software,” in 2008 IEEE International Conference\non Software Maintenance. IEEE, 2008, pp. 297-306.\n\nC. Kulkarni, “Notice of violation of ieee publication principles a\nqualitative approach for refactoring of code clone opportunities\nusing graph and tree methods,” in 2016 International Conference\non Information Technology (InCITe)-The Next Generation IT Summit\non the Theme-Internet of Things: Connect your Worlds. IEEE, 2016,\npp. 154-159.\n\nA. E. Tappenden, T. Huynh, J. Miller, A. Geras, and M. Smith,\n“Agile development of secure web-based applications,” Inter-\nnational Journal of Information Technology and Web Engineering\n(IJITWE), vol. 1, no. 2, pp. 1-24, 2006.\n\nP. M. Cousot, R. Cousot, F. Logozzo, and M. Barnett, “An ab-\nstract interpretation framework for refactoring with application\nto extract methods with contracts,” in Proceedings of the ACM\ninternational conference on Object oriented programming systems\nlanguages and applications, 2012, pp. 213-232.\n\nP. Borba, “An introduction to software product line refactoring,”\nin International Summer School on Generative and Transformational\nTechniques in Software Engineering. Springer, 2009, pp. 1-26.\n\nO. Macek and K. Richta, “Application and relational database\nco-refactoring,” Computer Science and Information Systems, vol. 11,\nno. 2, pp. 503-524, 2014.\n\nM. S. Feather and L. Z. Markosian, “Architecting and gener-\nalizing a safety case for critical condition detection software an\nexperience report,” in 2013 1st International Workshop on Assurance\nCases for Software-Intensive Systems (ASSURE). IEEE, 2013, pp.\n29-33.\n\nP. Muntean, M. Monperrus, H. Sun, J. Grossklags, and C. Eck-\nert, “Intrepair: Informed repairing of integer overflows,” IEEE\nTransactions on Software Engineering, 2019.\n\nS. Demeyer, “Refactor conditionals into polymorphism: what's\nthe performance cost of introducing virtual calls?” in 21st IEEE\nInternational Conference on Software Maintenance (ICSM’05). IEEE,\n2005, pp. 627-630.\n\nA. Kumar, A. Sutton, and B. Stroustrup, “Rejuvenating c++ pro-\ngrams through demacrofication,” in 2012 28th IEEE International\nConference on Software Maintenance (ICSM). IEEE, 2012, pp. 98-\n107.\n\n——,, “The demacrofier,” in 2012 28th IEEE International Confer-\nence on Software Maintenance (ICSM). IEEE, 2012, pp. 658-661.\n\nI. Sora, “A meta-model for representing language-independent\nprimary dependency structures.” in ENASE, 2012, pp. 65-74.\n\nM. F. Zibran, R. K. Saha, M. Asaduzzaman, and C. K. Roy, “An-\nalyzing and forecasting near-miss clones in evolving software:\nAn empirical study,” in 2011 16th IEEE International Conference on\nEngineering of Complex Computer Systems. IEEE, 2011, pp. 295-\n304.\n\nR. Rolim, “Automating repetitive code changes using exam-\nples,” in Proceedings of the 2016 24th ACM SIGSOFT International\nSymposium on Foundations of Software Engineering, 2016, pp. 1063—\n1065.\n\nW. S. Evans, C. W. Fraser, and F. Ma, “Clone detection via\nstructural abstraction,” Software Quality Journal, vol. 17, no. 4, pp.\n309-330, 2009.\n\nA. Khan, H. A. Basit, S. M. Sarwar, and M. M. Yousaf, “Cloning\nin popular server side technologies using agile development:\nAn empirical study,” Pakistan Journal of Engineering and Applied\nSciences, no. 1, 2018.\n\nT. D. Oyetoyan, R. Conradi, and D. S. Cruzes, “Criticality of\ndefects in cyclic dependent components,” in 2013 IEEE 13th\nInternational Working Conference on Source Code Analysis and Ma-\nnipulation (SCAM). IEEE, 2013, pp. 21-30.\n\nM. Gatrell, S. Counsell, and T. Hall, “Empirical support for\ntwo refactoring studies using commercial c# software,” in 13th\nInternational Conference on Evaluation and Assessment in Software\nEngineering (EASE) 13, 2009, pp. 1-10.\n\nR. K. Saha, M. Asaduzzaman, M. F. Zibran, C. K. Roy, and K. A.\nSchneider, “Evaluating code clone genealogies at release level:\n\n20\n\nAn empirical study,” in 2010 10th IEEE Working Conference on\nSource Code Analysis and Manipulation. IEEE, 2010, pp. 87-96.\n\n$173] A. Dereziriska and M. Byczkowski, “Evaluation of design pat-\ntern utilization and software metrics in c# programs,” in Interna-\ntional Conference on Dependability and Complex Systems. Springer,\n2019, pp. 132-142.\n\n$174] Y. A. Liu, M. Gorbovitski, and S. D. Stoller, “A language and\nframework for invariant-driven transformations,” ACM Sigplan\nNotices, vol. 45, no. 2, pp. 55-64, 2009.\n\n$175] I. Lanc, P. Bui, D. Thain, and S. Emrich, “Adapting bioinfor-\nmatics applications for heterogeneous systems: a case study,”\nConcurrency and Computation: Practice and Experience, vol. 26, no. 4,\npp. 866-877, 2014.\n\n$176] L. E. d. S. Amorim, M. J. Steindorfer, S. Erdweg, and E. Visser,\n“Declarative specification of indentation rules: a tooling perspec-\ntive on parsing and pretty-printing layout-sensitive languages,”\nin Proceedings of the 11th ACM SIGPLAN International Conference\non Software Language Engineering, 2018, pp. 3-15.\n\n$177] Z. Chen, L. Chen, W. Ma, and B. Xu, “Detecting code smells\nin python programs,” in 2016 International Conference on Software\nAnalysis, Testing and Evolution (SATE). IEEE, 2016, pp. 18-23.\n\n$178] C. Chapman and K. T. Stolee, “Exploring regular expression us-\nage and context in python,” in Proceedings of the 25th International\nSymposium on Software Testing and Analysis, 2016, pp. 282-293.\n\n$179] J. B. Cabral, B. Sanchez, F. Ramos, S. Gurovich, P. M. Granitto,\nand J. Vanderplas, “From fats to feets: Further improvements\nto an astronomical feature extraction tool based on machine\nlearning,” Astronomy and computing, vol. 25, pp. 213-220, 2018.\n\n$180] M. Zhu, F. McKenna, and M. H. Scott, “Openseespy: Python\nlibrary for the opensees finite element framework,” SoftwareX,\nvol. 7, pp. 6-11, 2018.\n\n$181] M. Furr, J.-h. An, and J. S. Foster, “Profile-guided static typing\nfor dynamic scripting languages,” in Proceedings of the 24th ACM\nSIGPLAN conference on Object oriented programming systems lan-\nguages and applications, 2009, pp. 283-300.\n\n$182] Z. W. Bell, G. G. Davidson, T. M. D’Azevedo, W. Joubert, J. K.\nMunro Jr, D. R. Patlolla, and B. Vacaliuc, “Python for develop-\nment of openmp and cuda kernels for multidimensional data,”\nin Symposium on Application Accelerators in HPC, 2011.\n\n$183] Y. Hu, U. Z. Ahmed, S. Mechtaev, B. Leong, and A. Roy-\nchoudhury, “Re-factoring based program repair applied to pro-\ngramming assignments,” in 2019 34th IEEE/ACM International\nConference on Automated Software Engineering (ASE). IEEE, 2019,\npp. 388-398.\n\n$184] C. Wang, S. Hirasawa, H. Takizawa, and H. Kobayashi, “A\nplatform-specific code smell alert system for high performance\ncomputing applications,” in 2014 IEEE International Parallel &\nDistributed Processing Symposium Workshops. YEEE, 2014, pp. 652—\n661.\n\n$185] C. Biray and F. Buzluca, “A learning-based method for detect-\ning defective classes in object-oriented systems,” in 2015 IEEE\nEighth International Conference on Software Testing, Verification and\nValidation Workshops (ICSTW). IEEE, 2015, pp. 1-8.\n\n$186] W. Hasanain, Y. Labiche, and S. Eldh, “An analysis of complex\nindustrial test code using clone analysis,” in 2018 IEEE Interna-\ntional Conference on Software Quality, Reliability and Security (QRS).\nIEEE, 2018, pp. 482-489.\n\n$187] D. Mazinanian and N. Tsantalis, “An empirical study on the use\nof css preprocessors,” in 2016 IEEE 23rd International Conference\non Software Analysis, Evolution, and Reengineering (SANER), vol. 1.\nIEEE, 2016, pp. 168-178.\n\n, “Cssdev: refactoring duplication in cascading style\nsheets,” in 2017 IEEE/ACM 39th International Conference on Soft-\nware Engineering Companion (ICSE-C). IEEE, 2017, pp. 63-66.\n\n$189] D. Mazinanian, N. Tsantalis, and A. Mesbah, “Discovering refac-\ntoring opportunities in cascading style sheets,” in Proceedings of\nthe 22nd ACM SIGSOFT International Symposium on Foundations of\nSoftware Engineering, 2014, pp. 496-506.\n\n$190] D. D. Perez and W. Le, “Generating predicate callback sum-\nmaries for the android framework,” in 2017 IEEE/ACM 4th In-\nternational Conference on Mobile Software Engineering and Systems\n(MOBILESoft). IEEE, 2017, pp. 68-78.\n\n$191] S. Negara, M. Vakilian, N. Chen, R. E. Johnson, and D. Dig,\n“Is it dangerous to use version control histories to study source\ncode evolution?” in European Conference on Object-Oriented Pro-\ngramming. Springer, 2012, pp. 79-103.\n\n$188]\n", 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020 21\n\n$192] M. Bosch, P. Geneveés, and N. Layaida, “Reasoning with style,” $213] S. Schlesinger, P. Herber, T. Géthel, and S. Glesner, “Proving\n\nin Twenty-Fourth International Joint Conference on Artificial Intelli- correctness of refactorings for hybrid simulink models with\ngence, 2015. control flow,” in International Workshop on Design, Modeling, and\n$193] H. A. Nguyen, H. V. Nguyen, T. T. Nguyen, and T. N. Nguyen, Evaluation of Cyber Physical Systems. Springer, 2016, pp. 71-86.\n“Output-oriented refactoring in php-based dynamic web appli- [S214] S. Makka and B. Sagar, “Simulation of a model for refactoring\ncations,” in 2013 IEEE International Conference on Software Mainte- approach for parallelism using parallel computing tool box,” in\nnance. IEEE, 2013, pp. 150-159. Proceedings of First International Conference on Information and Com-\n$194] B. Chen, Z. M. Jiang, P. Matos, and M. Lacaria, “An industrial ex- munication Technology for Intelligent Systems: Volume 2. Springer,\nperience report on performance-aware refactoring on a database- 2016, pp. 77-84.\ncentric web application,” in 2019 34th IEEE/ACM International $215] V. Pantelic, S. Postma, M. Lawford, M. Jaskolka, B. Mackenzie,\nConference on Automated Software Engineering (ASE). IEEE, 2019, A. Korobkine, M. Bender, J. Ong, G. Marks, and A. Wassyng,\nPpp. 653-664. “Software engineering practices and simulink: bridging the gap,”\n$195] L. Eshkevari, F. Dos Santos, J. R. Cordy, and G. Antoniol, “Are International Journal on Software Tools for Technology Transfer,\nphp applications ready for hack?” in 2015 IEEE 22nd Interna- vol. 20, no. 1, pp. 95-117, 2018.\ntional Conference on Software Analysis, Evolution, and Reengineering [S216] H. Zhu, Y. Yu, W. Qi, S. Liu, Y. Weng, T. Yuan, and H. Li, “The\n(SANER). IEEE, 2015, pp. 63-72. research on fault restoration and refactoring for active distribu-\n$196] J. L. Overbey and R. E. Johnson, “Differential precondition tion network,” in 2019 Chinese Automation Congress (CAC). IEEE,\nchecking: A lightweight, reusable analysis for refactoring tools,” 2019, pp. 4470-4474. . | |\nin 2011 26th IEEE/ACM International Conference on Automated [S217] V. N. Leonenko, N. V. Pertsev, and M. Artzrouni, “Using high\nSoftware Engineering (ASE 2011). IEEE, 2011, pp. 303-312. performance algorithms for the hybrid simulation of disease\nS197] J. L. Overbey, R. E. Johnson, and M. Hafiz, “Differential pre- dynamics on cpu and gpu.” in ICCS, 2015, pp. 150-159. ,\ncondition checking: a language-independent, reusable analysis $218] S. Tichelaar, S. Ducasse, S. Demeyer, and Oz Nierstrasz, “A meta-\nfor refactoring engines,” Automated Software Engineering, vol. 23, model for language-independent refactoring,” in P voce edings In-\nno. 1, pp. 77-104, 2016. ternational Symposium on Principles of Software Evolution. IEEE,\n$198] M. Hills, P. Klint, and J. J. Vinju, “Enabling php software 2000, pp. 154-164. , ;\nengineering research in rascal,” Science of Computer Programming, $219] T. M. T. T. F Munoz, “Beyond the refactoring browser: Ad-\n\nvol. 134, pp. 37-46, 2017. vanced tool support for software refactoring,” 2003.\n\n$199] F. Gauthier, D. Letarte, T. Lavoie, and E. Merlo, “Extraction and S220] A. Garrido and R. Johnson, “Challenges of refactoring C Pro”\ncomprehension of moodle’s access control model: A case study,” grams," in Proceedings of the international workshop on Principles of\nin 2011 Ninth Annual International Conference on Privacy, Security software evolution, 2002, PP. 6-14. .\n\nand Trust. IEEE, 2011, pp. 44-51. $221] K. Mens and T. Tourwé, “Delving source code with formal con-\n$200] M. Hills and P. Klint, “Php air: Analyzing php systems with cept analysis,” Computer Languages, Systems & Structures, vol. 31,\nrascal,” in 2014 Software Evolution Week-IEEE Conference on Soft- 5222 vv 54 PP. Nhe 200% RE. Johnson, “D: dedi [\nware Maintenance, Reengineering, and Reverse Engineering (CSMR- ]  Y. Lee, N. Chen, and R. E. Johnson, “Drag-and- Pe\nWCRE). IEEE, 2014, pp. 454-457. toring: intuitive and efficient program transformation,” in 2013\n\n$201] Y. Yu, Y. Wang, J. Mylopoulos, S. Liaskos, A. Lapouchnian, and 35th International Conference on Software Engineering (ICSE). IEEE,\n\nJ. C. S. do Prado Leite, “Reverse engineering goal models from 2013, pp. 23-32.\n\nlegacy code,” in 13th IEEE International Conference on Requirements 8 Mimente with prowctive declanstve. mcta-progeammning Pe\nEngineering (RE’05). | IEEE, 2005, pp. 363-372. Proceedings of the International Workshop on Smalltalk Technologies,\n\n$202] R. Lammel and J. Visser, “A strafunski application letter,” in In- 2009, pp. 68-76.\n\nternational Symposium on Practical Aspects of Declarative Languages. $224] M. Unterholzner, “Improving refactoring tools in smalltalk\n\nSpringer, 2003, pp. 357-375. using static type inference,” Science of Computer Programmin\n$203] G. M. Rama, “A desiderata for refactoring-based software mod- vol. 56, pp. 70-83, 2014. ’ P 8 &\n\nularity improvement,” in Proceedings of the 3rd India software\n\nengineering conference, 2010, pp. 93-102.\n\n$204] T. Mens and T. Tourwé, “A survey of software refactoring,” IEEE\n\nTransactions on software engineering, vol. 30, no. 2, pp. 126-139,\n\n2004. ype preaica ‘\n\n$205] A. Abadi, R. Ettinger, and Y. A. Feldman, “Fine slicing,” in Domai languages, td oe see TOE ACM Symposium on\nInternational Conference on Fundamental Approaches to Software $227] Pp Tesone, G. Polito, L. Fabresse, N. Bouraqadi, and S. Ducasse,\n\nEngineering. Springer, 2012, pp. 471-485. . “Preserving instance state during refactorings in live environ-\n$206] M. Lillack, C. Bucholdt, and D. Schilling, “Detection of code ments,” Future Generation Computer Systems, 2020.\n\n$225] D. Vainsencher, “Mudpie: layers in the ball of mud,” Computer\nLanguages, Systems & Structures, vol. 30, no. 1-2, pp. 5-19, 2004.\n\n$226] O. Callati, R. Robbes, E. Tanter, D. Réothlisberger, and A. Bergel,\n“On the use of type predicates in object-oriented software: The\n\nclones in software generators,” in Proceedings of the 6th Interna- $228] V. Arnaoudova and C. Constantinides, “Adaptation of refac-\ntional Workshop on Feature-Oriented Software Development, 2014, pp. toring strategies to multiple axes of modularity: characteristics\n37-44. oo . and criteria,” in 2008 Sixth International Conference on Software\n$207] H. M. Sneed and K. Erdoes, “Migrating as400-cobol to java: a Engineering Research, Management and Applications. IEEE, 2008,\nreport from the field,” in 2013 17th European Conference on Software pp. 105-114.\nMaintenance and Reengineering. IEEE, 2013, pp. 231-240. $229] E. Rodrigues Jr, R. S. Durelli, R. W. de Bettio, L. Montecchi, and\n$208] M. K. Smith and T. Laszewski, “Modernization case study: R. Terra, “Refactorings for replacing dynamic instructions with\nItalian ministry of instruction, university, and research,” in In- static ones: the case of ruby,” in Proceedings of the XXII Brazilian\nformation Systems Transformation. Elsevier, 2010, pp. 171-191. Symposium on Programming Languages, 2018, pp. 59-66.\n$209] T. Hatano and A. Matsuo, “Removing code clones from indus- $230] P. Sommerlad, G. Zgraggen, T. Corbat, and L. Felber, “Retaining\ntrial systems using compiler directives,” in 2017 IEEE/ACM 25th comments when refactoring code,” in Companion to the 23rd\nInternational Conference on Program Comprehension (ICPC). IEEE, ACM SIGPLAN conference on Object-oriented programming systems\n2017, pp. 336-345. languages and applications, 2008, pp. 653-662.\n$210] T. Gerlitz, Q. M. Tran, and C. Dziobek, “Detection and handling $231] T. Corbat, L. Felber, M. Stocker, and P. Sommerlad, “Ruby\nof model smells for matlab/simulink models.” in MASE@ MoD- refactoring plug-in for eclipse,” in Companion to the 22nd ACM\nELS, 2015, pp. 13-22. SIGPLAN conference on Object-oriented programming systems and\n$211] Z. Zhao, X. Li, L. He, C. Wu, and J. K. Hedrick, “Estimation applications companion, 2007, pp. 779-780.\nof torques transmitted by twin-clutch of dry dual-clutch trans- [$232] R. Chen and H. Miao, “A selenium based approach to automatic\nmission during vehicle’s launching process,” IEEE Transactions test script generation for refactoring javascript code,” in 2013\non Vehicular Technology, vol. 66, no. 6, pp. 4727-4741, 2016. IEEE/ACIS 12th International Conference on Computer and Informa-\n$212] K. Aishwarya, R. Ramesh, P. M. Sobarad, and V. Singh, “Lossy tion Science (ICIS). IEEE, 2013, pp. 341-346.\nimage compression using svd coding algorithm,” in 2016 Interna- [S233] H. V. Nguyen, H. A. Nguyen, T. T. Nguyen, and T. N. Nguyen,\ntional Conference on Wireless Contmunications, Signal Processing and “Babelref: detection and renaming tool for cross-language pro-\n\nNetworking (WiSPNET). IEEE, 2016, pp. 1384-1389. gram entities in dynamic web applications,” in 2012 34th Interna-\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n$234\n\n$235\n\nS236\n\n$237]\n\n$238\n\n$239\n\n$240\n\nS241\n\n$242\n\nS243\n\n$244\n\nS245\n\nS246\n\nS247]\n\nS248\n\n$249\n\n$250\n\n$251\n\n$252\n\n$253.\n\ntional Conference on Software Engineering (ICSE).\n1391-1394.\n\nK. An and E. Tilevich, “D-goldilocks: Automatic redistribution\nof remote functionalities for performance and efficiency,” in 2020\nIEEE 27th International Conference on Software Analysis, Evolution\nand Reengineering (SANER). IEEE, 2020, pp. 251-260.\n\nC.-Y. Hsieh, C. Le My, K. T. Ho, and Y. C. Cheng, “Identification\nand refactoring of exception handling code smells in javascript,”\nJournal of Internet Technology, vol. 18, no. 6, pp. 1461-1471, 2017.\n\nT. Mendes, M. T. Valente, and A. Hora, “Identifying utility\nfunctions in java and javascript,” in 2016 X Brazilian Symposium\non Software Components, Architectures and Reuse (SBCARS). IEEE,\n2016, pp. 121-130.\n\nN. Van Es, Q. Stievenart, J. Nicolay, T. D’Hondt, and\nC. De Roover, “Implementing a performant scheme interpreter\nfor the web in asm. js,” Computer Languages, Systems & Structures,\nvol. 49, pp. 62-81, 2017.\n\nL. Gong, M. Pradel, and K. Sen, “Jitprof: pinpointing jit-\nunfriendly javascript code,” in Proceedings of the 2015 10th Joint\nMeeting on Foundations of Software Engineering, 2015, pp. 357-368.\n\nA. M. Fard and A. Mesbah, “Jsnose: Detecting javascript code\nsmells,” in 2013 IEEE 13th International Working Conference on\nSource Code Analysis and Manipulation (SCAM). IEEE, 2013, pp.\n116-125.\n\nC. Schuster, T. Disney, and C. Flanagan, “Macrofication: Refac-\ntoring by reverse macro expansion,” in European Symposium on\nProgramming. Springer, 2016, pp. 644-671.\n\nJ. Portner, J. Kerr, and B. Chu, “Moving target defense against\ncross-site scripting attacks (position paper),” in International Sym-\nposium on Foundations and Practice of Security. Springer, 2014, pp.\n85-91.\n\nG. Ortiz, J. A. Caravaca, A. Garcia-de Prado, J. Boubeta-Puig\net al., “Real-time context-aware microservice architecture for pre-\ndictive analytics and smart decision-making,” IEEE Access, vol. 7,\npp. 183 177-183 194, 2019.\n\nM. U. Khan, M. Z. Iqbal, and S. Ali, “A heuristic-based approach\nto refactor crosscutting behaviors in uml state machines,” in\n2014 IEEE International Conference on Software Maintenance and\nEvolution. IEEE, 2014, pp. 557-560.\n\nR. Terra, M. T. Valente, and N. Anquetil, “A lightweight re-\nmodularization process based on structural similarity,” in 2016\nX Brazilian Symposium on Software Components, Architectures and\nReuse (SBCARS). IEEE, 2016, pp. 111-120.\n\nM. Bialy, M. Lawford, V. Pantelic, and A. Wassyng, “A method-\nology for the simplification of tabular designs in model-based\ndevelopment,” in 2015 IEEE/ACM 3rd FME Workshop on Formal\nMethods in Software Engineering. IEEE, 2015, pp. 47-53.\n\nP. Langer, M. Wimmer, P. Brosch, M. Herrmannsdérfer, M. Seid],\nK. Wieland, and G. Kappel, “A posteriori operation detection\nin evolving software models,” Journal of Systems and Software,\nvol. 86, no. 2, pp. 551-566, 2013.\n\nA. T. Sampson, J. M. Bjorndalen, and P. S. Andrews, “Birds\non the wall: Distributing a process-oriented simulation,” in 2009\nIEEE Congress on Evolutionary Computation. IEEE, 2009, pp. 225—\n231.\n\nY. Wang, H. Yu, Z. Zhu, W. Zhang, and Y. Zhao, “Automatic\nsoftware refactoring via weighted clustering in method-level\nnetworks,” IEEE Transactions on Software Engineering, vol. 44,\nno. 3, pp. 202-236, 2017.\n\nA. Ouni, M. Kessentini, M. O Cinnéide, H. Sahraoui, K. Deb, and\nK. Inoue, “More: A multi-objective refactoring recommendation\napproach to introducing design patterns and fixing code smells,”\nJournal of Software: Evolution and Process, vol. 29, no. 5, p. 1843,\n2017.\n\nH. Wang, M. Kessentini, and A. Ouni, “Bi-level identification\nof web service defects,” in International Conference on Service-\nOriented Computing. Springer, Cham, 2016, pp. 352-368.\n\nA. Ghannem, G. El Boussaidi, and M. Kessentini, “On the use\nof design defect examples to detect model refactoring opportuni-\nties,” Software Quality Journal, vol. 24, no. 4, pp. 947-965, 2016.\n\nB. Amal, M. Kessentini, S. Bechikh, J. Dea, and L. B. Said, “On\nthe use of machine learning and search-based software engi-\nneering for ill-defined fitness function: a case study on software\nrefactoring,” in International Symposium on Search Based Software\nEngineering. Springer, Cham, 2014, pp. 31-45.\n\nM. Kessentini, A. Ouni, P. Langer, M. Wimmer, and S. Bechikh,\n\nIEEE, 2012, pp.\n\n22\n\n“Search-based metamodel matching with structural and syntactic\nmeasures,” Journal of Systems and Software, vol. 97, pp. 1-14, 2014.\n\n$254] M. Kessentini, R. Mahaouachi, and K. Ghedira, “What you like\nin design use to correct bad-smells,” Software Quality Journal,\nvol. 21, no. 4, pp. 551-571, 2013.\n\n$255] A. Ghannem, M. Kessentini, and G. El Boussaidi, “Detecting\nmodel refactoring opportunities using heuristic search,” in Pro-\nceedings of the 2011 Conference of the Center for Advanced Studies on\nCollaborative Research, 2011, pp. 175-187.\n\n$256] M. Boussaa, W. Kessentini, M. Kessentini, S. Bechikh, and S. B.\nChikha, “Competitive coevolutionary code-smells detection,”\nin International Symposium on Search Based Software Engineering.\nSpringer, Berlin, Heidelberg, 2013, pp. 50-65.\n\n$257] E. Erturk and E. A. Sezer, “A comparison of some soft comput-\ning methods for software fault prediction,” Expert systems with\napplications, vol. 42, no. 4, pp. 1872-1879, 2015.\n\n$258] C. S. Melo, M. M. L. da Cruz, A. D. F. Martins, T. Matos, J. M.\nda Silva Monteiro Filho, and J. de Castro Machado, “A practical\nguide to support change-proneness prediction,” 2019.\n\n$259] L. Kumar, S. M. Satapathy, and A. Krishna, “Application of\nsmote and Issvm with various kernels for predicting refactoring\nat method level,” in International Conference on Neural Information\nProcessing. Springer, 2018, pp. 150-161.\n\n$260] R. Hill and J. Rideout, “Automatic method completion,” in\nProceedings. 19th International Conference on Automated Software\nEngineering, 2004. IEEE, 2004, pp. 228-235.\n\n$261] S. Haiduc, G. Bavota, A. Marcus, R. Oliveto, A. De Lucia, and\nT. Menzies, “Automatic query reformulations for text retrieval\nin software engineering,” in 2013 35th International Conference on\nSoftware Engineering (ICSE). YEEE, 2013, pp. 842-851.\n\n$262] G. M. Ubayawardana and D. D. Karunaratna, “Bug prediction\nmodel using code smells,” in 2018 18th International Conference\non Advances in ICT for Emerging Regions (ICTer). IEEE, 2018, pp.\n70-77.\n\n$263] Z. Aliyu, L. A. Rahim, and E. E. Mustapha, “A combine usability\nframework for imcat evaluation,” in 2014 International Conference\non Computer and Information Sciences (ICCOINS). IEEE, 2014, pp.\n15.\n\n$264] A. Herranz and J. J. Moreno-Navarro, “Formal extreme (and\nextremely formal) programming,” in International Conference on\nExtreme Programming and Agile Processes in Software Engineering.\nSpringer, 2003, pp. 88-96.\n\n$265] L. Quan, Q. Zongyan, and Z. Liu, “Formal use of design patterns\nand refactoring,” in International Symposium on Leveraging Appli-\ncations of Formal Methods, Verification and Validation. Springer,\n2008, pp. 323-338.\n\n$266] J. W. Ko and Y. J. Song, “Graph based model transforma-\ntion verification using mapping patterns and graph comparison\nalgorithm,” International Journal of Advancements in Computing\nTechnology, vol. 4, no. 8, 2012.\n\n$267] T. Ruhroth and H. Wehrheim, “Model evolution and refine-\nment,” Science of Computer Programming, vol. 77, no. 3, pp. 270-—\n289, 2012.\n\n$268] S. Stepney, F. Polack, and I. Toyn, “Patterns to guide practical\nrefactoring: examples targetting promotion in z,” in International\nConference of B and Z Users. Springer, 2003, pp. 20-39.\n\n$269] T. v. Enckevort, “Refactoring uml models: using openarchitec-\ntureware to measure uml model quality and perform pattern\nmatching on uml models with ocl queries,” in Proceedings of\nthe 24th ACM SIGPLAN conference companion on Object oriented\nprogramming systems languages and applications, 2009, pp. 635-646.\n\n$270] D. Luciv, D. Koznov, H. A. Basit, and A. N. Terekhov, “On fuzzy\nrepetitions detection in documentation reuse,” Programming and\nComputer Software, vol. 42, no. 4, pp. 216-224, 2016.\n\n$271] D. Arcelli, V. Cortellessa, and C. Trubiani, “Performance-based\nsoftware model refactoring in fuzzy contexts,” in International\nConference on Fundamental Approaches to Software Engineering.\nSpringer, 2015, pp. 149-164.\n\n$272] C. Wang and S. Kang, “Adfl: An improved algorithm for ameri-\ncan fuzzy lop in fuzz testing,” in International Conference on Cloud\nComputing and Security. Springer, 2018, pp. 27-36.\n\n$273] P. Lerthathairat and N. Prompoon, “An approach for source\ncode classification using software metrics and fuzzy logic to\nimprove code quality with refactoring techniques,” in Interna-\ntional Conference on Software Engineering and Computer Systems.\nSpringer, 2011, pp. 478-492.\n', 'IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n$274] Z. Avdagic, D. Boskovic, and A. Delic, “Code evaluation us-\ning fuzzy logic,” in Proceedings of the 9th WSEAS International\nConference on Fuzzy Systems. World Scientific and Engineering\nAcademy and Society (WSEAS), 2008, pp. 20-25.\n\nH. Liu, J. Jin, Z. Xu, Y. Bu, Y. Zou, and L. Zhang, “Deep learning\nbased code smell detection,” IEEE Transactions on Software Engi-\nneering, 2019.\n\nY. Wang, “What motivate software engineers to refactor source\ncode? evidences from professional developers,” in 2009 IEEE\nInternational Conference on Software Maintenance. IEEE, 2009, pp.\n413-416.\n\nJ. Grigera, A. Garrido, and G. Rossi, “Kobold: web usability\nas a service,” in 2017 32nd IEEE/ACM International Conference on\nAutomated Software Engineering (ASE). IEEE, 2017, pp. 990-995.\n\nM. W. Mkaouer, M. Kessentini, S. Bechikh, K. Deb, and\nM. O Cinnéide, “Recommendation system for software refactor-\ning using innovization and interactive dynamic optimization,”\nin Proceedings of the 29th ACM/IEEE international conference on\nAutomated software engineering, 2014, pp. 331-336.\n\nV. Alizadeh and M. Kessentini, “Reducing interactive refactor-\ning effort via clustering-based multi-objective search,” in Proceed-\nings of the 33rd ACM/IEEE International Conference on Automated\nSoftware Engineering, 2018, pp. 464-474.\n\n$275\n\nS276\n\n$277]\n\nS278\n\n$279\n\nChaima Abid is currently a PhD student in the\nintelligent Software Engineering group at the\nUniversity of Michigan. Her PhD project is con-\ncerned with the application of intelligent search\nand machine learning in different areas such as\nweb services, refactoring and security. Her cur-\nrent research interests are Search-Based Soft-\nware Engineering, web services, refactoring, se-\ncurity, data analytics and software quality.\n\nVahid Alizadeh is currently a Ph.D. student in\nthe intelligent Software Engineering group at the\nUniversity of Michigan. His Ph.D. project is con-\ncerned with the application of intelligent search\nand machine learning in different software engi-\nneering areas such as refactoring, testing, and\ndocumentation. His current research interests\nare Search-Based Software Engineering, Refac-\ntoring, Artificial Intelligence, data analytics and\nsoftware quality.\n\nMarouane Kessentini is a recipient of the pres-\ntigious 2018 President of Tunisia distinguished\nresearch award, the University distinguished\nteaching award, the University distinguished dig-\nital education award, the College of Engineering\nand Computer Science distinguished research\naward, 4 best paper awards, and his Al-based\nsoftware refactoring invention, licensed and de-\nployed by industrial partners, is selected as one\nof the Top 8 inventions at the University of Michi-\ngan for 2018 (including the three campuses),\namong over 500 inventions, by the UM Technology Transfer Office. He\nis currently a tenured associate professor and leading a research group\non Software Engineering Intelligence. Prior to joining UM in 2013, He\nreceived his Ph.D. from the University of Montreal in Canada in 2012.\nHe received several grants from both industry and federal agencies and\npublished over 110 papers in top journals and conferences. He has\nseveral collaborations with industry on the use of computational search,\nmachine learning and evolutionary algorithms to address software engi-\nneering and services computing problems.\n\n23\n\nThiago do Nascimento Ferreira is a Post-\ndoctoral Researcher at University of Michigan-\nDearborn under the supervision of Dr. Marouane\nKessentini in the ISELab. He received my PhD\nDegree in Computer Science from the Fed-\neral University of Parana in 2019. His research\nmainly focuses on the use of Preference and\nSearch Based Software Engineering to address\nseveral software engineering problems such as\nSoftware Testing and Software Refactoring.\n\n>:\n\nDanny Dig is an associate professor of com-\nputer science at the University of Colorado,\nand an adjunct professor at University of Illinois\nand Oregon State. He successfully pioneered\ninteractive program transformations by opening\nthe field of refactoring in cutting-edge domains\nincluding mobile, concurrency and parallelism,\ncomponent-based, testing, and end-user pro-\ngramming. He earned his Ph.D. from the Univer-\nsity of Illinois at Urbana-Champaign where his\nresearch won the best Ph.D. dissertation award,\nand the First Prize at the ACM Student Research Competition Grand\nFinals. He did a postdoc at MIT. He (co-)authored 50+ journal and\nconference papers that appeared in top places in SE/PL. According\nto Google Scholar his publications have been cited 4000+ times. His\nresearch was recognized with 8 best paper awards at the flagship and\ntop conferences in SE, 4 award runner-ups, and 1 most influential paper\naward (N-10 years) at ICSME’15. He received the NSF CAREER award,\nthe Google Faculty Research Award (twice), and the Microsoft Software\nEngineering Innovation Award (twice). He released 9 software systems,\namong them the world’s first open-source refactoring tool. Some of the\ntechniques he developed are shipping with the official release of the\npopular Eclipse, NetBeans, and Visual Studio development environ-\nments (of which Eclipse alone had more than 14M downloads in 2014).\n']}


**File**: S:\OneDrive\@Dev\!GPT\ScriptGPT\library\Refactoring\Source\A Survey of Deep Learning Based Software Refactoring'.pdf
- Time Taken: 82.14s
- Data Extracted: {'text': ['A Survey of Deep Learning Based Software Refactoring\n\nBRIDGET NYIRONGO, Beijing Institute of Technology, China\nYANJIE JIANG’, Peking University, China\nHE JIANG, Dalian University of Technology, China\n\nHUI LIU, Beijing Institute of Technology, China\n\nRefactoring is one of the most important activities in software engineering which is used to improve the quality (especially the\nmaintainability) of a software system. The traditional approaches to software refactoring involve designing a series of heuristics for\nrefactoring detection, solution suggestions, and refactoring execution. However, these approaches usually employ manually designed\nheuristics, which are often tedious, time-consuming, and challenging. With the advancement of deep learning techniques, researchers\nare attempting to apply deep learning techniques to software refactoring. Consequently, dozens of deep learning -based refactoring\napproaches have been proposed. However, there isa lack ofcomprehensive reviews on such works as well asa taxonomy for deep\nlearning-based refactoring. Tothis end, in this paper, we presenta survey ondeeplearning-based software refactoring. Weclassify\nrelated works into five categories according to the major tasks they cover, i.e., the detection of code smells, the recommendation of\nrefactoring solutions, the end-to-end code transformation as refactoring, quality assurance, and the mining ofrefactorings. Among\nthese categories, we further present key aspects (i.e.,code smell types, refactoring types, training strategies, and evaluation) to give\ninsight into the details of the technologies that have supported refactoring through deep learning. The classification indicates that\nthere is an imbalance in the adoption of deep learning techniques for the process of refactoring. Most of the deep learning techniques\nhave been used for the detection of code smells and the recommendation of refactoring solutions as foundin 56.25% and 33.33% of\nthe literature respectively. In contrast, only 6.25% and 4.17% were towards the end-to-end code transformationas refactoringand\nthe mining of refactorings, respectively. Notably, we found no literature representation for the quality assurance for refactoring. We\nalso observe that most of the deep learning techniques have been used to support refactoring processes occurring at the method\nlevel whereas classes and variables attracted minimal attention. Finally, we discuss the challenges and limitations associated with the\n\nemployment of deep learning-based refactorings and present some potential research opportunities for future work.\n\nCCS Concepts: + Software and its engineering — Software creation and management; - Software post-development issues; -\nSoftware quality assurance;\n\nAdditional Key Words and Phrases: Software refactoring, Deep learning, Code smells\n\n1 INTRODUCTION\n\nSoftware refactoring is used to improve software quality by changing the internal structure of a system software\nwithoutaltering its external behavior. Itisa way of maintaining andimproving the quality ofsoftware code thathelps\nin the following tasks [26]. First, ithelps in the discovery of bugs. When refactoring, more time and work are spent\n\non understanding what the program code does and incorporatingany new understanding into thecode. This process\n\n‘Corresponding author\n\nAuthors’ addresses: Bridget Nyirongo, larjean89 @outlook.com, Beijing Institute of Technology, Beijing, China; Yanjie Jiang, yanjiejiang@pku.edu.cn,\nPeking University, Beijing, China; He Jiang, jianghe @dlut. edu.cn, Dalian University of Technology, Dalian, China; Hui Liu, liuhui08@bit.edu.cn, Beijing\nInstitute of Technology, Beijing, China.\n', '2 Nyirongo and Jiang, etal.\n\nhelps to bring to light any assumptions that were previously made, making it less likely that bugs will go unnoticed.\nSecond, refactoring can help to improve the software design. With the development of software, various modifications\nmay be introduced due to misunderstanding of requirements or urgent task assignments. After such modifications,\nthe software will become harder to read and comprehend. Refactoring cleans up the program code as workis done to\nrearrange and remove parts that are not in order and are unnecessary. This helps to retain the program code’s structure\nthereby improving its design. Third, it helps in rapid software development. Refactoring helps in the faster development\nof software because it stops the design of the system from going bad. With frequent refactoring, not much time is\nspent on finding and fixing errors that might arise from poor code design. Fourth, it helps make software easier to\nunderstand. This is because a little more time spent on refactoring could make the code better communicate its purpose.\nThis code would say exactly what the developer writing it meant, as such any other developer using this code might\neasily understand it.\n\nResearchers have dedicated a great amount of time trying to find ways that could make the process of software refactoring\nless tedious and time-consuming. Different techniques, models, and concepts have been used. Semi-automatic and\nautomatic tools [22,86,89,91, 92,94, 100] have been developed to assist in the detection, recommendation, and safe\napplication ofthe refactorings. Most ofthese tools can easily be integrated as plugins in most of the modern IDEslike\nEclipse and Intellij IDE. The integration of these tools in the IDEs is usually alongside the already existing refactoring\nmenus within the IDEs thereby enriching the support rendered towards the process of refactoring. Even though this is\nthe case, it has been noted that most of the approaches used to come up with these tools rely on heuristics which are\nmanual in nature [51, 53]. Thus, researchers have adopted the use of machine learning techniques to minimize the use\nof manually designed heuristics [23, 25].\n\nDeep learning is a sub-field of machine learning that focuses on creating large neural network models that are capable\nof making accurate data-driven decisions. Deep learning is mostly suitable for contexts where data is complex and\nwhere large datasets are available. The unique aspect of deep learning is the approach it takes to feature design which is\ncharacterized by the automatic learning of hierarchical representations from raw data thereby eliminating the need for\nmanual feature engineering. Deep learning models can learn useful features from low-level raw data and complex non-\nlinear mappings from inputs and outputs [29, 43]. This is unlike most of the statistical machine learning models where\nfeature design is ahuman-intensive task that can require deep domain expertise and consumea lot of time and resources.\nAt the core of deep learning are Artificial Neural Networks (ANN or NN) which are composed of interconnected\nnodes organized into layers [1, 12,78]. Deep learning uses several types of Neural Networks each tailored for specific\ntasks. Some of the most common ones are as follows. Feedforward Neural Networks(FNN) also known as Multilayer\nPerceptrons(MLP). These are mostly used for general-purpose tasks including classification and regression. Convolutional\nNeural Networks (CNN). CNN utilizes convolutional layers to automatically learn hierarchical features from input images.\nThese were originally designed for image and grid-like data. Recurrent Neural Networks (RNN).RNN contain loops to\ncapture dependencies in sequential data. Two ofits variants, Long Short Term Memory (LSTM) and Gated Recurrent\nUnit (GRU) address the vanishing gradient problem and improve the modeling of long-range dependencies. Graph\nNeural Networks (GNN). GNN learns to process and extract information from graph-structured inputs. These are used\nin tasks like node classification, link prediction, and recommendation systems. Generative Adversarial Networks(GAN).\nGAN consists ofa generator networkanda discriminator network that are trained simultaneously. These are used for\ngenerating new data samples, such as images, text, and music. Autoencoders. Autoencoders are neural networks that are\n\nused for unsupervised learning and dimensionality reduction. Autoencoders comprise an encoder network to reduce\n', 'ASurvey of Deep Learning Based Software Refactoring 3\n\nthe input data’s dimensionality and a decoder network to reconstruct the input data from the reduced representation.\nTransformers. Transformers use a self-attention mechanism to capture relationships between input elements. They are\npopularised by models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative\npretrained Transformer) for NLP tasks, but they have been applied to various othertasks.\n\nRecently, a considerable amount of research [32, 51, 53] has been proposed to investigate and explore the application\nand adoption of deep learning techniques to automate and support the process of software refactoring. Researchers\nhave employed various deep learning models in the different tasks involved in the process of software refactoring.\nTo present the state-of-the-art on the employment of deep learning in refactoring, researchers conducted literature\nreviews [2,56,63, 110] centering around the use of deep learning for refactoring activities, e.g., the detection of code\nsmells. Naik etal. [63] presented and analyzed 17 related works published from 2016 to 2022 to identify which deep\nlearning techniques had been used for code refactoring and how well they worked. Alabza et al. [2] conducted a\nsystematic review focusing on the deep learning approaches for bad smell detection on 67 studies published until\nOctober 2022. Malhotra etal. [56] conducted a systematic literature review examining deep learning’s capability to\nspot code smells on 35 primary studies published from 2013 to July 2023. Zhang etal. [110] conducted a survey on\ncode smell detection based on supervised learning models by analyzing 86 studies published from 2010 to April 2023.\nAlthough such reviews have significantly facilitated the understanding of deep learning-based refactoring, we still\nlack a comprehensive survey that considers the majority of related works, covering all aspects of deep learning-based\nsoftware refactoring (not just confined to code smell detection), and provides a taxonomy for deep learning-based\nrefactoring.\n\nTo this end, in this paper, we conduct a survey by collecting 48 primary studies published from January 2018 to October\n2023 and classify them based on the specific refactoring tasks being supported by the deep learning technique, i.e.,\nthe detection of code smells, the recommendation of refactoring solutions, the end-to-end code transformation as\nrefactoring, quality assurance, and the mining of refactorings. Under each of these categories, we present key aspects (i.e.,\ncode smell types, refactoring types, training strategies) related to theapproaches togive insight into the technologies\nthat have supported software refactoring through deep learning. Based on such a presentation, we can provide a more\ncomprehensive perspective on deep learning-based refactoring. To the best of our knowledge, it is the first survey paper\nthat presents the hierarchical taxonomy for deep learning-based software refactoring. In our survey, we attempt to\ninvestigate the following research questions:\n\n+ RQ1: Which tasks in software refactoring have been supported by deep learning techniques, and how often\nthey have been targeted by the surveyed papers?\n\n¢ RQ2: What are the common deep learning techniques used in software refactoring?\n¢ RQ3: How effective is the use of deep learning models in the process of software refactoring?\n\n* RQ4: Whatarethe limitations and challenges associated with the use of deeplearningtechniquesinsoftware\nrefactoring?\n\nOur classification indicates that there is an imbalance in the refactoring tasks which have been supported by deep\nlearning techniques. Our survey indicates that most of it has been towards the detection of code smells and the\nrecommendation of refactoring solutions, unlike the end-to-end code transformation as refactoring, quality assurance,\n', '4 Nyirongo and Jiang, etal.\n\nand mining of refactorings. Thus, there is aneed for more future workto address the currentimbalance, specifically in\nareas of deep learning supporting the application, quality assurance, and mining ofrefactorings. The rest of the paper\nis structured as follows: Section 2 introduces related work. Section 3 presents the methodology used for the survey.\nSection 4 presents and discusses studies on the detection of code smells. Section 5 presents and discusses studies on the\nrecommendation of refactoring solutions. Section 6 presents and discusses studies on end-to-end code transformation\nas refactoring. Section 7 presents and discusses studies on the mining ofrefactorings. Section 8 discussessome of the\n\nchallenges and opportunities associated with deep learning-based refactoring, and section 9 concludes the survey.\n\n2 RELATED WORK\n\nNaik et al. [63] conducted a systematic review of the current studies on deep learning-based code refactoring. The\nsurvey presented a high-level analysis of 17 primary works published from 2016 to 2022. The key insight of this survey\nwas to present the state-of-the-art in deep learning-based code refactoring. The review addressed research questions\nwhose main focus was on the commonly used deep learning techniques and the performance of the deep learning-based\nrefactoring approaches. This reviewindicated thatCNN, RNN, andGNNarethe commonly used deep learning models\nfor code refactoring with Multilayer Percepton (MLP) performing the best. They also noted that most of the existing\nstudies focus on Java code, method-level refactoring, and single-language refactoring with various evaluation methods.\nCompared to the survey by Naik et al. [63], our survey covers substantially more related works, increasing the number\nofsurveyed papers from 17 to48. Our well-designed search strategy retrieved many closely related papers missed by\ntheir survey. Another difference is that we present a comprehensive and hierarchical taxonomy of deep learning-based\nrefactoring, and classify all related works based on the taxonomy.\n\nAlabza et al. [2] conducted a review on deep learning-based approaches for bad smell detection, and their focus was to\nsummarise and synthesize the studies that used deep learning for bad smell detection. They collected and analyzed 67\nstudies until October 2022. They analyzed deep learning models concerning the purpose of the model, the detected bad\nsmells, the employed training datasets, features, pre-processing techniques, and encoding techniques used for feature\ntransformation. Notably, this survey involved all kinds of code smells. This review indicated that code clonesare the\nmost recurring smell. The review showed that supervised learning is the most adopted learning approach used for\ndeep learning-based code smell detection. Also, they observed that CNN, RNN, DNN, LSTM, Attention models, and\nAutoencodersare the most popularly used deep learning models. Notably, this review focused only onthe use of deep\nlearning models for the detection of code smells which is one of the tasks used for the identification of refactoring\n\nopportunities. In contrast, our survey covers all aspects of refactoring.\n\nMalhotra et al. [56] conducted a literature review examining deep learning’s capability to spot code smells. They\npresented a total of 35 primary study works from the years 2013 to 2023. The consolidated studies highlighted four key\nconcepts, that is, the types of code smells addressed, the deep learning approach utilized in the experiment, evaluation\nstrategies employed in the studies, and the performance analysis of the model proposed. This review showed that the\nmost common code smells detected include feature envy, god class, long method, complex class, and large class. It\nalso indicated that the most common deep learning algorithms used are RNN and CNN, often combined with other\ntechniques for better results. Notably, this systematic analysis did not focus on the whole refactoring process (as what we\ndo). Instead, they only focused on the recommendation of refactoring solutions and the end-to-end code transformation\n\nas refactoring.\n', 'ASurvey of Deep Learning Based Software Refactoring 5\n\nZhang et al. [110] conducteda survey on code smell detection based on supervised learning models. They surveyed\n86 papers from January 2010 to April 2023. They formulated a total of 7 research questions which were empirically\nevaluated from different aspects such as dataset construction, data preprocessing, feature selection, and model training.\nBased on their analysis they concluded that most of the existing works suffer from issues such as sample imbalance,\ndifferent attention to types of code smell, and limited feature selection. They also made suggestions for future work,\none of which involves exploring the correlation between features and the perspective of code smells within the context\nof model interpretability. Notably, the core focus of this paper was on the types of deep learning models used for the\ndetection of code smells, and not the use of deep learning models in the process and support of software refactoring.\nOur paper differs from such reviews in that they focus on code smell detection only whereas we covera much larger\nscope, i.e., the whole process of software refactoring.\n\n3 METHODOLOGY\n\nWe surveyed deep learning-based software refactoring by exploring and searching the following databases: https:\n//dl.acm.org, http://ieeexplore.org, https://springer.com, https://sciencedirect.com, https://onlinelibrary.wiley.com/,\nand https://scholar.google.com. Weused these databases since they containa comprehensive coverage ofacademic\npublications, conferences, and journals in the field of software engineering. Utilizing these databases ensured a thorough\nand inclusive exploration of the existing literature, enabling a comprehensive understanding of the landscape of deep\nlearning-based refactoring techniques and advancements. We used a set of keywords based on the research questions\n\noutlined in Section 1 to retrieve studies from these databases. The main keywords were "deep learning", "software\nrefactoring", and "code smells". We also included synonyms of these keywords, such as "refactoring", "code refactoring",\nand "bad smells". These search terms were used with advanced search filters within the databases(e.g., specific year\nrange (2018 to 2023) and language specification (English)). For example, on ACM, we used the following search query:\n"query": (deep learning") AND ("refactoring"), "filter": publication date: 01/01/2018 TO 10/31/2023, owners.owner=HOSTED.\nWe obtained atotal of 1,755 papers after the filtering, and selected the top 100 (ifthere are more than 100) fromeach\ndatabase based on relevance, resulting in 486 papers. The selection of studies was limited to the top 100 papers from\neach database based on relevance because our initial analysis suggests that items outside the top 100 are often outside\nthe scope of the survey.\n\nThe firstauthor conducted a manual search to decide whether each papershould be included or not. This was done by\nreading through the title and abstract of each candidate paper. To ensure the correctness of the manual search, inclusion\ncriteria were defined, and continuous and open communication was maintained among the authors. The main criterion\nfor the classification was focused on papers that used deep learning for refactoring tasks, such as detecting code smells,\nrecommending refactorings, and the end-to-end code transformation as refactoring. The use of this criterion resulted\nin remaining with a total of 35 papers. To enhance the comprehensiveness of our literature search, we conducted a\nsnowballing process by scrutinizing the reference sections of the initial set of papers. This iterative process involved\nexploring citations within these papers to identify additional relevant studies. Google Scholar was utilized as the primary\ndatabase for this snowballing process, ensuring an expansive and thorough exploration. As a result of this snowballing\nprocedure, we identified and included 13 more papers that were deemed pertinent to our survey focus. Combining these\nnewly discovered papers with the initial set, a total of 48 papers were meticulously collected and considered as primary\nstudies for our survey. This approach allowed us to cast a wider net in the literature search, ensuring the inclusion\nof studies that may not have been initially captured, thereby enriching the depth and scope of our survey. Figure 1\n', '6 Nyirongo and Jiang, etal.\n\n10 al\n3\ni] 8\n2\ng\n4\ng 6\n5\na\nS\n3 4\n2\n£\nZz\n\n2\n\n2018 2019 2020 2021 2022 2023\nYears\n\nFig. 1. Primary studies through the years\n\npresents the papers on deep learning-based refactoring from January 2018 to October 2023. The figure shows that there\nis a growing interest among researchers to adopt deep learning techniques for software refactoring.\n\nFrom the collected data we have developed a taxonomy classification hierarchy as presented in Figure 2. The classification\nhierarchy for the taxonomy is based on the software refactoring tasks that could be supported by deep learning\ntechniques. These tasks include the detection of code smells, recommendation of refactoring solutions, end-to-end code\ntransformation as refactoring, quality assurance, and the mining of refactorings. Figure 2 presents the total number\nof papers collected for each specific task (as presented onthe lower right corners of the nodes). Our survey indicates\nthat there is an imbalance in the adoption of deep learning techniques for the process of refactoring. Most ofthe deep\nlearning techniques have been used to detect code smells and recommend refactoring solutions as found in 56.25% and\n33.33% of the literature respectively. In contrast, only 6.25% and 4.17% were towards the end-to-end code transformation\nas refactoring and the mining of refactorings, respectively. Notably, we found no literature representation for the\nquality assurance for refactoring. We have also observed that some researchers made contributions to more than one\nrefactoring task in the use of deep learning. For instance, 22.45% of the studies focused on applying deep learning\ntechniques on more than onerefactoring task, i.e., the detection of code smells and recommendations for refactoring\nsolutions. For such papers, we discuss them in different sections regarding their contribution to the different tasks. As\nsuggested by the legend in the left bottom of Figure 2, the leftmost node presents the root of the taxonomy, and the\nnodes in the following layer represent different steps involved in software refactoring. Nodes in the rightmost two\nlayers represent differentaspects (factors) that could be employed to further classify related works within the given\ncategory. For example, the "code smell types" node on the third layer suggests that we may classify related works on\n\n“detection of codesmells" into sub-categories according to the code smells’ types they support.\n', 'ASurvey of Deep Learning Based Software Refactoring 7\n\nime Result Metrics\n\nEvaluation\n\n| Datasets\n\nTraining Strategies\n\n[~ Detection of | _,| __Explainable and\n| L_Code smelis:27 [ Code Smell Types Feedback Centric\n\nt+| Hybrid Approach\n\nDetection Technologies\n\n}+| Graph Based\n\nL,|_ Sequence Modellin\n\nDatasets\nDeep Learning ; - | Result Metrics\nBased Recommendation Evaluation\nRefactoring:48 of Refactoring [Training Strategies\n9 Solutions:16 -\n_| Recommendation\n++ End to End Code Technologies\nTransformation\nas Refactoring:3\nt> Quality As-\nMining of Root Node of Taxonomy\n\nRefactorings:2\n\nRefactoring Tasks\n\nL,| Aspects /Sub-aspects for classification\n\nFig. 2. Taxonomy of deep learning-based refactoring\n\n4 DETECTION OF CODE SMELLS\n\nRefactoring is a crucial task in software engineering. To conduct refactoring, there needs to be awareness of the issues\nin the code that might call for the process of refactoring. One of the most common issues that trigger the need for\nrefactoring is code smells. Code smells are certain structures in the code that indicate the possibility of refactoring [26].\nDetecting code smells manually ona large codebase is a challenging task. Therefore, researchers have explored different\ntechniques, including automatic and semi-automatic ones, to aid in the detection of code smells. Despite various\ntechniques proposed by researchers, most of them rely on manually designed heuristics. The manual heuristics make\nthe process of detecting code smells for refactoring opportunities time-consuming. To solve this issue, researchers\nhave started exploring deep-learning techniques for the detection of code smells. Thus, researchers have proposed\nvarious deep-learning techniques for the detection of code smells. In this section, we discuss and presentthese deep\nlearning techniques based on the detection technologies, code smell types, training strategies, and evaluation techniques\nemployed.\n', '8 Nyirongo and Jiang, etal.\n\n4.1 Detection Technologies\n\nResearchers [51, 53, 80, 109] have used various deep-learning approaches for the detection of code smells. The details of\nthese approaches are presented as follows:\n\n4.1.1 Sequence Modelling Based Approaches. This category pertains to technological approaches that focus on the\nsequence of code, such as source code tokens or characters. The approaches in this group utilize deep learning\ntechniques to capture contextual dependencies within code snippets. Also, these approaches detect code smells based on\nthe sequential nature of code. CNN, RNN, LSTM, Transformers, etc. are some of the deep learning techniques employed\ninthis category. The deep learning techniques in this category can effectively learn patterns and relationships within\ncode sequences, thus aiding in the process of code smell detection.\n\nLiu etal. [53] proposed a deep learning-based approach to detecting feature envy, which is one of the most common\ncode smells. They used a Convolutional Neural Network (CNN) as their deep neural network-based classifier. The\nclassifier’s input was divided into two parts: textual input and numerical input. The textual input was a word sequence\nthat consisted of the method’s name, the name of its enclosing class, and the name of the potential target class. The\ninformation in the textual input had to pass through an embedding layer that converted the text description into\nnumerical vectors. The numerical vectors were then fed into the CNN. They had three CNN layers, each with filters=128,\nkernel size=1, and activation Tanh. The CNN classifier was trained automatically, without any human intervention. The\nlabeled samples for training were generated automatically based on open-source applications. To evaluate the approach,\nthey conducted a two-part verification process. First, they evaluated the approach on 7 well-known open-source\napplications (Junit, PMD, JExcelAPI, Areca, Freeplane, jEdit, and Weka) without automatically injecting feature envy\nsmells. This first evaluation gave an improvement on F-measure against state-of-the-art by 34.32%. The second part of\nthe evaluation was carried out on 3 open-source applications (XMD, JSmooth, and Neuroph) where no smells were\ninjected, and this outperformed the state-of-the-art.\n\nLiu et al. [21] proposed a method called feTruth to enhance the detection of feature envy in software using deep learning\n\nwith real-world examples. The feTruth technique used evolutionary histories of open-source projects stored in version\ncontrol systems such as GitHub to extract real examples of feature envy. The extracted real-world examples were then\nused to train a deep learning-based prediction model. During the testing phase, feTruth would examine the source code\nofa software project and generate a list of feature envy occurrences associated with methods in the project. feTruth\nincluded a heuristics-based filter and a learning-based filter. These two filters were used to exclude false positives\nreported by RefactoringMiner [93]. The heuristic-based filter would exclude false positives if the source class of the\npotential refactoring did not exist in the new version or if the target class of the potential refactoring did not existin\nthe oldversion. The learning-based filter leveraged a decision tree-based classifier to distinguish false positives from\ntrue positives based on a sequence of features of the refactorings. By using these techniques, the researchers were able\nto generate high-quality and large-scale training data for feature envy detection. They useda CNNin the design forthe\ndeep learning-based classifier. The CNN classifier leveraged new features not yet exploited by existing approaches.\nThe feTruth method was compared against Liu’s approach [51], JDeodorant [22], and JMove [89]. The subjects for\ntheir projects were divided into two parts. The first part consisted of 500 Java projects, which were used to discover\nreal-world examples of feature envy. These projects were collected from GitHub by selecting the top 500 most popular\nprojects with the largestnumber of stars. The second part consisted of 5 open-source Java projects, which were used\nto evaluate the proposed approachand the selected baseline. These 5 projects were chosen from Defects4J [42].The\n', 'ASurvey of Deep Learning Based Software Refactoring 9\n\nevaluation results on real-world open-source projects suggested that the proposed approach substantially outperforms\nthestate-of-the-artin the detection of feature envy smells. The approachimproves the precision and recallin feature\nenvy detection by 38.5%.\n\nDas et al. [18] proposed a deep learning approach to detect brain class and brain method code smells in software\napplications. They used Convolutional Neural Networks (CNN) to train a neural network-based classifier. The approach\nwas based ona large corpus of software applications, which generated a huge number of training samples. These\nsamples were labeled to indicate whether they were a code smell of kind brain class and brain method. The neural\nnetwork used in this approach had several layers - the first layer had a one-dimensional CNN layer with 256 filters and\nakernel size of 1. The activation function used wasTanh. Thesecond layer wasalsoa one-dimensional CNN layer with\n128 filters and Tanh as the activation function. They added a flattened layer as the third layer to connect the convolution\nlayer with dense layers. The fourth layer had a dense layer with 128 filters and ReLU as the activation function. The\nlastlayer was a dense layer with only one filter and Sigmoid as the activation function. This layer acted as the output\nlayer. They used 30 open-source Java Projects as subject applications, acquired through sharing activities in GitHub\nrepositories. The dataset of the Java projects was split into mutually exclusive training and test sets. The experiment\ndemonstrated high-accuracy results for both code smells.\n\nLin et al. [48] proposed anewapproach for detecting code smells. They used a full convolutional network that could\nidentify and use local correspondences by making use of semantic features. They defined a multidimensional array,\nh*w*d, to represent the convolutional network (where h and d are space dimensions and d is the channel). For their\nexperiment, they used an open-source database that was initially published by the author. They used this databaseto\ndetect various code smells such as long method, lazy class, speculative generality, refused bequest, duplicated code,\ncontrived complexity, shotgun surgery, and uncontrolled side effects.\n\nLiu et al. [53] proposed an approach for code smell detection using deep learning. However, this approach had some\nlimitations. To address these limitations, they presented a new approach in [51]. This approach was generic and\nevaluated on four code smells: feature envy, long method, god class, and misplaced class. They improved the deep\nneural networkused in [53] by using bootstrap aggregating. They used aclassifier that generated several bootstrap\nsamples simultaneously from a given training dataset. The classifier trained multiple binary classifiers that in turn\ndetermined the final classification by voting. The study highlighted that it is difficult to design and train a generic\nclassifier to detect all code smells since different features are needed for different smells. Therefore, they presented\ndifferent classifiers for different code smells. They used a Convolutional Neural Network (CNN) for the detection of\nfeature envy and misplaced class. A dense layer-based classifier was used for the detection of the long method. Dense\nlayers coupled with Long Short Term Memory(LSTM) were used for the detection of the god class. The classifier used\nfor feature envy was similar to the one in their earlier work [53]. The deep neural classifier used for the long method was\ncomposed of five dense layers besides the input and output layers. The resulting features which were extracted by the\nhidden layers were fed into the output layer. This process mapped the features into a single output to suggest if the\nfeature is associated with long method smell. The classifier for the god class was composed of two parts: a textual part\nand a numerical (code metric) input. The textual input was a word sequence formulated by concatenating the names of\nattributes and methods declared within the class under test. After converting the textual input into numerical vectors\nthrough embedding, the resulting vectors were handled by an LSTM layer. This was unlike with the code metrics which\nwere fed straight into a dense layer. This output was then merged with the output of the LSTM before being fed into\n', '10 Nyirongo and Jiang, etal.\n\nanother dense layer. The output of the dense layer was the one that indicated whether aclass should be decomposed\nor not. The classifier employed for the detection of the misplaced class was similar to that of feature envy because both\nthese smells are caused by misplaced software entities like methods, classes, etc. They evaluated the approach on 10\nopen-source applications. The approach was compared against JDeodorant [22], DECOR [61], and TACO [68]. The\nresults indicated that the approach outperformed the state-of-the-art. They also evaluated the proposed approach on\nreal-world applications without any injection of smells. The two-step evaluation of the approach using generated\ndata and real-world data gave considerable differences in the performance of their proposed approach. This led to the\nconclusion that perhaps the evaluation of code smell detection approaches should rely more on manually validated\ntesting data that are often more reliable than generated data.\n\n4.1.2. Graph Based Approaches. This category refers to the use of deep learning techniques to analyze the structural\nproperties of code representation. Specifically, models like GNN and GCN are utilized to detect code smells by analyzing\nthe structural relationship between different elements in the code. By using these models, complex dependencies and\ninteractions can be captured, thereby improving the detection of code smells.\n\nYu et al. [105] proposed a Graph Neural Network (GNN) based approach to address the issue of inherent calling\nrelationships between methods that often result in low detection efficiency for feature envy detection. To achieve this\napproach, the authors collected code metrics and calling relationships. The collected features were then converted\ninto agraph where nodes represented the code metrics of a method and edges represented the calling relationships\nbetween methods. To address the imbalance of positive and negative samples, they introduced a graph augmenter\nto obtain an enhanced graph. They then fed the enhanced graph into a GNN model for training and prediction. The\nGNN classifier had four layers: the input layer, the GraphSAGE layer, the dropout layer, a fully connected layer, and\nan output layer. The input layer received the augmented graph after oversampling. The GraphSAGE updated the\nembedding of nodes based on the embedding space. The dropout layer prevented the classifier from overfitting during\nnode classification training. The fully connected layers converted the output of the dropout layer into a one-dimensional\nvector for final classification. The output layer received the vector and outputted the prediction of the classifier through\nthe activation function Sigmoid. For their experiment, the authors used five open-source projects (BinNavi, ActiveMQ,\nKafka, Alluxio, and Realm-java) collected from a dataset labeled by Sharma and Kessentini [82]. The dataset contained\n86,652 open-source projects mainly written in Java and C# on GitHub. To select the five projects, they considered\nprojects whose updates were: 1) within two years, 2) had more than 2,000 stars on GitHub, and 3) had more than 5,000\nmethodsand 500 classes. The approach for detecting the feature envy smell achieved an average F1-score of 78.90%,\nwhich is 37.98% higher than other comparison approaches.\n\nHanyu et al. [35] proposed a graph-based deep learning approach to detect long methods. Their approach extended the\nProgram Dependency Graph (PDG) into a Directed-Heterogeneous Graph. The Directed-Heterogeneous Graph was then\nused as the input graph. They employed the Graph Convolutional Network (GCN) to construct a graph neural network\nfor long method detection. The input in this approach consisted of two kinds of nodes (method node and statement\nnode) and four types of edges (include edge, control flow edge, control dependency edge, and data dependency edge).\nThe Graph Convolutional Network had two layers and one linear layer. To obtain enough data samples for the deep\nlearning classifier, they introduced a semi-automatic approach to generate a large number of data samples. To validate\ntheir approach, they compared it with existing methods using five groups of manually reviewed datasets.\n', 'ASurvey of Deep Learning Based Software Refactoring 11\n\n4.1.3 Hybrid Approaches. This category refers to the use of deep learning techniques in combination with other\nmethods to enhance the accuracy and effectiveness of code smell detection. For instance, deep learning techniques such\nas CNN, GNN, and attention mechanisms can be combined to leverage the strengths of each approach. Hybrid-based\napproaches aim to gain a more comprehensive understanding of the code, potentially leading to better performance in\ndetecting code smells.\n\nCombination of Structural and Semantic Features. Zhang et al. [109] proposed a new approach called DeleSmell to detect\ncode smells using a deep learning model and Latent Semantic Analysis (LSA). They argued that most of the existing\napproaches suffer from two things: 1) incomplete feature extraction and 2) an unbalanced distribution between positive\nand negative samples. Toaddress these issues, they developeda refactoring tool to transform good source code into\nsmelly code and generate positive samples based on real-world project cases. They builta dataset with over 200,000\nsamples from 24 real-world projects to improve dataset imbalance. DeleSmell collected both structural features through\niPlasma and semantic features via Latent Semantic Analysis and Word2Vec. DeleSmell’s model comprised a CNN\nbranch,a Gate Recurrent Unit (GRU)-attention branch, dense layers, and an SVM branch. The input was processed by\nthe GRU-attention branch and CNN branchin parallel. The CNN branch hada feature extraction component followed\nby a classification component. The feature extraction component included a set of hidden layers, including convolution,\nbatch normalization, and dropout layers. The output layer of the last dropout layer was connected to the input of a\ndensely connected network that comprised a stack of two dense layers. An attention mechanism was introduced in the\nGRU branch to learn the important features in the dataset while suppressing the interference of irrelevant information\non the classification results. For the SVM, the kernel method was used to map the nonlinear samples to high dimensional\nspace and identify the optimal hyperplane by maximizing the classification interval between the two samples. The\ninput of the last dense layer consisted of features concatenated by the GRU attention branch and the CNN branch\nand was connected to the SVM for the final classification. Grid search was used to tune the hyperparameters of the\nclassifiers. ReLU was used as the activation function of this approach. DeleSmell was used to detect brain class and\n\nbrain method code smells.\n\nIn their research, Ma et al. [55] explored the use of a pre-trained model called CodeT5 to detect feature envy, one\nofthe mostcommon code smells. They also investigated the performance of different pre-trained models on feature\nenvy detection by comparing CodeT5 with two other models, CodeBERT and CodeGPT. CodeTS is an encoder-decoder\nmodel that considers the token type information in code. CodeGPT is a transformer-based language model that is\npre-trained on programming languages for code completion and text-to-code generation tasks. CodeBERT is a multilayer\ntransformer model that uses the same JavaTokenizer as CodeTS to extract token sequences from source code. The\nresearchers used these models to extract semantic relationships between code snippets and compared their performance.\nThey evaluated their approach on ten open-source projects (Junit, PMD, JExtractAPI, Areca, Freeplane, JEdit, Weka,\nAdbextract, Aoi, and Grinder). Liu etal.’s[51] approach was used as their baseline forcomparison. The results showed\nthat their approach improved the F-measure by 29.32% on feature envy detection compared to thestate- of-the-art.\n\nIn their study, Hadj-Kacem and Bouassida proposed a hybrid approach for detecting code smells using deep autoencoder\nand Artificial Neural Network (ANN) [32]. Both unsupervised and supervised algorithms were used to identify the code\nsmells. The approach had two phases. In the first phase, a deep autoencoder was used for dimensionality reduction,\nwhich extracted the most relevant features. Once the feature space was reduced with a small reconstruction error,\nthe ANN classifier would then learn the newly generated data and output the final results. The second phase used a\n', '12 Nyirongo and Jiang, etal.\n\nsupervised learning classification by using the ANN. The approach was applied to four code smells: god class, data\nclass, feature envy, and long method. The study adopted a set of four datasets that were extracted from 74 open-source\nsystems. The results showed high accuracy with precision and recall values. The best F-measure value was 98.93%,\nwhich was achieved with the god class code smell. Even at the method level, the F-measure surpassed 96%. These results\n\nvalidated the effectiveness of the approach.\n\nSharma et al. [80, 81] conducted a study on the feasibility of using deep learning models for detecting code smells\nwithout the need for extensive feature engineering. They investigated the possibility of applying transfer learningin\nthis context. The researchers trained smell detection models based on Convolutional Neural Networks (CNN), Recurrent\nNeural Networks (RNN),andautoencoder models. The CNN layer consisted ofa feature extraction partfollowed bya\nclassification part. The feature extraction part was composed of an ensemble of layers, including convolution, batch\nnormalization, and max pooling layers. These layers formed the hidden layers of their architecture. The convolution\nlayer performed convolution operations based on the specified filter and kernel parameters. The convolution layer also\ncomputed the network’s weights to the next layer. The max pooling layer reduced the dimensionality of the feature\nspace. The batch normalization layer mitigated the effects of varied input distribution for each training mini-batch\nwhich optimized the training. The output of the max pooling layer was connected to the dropout layer, which performed\nanother regularization byignoring some randomly selected nodes during training to prevent overfitting. The output\nof the last dropout layer was fed into a densely connected classifier network that had a stack of two dense layers.\nThese classifiers processed one-dimensional vectors, whereas the incoming output from the last hidden layer was a\nthree-dimensional tensor. For this reason, the flattened layer was used first to transform the data into the appropriate\nformat before feeding them into the first dense layer with 32 units and ReLU activation. This was followed by the\nsecond dense layer with one unit and Sigmoid activation. This second layer comprised the output layer and contained a\nsingle neuron to make predictions on whether a given instance belongs to the positive or negative class in terms of\nsmell investigation. The layer used the Sigmoid function to produce a probability within a range of 0 to 1. The RNN\nwas comprised of an embedding layer followed by a feature learning part (a hidden LSTM layer). It was succeeded by a\nregularization (a dropout layer) and classification (a dense layer) part. The embedding layer mapped discrete tokens into\ncompact vector representations. To avoid the noise produced by the padded zeros in the input arrays, they set the mask\nzero parameters by the Keras embedding layerimplementation. Thus, the padding wasignored, and onlymeaningful\nparts of the input data were taken into account. The dropout and the recurrent dropout parameters of LSTM were set to\nlayer0.1.The output from the embedding layer was fed into the LSTM layer, which, in turn, gave output to the dropout\nlayer. The trainingand evaluation samples for this approach were generated by downloading repositories containing\nC#and Java code from GitHub after filtering out low-quality repositories by RepoReapers. They downloaded 922 C#\n\nand 922 Java repositories in total. They applied this technique for detecting complex method, complex conditional,\nfeature envy, and multifaceted abstraction. Through this study, they discovered that although deep learning methods\ncould be used for code smell detection, the performance is smell-specific. That is, it is very difficult to have a simple and\ndirect solution for this. They also noted that they could not find a clear superior method between one-dimensional\nand two-dimensional CNNs. One-dimensional CNNs performed slightly better for the smells’empty catch block and\nmultifaceted abstraction’, while two-dimensional CNNs performed better than their one-dimensional counterpart\n\nfor ‘complex method and magic number’ [80].\n\nIntheir paper, Hadj-Kacem and Bouassida proposed a method for detecting code smells in software using a deep learning\nalgorithm [33]. They used an abstract syntax tree and a variational autoencoder to extract semantic information from\n', "ASurvey of Deep Learning Based Software Refactoring 13\n\nthe source code. Firstly, they parsed the source code into the AST and transformed each tree into a vector representation\nthat was then fed into the variational autoencoder. The autoencoder generated a latent representation which was used\ntoreconstructthe original input data. A logisticregression classifier was then applied to determine whether the code\nwas a code smell or not. The approach was evaluated on the Landfill dataset [67]. The results showed that the proposed\nmethod was effective in detecting code smells such as blob, feature envy, and long method.\n\nXuand Zhang [102] proposed adeep-learning approach to detect code smells based on Abstract Syntax Trees (ASTs).\nThe approach captures the structural and semantic features of code fragments from the ASTs, by utilizing sequences\nof statement trees. The sequences ofstatement trees were encoded using bi-directional GRU and maximum pooling.\nThen, semantic and structural features were extracted from the encoded sequence to obtain final vector representations\nof the code fragments. The approach was applied to four types of code smells: insufficient modularization, deficient\nencapsulation, feature envy, and empty catch block. The final detection results were obtained through fully connected\nlayers. The approach was applied to 500 high-quality Java projects from GitHub, outperforming state-of-the-artdeep\nlearning models for both small-grained and larger-grained code smells.\n\nAttention Mechanism and Enhanced Neural Networks. Zhang and Dong proposed a new approach for detecting code\nsmells called MARS, whichis based ona Metric-Attention-based Residual network [108]. Thisapproach was used to\nidentify brain class and brain method code smells. MARS addresses the issue of gradient degradation by utilizingan\nimproved Residual Network (ResNet). The reason they chose ResNetis that it can reduce model parameters while\nspeeding up the training process. ResNet extracts deep feature information to enhance the accuracy of code smell\ndetection. The approach increases the weight value of important code metrics to label smelly samples by introducing a\nmetric attention mechanism. The attention mechanism used in this approach was inspired by SENet [37]. The approach\ncomprised a fully connected layer, used Tanh as the activation function to accelerate the convergence speed of the\nmodel, and used Sigmoidas the gated function. The improved ResNet had a convolution layer, a batch normalization\nlayer that accelerated the convergence speed of the network, used ReLU as the activation function, and had an addition\nas the sum operation. To train MARS, they extracted more than 270,000 samples from 20 real-world applications to\ngenerate a dataset called BrainCode, which is publicly available. They evaluated the effectiveness of the proposed\napproach by answering five research questions. The results showed that MARS achieved an average of 2.01% higher\naccuracy than the existing approaches.\n\nZhao etal. [111] developed a model to detect feature envy, which is based on dual attention and correlation feature\nmining. Firstly, they proposed a strategy for entity representation using multiple views. This strategy increased the\nmodel's robustness and improved the correlation feature and the model’s suitability. Secondly, they added an attention\nmechanism to CNN’s channel and spatial dimensions. The addition of the attention mechanism enabled the accurate\ncapturing of the correlation features between entities and controlled the information flow. They compared theirapproach\nagainst Liu’s [53] method, JMove [89] and JDeodorant [22]. Five open-source projects were used as subject applications.\nThe experimental evaluation was divided into two parts: 1) large-scale data with feature envy automatically injected for\ntraining and classifier verification, and 2) small-scale data without feature envy injected for evaluating the approach’s\neffectiveness on real projects. The evaluation results for both feature envy-injected and non-injected projects showed\nthat the proposed approach outperformed the state-of-the-art.\n\nIn their paper, Wang et al. proposed a new model for detecting feature envy using a Bi-LSTM with self-attention\nmechanism [98]. They approached the problem as a deep learning task and used two input parts: a simpler distance\n", '14 Nyirongo and Jiang, etal.\n\nmetric and text features extracted from the source code. Their approach consisted of three modules: text feature extract\nfor processing the text input, distance value enhancement for handling the distance metric input, anda feedforward\nneural network for classification. Notably, they introduced a basic attention function (AttentionBasic) thatis widely\nused in language modeling, in addition to the three score functions related to attention mechanism (AttentionAdd,\nAttentionDot, and AttentionMinus) [98]. They evaluated their approach using a dataset generated from seven open-\nsource Java projects, which was released by Liu et al. [51]. The results showed that their approach outperformed\n\nJDeodorant [22], JMove [89], and the first deep learning method proposed by Liu et al. [51].\n\nGuo et al. [31] proposed a method to detect feature envy code smell using a deep semantics-based approach that\ncombined method representation and a CNN model. The method representation technique was based on an attention\nmechanism and an LSTM network. The LSTM network represented the textual information of methods in source code.\nThis technique extracted semantic features from textual information and reflected the contextual relationships among\ncode parts. The attention mechanism helped in extracting specific features that were significant to code smell detection.\nThe semantic features supplemented the limited structural information in the code metrics and improved the accuracy\nof code smell detection. To achieve this approach, they first converted the textual information into vectors representing\nthe description of the method using the method representation. In the second part, the metric cluster was fed into a\nCNN-based model which had three convolutional layers and did not set the pooling layer. The CNN could fully extract\nthe features from the structural information in the code metrics to reflect the relations between adjacent metrics. After\nthis, they applied a flattened layer to turn the shape of the input into a one-dimensional layer. In the third part, they\nuseda Multilayer Perceptron-based neural network. The outputs of the CNN modeland method representation were\nconnected at the connection layer, which concatenated all inputs including the features from the textual input and\nthe metrics input. Behind the connection layer, they set two dense layers and one output layer to facilitate the final\nclassification which mapped the textual input and the metrics input into a single output. The output layer had only one\nneuron which represented the result of the identifier, i.e., smelly or non-smelly. Sigmoid was used as the activation\nfunction. The approach was evaluated using a dataset from 74 open-source projects. The results suggested that the\n\napproach achieved significantly better performance than the state-of-the-artapproaches.\n\nIna study by Liu et al. [54], an automated method was proposed to spot and refactor inconsistent method names. They\nused a graph vector and Convolutional Neural Network (CNN) to extract deep representations of the method names\nand bodies, respectively. The approach worked by computing two sets of similarnames when givena method name.\nThe first set included those that could be identified by the trained model of method names. The second set included\nnames of methods whose bodies were positively identified as similar to the body of the input method. If the two sets\nintersected to some extent, the method name was identified to be consistent. If not, it was identified as inconsistent.\nThey then leveraged the second set of consistent names to suggest new names when the input method was flaggedas\ninconsistent. The CNN used in this approach had two pairs of convolutional and subsampling layers. These layers were\nusedto capture the local features of methodsand decrease the dimensions ofinput data. The network layers fromthe\nsecond subsampling layertothe subsequent layers were fully connected. This meantthatthey could combinealllocal\nfeatures captured by convolutional and subsampling layers. For this approach, the output of dense layers was chosen\nto be the vector representation of method bodies, which synthesized all local features captured by the other layers.\nThe researchers collected both thetraining and test data from open-source projects from four differentcommunities\n(Apache, Spring, Hibernate, and Google). They only considered 430 Java projects with at least 100 commits to ensure\n', 'ASurvey of Deep Learning Based Software Refactoring 15\n\nthat the projects had been well maintained. The experimental results showed that the approach achieved an F-measure\nof 67.9% on identifying inconsistent method names, improving about 15 percentage points over the state-of-the-art.\n\nLiand Zhang [47] proposed a hybrid model with a multi-level code representation to optimize code smell detection.\nThey first parsed the code into an Abstract Syntax Tree (AST) with control and data flow edges. Then, they applied a\nGraph Convolutional Network to get the prediction at the syntactic and semantic level. Next, they analyzed the code\ntokenatthe token level using the bidirectional LongShort Term Memory networkwithanattention mechanism. They\napplied this approach to a total of 9 code smells (magic number, long identifier, long statement, missing default, complex\nmethod, long parameter list, complex conditional, long method, empty catch clause, and multi smells) and combined\nthem to come up witha multi-labeled dataset.\n\nZhang and Jia [107] proposed a new technique to detect feature envy using self-attention and Long Short Time Memory\n(LSTM). They were inspired by the transformer model in the formulation of this approach. The researchers added posi-\ntional encoding to preserve the meaning of sequences and compensate for the lack of positional information in the pure\nattention mechanism. They built on the existing deep learning model proposed by Liuetal.[51].Also, the researchers\nconsidered attention mechanism, LSTM structure, and snapshot ensemble to improve the detection performance. In\ntheir approach, they utilized self-attention as the attention mechanism. This mechanism was implemented with the\npositional encoding layer right after the embedding layer. The self-attention score was calculated using the Softmax\nfunction and then multiplied with the original inputto obtain the word embedding vector with attention score. They\nalso included an LSTM block after the attention layer to extract deeper semantic information. For the CNN model, they\ninitially tried adding a pooling layer but found it to be less effective than a dense layer. They changed the convolution\nblock’s structure from 128 dimensions to 64 dimensions and then to 32 dimensions. The researchers believed that this\nstructure could filter out the critical features while reducing the consumption of time and computing time. After the\nCNN, they inserted a dense layer with 1024 nodes right after concatenation. They also included the concept of integrated\nlearning by incorporating a snapshot ensemble in their approach, inspired by Huang et al. [39]. However, they proposed\nanew periodic function instead of using the cosine function to adjust the learning rate. The results showed that their\nmodelachieved better performance on four evaluation metrics, with precision increasing by 0.048, recall increasing\nby 0.035, F-measure increasing by 0.043, and AUC increasing by 0.056. The introduction of the attention mechanism\nand LSTM illustrated the correlation between code smell detection and natural language processing. Compared to the\nmodel of feature envy detection in Liuetal. [51], this study optimized it from three aspects: modifying and expanding\nthe model structure, introducing the self-attention mechanism, and applying a snapshot ensemble.\n\nTraditional Machine Learning and Deep Learning. Menshawy et al. [60] proposed a mechanism to detect feature envy\ncode smell using machine and deep learning techniques. The study applied 6 deep learning techniques (CNN, Long\nShort Time Memory (LSTM), Bidirectional LSTM (BILSTM), Gated Recurrent Unit (GRU), Bidirectional Gated Recurrent\nUnit (BIGRU), and Autoencoder) and 11 machine learning techniques based on code structural features. The deep\nlearning models were implemented using TensorFlow and Keras frameworks [41]. The CNN model was inspired by an\nimage classification model. The CNN comprised of an input layer that passed the input features to the embedding layer.\nThe role ofthis layerwas to map vocabularies in high-dimension space to vectors of fixed size. The embedding output\nwas fed to the convolution one-dimensional layer of a specific kernel and filter parameters. The new weights were\ncomputed to the next max pooling one-dimensional layer which reduced the dimensionality of the feature space. To\navoid overfitting, the weights were passed to a dropout layer to randomly disregard a specific percentage of nodes\n', '16 Nyirongo and Jiang, etal.\n\nduring training. The weights were connected to a flattened layer and then a stack of three dense layers to predict\nifa given instance was smelly or belonged to the non-smelly data. The LSTM and GRU architectures were inspired\nby a typical NLP model. The input layer fed the next embedding layer with input textual features. The embedding\nlayer mapped the input tokens to vectors. The new vectors were passed to the LSTM layer in the LSTM model or\nthe GRU layer in the GRU model. Both networks (LSTM and GRU) had recurrent dropout and dropout values of 0.1.\nThe new weights were fed to the dropout layer to avoid overfitting and then fed to a flattened layer and a stack of\nthree dense layers to avoid underfitting. Similar to the CNN architecture, the input to the dense layers was one unit.\nThe Sigmoid function was applied to decide whether the output belongs to the positive class or the negative class.\nA bidirectional network was applied to the LSTM and GRU layers with the same layers and hyperparameters of the\nLSTM and GRU models respectively. For the machine learning approach, eleven individual algorithms of different\nclassifier families were applied. These included Decision Table, Instance-based learning with parameter K, J48, JRip,\nMultilayer Perception, Naive Bayes, Random Forest, Simple Logistics, Sequential minimal optimization, AdaBoost, and\nBagging. Both approaches (deep learning and machine learning) were applied to open-source Java projects from the\nQualitas Corpus dataset. In the machine learning approach, the Designate Java tool was used to detect the code smell\nand to extract the corresponding metrics in CSV files. The machine learning data processor module splits the extracted\ndata into positive and negative samples. The machine learning algorithms were then applied to the processed output\nCSV samples to train the models and to evaluate the classifier’s performance. In the deep learning approach, the data\nwas tokenized by the JavaTokenizer tool which exported tokenized text files. The deep learning data processingstage\nsplits the tokenized files into positive and negative samples according to the extracted detection information from\nDesignateJava. The tokenized input was then fed to the deep learning models to detect the code smells. The results\nshowed that deep learning techniques are promising and that they tend to achieve good results compared with the\nmachine learning approach. Based on their evaluation of the 6 deep learning techniques against machine learning\nmodels, the autoencoder models achieved superiority among all the deep learning techniques. In contrast, CNN achieved\nthe lowest F-score. Overall, the deep learning techniques showed high potential in predicting feature envy. However,\nthe deep learning techniques based on semantic features are not capable of detecting all code smell types.\n\nHamdy and Tazy [34] proposed an approach for detecting the occurrence of the god class smell in source code. Their\napproach utilized both the source code textual features and metrics to train three deep learning models (Long Short\nTerm Memory, Gated Recurrent Unit, and Convolutional Neural Network). They built a dataset for the god class\nsmell in source code acquired from the Qualitas Corpus repository. They extracted the textual features of the source\ncode using natural language processing techniques and integrated them with metric features. They then trained the\nthreedeep learning models using different types of source code features, suchas metrics, textual features,andhybrid\nmetrics-textual features. The Convolutional Neural Network (CNN) used in the deep learning approach comprised a\nstack of convolution stages, for feature selection, followed by a stack of dense layers for classification. The CNN applied\naset of filters on the input sequence and produced the feature map, which represented an input to the max pooling layer.\nThe output of the last max pooling layer was connected to a dropout layer, which performed regularization by ignoring\nsome random nodes during training to prevent overfitting. In the experiment, they setthe dropout rate tobe equal to\n\n0.5. The output of the last dropout layer was fed into a dense layer, which had a fully connected multi-perceptron neural\nnetwork that worked like a classifier. They used a stack of two dense layers: the first dense layer had 32 units and ReLU\nactivation, followed bya second dense layer with the number of outputs set to equal to one. The second dense layer\nmade predictions on whether the god class smell occurs or not in a given source code. This layer used the Sigmoid\n', 'ASurvey of Deep Learning Based Software Refactoring 17\n\nactivation function toproducea probability within the range of 0 and 1.The LSTM and GRU models comprised a stack\nof LSTM/GRU layers,adropoutlayer,and adense layer. The LSTM/GRU layer learned the representation ofeachclass.\nThe regular dropout in the dropout layer was set to 0.5, while the recurrent dropout parameters of the LSTM/GRU were\nsetto 0.1.For the machinelearning approach, three traditional machine learning techniques were used: Naive Bayes,\nRandom Forest, and Decision tree (C4.5). These traditional approaches were mainly used to compare the effectiveness\nof the proposed deep learning technique. The evaluation results showed that the deep learningtechnique performed\nbetter than the latter.\n\nDewangan et al. [19] proposed a machine learning-based approach to predict code smells in software and identify the\nmetrics that play a significant role in detecting them. They used four code smell datasets, i.e., god class, data class,\nfeature envy, and long method, generated from 74 open-source systems (Qualitas Corpus) obtained from Fontana\netal. [24]. Six different algorithms, including Naive Bayes, KNN, Decision tree, logistic regression, Random Forest,\nand Multilayer Perceptron, were used in the statistical-heuristic machine learning models. The performance of each\ntechnique was evaluated individually for the four code smells. The Multilayer perceptron (MLP) algorithm was found\n\nto perform the best in terms of accuracy for detecting the data class code smell.\n\nBarbezet al. [10] proposed a machine learning-based ensemble method called Smart Aggregation of Anti-patterns\nDetectors (SMAD) to create an improved classifier compared to standalone approaches. To train and evaluate the model,\nthey created an oracle that contained the occurrences of god class and feature envy in eight open-source systems.\nHowever, neural networks often perform poorly onimbalanced datasets like their oracle, so they designeda training\nprocedure that maximizes the expected Matthews Correlation Coefficient (MCC). They then evaluated SMAD on the\noracle and compared its performance with other aggregated tools and competing ensemble methods. Although SMAD\nis intended for detecting antipatterns, it can serve as a benchmark for researchers looking to develop standalone tools\n\nfor identifying common code smells during the refactoring process.\n\n4.1.4 Explainable and Feedback Centric. This category refers to studies that have used explanation mechanisms or\nintegrated user feedback in their approach to detecting code smells using deep learning. Explanation mechanisms\ntypically aim to incorporate methods and techniques that facilitate the understanding and interpretability of the deep\nlearning model’s decision-making process during code smell detection. This is particularly important for developers, as\nitenablesthem to have faith in the process, especially when the detected code smells lead to critical code changes. On\nthe other hand, user feedback integration involves actively seeking and incorporating feedback from developers into\nthe deep learning-based code smell detection process. Continuous interaction with the developers ensures adaptability\n\nto evolving coding practices, preferences, and domain-specific knowledge.\n\nFor the use ofexplainable mechanisms, Yin et al. [104] have proposed an explainable approach to detecting feature envy\nbased on localand global features. Tomake the most of the code information, they designed differentrepresentation\nmodels for global and local code. They extracted different feature envy features and automatically combined those\nthat were beneficial for detection accuracy. They further designed a Code Semantic Dependency (CSD) to make the\ndetection result easy to explain. The global feature contained a global semantic feature and a global metrics feature.\nThey splice the extracted method name, enclosing class name, and target class name together to create the global\nsemantic features. The global semantic features were metric values calculated with plugins. LSTM was used for the\nglobal semantic features to extract context from the input statements. Then, it extracted the semantic relations from\ncontext features. LSTM focused on the cell state in the neural network and used three gates to determine how much\n', "18 Nyirongo and Jiang, etal.\n\ncell state information was retained. The gate structure was used to select the appropriate call state information so\nthat LSTM could easily capture the context information between the legal name, the class name, and the target class\nname. For the global metrics, CNN was used to obtain complex mapping relations from simple metric information. They\nopted for CNN due to its ability to automatically extract features from the original features, which could reveal the\nrelationship between metrics information and code smell. CNN is also highly suitable for parallel training on GPU\nwhich could greatly reduce the training time. To achieve the CSD, a siamese network was formed based on a local\nfeature representation model. It had the same branch with different inputs. The code input would first pass through an\nembedding layer and then through an attention layer, meanwhile, the parameters were also being shared. Atthe final\nstage, they would calculate the code dependency ofthe feature obtained from each branch. Toevaluate the approach,\nthey used manually constructed code smell projects (Junit, PMD, JExcelAPI, Areca, Freeplane, jEdit,and Weka) and 3\nreal-world projects (Xmd, JSmooth, and Neuroph). They compared their approach against Liu et al.’s [53] approach,\nJDeodorant [22], and JMove [89]. The F-measure for the two experimental setups were 2.85% and 6.46%, respectively,\nwhich was higher compared to the state-of-the-art approaches.\n\nFor theintegration of feedback, Nanadani etal. [64] conducted a study to investigate the effects ofhuman feedbackon the\nperformance oftrained models in detecting code smells, which are subjective perceptions of developers. Tocreatearobust\nandadaptable system, the study combined deep learning techniques, user feedback, and acontainerized deployment\narchitecture fora locally-run web server. The deep learning techniques used in the study were an autoencoder witha\ndense multilayer perceptron, an autoencoder witha Long Short Term Memory, and a variational autoencoder witha\nthreshold strategy for classification. The first step in this approach was to train the autoencoder, which was used to\ncompress the input data into a lower dimensional representation called the latent representation. The autoencoder\nhad an encoder and a decoder, with the encoder starting with an input layer followed by a series of dense layers. To\nimprove the training stability and efficiency, batch normalization was added to standardize inputs for each mini-batch.\nThe decoder was then constructed in thereverse order ofthe layers. The variational autoencoder was used toserve as\na deep generative model that employed Bayesian inference to estimate the latent representation. The approach was\nused to detect complex method, long parameter lists, and multifaceted abstraction code smells. A plugin for IntelliJ\nIDEA was created, and a container-based web server was developed to offer services of the baseline deep learning\nmodel. The setup allowed developers to see code smells within the IDEA and provide feedback. Using this setup, the\nresearchers conducted a controlled experiment with 14 participants divided into experimental and control groups. In\nthe first round of the experiment, the code smells predicted by using the baseline deep learning model were shown,\nand feedback was collected from the participants. In the second round, the researchers fine-tuned and reevaluated the\nmodel's performance before and after adjustment. The results showed that calibration improves the performance of the\nsmell detection model by an average of 15.49% in F1 score across the participants of the experimental group.\n\n4.2 Code Smell Types of Refactoring Opportunities\n\nCode smells are defined as certain structures in the code that call for the process of refactoring. Based on this definition\nFowler [26] proposed 22 types of code smells, for example, long method, duplicate class, feature envy, duplicate class, etc.\nCode smells have been analyzed and categorized based on the implementation, design, and architectural levels (based on\ntheir scope, granularity, and impact) [26-28, 81]. Essentially, code smells may occur at different levels of the codebase,\nthat is class, method, or variable levels. This occurrence at different levels affects how these smells may be detected\n\nwhen employing deep learning models. From our literature collection, we note and observe, as presented in Table 1, that\n", 'ASurvey of Deep Learning Based Software Refactoring 19\n\natotal of 26 different types of code smells have been detected using deep learning techniques which could potentially\nlead to the identification of refactoring opportunities. The data collected shows that the smells detected were occurring\nat different levels of the code base from class up to variable level. Also, we observe that some researchers focused on the\ndetection ofonly one type of codesmell [21,53, 105] while others [18, 32,48] focused onthe detection ofatleast more\nthan one type of code smell. Overall, our data suggest that feature envy being one of the mostcommon code smells is\nthe one whose detection researchers are employing the use of deep learning techniques more often. Feature envy code\nsmell appeared in at least 27.87% of the primary studies, seconded by long method 9.84%, god class 6.56%, complex\nmethod 6.56%, andthe rest ofthe smells. These smells, i.e., feature envy andlong method could potentially lead to the\nrecommendation and suggestion of refactorings which involve 1) moving features between objects, e.g., move method\nrefactoring. 2) composing methods to ensure that they are much easier to understand, e.g., extract method refactoring.\nThrough our literature search we have noted that other researchers like Xuand Zhang [102] combined several code\n\nsmells to create multi smells as a way of enhancing the efficiency and validation of their approach.\n\n4.3 Training Strategies\n\nDetecting code smells using deep learning models requires the adoption of effective training strategies. These strategies\ninvolve an organized approach to teaching the model to recognize patterns, make predictions, and perform tasks\nbased on the data. Training strategies are crucial in developingarobust and accurate model for detecting codesmells.\nResearchers use various techniques to create effective training strategies that produce efficient models. Our analysis of\nthe literature focuses on how researchers preprocess and engineer features from various metrics to effectively represent\ncode smells. Wealso explore the embeddings and representations used to capture semantic relationships within the\ncode, which aids the model’s ability to learn code smells. Researchers use various data preprocessing, feature extraction,\nand data balancing techniques to enable the deep learning model to identify different code smells effectively.\n\n4.3.1 Data Preprocessing. Data preprocessing is an essential step to improve the quality of data that will be fed into deep\nlearning models. During the process of data preprocessing, several techniques are utilized, including data cleaning, data\nintegration, data transformation, and data regularization. Based on the nature of the datasets used, various techniques\nmay be applied to preprocess the candidate metrics such as code, text, graph data, etc. For instance, Sharma et al. [80, 81]\nperformed their preprocessing by analyzing the data using Designate and DesignateJava on their C# and Java code,\nrespectively. Then they split their code using Codesplit before applying tokenization. They performed duplicate removal\nafter tokenization toensure thatno duplicate data was fed into the deep learning model. Incontrast, Himeshetal. [64]\nconducted their duplicate removal before tokenization using a hash function to compute a unique hash value for each\ncode instance and compared the hash values to identify any duplicates. Regularization is another widely adopted\ntechnique used during data preprocessing to prevent deep models from learning specific and irrelevant features of their\ndata. Barbez etal. [10] usedregularization to adda special term to the loss function, which encourages the weights to\nbe small this was as defined by Witten [99]. Overall, data preprocessing is crucial in enhancing the quality and integrity\n\nof data, which in turn leads to better deep learning model performance [103].\n\n43.2 Feature Extraction. Various features from data candidates are extracted for deep learning models to detect\nvarious code smells. The selection of features to be extracted playsa key role inthe way a deep learning model makes\npredictions. The features that could be extracted for code smell detection include but are not limited to the following\n', '20 Nyirongo and Jiang, etal.\n\nTable 1. List of code smells detected in the primary studies.\n\nCode Smells Frequencies References\nFeature envy 17 [10, 19, 21, 31, 33, 51,53, 55, 60, 80, 81, 98, 102, 104, 105, 107,111]\nLong method 6 [19, 33, 35, 47, 48, 51]\nGod class 4 [10, 19, 32, 34]\nMisplaced class 1 [51]\nBrain class and Brain method 3 [18, 108, 109]\nComplex method 4 [47, 64, 80, 81]\nComplex conditional 2 [47, 81]\nMultifaceted abstraction 3 [64, 80, 81]\nLazy class [48]\nSpeculative generality [48]\nRefused bequest [48]\nDuplicate code [48]\nShotgun surgery [48]\nContrived complexity [48]\nUncontrolled side effects [48]\nInsufficient modularization [102]\nDeficient encapsulation [102]\nEmpty catch block/clause 3 [47, 80, 102]\nMagic number 2 [47, 80]\nLong identifier [47]\nLong statement [47]\nMissing default [47]\nLong parameter list 2 [47, 64]\nBlob [33]\nInconsistent method names [54]\nData class 2 [19, 32]\n**Multi Smells [47]\n\nstructural features, semantic features, naming conventions and documentation, patterns, etc. Several ofourprimary\nstudies [18, 108, 109] were found to detect similar code smells such as feature envy, long method, god class, brain\nclass/method, etc. respectively. However, we note that different features were employed for the same code smell across\ntheliterature. Liu etal. [51,53] leveraged the use ofsemanticand structural features to detectfeature envy code smell\nbut ignored the semantic information contained in input sequences. Thus, Zhang et al. [107] proceeded to include\nthe semantic information contained in the inputsequences as a way ofimproving the efficiency of the deep learning\nmodel. Apart from just using structural and semantic features, we note that in the detection of feature envy code smells,\nresearchers used global metric features [104], calling relationships [105], and others even extracted ASTs from the code\nfragments and formed sequences of statementtrees like the case of Xuand Zhang [102]. For the detection of the long\nmethod, we notethatinasmuchas most ofthe researchers employed the use of source code metrics, there wasa slight\ndifference in how these were processed for them to be extracted as features. For example, Lin et al. [48] converted\n', 'ASurvey of Deep Learning Based Software Refactoring 21\n\nTable 2. Representative feature sets.\n\nCode Smells Features References\n\ncode metrics,textualfeatures [10, 19, 21, 31,33, 51, 53,55, 60, 80, 81, 98, 102, 104, 105, 107,111]\nFeature envy\n\nglobal metrics,asts\n\nLong method code metrics, source code (xml) [19, 33, 35, 47, 48, 51]\nGod class code metrics, textual features [10, 19, 32, 34]\nBrain class/method code metrics, textual features [18, 108, 109]\nComplex method source code, code metrics [47, 64, 80, 81]\nComplex conditional source code, code metrics [47, 81]\nMultifaceted abstraction source code code metrics [64, 80, 81]\nEmpty catch block source code, code metrics [47, 80, 102]\nMagic number source code, code metrics [47, 80]\n\nLong parameter list code metrics,source code [47, 64]\n\nData class code metrics [19, 32]\n\nthe metrics into XML while Hadj-Kacem and Bouassida [33] parsed the metrics into an AST. To detect god class code\nsmell, Hamdy etal. [34] used textual features from sourcecode while Hadj-Kacem and Bouassida [32] and Dewangan\netal. [19] extracted their features from code metrics. Similarly, for brain class/method Zhang et al. [109] used both\nsemantic and structural features while Zhang and Dong [108]and Das etal. [18] used code metrics. Table 2 gives a\nrepresentative tabulation of the scenarios being highlighted. From Table 3 we note that it is only in the detection of\ndata class where researchers used the same feature set.\n\n4.3.3 Feature Embedding. Deep learning models require the features extracted to be converted intoa suitable format\nfor the model to extract the relevant features. Tokenization and vectorization are common processes used to achieve this.\nTokenization breaks down text into smaller units (tokens), while vectorization converts these tokens into numerical\nvectors. Researchers also use embedding techniques to convert the candidate data into the required format. Embedding\nis a specific type of vectorization that represents words as dense vectors in a continuous space, capturing semantic\nrelationships between words. Table 3 lists some of the tools that researchers used to execute these processes. Our data\nindicates that most researchers used Word2Vecas an aid to conducting embedding for their data candidates, with 23.08%\nof the primary studies using it, followed by iplasma at 19.23%. Wealso discovered that at least 15% of the studies did not\nexplicitly highlight the tools used for the feature extraction processes.\n\n4.3.4 Data Balancing. Data balancing is the process of ensuring that the different categories or labels in a dataset\nare represented equally in terms of their frequency. This is particularly important in classification problems where\nthe model tries to predict different classes. A balanced dataset prevents the model from being biased towards the\n', '22 Nyirongo and Jiang, etal.\n\nTable 3. Feature extraction tools.\n\nTools References\nWord2Vec [51, 53, 54, 102, 104, 109]\niplasma [18, 31, 32, 108, 109]\nJavalang 34\n\nNLTK 34\nTokenizer [80, 81]\n\nFluid tool [81, 32]\nJavatokenizer 60\nGraphSAGE [105]\nWrapper 19\n\nCodeT5S 55\n\nmajority classand ensures that it performs wellinall classes. Imbalanced datasets, onthe other hand, can lead to poor\nperformance of the model, especially for the minority class. The detection of code smells also requires a balanced dataset\nwhere the number of positive samples (with code smell) and negative samples (without code smell) are maintained.\nDifferent techniques can be used to balance the dataset such as under-sampling or oversampling. Under-sampling\nremoves samples from the majority classes while oversampling generates new samples for the minority class. Synthetic\ndata generation and ensemble methods can also be used to reduce the disadvantages of having an imbalanced dataset.\nResearchers have used the Synthetic Minority Over Sampling Algorithm (SMOTE) to balance their data in the detection\nof code smells. SMOTE is an effective oversampling technique that has been widely used in practice. However, its\neffectiveness may vary depending on the specific characteristics of the data and the problem at hand. It is worth noting\n\nthat while 78.5% of the primary studies were clear about the techniques they used to balance their data, 21.5% did not\nmention any issues with data imbalance.\n\n4.3.5 Model Training. Various approaches exist for training deep learning models. According to our literature collection,\nmodels can undergo training using labeled data, unlabeled data, or a blend of both. The process of labeling data can\noccur automatically, semi-automatically, or manually. These training methodologies are commonly categorized as\nsupervised, unsupervised, or a combination of both, where a simultaneous application of both techniques is employed.\nThese techniques are presented in Figure 3. Our survey found that the majority of primary studies (65.38%) used\nsupervised learning techniques, while 23.08% used unsupervised techniques and 11.54% used a combination of both.\n\n4.4 Evaluation\n\n4.4.1 Datasets. Based on our survey findings, a majority of the primary studies utilized applications exclusively\ndeveloped ina single programming language, such as Java, for conducting experiments and evaluations. In contrast,\nsome studies opted for a diverse approach by employing a combination of applications developed in different languages,\nsuch as Java and C#, for their experiments and performance assessments. As depicted in Figure 4, the data reveals\nthat 87.5% of the primary studies relied on projects developed in a singular programming language, while only 12.5%\n', 'ASurvey of Deep Learning Based Software Refactoring 23\n\nSupervised\n\nCombination of both\n\nUnsupervised\n\nFig. 3. Model training strategies\n\nemployed a combination of programming languages in their application projects. Additionally, as depicted in Figure 5,\na significant majority, specifically 91.67%, of the studies employed projects that are publicly accessible, i.e., general\nopen-source projects available in various repositories, and real-world open-source projects. In contrast, 8.33% utilized\n\nprivate projects with restricted access.\n\nSingle language\nPublic\n\nPrivate\nMultiple languages\n\nFig. 4.Programming languages Fig. 5. Dataset types\n\n4.4.2 ResultMetrics. Researchers use metrics to evaluate the performance ofa particular approach.Commonly used\nmetrics for this calculationinclude precision, recall, F-measure,and accuracy. Precision measuresthe accuracy ofthe\npositive predictions. Its formula is defined as (Precision = TP / (TP + FP)), where TP represents true positive and FP\nrepresents false positive. Itis important in situations where false positives should be minimized, and there is a cost\nassociated with making incorrect positive predictions. Recall measures the model’s ability to capture all relevant cases.\nIts formula is defined as (Recall = TP / (TP + FN)), where FN represents a false negative. It is important when the\ngoal is to capture as many positive cases as possible, and there is acost associated with missing positive predictions.\nThe F1 score or F-measure provides a balance between precision and recall. Its formula is defined as (F1 Score = 2 *\n(Precision * Recall) / (Precision + Recall)). It is useful when both false positives and false negatives need to be minimized.\nAccuracy measures the ratio of correctly predicted observations to the total observations. Its formula is defined as\n(Accuracy = (TP + TN) / (TP +TN + FP +FN)), where TN represents true negative. While accuracy isa common metric,\nit may not be suitable for imbalanced datasets as it can be misleading. Accuracy is widely used when the classes are\n', '24 Nyirongo and Jiang, etal.\n\n25%\n\n204\n\nNumber of papers whichused the metric\n\n0 >\nPrecision Recall Fl-score Accuracy AUC MCC Kappa\n\nPerformance assessment\n\nFig. 6. Metrics used for performance assessment\n\nbalanced. However, in imbalanced datasets, accuracy alone may not provide a complete picture of model performance.\nFrom our primary studies, we have noted that researchers use other metrics apart from the outlined (precision, recall,\nf-measure, and accuracy). For example, researchers [35, 51,80, 107] have used Area Under Cover(AUC) to represent\nthe trade-off between true positive rate and false positive rate at various thresholds. For instance [10, 51], used the\nMatthews Correlation Coefficient (MCC). MCC isa correlation coefficientbetween the observedand predicted binary\nclassifications. It takes into account true and false positives and negatives and is particularly useful when dealing with\nimbalanced datasets. Its formula is defined as (MCC = (TP *TN - FP* FN) /sqrt((TP + FP) *(TP+ FN) *(TN+FP)*(TN\n+FN))).MCCisa balanced metricthat works well forimbalanced datasets. Lin etal. used [48] Kappa (Cohen’s Kappa).\nappa measures the agreement between observed and predicted classifications, correcting for the agreement that could\noccur by chance. Its formula is defined by ( Kappa = (Po - Pe) / (1 - Pe)), where Po is the observed agreement, and Pe\nis the expected agreement. Kappa is useful for classification problems where class distribution is imbalanced, and it\naccounts for the agreement that could occur by chance. From Figure 6 itbecomes apparent that precision, recall, and\nF-measure are commonly used for performance assessment. Notably, 96.15% of our primary studies utilized precision,\nwhile 92.31% and 88.46% employed recall and F-measure, respectively.\n\n5 RECOMMENDATION OF REFACTORING SOLUTIONS\n\nhe recommendation of refactoring solutions involves identifying areas of code that could benefit from refactoring and\nsuggesting potential approaches or strategies for improving them. Recommendations are often made based oncode\nreviews, automated analysis tools, or architectural discussions. The recommendation of refactoring solutions does not\ninvolvedirectly altering the code but rather advising on possible improvements. The recommendation of refactoring\nsolutions is more about identifying problems and proposing solutions than actually implementing them. In this section,\nwe will explore how deep learning techniques have been used by researchers to suggest refactoring solutions. We will\ndiscuss the recommendation technologies, the suggested refactorings, the training strategies, and the evaluation metrics\n\nused to assess the performance of the approach.\n', 'ASurvey of Deep Learning Based Software Refactoring 25\n\n5.1 Recommendation technologies\n\nThrough the use of various deep-learning techniques, researchers have been able to suggest solutions for refactoring.\nFrom our literature search, we note that some researchers [3, 9, 44] have utilized deep learning models to predict\nspecific refactoring solutions. Conversely, others [51,53,55] haveutilized deep learning models to initially detect the\npresence of code smells in a given codebase and subsequently recommend refactoring solutions based on the identified\ncode smells. Consequently, we have categorized these approaches into two distinct groups: refactoring-based and code\nsmell-based. We will discuss the technologies used in these studies in the following sections.\n\n5.1.1 Refactoring Based Technologies. Alenezi et al. [3] conducted a study on the effectiveness of deep learning\nalgorithms in building refactoring prediction models at the class level. They used a Gated Recurrent Unit (GRU) as\na key structural parameter. The GRU comprised a hidden layer, an epoch, and a normalization batch. The GRU had\nthree hidden layers and utilized the number of epochs between 10 to 2500, with a batch size of 2 to 25. The deep\nlearning algorithm proposed by Qasem et al. [76] was used after the three hidden layers. The study used 7 open-source\nJava-based projects to assess the effectiveness of the proposed algorithm, which had two main stages. In the first stage,\na set of necessary preprocessing procedures were performed on the datasets. During this initial stage, SMOTE was used\nto prevent any imbalance. In the second stage, the deep learning algorithm was applied to the dataset to predict the\nneed for refactoring at the class level by using the Gated Recurrent Unit algorithm. For the experiment, two sets of\ndatasets, balanced and unbalanced, were used to check if the balance of the dataset would affect the predictability of the\ndeep learning model. The results indicated that a balanced dataset enhances the prediction of class-level refactorings.\n\nAniche etal. [9] conducted a large-scale study to evaluate the effectiveness of various supervised machine learning\nalgorithms in predicting software refactoring. They aimed to demonstrate that machine learning methods can accurately\nmodel the refactoring recommendation problem. The study used a dataset containing over two million real-world\nrefactorings extracted from more than 11,000 real-world projects from Apache, F-Droid, and GitHub repositories. The\nstudy evaluated six machine learning algorithms, of which five were traditional machine learning approaches (logistic\nregression, naive Bayes, support vector machine, decision trees, and random forest), and one was a neural network used\nas the deep learning technique. The neural networkused was asequential network ofthree dense layers with 128,64,\nand 1 units, respectively. To avoid overfitting, dropout layers were added between the sequential dense layers, keeping\nlearning in 80% of the units in dense layers. The number of epochs was set to 1000. All the algorithms employed for\nthis technique werethen individually used to predictrefactoringsat different levels: class, method, and variable. The\nresulting models were able to predict 20 refactorings at the different levels with an accuracy higher than 90%.\n\nKumar et al. [44] developed a recommendation system that suggests which methods require refactoring. They used 25\ndifferent sourcecode metrics at the method levelas input features ina machinelearning framework. The system then\npredicts the need for refactoring. The authors conducted a series of experiments ona publicly available annotated dataset\nof five software systems to investigate the performance of the approach. The approach used ten different machine\nlearning classifiers, of which seven (AdaBoost, LegitBoost, Log, NB, BayesNet, RF, and RBEN) were traditional machine\nlearning techniques. Only three out of the ten were based on deep learning, namely MLP, ANN-GD, and ANN-LM.\nThe first phase of the approach focused on analyzing relevant features. The considered dataset was preprocessed to\nextract relevant features. At this stage, the Wilcoxon rank sum test was applied to handle uncertainty in the dataset,\nand also for extracting features. Subsequently, ULR analysis was applied to identify the final set of source code metrics\nconsidered for further implementation. The second phase involved the model-building process and the assessment of the\n', "26 Nyirongo and Jiang, etal.\n\nperformance of the proposed model. During the model-building process, the dataset was normalized through min-max\nnormalization. The class imbalance issue of the dataset was addressed using SMOTE, RUSBoost, and Up-Sample. Then,\na 10-fold cross-validation was employed with the 10 machine learning techniques to implement the proposed approach.\nThe results obtained from each technique were compared with different performance measures to evaluate them. The\nauthors concluded that overall, from the experiments conducted, method-level refactoring prediction using source code\n\nmetrics and machine learning classifiers is possible.\n\nPanigrahietal. [69] designed arecommendation system for predicting refactoring instances at the class level using\nmachine learning techniques, various data sampling approaches to address the data imbalance problem, and feature\nselection techniques. The approach centered around five main research questions: 1) how does data balancing improve\nthe prediction model’s capability? 2) how does feature selection improve the results of the refactoring prediction model?\n3) how do machine learning classifiers predict the refactoring model's results across different performance parameters?\n4) how can refactoring instances be predicted in the case of cross-project? 5) how can refactoring instances be predicted\nin the case of intra-project? The authors highlighted the importance of using machine learning to refactor prediction\nmodels for large object-oriented software systems. They recommended using ensemble approaches and enhanced\nmachine learning classification algorithms to predict superior performance across different performance parameters.\nThe classifiers recommended for use as standalone or ensemble classifiers include SVM, LSSVM, Naive Bayes, Random\nForest, ELM-based, K-nearest-neighbors, and deep learning models such as LSTM and CNN. Forthe approach in this\nstudy, the authors used LSTM and CNN classifiers. The first phase involved calculating software metrics using the\nsource meter tool, while the second phase involved data normalization. Relevant features were selected from all the\nfeatures extracted from the dataset using Principal Component Analysis (PCA), correlation tests, and Wilcoxon rank\ntests. The classifiers were then utilized to predict refactoring instances. To improve the refactoring model’s performance,\nthey implemented ensemble techniques, LSTM, and CNN. Performance was evaluated using different performance\nmetrics suchas recall, accuracy, F-measure, and precision. For unbalanced data, the Area Under the Curve (AUC) was\n\ncomputed.\n\nSagar et al. [79] conducted research to determine whether code metrics can be used to predict refactoring activities in\nthe source code. They approached this by formulating refactoring operation type prediction asa multi-classification\nproblem and implementing both supervised learning and LSTM models. They used the metrics extracted from committed\ncode changes to extract the features that best represent each class and predict the method level refactoring being applied\n(move method, rename method, extract method, inline method, pull up method, and push down method) for any\nproject. For this approach, they developed two types of models - a metric-based model that included traditional machine\nlearning techniques (random forest, SVM, and logistic regression classifiers) and a text-based model with LSTM. The\ntext-based model had an input layer of word embedding metrics and an LSTM layer. The LSTM layer provided the\nfinal dense layer output. For the LSTM layer, they used 128 neurons for the dense layer, and five neurons since there\nwere five differentrefactoring classes. They used Softmax as an activation functionin the dense layer and categorical\ncross-entropy as the loss function. The metric-based model was built with supervised machine learning models to\npredict the refactoring class. The random forest, SVM, and logistic regression were trained with 70% of the data. Initially,\nthe proposed metric model was implemented with only commit messages as input, but the authors realized that this\napproach was insufficient. Therefore, they combined commit messages with code metrics in the second experiment.\nThe model built with LSTM produced 54.3% accuracy. The model built with sixty-four different code metrics dealing\n", 'ASurvey of Deep Learning Based Software Refactoring 27\n\nwith cohesion and coupling characteristics of the code produced 75% accuracy when tested with 30% of data. The study\nshowed that commit messages with little vocabulary are not sufficient for training machine learning models.\n\nCui et al. [17] have proposed an approach for recommending move method refactoring called Rmove. The proposed\napproach involves automatically learning both structural and semantic representations from code snippets. To achieve\nthis, they first extracted method structural and semantic information from a dataset. Next, they created the structural\nand semantic representation and concatenated them. Finally, they trained a machine learning classifier to guide the\nmovement ofthe method to a suitable class. The authors used a total of nine classifiers, of which six (Decision Tree, Naive\nBayes, SVM, Logistic Regression, Random Forest, and Extreme Gradient Boosting) were machine learning -based. Only\nthree were based on deep learning, namely CNN, LSTM, and GRU. The approach demonstrated significant improvement,\nwith an increase of 14%-36% in precision, 19%-45% in recall, and 27%-44% in F-measure compared to the state-of-the-art\ntechniques, such as PathMove [45], JDeodorant [22], and JMove [89].\n\nNyamawe etal. [66] proposed alearning-based approach for recommending refactoring types based on the history\nof feature requests, code smells information, and the applied refactorings on the respective commits. The proposed\napproach learned from the training dataset associated with a set of applications. The approach could be used to\nsuggest refactoring types for feature requests associated with other applications or that are associated with the training\napplications. The proposed approach had six main steps. Firstly, the feature requests were extracted from the issue\ntracker JICA and their respective commits were retrieved from a repository on GitHub. Secondly, the previously applied\nrefactorings on the retrieved commits were recovered. Thirdly, RefDiff [84] and RMiner [95] were used to identify the\ncode smells associated with the source code in each of the retrieved commits. Fourthly, text processing was applied to\nthe contents of the file to prepare textual data into a numerical representation for training the classifiers. The fifth step\ninvolved the training of the classifiers which gave the prediction model for predicting and recommending refactorings\nfor new feature requests. Six classifiers were employed, out of which five (SVM, MNB, LR, RF, and DT) were machine\nlearning-based, while only one was deep learning-based (CNN). The study noted that CNN performed slightly lower\nthan the rest of the classifiers, partly because deep learning classifiers generally require significantly larger datasets to\nachieve competitive performance. Nonetheless, the overall evaluation of the approach based on two tasks (the need for\nrefactoring and recommending refactoring types) indicated that the approach attained an accuracy of up to 76.01% and\n83.19%, respectively.\n\nIna similar vein to the work done in Nyamawe et al. [66], Nyamawe [65] proposed a machine learning approach\nthat made use of commit messages toimprove software refactoring recommendations. This approach identified past\nrefactorings that were applied to commits used for implementing feature requests by analyzing the commit messages.\nThe approach employed six algorithms, including five based on machine learning (SVM, MNB, Random Forest, Logistic\nRegression, and Decision Tree) and one based on deep learning (CNN). Toevaluate the approach, a dataset of commit\nmessages from 65 open-source projects was used. The results showed that leveraging commit messages improved\nrefactoring recommendation accuracy significantly compared to the state-of-the-art.\n\nAstudy conducted by Mastropaolo et al. [59] explored the potential of data-driven approaches to automate variable\nrenaming. They experimented with three techniques - a statistical language model and two deep learning-based\nmodels. Three datasets were used to train and evaluate the models. The first dataset was used to train the models,\ntunetheir parameters, andassess their performance. The second and third datasets were used to further evaluate the\nperformance of the techniques. The researchers found that under certain conditions, these techniques can provide\n', '28 Nyirongo and Jiang, etal.\n\nvaluable recommendations and can be integrated into rename refactoring tools. The three representative techniques\nused were a statistical model, an n-gram cached language model proposed by Hellendoorn [36], TS proposed by Raffel\net al. [77], anda transformer-based model presented by Liu et al. [50]. The study demonstrated that deep learning\nmodels, particularly those that generate predictions with high confidence, can be valuable support for variable rename\nrefactoring.\n\nAlomar etal. [4] developed a tool called AntiCopyPaster, which is a plugin for IntelliJ IDEA. The tool aims to provide\nrecommendations for extract method refactoring opportunities as soon as duplicate code is introduced in the opened\nfile in the IDE. The tool takes into account various semantic and syntactic code metrics as input and makes a binary\ndecision on whether the code fragment should be extracted or not. The goal of this approach is to increase the adoption\nof extract method refactoring while maintaining the workflow of the developer. To achieve this goal, Alomar et al. in [5]\ninvestigated the effectiveness of machine learning and deep learning algorithms. They defined the detection of extract\nmethod refactoring as a binary classification problem. Their proposed approach relied on mining prior applied extract\nmethod refactorings and extracting their features to train a deep learning classifier that detected them in the user’s code.\nThe approach wasstructured into four phases: data collection, refactoring detection, code metrics selection,andtool\ndesign and evaluation. The deep learning model used in the approach was CNN. The CNN comprised multiple layers\nof fully connected nodes, structured into convolutional, deconvolutional, and dense layers. A dropout stage was also\nincluded to prevent overfitting. The input to the CNN was avector of 78 metric values which were batch-normalized\nto stabilize their distribution. The batch normalized inputs were then fed intoa convolution that reduced the feature\nspace from 78 to 32. ReLU was used as the activation function for the convolutional layers. The convoluted data was\nthen fed into the deconvolutional layer, which was followed by a max pooling layer with a filter size of 2 that took the\nlargestnumber inthe filter. The final layer was the dense layer, in which each nodereceivedinputfromall nodes of the\nprevious layer. The approach wasimplementedasa plugin in IntelliJ IDEA, whichis a popular IDE for Java. The plugin\nconsists of four main components: Duplicate detector, Code analyzer, Method extractor, and Refactoring launcher.\nThe results showed that CNN recommended the appropriate extract method refactorings with an F-measure of 0.82.\nThese results solidify that machine learning models can recommend extract method refactorings while maintaining the\nworkflow of the developer.\n\nCui et al. [16] have proposed an automated approach called Representation -based Extract Method Refactoring Recom-\nmender System (REMS) to suggest appropriate extract method refactoring opportunities. The approach involves mining\nmulti-view representations from a code property graph. First, code property graphs were extracted from training and\ntesting samples. Then, multi-view representations such as tree-view and flow-view representations were generated from\nthe code property graph. Compact bilinear pooling was used to fuse the tree-viewand the flow-viewrepresentations.\nFinally, machine learning classifiers were trained to guide the extraction ofsuitable lines of code asa new method. Six\nrelevant embedding techniques such as CodeBERT, GraphCodeBERT, CodeGPT, CodeT5, PLBART, and CoTexT were\nused to generate various representations of the abstract syntax tree, which were referred to as tree-view representations.\nThe researchers explored the impact of these representations on recommendation performance. The REMS operates in\nthree phases, including 1) feature extraction from code property graphs of training and testing samples, 2) model training\nbased on machine learning techniques, and 3) applicable analysis of behavior preservation and functional usefulness.\nSeven traditional machine learning models such as Decision Tree, K-nearest neighbor, Logistic Regression, Naive Bayes,\nRandom Forest, Support Vector Machine, and Extreme Gradient Boosting, were used. CNN and LSTM were the only\n', 'ASurvey of Deep Learning Based Software Refactoring 29\n\ndeep-learningtechniques employed.The results showed thatthe REMS approach outperformed five state-of-the-art\nrefactoring tools, including GEMS [101], JExtract [83], SEMI [14], JDeodorant [22], and Segmentation [90].\n\nPantiuchina [71] has developed techniques to create a new generation of refactoring recommenders. These recom-\nmenders can predict code components thatare likely to be affected by code smells inthe near future andrecommend\nmeaningful refactorings that emulate the ones that developers would perform. They refer to this approach as just-in-time\nrational refactoring, which has two main goals. First, predicting code quality decay aims to develop techniques that\nalert the developer when acode component is deviating from good design principles before design flaws are introduced.\nSecond, learning refactoring transformations investigates the possibility of applying deep learning models to learn code\nchanges performed by software developers. The researchers plan to investigate if neural machine translation models\ncan be used for the replication of refactoring operations performed by software developers.\n\nPinheiro et al. [74] investigated how trivial class-level refactorings could affect the prediction of non-trivial refactorings\nusing machine learning techniques. They selected 884 open-source projects and extracted the type of refactoring\nfrom classes involved in some operation and code metrics. The researchers grouped the refactorings into trivial and\nnon-trivial ones based on their level of change. Trivial refactorings included adding class annotations, changing access\nmodifiers, removing class annotations, etc. Non-trivial refactorings included extract class, move class, merge class, etc.\nAdditionally, they proposed contexts based on combinations of the refactoring types thatmade itpossible to increase\nthe accuracy of supervised learning models. They used four traditional machine learning models, Decision Tree, Logistic\nRegression, Naive Bayes, and Random Forest. They employed a Neural Network as the only deep-learning technique for\nthis approach. They followed asequence offive steps: selection of software projects, refactoring, and feature mining,\nselection of contexts, training and testing of the machine learning-based models, and evaluation of the results. The\nselection of contexts in step number three had to do with creating several datasets with different combinations of\nrefactoring types. The datasets constructed by the combinations of C1, C2, and C3 were used to predict the refactorings.\nThe four machine learning models were used through the Scikit-learn library, while the Neural Network was used\nthrough Tensorflow Keras. After training, each generated model was validated by predicting the refactorings of the\nfeatures in the test set. The main findings of this approach were: 1) machine learning with tree-based models, such\nas Random Forest and Decision Tree, performed very well when trained with code metrics to detect refactorings, 2)\nseparating trivial and non-trivial refactorings into different classes resulted in a more efficient model, indicative to\nimprove the accuracy of machine learning-based automated solutions, and 3) using balancing techniques that increase\nor decrease samples randomly is not the best strategy to improve datasets composed of code metrics.\n\nPanigrahi et al. [70] developed a refactoring prediction model using an ensemble-based approach. They identified the\noptimal set of code metrics and their association with refactoring proneness by analyzing the structural artifacts of\nthe software program. This approach involved refactoring data preparation, feature extraction, multiphased feature\nextraction, sampling, and heterogeneous ensemble structure refactoring prediction. The proposed model extracted 125\nsoftware metrics from object-oriented software systems using a robust multi-phased feature selection method, which\nincluded Wilcoxon significant text, Pearson correlation test, and Principal Component Analysis (PCA). The optimal\nfeatures characterizing inheritance, size, coupling, cohesion, and complexity were retained. A novel heterogeneous\nensemble classifier was developed using techniques such as ANN-Gradient Descent, ANN-Levenberg Marquardt, ANN-\nGDX, ANN-Radial Basis Function support vector machine, LSSVM-Linear, LSSVM-Polynomial, LSSVM-RBF, Decision\nTree algorithm, Logistic Regression algorithm, and Extreme Learning Machine (ELM) model as the base classifiers. The\n', '30 Nyirongo and Jiang, etal.\n\nresults indicated that the Maximum Voting Ensemble (MVE) achieved better accuracy, recall, precision, and F-measure\nvalues (99.76, 99.93, 98.96, 98.44) compared to the Base Trained Ensemble (BTE). Additionally, it experienced fewer\nerrors (MAE = 0.0057, MORE = 0.0701, RMSE = 0.0068, and SEM = 0.0107) during the implementation to develop\nthe refactoring model. The experimental results recommended that MVE with up-sampling could be implemented to\nimprove the performance of the refactoring prediction model at the class level.\n\n5.1.2 Code Smell Based Technologies. Apart from proposing a deep learning-based approach to identify feature envy\nsmells, one ofthe most common code smells, Liu etal. [53], also used their approach described in Section 4 to recommend\nmove method refactorings. For methods that were predicted to be smelly (with feature envy), they suggested that such\nmethods should be moved via move method refactorings. If only one (noted as inputj) of the testing items generated\nfor a method m was predicted as positive, they suggested moving 1 to the target class tcj that was associated with\nthe positive testing item inputj. Ifmore than one testing items were predictedas positive, they selected one (noted as\ninputi) with the greatest output and suggested moving method m to class tci that associated with inputi. Although their\nneural network described in Section 4 was a binary classifier the output of the neural network wasa decimal varying\nfrom zero to one. The neural network interpreted the predictionas positive if and only ifthe output was greater than\n0.5. [15]. Their results indicated that the approach was accurate in recommending destinations for the smelly methods.\nThe approach achieved, onaverage, an accuracy of 74.94%. They also observed that the approach was more accurate\nthan the state-of-the-art tool in the recommendation of move method refactorings, i.e., JMove [89], JDeodorant [22].\nThis study was extended in Liu et al. [51] where in addition to using deep learning to detect code smells they also\nexplored the recommendation of refactoring solutions for feature envy and misplaced class. The recommendation\nof refactoring solutions for feature envy was the same as in Liu et al. [53]. The CNN used for misplaced class was\nthe same as described in Section 4. To decide whether a given class should be moved from its enclosing package to\nanother package, they leveraged two categories of features code metrics and textual features. The used code metrics\nincluded coupling between objects and message-passing coupling. To evaluate the approach they compared their\nproposed approach to TACO [68] in recommendation of target packages for misplaced classes. The accuracy of the\nrecommendation was critical because if misplaced classes are moved to the wrong positions they remain misplaced.\nThe approach resulted ina greater number ofaccepted recommendations. In total, 488 ofits recommendations were\naccepted whereas the number was reduced to 342 for TACO [68]. Secondly, TACO [68] was more accurate than the\nproposed approach in recommending target packages. It improved the average accuracy from 49.80 to 62.98 percent.\nHowever, on the same code smells where both the proposed approach and TACO [68] made recommendations, their\naccuracy in recommending target packages was close to each other 62.6% for TACO [68] and 60.05% for the proposed\napproach. Based on this analysis they concluded that the proposed approach outperformed the baseline in identifying\nmisplaced classes and it could be comparable to the baseline in recommending target packages.\n\nMaetal. [55] in their pursuit of using pre-trained model CodeTS to extract the semantic relationship between code\nsnippets to detect feature envy code smell, also explored the effectiveness of their approach inrecommending refactoring\nsolutions for the code smell. They wanted to find out if their approach could exceed the state-of-the-art approachesin\nrecommending destinations for the methods to be moved. In their approach, for methods that were predicted as smelly\n(with feature envy, inputecwas predicted positive), they suggested where sucha method should be moved viamove\nmethod refactoring. Then, they fed all testing items inputptci=<code(m),code(ptci)> into the neural network. If only one\n(noted as inputptci) of the testing items generated for m is predicted as positive, then they suggested moving m to the\n\npotential target class (ptcj) that is associated with the positive testing item inputptcj. Ifmore than one testing item is\n', 'ASurvey of Deep Learning Based Software Refactoring 31\n\npredicted as positive, they selected the one (noted as inputptci) with the greatest output and suggested moving method\nm to class ptci that is associated inputptci. The neural network interpreted the prediction as positive ifand only if the\noutput was greater than 0.5. This approach was compared to Liuetal.’sapproach [51]. The results indicated thattheir\napproach improved the accuracy in recommending target classes as itattained an accuracy rate of greater than 90%\nwhile Liu et al. was below 90%.\n\nThe Bi-LSTM witha self-attention mechanism that was proposed by Wangetal. [98] to detect feature envy code smell\nwas also used to recommend refactoring destinations for the methods to be moved. For a method m that had been\nmarked as positive, they predicted its refactoring destination as follows. If there is only one positive example, they\nregard the target class related to this example as the destination. If there was more than one, they chose the target\nclass related to the highest probability in the outputset. The rationale behind this was that higher probability meant\nhigher confidence in the deep neural network obtained. This solution was presented as destination=Cmax where Cmax\nrelated to Pmax=Maxp1,p1,. ,pk. This functional mapping of input to output was presented and computed by the deep\n\nlearning model. This approach was compared to JMove [89], JDeodorant [22], and Liu et al.’s [53] approach. The results\nindicated that their approach was more accurate on destination recommendation than the state-of-the-art.\n\nYuetal. [105] did not onlysolve the problem ofinherentcalling relationships between methods which usually cause\nunimpressive detection efficiency by proposing a Graph Neural Network (GNN) based approach towards feature envy\ndetection butalso utilized the strength ofthe calling relationship of onemethod to anotherto recommend refactoring\nsolutions for the feature envy code smell. For the methods that were predicted to have a smell, they provided refactoring\nsuggestions to move these methods to the classes that best fit their functional implementation. If a smelly method\naccessed only one external class, they recommended movingit to that external class. If the smelly method was interested\nin two or more external classes, they employed an algorithm to suggest the most suitable external class. For this\nalgorithm, they regarded the calling strength as the weight of the edge and obtained the calling strength graph. The\ncalling strength graph was constructed as G2=V,E, W where Vrepresented the set ofnodes, E represented the set of edges\nand Wrepresented the weight matrix of edges. To validate their approach, they compared itagainst, JDeodorant [22],\nJMove [89], and Liuetal.’s[51] approach. The results indicated thattheirapproach had higheraccuracy than the three\nbenchmarksin refactoring recommendations. Specifically, compared with Liuetal.’s[51] work, JDeodorant [22],and\nJMove [89], it improved the accuracy by 10.10%, 5.13% and 11.00% respectively.\n\nLiu et al. [54] proposed an automated approach to detecting and improving inconsistent method names. In addition to\nidentifying inconsistent names, their approach provided a list of ranked suggestions for new names for a given method.\nThe ranked list of similar names was generated using four ranking strategies. The first strategy (R1) relied solely on\nthe similarities between method bodies, ranking the names of similar method bodies according to their similarity to\nthe given method body. The second strategy (R2) groupedidentical names, ranked distinctnames based on the size of\nthe associated groups, and broke ties based on the similarities between method bodies as per R1. The third strategy\n(R3) was similar to R2, but it ranked groups based on the average similarity, regardless of the group size. To avoid\nhighly-ranked but small groups, the fourth strategy (R4) re-ranked all groups produced in R3, downgradingall 1-size\ngroups to the lowest position. To evaluate the performance of their approach in suggesting new names for inconsistent\nnames, the suggested names were ranked using the aforementioned strategies. To ensure a comprehensive assessment,\nthree different scenarios were considered: inconsistency avoidance, first token accuracy, and full name accuracy. The\napproach achieved an accuracy of 34-50% in suggesting subtokens and 16-25% accuracy in suggesting full names.\n', '32 Nyirongo and Jiang, etal.\n\nLiuetal. [21] conducted a study to enhance the deeplearning-based feature envy detection approaches by providing\nreal-world examples. They used their approach to identify feature envy methods that should be moved from their\nenclosing classes to other classes that they envy. They achieved this by using a heuristic-based filtering method, as\noutlined in Section 4. Their approach, feTruth, utilized a trained classifier and a sequence of heuristic rules to predict\nwhether a given method in the testing project was associated with feature envy smell. If the method was associated\nwith feature envy smell, feTruth would suggest the class to which the method should be moved. The accuracy of feTruth\nwas compared against other approaches, namely JDeodorant [22], JMove [89], and Liu et al.’s [51] approach. The results\nshowed that feTruth was accurate in suggesting destination classes for feature envy methods with an accuracy of 93.1%.\nThis was higher than JDeodorant’s accuracy of 80% and comparable to Liu et al.’s[51] and JMove’s accuracy of 87.5%\n\nand 100%, respectively.\n\n5.2 Refactoring Types\n\nFromthe primary studies presentedin this section, we note that various refactoring solutions wererecommendedas\na way of addressing issues related to a particular codebase. When recommending arefactoring solution, a particular\nrefactoring that could be applied is suggested to resolve the identified issue. Refactorings are techniques that are applied\nto the codebase to improve its quality without altering its external behavior. Refactorings are usually applied at different\nlevels of the codebase namely class, method, and variable. Refactorings applied at the class level are usually aimed\nat enhancing the overall structure, maintainability, and readability of the codebase by making changes at the class\nlevel. Examples of these include extract class, collapse hierarchy, rename class, etc. Method-level refactorings involve\nmodifying the internal structure of methods to enhance readability, maintainability, and performance without altering\nthe external behavior of the code. The goal of these is to create cleaner, more efficient, and easier-to-understand methods.\nExamples include extract method, rename method, move method, inline method, etc. Variable-level refactorings involve\nmaking changes to the variables within the codebase to improve the quality, readability, and maintainability without\naltering the external behavior of the program. Examples of these include rename variable, extract variable, inline\n\nvariable, encapsulate field, etc.\n\nTable 4 presents the level of the refactoring solutions that were recommended by the primary studies in our survey\nplusthe representative refactorings suggested at thatlevel. Analyzing the presented data, itbecomesapparentthata\nmajority of the recommended refactoring solutions focus on the method level, with 58.06% of researchers proposing\nsolutions at this granularity. Class-level refactoring follows at 25.81%, and variable-level solutions are suggested in\n16.13% ofthe primary studies. Through this data, we also observed that underthe method level refactoring, the most\ncommon refactoring types being suggested were the extract method and the move method. We noted that extract\nmethod refactoring was found in 62.50% ofthe studies that employed refactoring based recommendation approach\nwhile move method was found in 85.71% of the studies that employed code smell-based approach to recommendation\n\nrefactoring.\n\n5.3 Training Strategies\n\nAs previously mentioned, training strategies play a crucial role in developing a robust and accurate deep-learning\nmodel. In this section, we will discuss the technicalities, tools, and procedures that researchers have used to build deep\n\nlearning models that are effective in recommending refactoring solutions. Researchers have adopted various procedures\n', 'ASurvey of Deep Learning Based Software Refactoring 33\n\nTable 4. Representative refactoring types.\n\nLevel Refactorings References\n\nClass Extract class, Move class, Rename class [3, 9, 65, 66, 69-71, 74]\nMethod Extract method, Move method, Rename method [4,6,9,16,17,21,44,51,53-55,65,66, 71,71,79,98, 105]\n\nVariable Extract variable,Rename variable, Move attribute [9, 59, 65, 66, 71]\n\nrelated to data preprocessing, feature extraction, and data balancing to ensure that they build a deep learning model\nthat is trained with high-quality data for accurate and efficient recommendations of refactoring solutions.\n\n5.3.1 Data Preprocessing. According to our survey, researchers have employed various processes such as tokenization,\nlemmatization, stop word removal, noise removal, and normalization to ensure that their data is well-preprocessed and\ncleaned. Tokenization breaks texts into words, phrases, symbols, or other meaningful elements called tokens. This is\nused to split text into constituent sets of words. Lemmatization replaces the suffix of a word or removes it to obtain\nthe basic word form. Itis used for part of speech identification, sentence separation, and key phrase extraction. The\ngoal of lemmatization is to group different inflected forms of a word so that they can be analyzed as a single item.\nStop word removal involves filtering out common words thatare considered to be of little value in understanding the\nmeaning ofatext. Noise removal refers to the process ofreducing or eliminating irrelevant or unwanted information,\noften referred toas "noise," froma dataset. The goal ofnoise removal isto improve the quality of the data or signal for\nmore accurate analysis or interpretation. For instance, Sagar etal. [79] had to remove and clean HTML tags sincetheir\ndata came from the web. Sagar et al. [79] also checked for special characters, numbers, and punctuation to remove any\nnoise. Normalization refers to the process of transforming data into a standard scale. The goalis to bring the values of\ndifferent variables or features into acomparablerange, preventing one variable from dominating the analysis simply\nbecause of its larger scale. In this process, textual data may be converted to the standard required case, either lowercase,\nuppercase, camel case, etc. We also note through our literature search that other researchers, for example, Alenezi et\nal. [3], employed the process of basic data cleaning by first deleting all unnecessary features from their dataset, then\nfinalizing the process by deleting refactoring type features and replacing the summation of refactoring feature.\n\n5.3.2 Feature Extraction. Various types of features can be extracted to enable a deep learning model to recommend\nappropriate refactoring solutions. These features can include semantic, structural, code metrics, commit messages, or\ndocumentation. Most studies extract features from source code, which includes different metrics depending on the\nrefactoring solution they want to suggest. However, some researchers, such as Aniche et al. [9], used process and\nownership metrics instead of merely employing source code features. For the process metrics, Aniche et al. [9] collected\nfive different types of metrics: the number of commits, the sum of lines added and removed, the number of bug fixes, and\nthe number of previous refactoring operations. They calculated the number of bug fixes by using a heuristic whenever\nany ofthe keywords "bug, error, mistake, fault, wrong, fail, and fix" appeared in the commitmessage,and counted one\nor more bug fixes to that class. The number of previous refactoring operations was calculated based on the refactorings\nthey gathered from the refactoring mining tool. For the code ownership metrics, Aniche et al. [9] adopted the suite\nownership metrics as proposed by Bird et al. [11]. The number of authors was the total number of developers who had\ncontributed to the given software artifact. The minor authors were the number of contributors who had authored less\nthan 5% (in terms of commits) of an artifact. The major authors were the number of developers who contributed at\n', '34 Nyirongo and Jiang, etal.\n\nleast 5% to anartifact. With this, ownership was calculated as the proportion of commits achieved by the mostactive\n\ndeveloper.\n\n5.3.3 Feature Embedding. The primary studies have employed various tools and techniques for feature embedding.\nEmbedding is a type of vectorization that represents words as dense vectors in a continuous space which captures\nsemantic relationships between them. For instance, Cui et al. [16,17] used code and graphembedding techniques to\ngenerate corresponding structural and semantic representations. They also used them to create hybrid representations.\nFor code embedding, they used Code2vec [8] and Code2Seq [7]. Code2Vec is a neural network that automatically\ngenerates vectors from source code, while Code2Seq is a neural network that produces sequences from code snippets.\nFor graph embedding techniques, they explored the use of Deepwalk [72], Node2Vec [30], Walklets [73], GraRep [13],\nLine [88], ProNE[106],and SDNE [97]. DeepWalkand Node2Vec employ random walk to construct sample neighbor-\nhoods for nodes in a graph based on a Skip-gram Natural Language Processing (NLP) model. The goal of Skip-gram is to\nmaximize the likelihood of words appearing in a sliding window co-occurring. Walklets is another random walk-based\ngraph embedding technique that explicitly encodes multi-scale relationships between nodes to produce multi-scale\nrepresentations for them. GraRep is a matrix factorization-based graph embedding technique that constructs matrices\nfrom connections between nodes and factorizes them to produce the embedding result. Line calculates graph embedding\nresults by specifying two functions, one for the first-order node proximity and the other for the second-order node prox-\nimity. ProNEisa fastand scalable graph embeddingtechnique that was recently introduced. Itincludestwosteps, the\nfirstis to effectively initialize graph embedding results by phrasing the problemas sparse matrix factorization, motivated\nby the long-tailed distribution of most graphs and their sparsity. The second stage is to propagate the initial embedding\nresult using the higher-order Cheeger’s inequality [46], aiming at capturing the graph’s localized clustering information.\nSDNE employs deep autoencoders to generate embedding results. Other techniques also emerged from the researchers\ninthe primary studies, for example, the employment of word embedding technologies, e.g., Word2Vec[51,53,79,98],\nvector space models [65, 66], and CodeT5 [105] to achieve embedding.\n\n5.3.4 DataBalancing. Researchers often use various techniques to address the issue of data imbalance forrecommending\nrefactoring solutions. These techniques include the Synthetic Oversampling Technique (SMOTE) and its variants\n(BLSMOTE, SVSMOTE, GraphSMOTE, etc.), UpSample, RUSBoost, Down sampling, and Random sampling. SMOTE\ntechnique is based on the oversampling approach in which synthetic examples are used for oversampling the minority\nclass rather than oversampling with replacement. UpSample is used to improve the number of samples of the minority\nclass by inserting zeros between the samples. RUSBoost is a hybrid approach of data sampling and boosting algorithm\nused to improvethe performance of models trained on skewed data. Toreduce the bias thatmay arise due to the use of\nimbalanced datasets, data balancing techniques are usually employed to create abalanced dataset. Mostresearchers\nemploy the use of sampling techniques to achieve data balance in their datasets. In our survey, 82.00% of primary\nstudies were found to employ sampling techniques in their variant forms. However, Pinheiro et al. [74] concluded that\nusing balancing techniques that increase or decrease samples randomly is not the best strategy for improving datasets\n\ncomposed of code metrics.\n\n5.4 Evaluation\n\n5.4.1 Datasets. Different types of datasets are utilized to suggest accurate refactoring solutions through deep learning\nmodels. These datasets are carefully chosen to ensure that the deep learning model receives the appropriate data for\nmaking the right recommendations. Based onour survey, we found thatresearchers typically clone or download the\n', 'ASurvey of Deep Learning Based Software Refactoring 35\n\nsubject projects from repositories and extract data relevant to their study refactorings. RefactoringMiner [93] was found\nto be the most popular tool for refactoring data mining tasks. RefactoringMiner was used by at least 91.30% of the\nprimary studies. As shown in Figure 7, 86.96% of the researchers used publicly available projects, while only 13.04%\nused private projects with restricted access. Interestingly, 100% of the studies in this category the applications that\nwere used for the experiments and evaluation were developed in the Java language. We also came across the work\nof Mastropaolo et al. [59], who created three datasets to train and evaluate their deep learning model. They built a\nlarge-scale dataset for training the model, tuning parameters, and performing an initialassessment of performance.\nAdditionally, they created reviewed and developers datasets to further evaluate the performance of theirtechnique.\n\nPublic\n\nPrivate\n\nFig. 7. Datasets\n\n5.4.2 Result Metrics. As previously discussed, result metrics are used to measure the effectiveness of a particular\napproach. The standard metrics for this calculation are precision, recall, F-measure, and accuracy. Our literature\nsearch findings on the metrics used by researchers in this category are shown in Figure 8. According to Figure 8, F-\nmeasure, recall, and precision are the most commonly used metrics for evaluating various approaches in recommending\nrefactoring solutions. F-measure was used in at least 95.65% of the primary works, while precision and recall were\nemployed in 95.65% and 86.95% of the studies, respectively.\n', '36 Nyirongo and Jiang, etal.\n\n2\n5 204\no\ng\no\nSs\nSs\nEs)\nQ 154\n3\na\na\n=\na 104\ng\no\na.\noO\na.\n‘S\n5 54\no\n2\nEE\nE}\nZz\n0 >\nPrecision Recall F1-score Accuracy AUC MCC\nPerformance assessment\n\nFig. 8. Metrics used for performance assessment.\n\n6 END-TO-END CODE TRANSFORMATION AS REFACTORING\n\nThe end-to-end code transformation as refactoring refers to the actual process of modifying an existing codebase\nto improve its structure, readability, maintainability, or performance without changing its external behavior. The\nend-to-end code transformation of refactorings differs from the other refactoring tasks (i.e., the detection of code\nsmells, the recommendation of refactoring solutions) in thatit involves the application of the suggested refactoring\nto the codebase, reviewing the refactoring code to ensure itadheres to coding standards, validating that the external\nbehavior of the codebase remains unchanged, and updating any documentation to reflect the changes made during the\nrefactoring, such as comments, inline documentation, and external documentation if necessary. This process focuses on\nactively making changes to the codebase which can include the actual renaming of variables, the moving of methods,\nextracting methods, simplifying complex expressions, restructuring code, and so on. In this section, we will explore\nhow researchers have utilized deep learning models to conduct end-to-end code transformations as refactorings. Our\nspecific focus will be on the technologies utilized for conducting the refactoring code transformations.\n\nSzalontai et al. [87] have developed a method using deep learning to refactor source code, which was initially developed\nfor the general-purpose programming language and runtime environment, Erlang. This approach has two main\ncomponents: a localizer and a refactoring component. Together, they enable the localization and refactoring of non-\nidiomatic code patterns into their idiomatic counterparts. The method processes the source code as a sequence of\ntokens, making it capable of transforming even incomplete or non-compilable code. To do this, the source code is\ntransformed into a sequence of tokens, using the Erlang module tok to tokenize the source code. The module obtains\ntoken types such as atom, integer, variable, etc. These tokens are then provided as input into the neural network to\nlocalize non-idiomatic functions. The neural network consists of convolutional, recurrent, and feedforward components.\nThe tokens provided as input are embedded into a 64-dimensional vector space, and thena one-dimensional convolution\nis applied to each code chunk using 126=8 filters and a kernel size of 5. Two pooling operators are applied to the\nconvolutional outputs, average and minimax. These two operations yield two intermediate representations foreach\n', 'ASurvey of Deep Learning Based Software Refactoring 37\n\nchunk of the source code. The idiomatic alternative is generated using a recurrent sequence -to-sequence architecture\nwithan attention mechanism. The non-idiomatic tokenized code is first fed to the encoder, which produces a hidden\nrepresentation of the input sequence. This is achieved through the use of a Recurrent Neural Network consisting of four\nBiLSTM layers with 64 units each. The decoder uses a single LSTM layer with 256 units to generate an output sequence\nelement by element, producing the idiomatic alternative. Both the localizer and the refactoring models were evaluated\nona test set that was separated from the training data before the training process. The accuracy of the localizer was\nmeasured as the ratio of classified code chunks to the total number of chunks in the test set and was found to be\n99.09%. For the refactoring component, the ratio of error-free transformations against the total number of attempted\ntransformations was measured resulting in an accuracy of 99.46%. These results indicate that the presented models were\ntrained successfully and are capable of performing refactorings that are similar to the onesin the training datasets.\n\nTufano et al. [96] quantitatively investigated the ability of a Neural Machine Translation (NMT) model to learn\nhow to automatically apply code changes implemented by developers during pull requests. They harnessed NMT to\nautomatically translate a code component from its state before the implementation ofthe pull request (pre-PR) and\nafter the pull request has been merged (post-PR), thereby emulating the code changes thatwould beimplemented by\ndevelopers in the pull request. This is the first work that used deep learning techniques to learn and create a taxonomy\nfrom a variety of code transformations taken from the developer’s pull requests. In this investigation, they first mined a\ndataset of complete and meaningful code changes performed by developers in merged pullrequests, extracted from\nthree Gerrit repositories (Android, Google, and Ovirt). Then they trained the NMT models to translate pre-PR code into\npost-PR code, effectively learning code transformations as performed by developers. RNN Encoder-Decoder and Beam\nSearch Decoding were used as NMT models for this approach. The RNN Encoder-Decoder architecture was coupled with\nan attention mechanism which is commonly adopted in NMT tasks. The RNN encoder was used for encoding a sequence\nof tokens x into vector representation while the RNN Decoder was used for decoding the representation into another\nsequence of tokens y. The primary purpose of the employed beam search decoder was to improve the quality of the\ngenerated token sequences by exploring multiple possible paths instead of simply selecting the most likely next token\nat each step. The NMT model was able to predict and learn from some transformations. This was used to develop a\ntaxonomy of the transformations with three subcategories grouping the code transformation into bug fixing, refactoring,\nand other. The refactoring subtree included all code transformations that modified the internal structure of the system\nby improving one or more of its non-functional attributes without changing the system’s external behavior. Under this\nsubtree five subcategories were formulated, namely, inheritance (forbid method overriding by adding the final keyword\nto the method declaration, invoke overriding method instead of overridden by removing the super keyword to the\nmethod invocation and making a method abstract through the abstract keyword and deleting the method body), methods\ninteraction (add parameter refactoring (i.e., a value previously computed in the method body is now passed as parameter\nto it), and broadening the return type of a method by using the Java wildcard (?) symbol), readability (braces added to if\nstatements with the only goal of clearly delimiting their scope, the merging of two statements defining and initializing\n\navariable intoa single statement, the addition/removal of the this qualifier, to match the project’s coding standards,\nreducing the verbosity ofa generic declaration by using the Java diamond operator, refactoring anonymous classes\nimplementing one method tolambda expressions, to make the code morereadable, simplifying Boolean expressions,\nand merging two catch blocks capturing different exceptions into one catch block capturing both exceptions using\nthe or operator), naming (renaming of methods, parameters, and variables), and encapsulation (modifying the access\nmodifiers, e.g., changing a public method to a private one). The results showed that NMT models are capable of learning\n', "38 Nyirongo and Jiang, etal.\n\ncode changes and perfectly predict code transformations in up to 21% of the cases when only a single translation is\ngenerated and upto32% when 10 possible guesses are generated. These results highlight the ability ofthe models to\nlearn froma heterogeneous set of pull requests belonging to different datasets, indicating the possibility of transfer\nlearning access projects and domains.\n\nTo facilitate the rename refactoring process and reduce the cognitive load of developers, Liu et al. [52] proposed a\ntwo-stage pre-trained framework called RefBERT. This framework is based on the BERT architecture and was designed\nto automatically suggest a meaningful variable name, which is considered a challenging task. The researchers focused on\nrefactoring variable names, which is more complex than refactoring other types of identifiers, such as method names and\ntype names. RefBERT uses 12 RoBERTa layers, which area replicated version of the original BERT model with improved\nperformance. The approach is based on three observations. First, rename refactoringis similar to Masked Language\nModelling (MLM),a pretext task commonly used in pre-training BERT. MLM fills the masked part ofa text according\nto its context. Similarly, rename refactoring aims to suggest a meaningful variable name according to the context.\nTherefore, MLM can be adopted for training an automatic rename refactoring model. Second, unlike the variable name\nprediction task, where only the context of the target variable is known, in rename refactoring, both the context of the\ntarget variable and the variable name before refactoring are known. Contrastive learning, which contrasts positive and\nnegative samples for improving representation learning, is an ideal learning paradigm for automatic rename refactoring.\nThe researchers expected the generated name to be close tothe variable nameafter refactoring but far away from the\nvariable name before refactoring. Third, unlike natural language text where words should follow a strict order to ensure\ngrammatical correctness, subtokens in avariable name do not have such a restriction. Different orders of subtokens\nfor a variable name do not significantly affect our understanding of the variable. Thus, the standard cross-entropy\nloss that emphasizes the strict alignment between the prediction and the targetis suboptimal for automatic rename\nrefactoring. RefBERT was trained to generate refactorings in two steps: Length Prediction (LP), where itpredicts the\nnumber of tokens in the refactored variable name, and Token Generation (TG), where given the predicted number\nof tokens, RefBERT generates tokens in the refactored variable name. To train RefBERT, the researchers used the\nCodeSearchnet [4.0] and Java-Small [7] datasets in the pretraining stage. During the fine-tuning stage, they also used\nJavaRef and TL-Codesum [38] datasets. JavaRef was constructed by the researchers by applying data collection and\npreprocessing procedures on open-source datasets collected from GitHub. The experimental results demonstrated the\neffectiveness of RefBERT in automatic rename refactoring.\n\nFrom the literature presented, we note from Table 5, that Recurrent Neural Networks (RNN) through its variants (LSTM\nand GRU) have mostly been used for the end-to-end transformation as refactorings. The usage of RNN was found inat\nleast 75% of the studies. This was followed by Transformer technologies (e.g., BERT), which were utilized in 25% of the\nstudies. Notably, to enhance the performance of the proposed techniques the researchers adopted the inclusion of other\ntechniques in their approaches. Szantotai et al. [87] and Tufano et al. [96] used an attention mechanism to improve the\nmodel's ability to focus on relevant parts of the input sequence when generating the output sequence. In contrast, Liu\netal. [52] used contrastive learning to contrast positive and negative samples forimproving representation learning,\nand was used for automatic rename refactoring. The inclusion of contrastive learning helped the model to understand\nthe context of the target variable and the variable name before refactoring.\n\nFrom the presented studies, we note that researchers have employed various deep-learning techniques to perform\nthe code transformation for different types of refactorings. Szalontai et al. focused on using a deep learning model\n", 'ASurvey of Deep Learning Based Software Refactoring 39\n\nTable 5. Technologies used for the end-to-end refactorings transformation\n\nTechnologies Refactorings Deployment tools/plaforms References\nLSTM+GRU+attention mechanism Non idiomatic components Erlang [87]\nNMT(RNN+Beam search)+attention mechanism Non functional attributes Java [96]\nBERT(12RoBERTA)+contrastive learning Rename refactoring RefBERT-Java [52]\n\nto refactor nonidiomatic code patterns into idiomatic ones across various levels of code organization such as class,\nmethod, and variable. Typically, the choice of the appropriate level depends on the specific issues identified in the\ncodebase, with refactorings like extract class, extract method, and rename variable being associated with these patterns.\nSzalontai etal.’s approach primarily targeted the general-purpose programming language Erlang. Conversely, Liu et\nal.’s [52] and Tufano et al. [96] approaches specifically targeted the renaming refactoring for variables and refactoring\nofnon-function attributes, respectively, using the Java language. Thus, from Table 5, we observe that researchers are\nemploying a specific approach to the conduction ofend-to-end transformation of refactorings as found in 66.67% of\nthe studies i.e., variable renaming and non-function attribute refactoring. In contrast, 33.33% of the studies opted for a\ngeneral approach in the conduction of the refactoring transformation where the changes could be used at different\n\nlevels (i-e., class, method, variable).\n\n7 MINING OF REFACTORINGS\n\nMining of refactorings refers to the automatic process of identifying and extracting instances of refactorings from\nexisting codebases. To conduct the mining of refactorings for deep learning-based refactoring, traditional refactoring\nminers are used. The refactoring miners utilize various techniques such as static analysis, pattern recognition, and\nheuristic-based methods to identify and discover refactoring activities that were carried out within codebases. The\noutputs generated by these miners might consist of labeled examples that indicate where and howrefactorings have\nbeen applied. These labeled examples serve as ground truth data, forming the foundation for training and evaluating\ndeep learning models. By leveraging the outputs of traditional refactoring miners, large datasets of labeled refactorings,\nenabling the development ofaccurate and robust deep-learning models capable of automating software refactoring\nprocesses can be created. This integration of mining approaches with advanced deep-learning methodologies accelerates\nthe advancement of intelligent tools aimed at enhancing code quality and maintainability.\n\nSeveral traditional refactoring miners have been proposed by researchers to aid in the mining of refactorings. Tsantalis\nproposed RefatoringMiner [93] which represents the implementation of software entities as abstract syntax trees (ASTs),\nand computes the similarity between two entities according to the name-based similarity and the statement-based\nsimilarity. With such similarities, RefactoringMiner maps entities between two successive versions and leverages a\nsequence of heuristics to discover software refactorings based on the mapping. RefactoringCrawler developed by Dig et\nal. [20], is an analysis tool that detects refactorings that happened between two versions of a component. The strength\nof the tool lies in the combination ofa fast syntactic analysis to detect refactoring candidates, and a more expensive\nsemanticanalysis to refine these candidates. Silva et al. [85] proposed RefDiffwhich utilizes static analysis and code\nsimilarity to detect various refactorings. It begins by tokenizing the source code of the project. Each code element (such\n', '40 Nyirongo and Jiang, etal.\n\nas classes, methods, and fields) is transformed into a bag of tokens. Ref-Finder proposed by Prete et al. [75] encodes code\nelements (e.g,, classes, methods, and fields) and their relationships using logic predicates to detect the refactorings. Liu\net al. [49] proposed ReMapper an automated iterative approach used to match software entities between two successive\nversions for the discovery of refactorings. ReMapper takes full advantage of the qualified names, the implementations,\nand the references of software entities. It leverages an iterative matching algorithm to handle the interdependence\nbetween entity matching and the computation of reference-based similarity. Researchers have utilized some of these\n\ntraditional refactoring miners to mine refactorings to train deep learning models in the process of refactoring as follows.\n\nAlthough deep learning technologies have notyet been employed to distinguish refactorings from other source code\nmodifications as RefactoringMiner or Ref-Finder do, deep learning technologies have been successfully employed to\nidentify refactoring-containing commits by analyzing their associated commit messages. For example, Marmolejos et\nal. [58] developed a framework that used text-mining, natural language preprocessing, and supervised machine learning\ntechniques to automatically identify and classify refactoring activities in commit messages. The framework focused on\ndetecting Self-Affirmed Refactorings (SAR), which are refactoring activities reported in commit messages. The approach\nused a binary classification method to overcome the limitations of the manual process proposed in previous studies. The\nframework had four main parts. The first part involved preparing the data and processing the content of the commit\nmessages to remove unnecessary and irrelevant information, as well as normalize the data. In the second part, the\ndata was converted into hash values, with each hash value representing one or more features in the commit messages.\nThe third part involved filtering the features to select only the most important ones from the dataset. Finally, in the\nfourth part, machine learning algorithms were trained and tested based on the selected features. The resulting two-class\nclassifier was able to operate over unlabelled texts. For this approach, the authors used four classifiers, including Bayes\nPoint Machine, Logistic Regression, Boosted Decision Tree, and Average Perceptron, as well as one deep learning-based\nclassifier, Neural Network. The datasetused in this approach contained 1,208,970 refactoring operations, extracted\nusing Refactoring Miner [93] from 3,795 open-source Java projects. From this dataset, the authors extracted commit\nmessages containing the required patterns to create their refactoring dataset. Since the employed machine learning\ntechniques could not directly identify text, the authors converted the collected data into hashes. They used the feature\nhashing technique, also known as a hashing trick, to derive features. In this technique, various words with varying\nlengths were mapped to different features based on the hash value. To determine the relevance of each attribute in\nthe dataset, Chi-Square (CHI) was used to give a score, while Fisher Score (FS) was used to select a subset of features\nand score the distance between them. The machine learning classifiers were trained usinga stratified train-testsplit\nmethodology, where 70% of the rows of the transformed dataset from the selected features were used for training and\nthe remaining 30% were used to measure the error rate. The approach proved to be efficient, as the authors obtained\n\nsubstantial accuracy.\n\nAlomar et al. [6] aimed to investigate whether different words and phrases used in refactoring commit messages\nare unique to different types of refactorings. To achieve this, they employed machine learning techniques to predict\nrefactoring operation types based on the commit messages. The prediction of the refactoring operation was formulated\nas a multiclass classification problem, which relied on textual mining of commit messages to extract relevant features\nfor each class. The researchers collected a dataset of refactorings from 800 projects, where each instance presented\nacommit message and arefactoring type. They identified six preferred method-level refactorings, including extract\nmethod, inline method, move method, pull-up method, push-down method, and rename method. To identify relevant\nfeatures, they used the n-gram technique proposed by Manningand Hinrich [57]. Nine supervised machine learning\n', 'ASurvey of Deep Learning Based Software Refactoring 41\n\nalgorithms were applied, and the results were compared against a keyword-based baseline approach used in Murphy\net al. [62]. The results revealed that the predictive accuracy for rename method, extract method, and move method\nranged from 63% to 93% in terms of F-measure. Nevertheless, the model encountered challenges in accurately discerning\nbetween Inline Method, Pull-up Method, and Push-down Method, with F-measure scores falling within the range of\n42% to 45%. Additionally, it’s noteworthy that the keyword-based approach exhibited significantly lower performance\ncompared to the machine learning models.\n\n8 CHALLENGES AND OPPORTUNITIES\n\nAs the use of deep learning models in the domain of software refactoring continues to grow, itbecomes imperative to\nclosely look at the challenges and opportunities linked to their adoption. Despite showcasing promising capabilities in\naiding different tasks of the refactoring process, there are still some challenges associated with their application. This\nsection explores the challenges confronted by deep learning models in supporting the process of software refactoring,\nconcurrently shedding light on prospective opportunities for future work.\n\n8.1 Challenges\n\nWhile deep learning models have proven to be effective in supporting the process of software refactoring, their adoption\ninto this field is not without challenges. From our survey, we note the following challenges.\n\n+ Limited generalization of deep learning techniques across diverse paradigms is a significant concern. Many\nstudies have developed approaches concentrating on specific code smells (e.g., feature envy, brain class, brain\nmethod) within a particular language, such as Java. This specialization restricts the applicability of these models\ntodifferent code smells or programming languages. Given that codesmells can manifestdifferently in diverse\ncontexts, amodel trained on one set of smells may not exhibit robust performance on others. Moreover, code\nsmells often coexist and exhibit interactions. For instance, a lengthy method may signal a broader design issue,\nlike a godclass. Approaches focused onindividual smells in the detection of code smells might overlook these\nintricate interactions, resulting in incomplete or inaccurate outcomes. Notably, based on the compiled primary\nworks, asubstantial majority (at least87.50%) employed datasets developed ina singular programming language,\ni.e., Java, posing a challenge for the generalization of these approaches to other programming languages.\n\n* Challenges in creating generic classification and feature engineering. Developing a universal classifier for\ndiverse refactoring processes has proven challenging. This challenge is particularly evident in the detection of\ncode smells, where different types of code smells necessitate distinct features and characteristics for accurate\nidentification. Employing a one-size-fits-all approach may lead to diminished precision and recall, especially\nin studies utilizing sequential modeling-based deep-learning approaches to support software refactoring.\nAdditionally, extracting pertinent features from abstract syntax trees or sequences of statements presents\ndifficulties. The model’s effectiveness relies on the accurate capture of both semanticand structural features.\nEstablishing suitable mechanisms for feature extraction is pivotal for the success of deep learning-based\napproaches.\n\n+ Concerns about data quality and representativeness are pivotal factors influencing the performance of deep\nlearning models. Certain approaches utilized automatically generated labeled data, potentially lacking accurate\n', '42 Nyirongo and Jiang, etal.\n\nrepresentation of real-world scenarios. Incorporating real-world examples could enhance the effectiveness of\ndeep learning models.\n\n+ Limited adoption. Developers may be resistant to adopting automated refactoring tools supported by deep\nlearning due to concerns about the reliability and trustworthiness of the generated code changes. Notably, most\nof the current studies on the end-to-end code transformation for refactorings by deep learning models have\nused prototype tools and non-industrial datasets which do notreflect the actual tools and data pools used by\n\nprogrammers.\n\n+ Need for continuous learning. Software systems evolve as software systems undergo maintenance and updates.\nModels trained ona static dataset may become outdated and may not effectively support the process of software\nrefactoring in such without continuous learning mechanisms.\n\n8.2 Opportunities\n\nAmidst these challenges presented in Section 8.1, there exist some opportunities for advancing the integration of deep\nlearning models in software refactoring. Some of the opportunities, according to our survey, are as follows.\n\n+ According to the taxonomy presented, deep learning models have been used for various tasks, including\ndetecting code smells, recommending refactoring solutions, conducting refactorings, and mining refactorings.\nOur survey of the literature shows that deep learning models are primarily employed for detecting code smells,\naccounting forat least 56.25% inthis category. The recommendation ofrefactoring solutions accounts for 33.33%,\nend-to-end codetransformationas refactoring for 6.25%, and mining ofrefactorings for 4.17%. However, we\ndid not find any significant study on the use of deep learning for software refactoring quality assurance inour\nliteraturesearch. This revelation shows thatthere is an imbalance in howdeep learning has beenemployed in\nsupporting refactoring tasks and thus, presents an opportunity for future work. Itis highly valuable to fill this\n\ngap and ensure that all the tasks of software refactoring are fully supported by deep learning models.\n\n+ According to the primary studies presented, various types of deep learning models have been used for software\nrefactoring. Most of the studies focused on utilizing hybrid models that combine deep learning models with\nother techniques to improve model performance. The data shows that at least 58.69% of the primary studies used\nhybridapproaches, while the remaining 41.31% used generic deep learning models. Thelatter encompassed\nsequential modeling, explainable and feedback-centric approaches that fit into developers’ workflow. Among\nthe deep learning models, CNN was the most commonly used model, found in at least 43.48% of the primary\nworks, followed by RNN at 34.78%, and its variants (such as GRU, LSTM, GRU, etc.). The other 21.74% ofthe\nstudies employed other deep learning models, such as GCN, GNN, ResNet, MLP, etc. However, only a few\nstudies, such as Sharma et al. [80, 81], explored the use of transfer learning techniques to enable deep learning\nmodels trained on one project to be effectively used for refactoring in different projects. There is a need to carry\nout more exploration of transfer learning approaches as they can result in better generalization and minimize\nthe need for extensive project-specific training data. Notably, Himesh et al. [64] and Yin et al. [104] werea\nfew of the researchers who explored the inclusion of feedback and developments of explainable deep learning\nmodels for refactoring. To improve the adoption of deep learning models in the software refactoring process by\nsoftware developers, itis necessary to do more explorations in the inclusion of feedback and explainability in\nthe deep learning models.\n', 'ASurvey of Deep Learning Based Software Refactoring 43\n\n* Our literature search has revealed that deep learning models have been predominantly used for method-level\nrefactorings. Among the primary studies we surveyed, 55.41% applied deep learning models for refactorings\natthe method level, followed by 30.45% forclass level, 10.12% for variable level,and 4.02% for other types of\nrefactorings. The most frequent use of deep learning techniques was for detecting and applying move method\nand extract method refactorings, both of which occur at the method level of the codebase. Itis worth noting\nthat most of the work on identifying refactoring opportunities by detecting code smells has been focused on\nmethod-level code smells, particularly the feature envy smell which often leads to the recommendation and\nsuggestion of move method refactoring. This opens up room for more future work in employing deep learning\nmodels for refactorings occurring at other levels than just the method level.\n\n+ Based on the literature, it has been found that deep learning models are effective in supporting the process of\nsoftware refactoring. These models have outperformed the existing approaches or tools used in refactoring by\nachieving an average F-measure of 76%, whichis a significant improvement of 30% onaverage compared to\nthe state-of-the-artapproaches. However, thereis stilla need to develop dynamicand adaptive deep-learning\nmodels that can continuously learn and adjust to changes in coding standards, project goals, and evolving best\npractices. This could involve using reinforcementlearning approaches or other adaptive learningstrategies.\nAdditionally, according to our survey, most of the primary studies did not use industrial datasets, which makes it\ndifficult to generalize the findings of most of the techniques. Toimprove the effectiveness of deep learning-based\n\ntechniques for refactoring, it is necessary to incorporate more real-world industrial data.\n\n9 CONCLUSIONS\n\nIn this paper, we have presented a survey on deep learning-based software refactoring. Our focus was on the process of\nsoftware refactoring and how it can be supported by deep learning models. We have categorized the studies based on the\nspecific refactoring task that was supported by deep learning models. Our taxonomy has identified five main categories\nwhich include the detection of code smells, recommendation of refactoring solutions, end-to-end code transformation\nas refactoring, quality assurance for refactoring, and mining of refactorings. We have presented key aspects under\nthese categories which have provided insight into the research direction in the deployment of deep learning models for\n\nsoftware refactoring.\n\nREFERENCES\n\n1] Charu C. Aggarwal. 2018. Neural Networks and Deep Learning: a textbook. Cham (Switzerland) Springer 2018. https: /linkspringer.com/book/10.\n1007/978-3-319-94463-0\n\n2] AmalAlazba, Hamoud Aljamaan, and Mohammad R.Alshayeb. 2023. Deep learning approaches forbad smell detection:a systematic literature\nreview. Empirical Software Engineering 28 (2023). https://api.semanticscholar.org/CorpusID:258591793\n\n3] Mamdouh Alenezi, Mohammed Akour, and Osama Al Qasem. 2020. Harnessing deep learning algorithms to predict software refactoring.\nTELKOMNIKA Telecommunication Computing Electronics and Control 18 (2020), 2977-2982. https://api.semanticscholar.org/CorpusID:225015544\n4] Eman Abdullah Alomar, Anton Ivanov, Zarina Kurbatova, Yaroslav Golubev, Mohamed Wiem Mkaouer, Ali Ouni, Timofey Bryksin, Le Nguyen,\nAmit Dilip Kini, and Aditya Thakur. 2021. AntiCopyPaster: Extracting Code Duplicates AsSoon As They Are Introduced in the IDE. Proceedings of\nthe 37th IEEE/ACM International Conference on Automated Software Engineering (2021). https: //api.semanticscholar.org/CorpusID:245634394\n\n5] Eman Abdullah Alomar, Anton Ivanov, Zarina Kurbatova, Yaroslav Golubev, Mohamed Wiem Mkaouer, Ali Ouni, Timofey Bryksin, Le Nguyen,\nAmit Dilip Kini, and Aditya Thakur. 2023. Just-in-Time Code Duplicates Extraction. Inf. Softw. Technol. 158 (2023), 107169. https://api.\nsemanticscholar.org/CorpusID:256621589\n\n6] Eman Abdullah Alomar, Jiaqian Liu, Kenneth Addo, Mohamed Wiem Mkaouer, Christian D. Newman, Ali Ouni, and Zhe Yu. 2021. On the\ndocumentation of refactoring types. Automated Software Engineering 29 (2021). https://api.semanticscholar.org/Corpus|D:244896267\n\n', '44\n\n17]\n[8\n\n(9\n\n[10]\n\n11\n\n12\n\n13\n\n14\n\n15\n16\n\n17\n\n18\n\n19\n\n20)\n\n21\n\n22\n\n23\n\n24)\n\n25)\n\n26)\n\n27)\n\n28)\n29)\n\n30)\n\n31\n\n32\n\n33\n\nNyirongo and Jiang, etal.\n\nUri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Generating Sequences from Structured Representations of Code. ArXiv\nabs /1808.01400 (2018). https://apisemanticscholar.org/CorpusID:51926976\n\nUri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. code2vec: learning distributed representations of code. Proceedings of the ACM on\nProgramming Languages 3 (2018), 1 - 29. https://apisemanticscholar.org/Corpus!D:4710028\n\nMauricio Finavaro Aniche, Erick Galani Maziero, Rafael Serapilha Durelli, and Vinicius H. S. Durelli. 2020. The Effectiveness of Supervised\nMachine Learning Algorithms in Predicting Software Refactoring. IEEE Transactions on Software Engineering 48 (2020), 1432-1450. https:\n//api.semanticscholar.org/Corpus!D:210157308\n\nAntoine Barbez, Foutse Khomh, and Yann-Gaél Guéhéneuc. 2019. A Machine-learning Based Ensemble Method For Anti-patterns Detection. ArXiv\nabs /1903.01899 (2019). https://apisemanticscholar.org/Corpus|D:67877051\n\nChristian Bird, Nachiappan Nagappan, Brendan Murphy, Harald Gall, and Premkumar Devanbu. 2011. Don’t Touch My Code! Examining the\nEffects of Ownership on Software Quality. SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software\nEngineering, 4-14. https://doi.org/10.1145/2025113.2025119\n\nChristopher M. Bishop. 1995, Neural networks for pattern recognition. https://api.semanticscholar.org/CorpusID:60563397\n\nShaosheng Cao, WeiLu,and Qiongkai Xu. 2015. GraRep: Learning Graph Representations with Global Structural Information. Proceedings of the\n24th ACM International on Conference on Information and Knowledge Management (2015). https://apisemanticscholar.org /CorpusID:17341970\nSofia Charalampidou, Apostolos Ampatzoglou, Alexander Chatzigeorgiou, Antonios Gkortzis, and Paris Avgeriou. 2017. IdentifyingExtract\nMethod Refactoring Opportunities Based on Functional Relevance. IEEE Transactions on Software Engineering 43 (2017), 954-974. https:\n//api.semanticscholar.org/Corpus!D:4642697\n\nFrangois Chollet. 2018. Keras: The Python Deep Learning library. https://api.semanticscholar.org/CorpusID:215844202\n\nDi Cui, Qiangqiang Wang, Siqi Wang, Jianlei Chi, Jianan Li, Lu Wang, and Qingshan Li. 2023, REMS: Recommending Extract Method Refactoring\nOpportunities via Multi-view Representation of Code Property Graph. 2023 IEEE/ACM 31st International Conference on Program Comprehension\n(ICPC) (2023), 191-202. https: //apisemanticscholar.org/Corpus!D:259860601\n\nDiCui, Siqi Wang, YongLuo, Xingyu Li, Jie Dai, LuWang, and Qingshan Li. 2022. RMove: Recommending Move Method Refactoring Opportunities\nusing Structural and Semantic Representations of Code. 2022 IEEE International Conference on Software Maintenance and Evolution (ICSME) (2022),\n281-292. https://api.semanticscholar.org/CorpusID:254902806\n\nAnanta Kumar Das, Shikhar Yadav, and Subhasish Dhal. 2019, Detecting Code Smells using Deep Learning. In TENCON 2019 - 2019 IEEE Region 10\nConference (TENCON). 2081-2086. https://doi.org/10.1109/TENCON.2019.8929628\n\nSeema Dewangan, Rajwant Singh Rao, Alok Mishra, and Manjari Gupta. 2021. A Novel Approach for Code Smell Detection: An Empirical Study.\nIEEE Access PP (2021), 1-1. https://api.semanticscholar.org/CorpusID:245065600\n\nDanny Dig, Can Comertoglu, Darko Marinov, and Ralph E. Johnson. 2006. Automated Detection of Refactorings in Evolving Components. In\nEuropean Conference on Object-Oriented Programming. https: //apisemanticscholar.org/CorpuslD:12303996\n\nBo Liu et al. 22023, Deep Learning Based Feature Envy Detection Boosted by Real-World Examples. (22023), https://lyoubo.github.io/papers/\nDeep_Learning_Based_Feature_Envy_Detection_Boosted_by_Real-World_Examples.pdf\n\nMarios Fokaefs, Nikolaos Tsantalis, and Alexander Chatzigeorgiou. 2007. JDeodorant: Identification and Removal of Feature Envy Bad Smells. In\nInternational Conference on Smart Multimedia, https://apisemanticscholar.org/CorpusID:19001314\n\nFrancesca Arcelli Fontana, Mika Mantyla, Marco Zanoni, and Alessandro Marino. 2016. Comparingand experimenting machine learning techniques\nforcodesmell detection. Empirical Software Engineering 21 (2016), 1143-1191. https://apisemanticscholar.org/CorpusID:16222152\nFrancesca Arcelli Fontana, Mika Mantyla, Marco Zanoni, and Alessandro Marino. 2016. Comparingand experimenting machine learning techniques\nforcodesmell detection. Empirical Software Engineering 21 (2016), 1143-1191. https://apisemanticscholar.org/CorpusID:16222152\nFrancesca Arcelli Fontana and Marco Zanoni. 2017. Code smell severity classification using machine learning techniques. Knowl. Based Syst. 128\n(2017), 43-58. https://api.semanticscholar.org/CorpusID:39781104\n\nMartin Fowler, 2002. Refactoring: Improving the Design of Existing Code. In Extreme Programming and Agile Methods — XP/Agile Universe 2002,\nDon Wells and Laurie Williams (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 256-256.\n\nJoshua Garcia, Daniel Popescu, George T. Edwards, and Nenad Medvidovié, 2009. Identifying Architectural Bad Smells. 2009 13th European\nConference on Software Maintenance and Reengineering (2009), 255-258. https: //api.semanticscholar.org/CorpusID:1847981\nSuryanarayana Girish, Samarthyam Ganesh, and Sharma Tushar. 2015. Refactoring for Software Design Smells: Managing Technical Debt.\n\nIan Goodfellow, Yoshua Bengio,and Aaron Courville.2016. Deep Learning. Cambridge (Massachusetts): MIT Press. https://doiorg/10.1007/s10710-\n017-9314-z\n\nAditya GroverandJure Leskovec.2016. node2vec: Scalable Feature Learning forNetworks. Proceedings ofthe 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (2016). https://api.semanticscholar.org/Corpus!D:207238980\n\nXueliang Guo, Chongyang Shi, and He Jiang. 2019. Deep semantic-Based Feature Envy Identification. Proceedings of the 11th Asia-Pacific Symposium\non Internetware (2019). https://api.semanticscholar.org/CorpusID:207811924\n\nMouna Hadj-Kacemand Nadia Bouassida. 2018. A Hybrid Approach To Detect Code Smells using Deep Learning. In International Conference on\nEvaluation of Novel Approaches to Software Engineering. https://api.semanticscholar.org/CorpusID:14006917\n\nMouna Hadj-Kacem and Nadia Bouassida. 2019. Deep Representation Learning for Code Smells Detection using Variational Auto-Encoder. 2019\nInternational Joint Conference on Neural Networks (IJCNN) (2019), 1-8. https://api.semanticscholar.org/Corpus!D:203605428\n', 'ASurvey of Deep Learning Based Software Refactoring 45\n\n34)\n35)\n\n36)\n\n37)\n\n38)\n\n39)\n\n40)\n\n41\n\n42\n\n43\n\n44)\n\n45)\n\n46)\n\n47)\n\n48)\n\n49)\n\n50)\n\n51\n\n52\n\n53\n\n54)\n\n55)\n\n56)\n\n57)\n58)\n\n59)\n\nAbeer Hamdy and Mostafa Tazy. 2020. Deep Hybrid Features for Code Smells Detection. https://api.semanticscholar.org/CorpusID:221505409\nZhang Hanyu and Tomoji Kishi. 2023. Long Method Detection Using Graph Convolutional Networks. Journal of Information Processing 31 (08\n2023), 469-477. https://doiorg/10.2197/ipsjjip.31.469\n\nVincent J. Hellendoorn and Premkumar T. Devanbu. 2017. Are deep neural networks the best choice for modeling source code? Proceedings of the\n2017 11th Joint Meeting on Foundations of Software Engineering (2017). https://api.semanticscholar.org/Corpus|D:21164835\n\nJie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. 2017. Squeeze-and-Excitation Networks. 2018 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (2017), 7132-7141. https://api.semanticscholar.org/CorpusID:140309863\n\nXing Hu, Ge Li, Xin Xia, D. Lo, and Zhi Jin. 2019. Deep code comment generation with hybrid lexical and syntactical information. Empirical\nSoftware Engineering 25 (2019), 2179 - 2217. https: //api.semanticscholar.org/CorpusID:189927337\n\nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. 2017. Snapshot Ensembles: Train 1, get M forfree.\nArXiv abs/1704.00109 (2017). https://api.semanticscholar.org/CorpusID:6820006\n\nHamel Husain, Hongqiu Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of\nSemantic Code Search. ArXiv abs/1909.09436 (2019). https: //api.semanticscholar.org/CorpusID:202712680\n\nFerdin Joe John Joseph, Sarayut Nonsiri, and Annop Monsakul. 2021. Keras and TensorFlow: A Hands-On Experience. Advanced Deep Learning for\nEngineers and Scientists (2021). https://api.semanticscholar.org/CorpusID:237998052\n\nRené Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: a database of existing faults to enable controlled testing studies for Java programs.\nIn International Symposium on Software Testing and Analysis. https://api.semanticscholar.org/CorpusID:12796895\n\nJohn D Keller. 2019. Deep Learning. Cambridge (Massachusetts): MIT Press. https://direct.mitedu/books/book/4556/Deep-Learning\nLov Kumar, Shashank Mouli Satapathy, and Lalita Bhanu Murthy Neti. 2019. Method Level Refactoring Prediction on Five Open Source Java\nProjects using Machine Learning Techniques. Proceedings of the 12th Innovations on Software Engineering Conference (formerly known as India\nSoftware Engineering Conference) (2019). https://api.semanticscholar.org/CorpusID:60441115\n\nZarina Kurbatova, Ivan Veselov, Yaroslav Golubev, and Timofey Bryksin. 2020. Recommendation of Move Method Refactoring Using Path-\nBased Representation of Code. Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops (2020). https:\n//api.semanticscholar.org/CorpusID:211133313\n\nJames R. Lee, Shayan Oveis Gharan, and Luca Trevisan. 2011. Multi-way spectral partitioning and higher-order cheeger inequalities. In Symposium\non the Theory of Computing. https://apisemanticscholar.org/CorpusID:8212381\n\nYichen Liand Xiaofang Zhang. 2022. Multi-Label Code Smell Detection with Hybrid Model based on Deep Learning, In International Conference on\nSoftware Engineering and Knowledge Engineering. https://api.semanticscholar.org/CorpusID:252098564\n\nTao Lin, Xue Fu, Fu Chen, and Luqun Li. 2021. A Novel Approach for Code Smells Detection Based on Deep Leaning. Lecture Notes ofthe Institute\n‘for Computer Sciences, Social Informatics and Telecommunications Engineering (2021). https://apisemanticscholar.org/CorpusID:238020089\n\nBo Liu, Hui Liu, Nan Niu, Yuxia Zhang, Guangjie Li, and Yanjie Jiang. 2023. Automated Software Entity Matching Between Successive Versions. 2023\n38th IEEE/ACM International Conference on Automated Software Engineering (ASE) (2023), 1615-1627. https://api.semanticscholar.org/Corpus|D:\n265056437\n\nF. Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task Learning based Pre-trained Language Model for Code Completion. 2020 35th IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE) (2020), 473-485. https: //apisemanticscholar.org/CorpusID:229703606\n\nHui Liu, Jiahao Jin, Zhifeng Xu, Yanzhen Zou, Yifan Bu, and Lu Zhang. 2021. Deep Learning Based Code Smell Detection. [EEE Transactions on\nSoftware Engineering 47,9 (2021), 1811-1837. https://doi.org/10.1109/TSE.2019.2936376\n\nHao Liu, Yanlin Wang, Zhao Wei, Yongxue Xu, Juhong Wang, Hui Li, and Rongrong Ji. 2023. RefBERT: A Two-Stage Pre-trained Framework\nfor Automatic Rename Refactoring. Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis (2023).\nhttps://api.semanticscholar.org/Corpus!D:258960338\n\nHui Liu, Zhifeng Xu, and Yanzhen Zou. 2018. Deep Learning Based Feature Envy Detection. In 2018 33rd IEEE/ACM International Conferenceon\nAutomated Software Engineering (ASE). 385-396. https://doi.org/10.1145/3238147.3238166\n\nKui Liu, Dongsun Kim, Tegawendé F. Bissyandé, Tae young Kim, Kisub Kim, Anil Koyuncu, Suntae Kim, and Yves Le Traon. 2019. Learning\nto Spot and Refactor Inconsistent Method Names. 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (2019), 1-12.\nhttps://api.semanticscholar.org/CorpusID:155144146\n\nWenhao Ma, Yaoxiang Yu, Xiaoming Ruan, and Bo Cai. 2023. Pre-trained Model Based Feature Envy Detection. 2023 IEEE/ACM 20thInternational\nConference on Mining Software Repositories (MSR) (2023), 430-440. https://api.semanticscholar.org/CorpusID:259835399\n\nRuchika Malhotra, Bhawna Jain, and Marouane Kessentini. 2023. Examining deep learnings capability to spot code smells: a systematic literature\nreview. Cluster Computing 26 (2023), 3473 - 3501. https: //api.semanticscholar.org/CorpusID:263654376\n\nChristopher D. Manning and Hinrich Schiitze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA, USA.\nLicelot Marmolejos, Eman Abdullah Alomar, Mohamed Wiem Mkaouer, Christian D. Newman, and Ali Ouni. 2021. Onthe use of textual feature\nextraction techniques to support the automated detection of refactoring documentation. Innovations in Systems and Software Engineering 18 (2021),\n233 - 249. https://api.semanticscholar.org/CorpusID:233807785\n\nAntonio Mastropaolo, Emad Aghajani, Luca Pascarella, and Gabriele Bavota. 2022. Automated variable renaming: are we there yet? Empirical\nSoftware Engineering 28 (2022). https://api.semanticscholar.org/CorpusID:254564632\n', '46\n\n60)\n\n61\n\n62\n\n63\n\n64)\n\n65)\n\n66)\n\n67)\n\n68)\n\n69)\n\n70)\n\n[71]\n\n72\n\n73\n\n74,\n\n75)\n\n76)\n\n77)\n\n78)\n\n79)\n\n80)\n\n81\n\n82\n\n83\n\n84)\n\nNyirongo and Jiang, etal.\n\nRana S, Menshawy, Ahmed H. Yousef, and Ashraf Salem. [n. d.]. Comparing the Effectiveness of Machine Learning and Deep Learning Techniques\nfor Feature Envy Detection in Software Systems. In 2023 Intelligent Methods, Systems, and Applications (IMSA). 470-475. https://doi.org/10.1109/\nIMSA58542.2023.10217458\n\nNaouel Moha, Yann-Gael Gueheneuc, Laurence Duchien, and Anne-Francoise Le Meur. 2010. DECOR: A Method for the Specification and Detection\nof Code and Design Smells. IEEE Transactions on Software Engineering 36, 1 (2010), 20-36. https://doi.org/10.1109/TSE.2009.50\n\nEmerson R. Murphy-Hill, Chris Parnin, and Andrew P. Black, 2009. How we refactor, and how we know it. 2009 IEEE 31st International Conference\non Software Engineering (2009), 287-297. https://api.semanticscholar.org/Corpus|D:5856772\n\nPurnima Naik, Salomi Nelaballi, Venkata Sai Pusuluri, and Dae-Kyoo Kim. 2023. Deep Learning-Based Code Refactoring: A Review of Current\nKnowledge. SSRN Electronic Journal (2023). https://api.semanticscholar.org/Corpus!D:254267544\n\nHimesh Nanadani, Mootez Saad, and Tushar Sharma. 2023. Calibrating Deep Learning-based Code Smell Detection using Human Feedback. (2023).\nhttps://tusharma in/preprints /SCAM23_HumanFeedbackOnSmells.pdf\n\nAllyS. Nyamawe. 2022. Mining commit messages to enhance software refactorings recommendation: A machine learning approach. Machine\nLearning with Applications (2022). https://api.semanticscholar.org/CorpusID:248807768\n\nAlly S. Nyamawe, Hui Liu, Nan Niu, Qasim Umer, and Zhendong Niu. 2020, Feature requests-based recommendation of software refactorings.\nEmpirical Software Engineering 25 (2020), 4315-4347. https: //api.semanticscholar.org/CorpusID:221521552\n\nFabio Palomba, Dario Di Nucci, Michele Tufano, Gabriele Bavota, Rocco Oliveto, Denys Poshyvanyk, and Andrea De Lucia. 2015. Landfill: An\nOpen Dataset of Code Smells with Public Evaluation, 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories (2015), 482-485.\nhttps://apisemanticscholar.org/CorpusID:16120092\n\nFabio Palomba, Annibale Panichella, Andrea De Lucia, Rocco Oliveto, and Andy Zaidman, 2016. A textual-based technique for Smell Detection.\n2016 IEEE 24th International Conference on Program Comprehension (ICPC) (2016), 1-10. https: //apisemanticscholar.org/Corpus!D:36114894\nRasmita Panigrahi, Sanjay Kumar Kuanar, and Lov Kumar.2022. Machine Learning Implementation for Refactoring Prediction. In 2022 IEEE 4th PhD\nColloquium on Emerging Domain Innovation and Technology for Society (PhD EDITS). 1-2. https://doi.org/10.1109 /PhDEDITS56681.2022.9955297\nRasmita Panigrahi, Sanjay Kumar Kuanar, Sanjay Misra, and Lov Kumar. 2022. Class-Level Refactoring Prediction by Ensemble Learning with\nVarious Feature Selection Techniques. Applied Sciences (2022). https://api.semanticscholar.org/CorpusID:254363225\n\nJevgenija Pantiuchina, 2019. Towards Just-In-Time Rational Refactoring. 2019 IEEE/ACM 41st International Conference on Software Engineering:\nCompanion Proceedings (ICSE-Companion) (2019), 180-181. https://api.semanticscholar.org/CorpusID:174799906\n\nBryan Perozzi, Rami Al-Rfou, and Steven S. Skiena. 2014. DeepWalk: online learning of social representations. Proceedings of the 20th ACM SIGKDD\ninternational conference on Knowledge discovery and data mining (2014). https://apisemanticscholar.org/CorpusID:3051291\n\nBryan Perozzi, Vivek Kulkarni, Haochen Chen, and Steven S, Skiena. 2016. Don’t Walk, Skip!: Online Learning of Multi-scale Network\nEmbeddings. Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017 (2016).\nhttps://api.semanticscholar.org/CorpusID:207699173\n\nDarwin Pinheiro, Carla Ilane Moreira Bezerra, and Anderson G.Uchéa. 2022, How do Trivial Refactorings Affect Classification Prediction Models?\nProceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse (2022). https://apisemanticscholar.org/CorpusID:\n252497875\n\nKyle Prete, Napol Rachatasumrit, Nikita Sudan, and Miryung Kim. 2010. Template-based reconstruction of complex refactorings. 2010 IEEE\nInternational Conference on Software Maintenance (2010), 1-10. https://api.semanticscholar.org/CorpusID:2659467\n\nOsama Al Qasem, Mohammed Akour, and M. Alenezi. 2020. The Influence of Deep Learning Algorithms Factors in Software Fault Prediction, IEEE\nAccess 8 (2020), 63945-63960. https://api.semanticscholar.org/CorpusID:215816960\n\nColin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019.\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res. 21 (2019), 140:1-140:67. https:\n//api.semanticscholar.org/Corpus!D:204838007\n\nSanjiban Roy, Valentina Balas, Pijush Samui, and Sharma D. 2019. Handbook of Deep Learning Applications.\n\nPriyadarshni Suresh Sagar, Eman Abdullah Alomar, Mohamed Wiem Mkaouer, Ali Ouni, and Christian D. Newman. 2021. Comparing Commit\nMessages and Source Code Metrics for the Prediction Refactoring Activities. Algorithms 14 (2021), 289. https://api.semanticscholar.org/CorpuslD:\n244175361\n\nTushar Sharma, Vasiliki Efstathiou, Panagiotis Louridas, and Diomidis D. Spinellis. 2019. On the Feasibility of Transfer-learning Code Smells using\nDeep Learning. ArXiv abs/1904.03031 (2019). https://api.semanticscholar.org/CorpusID:102351639\n\nTushar Sharma, Vasiliki Efstathiou, Panos Louridas,and DiomidisD.Spinellis.2021. Code smell detection by deep direct-learningandtransfer-\nlearning. J. Syst. Softw. 176 (2021), 110936. https://api.semanticscholar.org/Corpus!D:233329781\n\nTushar Sharma and Marouane Kessentini. 2021. QScored: A Large Dataset of Code Smells and Quality Metrics, 2021 IEEE/ACM 18th International\nConference on Mining Software Repositories (MSR) (2021), 590-594. https://api.semanticscholar.org/CorpusID:232165224\n\nDanilo Silva, Ricardo Terra, and Marco Tulio Valente. 2015. JExtract: An Eclipse Plug-in for Recommending Automated Extract Method Refactorings.\nArXiv abs/1506.06086 (2015). https://api.semanticscholar.org/CorpusID:707971\n\nDanilo Silva and Marco Tilio Valente. 2017. RefDiff: Detecting Refactorings in Version Histories. 2017 IEEE/ACM 14th International Conference on\nMining Software Repositories (MSR) (2017), 269-279. https://api.semanticscholar.org/CorpusID:11506870\n', 'ASurvey of Deep Learning Based Software Refactoring 47\n\n85] Danilo Silva and Marco Tulio Valente, 2017. RefDiff: Detecting Refactorings in Version Histories. In 2017 IEEE/ACM 14th International Conference\non Mining Software Repositories (MSR). 269-279. https://doi.org/10.1109/MSR.2017.14\n\n86] Gustavo Soares. 2010. Making program refactoring safer. In 2010 ACM/IEEE 32nd International Conference on Software Engineering, Vol. 2. 521-522.\nhttps://doi.org/10.1145/1810295.1810461\n\n87] Balazs Szalontai, Péter Bereczky, and Daniel Horpacsi. 2023. Deep Learning-Based Refactoring with Formally Verified Training Data. Infocommu-\nnications journal (2023). https://api.semanticscholar.org/Corpus!D:261570010\n\n88] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei, 2015. LINE: Large-scale Information Network Embedding, Proceedings\nof the 24th International Conference on World Wide Web (2015). https://api.semanticscholar.org/CorpusID:8399404\n\n89] Ricardo Terra, Marco Tilio Valente, Sergio Miranda, and Vitor Sales. 2018. JMove: Anovel heuristic and tool to detect move method refactoring\nopportunities. J. Syst. Softw. 138 (2018), 19-36. https://apisemanticscholar.org/CorpusID:4412526\n\n[90] Omkarendra Tiwari and Rushikesh K. Joshi. 2022. Identifying Extract Method Refactorings. 15th Innovations in Software Engineering Conference\n(2022). https://api.semanticscholar.org/CorpusID:246828642\n\n91] Nikolaos Tsantalis and Alexander Chatzigeorgiou. 2009. Identification of Extract Method Refactoring Opportunities. In 2009 13th European\nConference on Software Maintenance and Reengineering. 119-128, https://doiorg/10.1109/CSMR.2009.23\n\n92] Nikolaos Tsantalis and Alexander Chatzigeorgiou. 2009. Identification of Move Method Refactoring Opportunities, [EEE Transactions on Software\nEngineering 35, 3 (2009), 347-367. https://doi.org/10.1109/TSE.2009.1\n\n93] Nikolaos Tsantalis, Ameya Ketkar, and Danny Dig, 2022. RefactoringMiner 2.0. IEEE Transactions on Software Engineering 48, 3 (2022), 930-950.\nhttps://doi.org/10.1109/TSE.2020.3007722\n\n94] Nikolaos Tsantalis, Matin Mansouri, Laleh Eshkevari, Davood Mazinanian, and Danny Dig, 2018. Accurate and Efficient Refactoring Detection in\nCommit History. In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). 483-494, https://doi.org/10.1145/3180155.3180206\n95] Nikolaos Tsantalis, Matin Mansouri, Laleh Mousavi Eshkevari, Davood Mazinanian, and Danny Dig, 2018. Accurate and Efficient Refactoring\nDetection in Commit History. 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE) (2018), 483-494. https://api.\nsemanticscholar.org/CorpuslD:49665673\n\n96] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and Denys Poshyvanyk. 2019. On Learning Meaningful Code Changes Via\nNeural Machine Translation. 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (2019), 25-36. https://apisemanticscholar.\norg/CorpusID:59316445\n\n97] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural Deep Network Embedding, Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (2016). https://api.semanticscholar.org/CorpusID:207238964\n\n98] Hongze Wang, Jing Liu, Jiexiang Kang, Wei Yin, Haiying Sun, and Hui Wang. 2020. Feature Envy Detection based on Bi-LSTM with Self-Attention\nMechanism. 2020 IEEE Intl Confon Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing &\nCommunications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom) (2020), 448-457. https://api.semanticscholar.org/Corpus|D:\n235340022\n\n99] Ian H. Witten, Eibe Frank, and Mark A. Hall. 2011. Data Mining: Practical Machine Learning Tools and Techniques (3rd ed.). Morgan Kaufmann\nPublishers Inc, San Francisco, CA, USA.\n\n100] Sihan Xu, Aishwarya Sivaraman, Siau-Cheng Khoo, and Jing Xu. 2017. GEMS: An Extract Method Refactoring Recommender. In 2017 IEEE 28th\nInternational Symposium on Software Reliability Engineering (ISSRE). 24-34. https: //doi.org/10.1109/ISSRE.2017.35\n\n101] Sihan Xu, Aishwarya Sivaraman, Siau-Cheng Khoo, and Jing Xu. 2017. GEMS: An Extract Method Refactoring Recommender. 2017 IEEE 28th\nInternational Symposium on Software Reliability Engineering (ISSRE) (2017), 24-34. https://api.semanticscholar.org/Corpus|D:38648648\n102] Weiwei Xu and Xiaofang Zhang, 2021, Multi-Granularity Code Smell Detection using Deep Learning Method based on Abstract Syntax Tree. In\n\nInternational Conference on Software Engineering and Knowledge Engineering, https://api.semanticscholar.org/Corpus|D:239678857\n\n103] Yanming Yang, Xin Xia, David Lo, and John Grundy. 2020. A Survey on Deep Learning for Software Engineering.\n\n104] Xin Yin, Chongyang Shi, and Shuxin Zhao. 2021. Local and Global Feature Based Explainable Feature Envy Detection. 2021 IEEE 45th Annual\n\nComputers, Software, and Applications Conference (COMPSAC) (2021), 942-951, https://api.semanticscholar.org/CorpusID:237474266\n\n105] Dongjin Yu, Yihang Xu, Lehui Weng, Jie Chen, Xin Chen, and Quanxin Yang. 2022, Detecting and Refactoring Feature Envy Based on Graph\n\nNeural Network. 2022 IEEE 33rd International Symposium on Software Reliability Engineering (ISSRE) (2022), 458-469. https://apisemanticscholar.\n\norg/CorpusID:254930430\n\n106] Jie Zhang, Yuxiao Dong, Yan Wang, Jie Tang, and Ming Ding. 2019. ProNE: Fastand Scalable Network Representation Learning, In International\n\nJoint Conference on Artificial Intelligence. https://api.semanticscholar.org/CorpusID:189808933\n\n107] Minnan Zhang and Jingdong Jia. 2022. Feature Envy Detection with Deep Learning and Snapshot Ensemble. 2022 9th International Conferenceon\n\nDependable Systems and Their Applications (DSA) (2022), 215-223. https://apisemanticscholar.org/CorpusID:253124697\n\n108] Yang Zhang and Chunhao Dong, 2021. MARS: Detecting brain class /method code smell based on metric-attention mechanism and residual\n\nnetwork. Journal of Software: Evolution and Process (2021). https://api.semanticscholar.org/CorpuslD:243792860\n\n109] Yang Zhang, Chuyan Ge, Shuai Hong, Ruili Tian, Chun-Ru Dong, and J. Liu. 2022. DeleSmell: Code smell detection based on deep learning and\nlatent semantic analysis. Knowl. Based Syst. 255 (2022), 109737. https://api.semanticscholar.org/CorpusID:251751777\n\n[110] Yang Zhang, Chuyan Ge, Haiyang Liu, and Kun Zheng, 2024. Code smell detection based on supervised learning models: A survey. Neurocomputing\n\n565 (2024), 127014. https://doi.org/10.1016 /jneucom.2023.127014\n\n', '48 Nyirongo and Jiang, etal.\n\n[111] Shuxin Zhao, Chongyang Shi, Shaojun Ren, and Hufsa Mohsin. 2022. Correlation Feature Mining Model Based on Dual Attention for Feature Envy\nDetection. In International Conference on Software Engineering and Knowledge Engineering. https://api.semanticscholar.org/CorpusID:252100954\n']}


**File**: S:\OneDrive\@Dev\!GPT\ScriptGPT\library\Refactoring\Source\TOSEM-refactoring-ouni-2016.pdf
- Time Taken: 92.93s
- Data Extracted: {'text': ['Multi-criteria Code Refactoring Using Search-Based Software\nEngineering: An Industrial Case Study\n\nALI OUNI, Osaka University\n\nMAROUANE KESSENTINI, University of Michigan-Dearborn\nHOUARI SAHRAOUI, University of Montreal\n\nKATSURO INOUE, Osaka University\n\nKALYANMOY DEB, Michigan State University\n\nOne of the most widely used techniques to improve the quality of existing software systems is refactoring —\nthe process of improving the design of existing code by changing its internal structure without altering its\nexternal behavior. While it is important to suggest refactorings that improve the quality and structure of\nthe system, many other criteria are also important to consider such as reducing the number of code changes,\npreserving the semantics of the software design and not only its behavior, and maintaining consistency with\nthe previously applied refactorings. In this paper, we propose a multi-objective search-based approach for\nautomating the recommendation of refactorings. The process aims at finding the optimal sequence of refac-\ntorings that (i) improves the quality by minimizing the number of design defects, (ii) minimizes code changes\nrequired to fix those defects, (ii) preserves design semantics, and (iv) maximizes the consistency with the\npreviously code changes. We evaluated the efficiency of our approach using a benchmark of six open-source\nsystems, 11 different types of refactorings (move method, move field, pull up method, pull up field, push\ndown method, push down field, inline class, move class, extract class, extract method and extract interface)\nand 6 commonly occurring design defect types (blob, spaghetti code, functional decomposition, data class,\nshotgun surgery and feature envy) through an empirical study conducted with experts. In addition, we per-\nformed an industrial validation of our technique, with 10 software engineers, on a large project provided by\nour industrial partner. We found that the proposed refactorings succeed in preserving the design coherence\nof the code, with an acceptable level of code change score while reusing knowledge from recorded refactorings\napplied in the past to similar contexts.\n\nGeneral Terms: Algorithms, Reliability\n\nAdditional Key Words and Phrases: Search-based Software Engineering, Refactoring, Software Mainte-\nnance, Multi-Objective Optimization, Software Evolution\n\nACM Reference Format:\n\nAli Ouni, Marouane Kessentini, Houari Sahraoui, Katsuro Inoue, Kalynmoy Deb, 2015. Multi-criteria Code\nRefactoring using Search-Based Software Engineering: An Industrial Case Study. ACM Trans. Softw. Eng.\nMethodol. 9, 4, Article 39 (March 2015), 54 pages.\n\nDOI: 0000001.0000001\n\n1. INTRODUCTION\n\nLarge scale software systems exhibit high complexity and become difficult to main-\ntain. In fact, it has been reported that the software cost attributable to maintenance\nand evolution activities is more than 80% of total software costs |Erlikh 2000]. To facil-\nitate maintenance tasks, one of the most widely used techniques is refactoring which\n\nimproves design structure while preserving external behavior |Mens and Tourwé 2004\nOpdyke 1992]\n\nAuthor’s addresses: Ali Ouni, Osaka University, ali@ist.osaka-u.acjp, Marouane Kessentini, Uni-\nversity of Michigan-Dearborn, marouane@umich.edu, Houari Sahraoui, University of Montreal,\nsahraouh@iro.umontreal.ca, Katsuro Inoue, Osaka University, inoue@ist.osaka-u.ac.jp, and Kalyanmoy Deb,\nMichigan State University, kdeb@egr.msu.edu.\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted\nwithout fee provided that copies are not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights for third-party components of this\nwork must be honored. For all other uses, contact the owner/author(s).\n\n© 2015 Copyright held by the owner/author(s). 1049-331X/2015/03-ART39 $15.00\n\nDOI: 0000001.0000001\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:2 A. Ouni et al.\n\nEven though most of the existing refactoring recommendation approaches are pow-\nerful enough to suggest refactoring solutions to be applied, several issues are still need\nto be addressed. One of the most important issues is the semantic coherence of the\nrefactored program, which is not considered by most of the existing approaches [Ouni|\n" Comse-\nquently, the refactored program could be syntactically correct, implement the correct\nbehavior, but be semantically incoherent. For example, a refactoring solution might\nmove a method calculateSalary() from class Employee to class Car. This refactoring\ncould improve the program structure by reducing the complexity and coupling of class\nEmployee and satisfy the pre- and post-conditions to preserve program behavior. How-\never, having a method calculateSalary() in class Car does not make any sense from\nthe domain semantics standpoint, and is likely to lead to comprehension problems in\nthe future. Another issue is related to the number of code changes required to ap-\nply refactorings, something that is not considered in existing refactoring approaches\nwhose only aim is to improve code quality independently of the cost of code changes.\nConsequently, applying a particular refactoring may require a radical change in the\nsystem or even its re-implementation from scratch. Thus, it is important to minimize\ncode changes to help developers in understanding the design after applying the pro-\nposed refactorings. In addition, the use of development history can be an efficient aid\nwhen proposing refactorings. Code fragments that have previously been modified in\nthe same time period are likely to be semantically related (e.g., refer to the same fea-\nture). Furthermore, code fragments that have been extensively refactored in the past\nhave a high probability of being refactored again in the future. Moreover, the code to\nrefactor can be similar to some refactoring patterns that are to be found in the devel-\nopment history, thus, developers can easily adapt and reuse them.\n\nOne of the limitations of the existing works in software refactoring |Du Bois et al.\n2004} |Qayum and Heckel 2009} |Fokaefs et al. 2011} Harman and Tratt 2007} /Moha|\net al. 200 eng et al. 2006] is that the definition of semantic coherence is closely\n\nrelated to behavior preservation. Preserving the behavior does not means that the de-\nsign semantics of the refactored program is also preserved. Another issue is that the\nexisting techniques are limited to a small number of refactorings and thus it could\nnot be generalized and adapted for an exhaustive list of refactorings. Indeed, semantic\ncoherence is still hard to ensure since existing approaches do not provide a pragmatic\ntechnique or an empirical study to prove whether the semantic coherence of the refac-\ntored program is preserved.\n\nIn this paper, we propose a multi-objective search-based approach to address the\nabove-mentioned limitations. The process aims at finding the sequence of refactorings\nthat: (1) improves design quality; (2) preserves the design coherence and consistency\nof the refactored program; (3) minimizes code changes; and (4) maximizes the consis-\ntency with development change history. We evaluated our approach on six open-source\n\nsystems using an existing benchmark [Ouni et al. 2012al\nphos) We report the results of the efficiency and effectiveness of our approach, com-\npared to existing approaches [|[Harman and Tratt 2007;|Kessentini et al. 2011]. In ad-\ndition, we provide an industrial validation of our approach on a large-scale project\nin which the results were manually evaluated by 10 active software engineers. The\nstudy also evaluated the relevance and usefulness of our refactoring technique in an\nindustrial setting.\n\nThe remainder of this paper is structured as follows. Section |2| provides the nec-\nessary background and challenges related to refactoring and code smells. Section\ndefines refactoring recommendation as a multi-objective optimization problem,\n\nwhile Section |4]introduces our search-based approach to this problem using the non-\ndominated sorting genetic algorithm (NSGA-II) [Deb et al. 2002]. Section [5] describes\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:3\n\nthe method used in our empirical studies and presents the obtained results, while Sec-\ntion [6] provides further discussions. Section |7| presents an industrial case study long\nwith a discussion of the obtained results. Section 8] discusses the threats to validity\nand the limitations of the proposed approach, while Section |9| describes the related\nwork. Finally, Section[I0|concludes and presents directions for future work.\n\n2. CHALLENGES IN AUTOMATED REFACTORING RECOMMENDATION\nIn this section, we define the issues and challenges related to software refactoring.\n\n2.1. Background and definitions\nRefactoring is defined as the process of improving a code after it has been written by\n\nchanging its internal structure without changing its external behavior [Opdyke 1992].\nThe idea is to reorganize variables, classes and methods, mainly to facilitate future\nadaptations and extensions. This reorganization is used to improve various aspects of\n\nsoftware quality such as maintainability, extensibility, reusability, etc. [Fowler 1999)\nBaar and Markovié 2007]. The refactoring process consists of 6 distinct steps [Baar\nand Markovic 2007]:\n\n(1) Identify where the software should be refactored.\n\n(2) Determine which refactoring(s) should be applied to the identified places.\n\n(3) Guarantee that the applied refactoring preserves behavior.\n\n(4) Apply the refactoring.\n\n(5) Assess the effect of the refactoring on quality characteristics of the software (e.g.,\ncomplexity, understandability, maintainability) or the process (e.g., productivity,\ncost, effort).\n\n(6) Maintain the consistency between the refactored program code and other software\nartifacts (such as documentation, design documents, requirement specifications,\ntests, etc.).\n\nWe focus in this paper on steps 1, 2 and 5. In order to find out which parts of the\nsource code need to be refactored, most existing work [Dhambri et al. 2008}|Moha et al.\nrelies on the notion of design\ndefects or bad smells. In this paper, we do not focus on the first step related to the\ndetection of refactoring opportunities. We assume that a number of different design\ndefects have already been detected, and need to be corrected. Typically, design defects,\n\nalso called anomalies [Brown et al. 1998], design flaws |Marinescu 2004], bad smells\n[Fenton and Pfleeger 1998], or anti-patterns [Fowler 1999], refer to design situations\n\nthat adversely affect the development of software. As stated by Fenton and Pfleeger\n\n[Fenton and Pfleeger 1998], design defects are unlikely to cause failures directly, but\nmay do so indirectly [Yamashita and Moonen 2013]. In general, they make a system\ndifficult to change, which may often introduce bugs. In this paper, we focus on the\n\nfollowing six design defect types [Brown et al. 1998} |Murphy-Hill and Black 2010)\nMantyla et al. 2003 :\n\n] to evaluate our approac\n\n— Blob: It is found in designs where much of the functionality of a system (or part of\nit) is centralized in one large class, while the other related classes primarily expose\ndata and provide little functionality.\n\n— Spaghetti Code: This involves a code fragment with a complex and tangled control\nstructure. This code smell is characteristic of procedural thinking in object-oriented\nprogramming. Spaghetti Code is revealed by classes declaring long methods with no\nparameters, and utilising global variables. Names of classes and methods may sug-\ngest procedural programming. Spaghetti Code does not exploit, and indeed prevents\nthe use of, object-oriented mechanisms such as inheritance and polymorphism.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', "39:4 A. Ouni et al.\n\n—Functional Decomposition: This design defect consists of a main class in which in-\nheritance and polymorphism are hardly used, that is associated with small classes,\nwhich declare many private fields and implement only a few methods. This is fre-\nquently found in code produced by inexperienced object-oriented developers.\n\n—Data Class: It is a class that contains only data and performs no processing on\nthese data. It is typically composed of highly cohesive fields and accessors. However,\ndepending on the programming context some Data Classes might suit perfectly and,\ntherefore, not design defects.\n\n— Shotgun Surgery: This occurs when a method has a large number of external meth-\nods calling it, and these methods are spread over a significant number of classes. As\na result, the impact of a change in this method will be large and widespread.\n\n—Feature Envy: It is found when a method heavily uses attributes and data from\none or more external classes, directly or via accessor operations. Furthermore, in\naccessing external data, the method uses data intensively from at least one external\nsource.\n\nWe choose these design defect types in our experiments because they are the most\nimportant and common ones in object-oriented industrial projects based on recent em-\npirical studies |Ouni et al. 2012a} Moha et al. 2008} Ouni et al. 2013]. Moreover, it\nis widely believed that design defects have a negative impact on software quality that\noften leads to bugs and failures [Li and Shatnawi 2007}|D’Ambros et al. 2010}|Deligian-\n. Consequently, design defects should be identifie\nand corrected by the development team as early as possible for maintainability and\nevolution considerations. For example, after detecting a blob defect, many refactoring\noperations can be used to reduce the number of functionalities in a specific class, such\nas move method and extract class.\n\nIn the next subsection, we discuss the different challenges related to fixing design\ndefects using refactoring.\n\n2.2. Problem statement\n\nEven though most existing refactoring approaches are powerful enough to provide\nrefactoring solutions, some open issues need to be targeted to provide an efficient and\nfully automated refactoring recommendation.\n\nQuality improvement: Most of the existing approaches |Qayum and Heckel 2009\nO'Keeffe and Cinnéide 2008}|Moha et al. 2008}|Seng et al. 2006] consider refactoring as\nthe process to improve code quality by improving structural metrics. However, these\n\nmetrics can be conflicting and it is difficult to find a compromise between them. For\nexample, moving methods to reduce the size or complexity of a class may increase the\nglobal coupling. Furthermore, improving some quality metrics does not guarantee that\nthe detected design defects are fixed. Moreover, there is no consensus about the metrics\nthat need to be improved in order to fix defects. Indeed, the same type of defect can be\nfixed by improving completely different metrics.\n\nSemantic coherence: In object-oriented programs, objects reify domain concepts\nand/or physical objects, implementing their characteristics and behavior. Methods and\nfields of classes characterize the structure and behavior of the implemented domain el-\nements. Consequently, a program could be syntactically correct, implement the appro-\npriate behavior, but violate the domain semantics if the reification of domain elements\nis incorrect. During the initial design/implementation, programs usually capture well\nthe domain semantics when object-oriented principles are applied. However, when\nthese programs are (semi-)automatically modified/refactored during maintenance, the\nadequacy with regards to domain semantics could be compromised. Indeed, semantic\ncoherence is an important issue to consider when applying refactorings.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n", 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:5\n\nMost of the existing approaches suggest refactorings mainly with the perspective of\nonly improving some design/quality metrics. As explained, this may not be sufficient.\nWe need to preserve the rationale behind why and how code elements are grouped and\nconnected when applying refactoring operations to improve code quality.\n\nCode changes: When applying refactorings, various code changes are performed.\nThe amount of code changes corresponds to the number of code elements (e.g., classes,\nmethods, fields, relationships, field references, etc.) modified through adding, deleting,\nor moving operations. Minimizing code changes when suggesting refactorings is im-\nportant to reduce the effort and help developers understand the modified/improved\ndesign. In fact, most developers want to keep as much as possible with the original\ndesign structure when fixing design defects Prowler 1989). Hence, improving software\nquality and reducing code changes are conflicting. In some cases, correcting some de-\nsign defects corresponds to changing radically a large portion of the system or is some-\ntimes equivalent to re-implementing a large part of the system. Indeed, a refactoring\nsolution that fixes all defects is not necessarily the optimal one due to the high code\nadaptation/modification that may be required.\n\nConsistency with development/maintenance history: The majority of the ex-\nisting work does not consider the history of changes applied in the past when propos-\ning new refactoring solutions. However, the history of code changes can be helpful in\nincreasing the confidence of new refactoring recommendations. To better guide the\nsearch process, recorded code changes applied in the past can be considered when\nproposing new refactorings in similar contexts. This knowledge can be combined with\nstructural and textual information to improve the automation of refactoring sugges-\ntions.\n\n2.3. Motivating example\n\nTo illustrate some of these issues, Figure [I] shows a concrete example extracted from\nJFreeChart}| v1.0.9, a well-known Java open-source charting library. We consider\na design fragment containing four classes XYLineAndShapeRenderer, XYDotRenderer,\nSegmentedTimeline, and XYSplineRenderer. Using design defect detection rules pro-\nposed in our previous work (Ressentini et al. 2017) the class XYLineAndShapeRenderer\nis detected as a design defect: blob (i.e., a large class that monopolizes the behavior of\na large part of the system).\n\nWe consider the scenario of a refactoring solution that consists of moving the\nmethod drawItem() from class XYLineAndShapeRenderer to class SegmentedTimeline.\nThis refactoring can improve the design quality by reducing the number of func-\ntionalities in this blob class. However, from the design semantics standpoint, this\nrefactoring is incoherent since SegmentedTimeline functionalities are related to pre-\nsenting a series of values to be used for a curve axis (mainly for Date related\naxis) and not for the task of drawing objects/items. Based on textual and structural\ninformation, using respectively a semantic lexicon (Amaro_et al. 2006}, and cohe-\nsion/coupling [Ouni et al. 2012b], many other target classes are possible including\nXYDotRenderer and XYSplineRenderer. These two classes have approximately the same\nstructure that can be formalized using quality metrics (e.g., number of methods, num-\nber of attributes, etc.) and their textual similarity is close to XYLineAndShapeRenderer\nusing a vocabulary-based measure. Thus, moving elements between these three\nclasses is likely to be semantically coherent and meaningful. On the other hand,\nfrom previous versions of JFreeChart, we recorded that there are some meth-\nods such as drawPrimaryLineAsPath(), initialise(), and equals() that have been\nmoved from class XYLineAndShapeRenderer to class XYSplineRenderer. As a conse-\n\n‘http //www.jfree.org/jfreechart/\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n\n', "39:6 A. Ouni et al.\n\nSegmentedTimeline\n\nXYLineAndShapeRenderer | _|workingCalendar: Calendar\nsegmentSize : long\nstartTime : long\n\nserialVersionUID : long\nlinesVisible : Boolean\nlegendLine : Shape\n\nshapesVisible : Boolean getStarttime()\n\nuseFillPaint : boolean getBaseTimeline()\n\nuseOutlinePaint : boolean | | totimelineValue()\n\nbaseShapesfilled : boolean | | :oxiliisecond() XYDotRenderer\ndrawOutlines : boolean getSegmentsSize() serialVersionUID : long\nshapesFilled : Boolean clone() JdotWidth : int\nbaseShapesVisible: boolean! | equals() dotHeight : int\n\nlegendShape : Shape\n\ngetDrawSeriesLineAsPath() - =\n\nsetDrawSeriesLineAsPath() XYDotRenderer()\n\ngetPassCount() getDotWidth()\n\ngetLegendLine() t——— setLegendShape()\n\ngetBaseShapesVisible() drawltem()\n\ngetSeriesShapesFilled() equals(Object)\n\ngetUseFillPaint() clone()\n\ninitialise() readObject()\n\ngetLinesVisible() writeObject()\n\nsetLinesVisible() we\n\ndrawitem()\n\ngetLegenditem() 3\nclone() juggested refactorings: '\n\ndrawPrimaryLine()\nsetDrawOutlines()\ngetUseFillPaint()\nsetUseOutlinePaint()\n\nmove method(XYLineAndShapeRenderer:: drawltem(), XYSplineRenderer) !\n\ndrawSecondaryPass() XYSplineRenderer\ngetLegenditem(int, int) points : Vector\nreadObject()\n\nprecision : int\n\nwriteObject()\ndrawPrimaryLine()\ndrawFirstPassShape()\n\nXYSplineRenderer()\ngetPrecision()\nsetPrecision()\n\ninitialise()\ndrawPrimaryLineAsPath()\nequals()\n\nsolveTridiag()\n\nDesign defect: Blob\n\nmove method(XYLineAndShapeRenderer:: initialise(), XYSplineRenderer)\nmove method(XYLineAndShapeRenderer:: equals(), XYSplineRenderer)\n\nFig. 1: Design fragment extracted from JFreeChart v1.0.9.\n\nquence, moving methods and/or attributes from class XYLineAndShapeRenderer to class\nXYSplineRenderer has higher correctness probability than moving methods or at-\ntributes to class XYDotRenderer or SegmentedTimeline.\n\nBased on these observations, we believe that it is important to consider additional\nobjectives rather than using only structural metrics to ensure quality improvement.\nHowever, in most of the existing work, design semantics, amount of code changes, and\ndevelopment history are not considered. Improving code structure, minimizing design\nincoherencies, reducing code changes, and maintaining consistency with development\nchange history are conflicting goals. In some cases, improving the program structure\ncould provide a design that does not make sense semantically or could change radically\nthe initial design. For this reasons, an effective refactoring strategy needs to find a\ncompromise between all of these objectives. These observations are the motivation for\nthe work described in this paper.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n", 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:7\n\n3. REFACTORING: A MULTI-OBJECTIVE PERSPECTIVE\n3.1. Overview\n\nOur approach aims at exploring a large search space to find refactoring solutions, i.e.,\na sequence of refactoring operations, to correct bad smells. The search space is deter-\nmined not only by the number of possible refactoring combinations, but also by the\norder in which they are applied. A heuristic-based optimization method is used to gen-\nerate refactoring solutions. We have four objectives to optimize: 1) maximize quality\nimprovement (bad smells correction); 2) minimize the number of design coherence er-\nrors by preserving the way code elements are semantically grouped and connected\ntogether; 3) minimize code changes needed to apply the refactorings; and 4) maximize\nthe consistency with development change history. We thus consider the refactoring\ntask as a multi-objective optimization problem using the non-dominated sorting ge-\nnetic algorithm (NSGA-II) (Deb et al. 2002)\n\nThe general structure of our approach is sketched in Figure [2] It takes as input the\nsource code of the program to be refactored, a list of possible refactorings that can\nbe applied (label A), a set of bad smell detection rules (label B) (Quni et-al- 20728,\nour technique for approximating code changes needed to apply refactorings (label C),\na set of textual and design coherence measures described in Section 3 (label D), and\na history of applied refactorings to previous versions of the system (label E). Our ap-\nproach generates as output a near-optimal sequence of refactorings that improves the\nsoftware quality by minimizing as much as possible the number of design defects, min-\nimizing code changes required to apply the refactorings, preserving design semantics,\nand maximizing the consistency with development change history. Our approach cur-\nrently supports eleven refactoring operations including move method, move field, pull\nup field, pull up method, push down field, push down method, inline class, extract\nmethod, extract class, move class, and extract interface (cf. Table[IIp [Fowler 1999], but\nnot all refactorings in the literatur¢?| We selected these refactorings because they are\nthe most frequently used refactorings and they are implemented in most modern IDEs\nsuch as Eclipse and Netbeans. In the following, we describe the formal formulation of\nthe four objectives to optimize.\n\n3.2. Modeling the refactoring process as a multi-objective problem\n\n3.2.1. Quality. The Quality criterion is evaluated using the fitness function given in\nEquation |1| The quality value increases when the number of defects in the code is\nreduced after refactoring. This function returns the complement of the ratio of the\nnumber of design defects after refactoring (detected using bad smells detection rules)\nover the total number of defects that are detected before refactoring. The detection of\ndefects is based on some metrics-based rules according to which a code fragment can\nbe classified as a design defect or not (without a probability/risk score), i.e., 0 or 1, as\ndefined in our previous work [Kessentini et al. 2011} Ouni et al. 2012a]. The accuracy of\nthe genetic programming approach for code smells detection proposed in our previous\n\nstudies was an average of 91% of precision and 87% of recall on 8 large-scale systems.\nThe defect correction ratio function is defined as follows:\n\n# defects after applying refactorings\n\nDCR =1-—\n# defects before applying refactorings\n\n()\n\n3.2.2. Code changes. Refactoring Operations (ROs) are classified into two types: Low-\n\nLevel ROs (LLR) and High-Level ROs (HLR) [Ouni et al. 2012a]. A HLR is a sequence\n*http://refactoring.com/catalog/\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n\n', '39:8 A. Ouni et al.\n\nList of possible\nrefactorings\n\nDesign defects\ndetection rules\n\nOutpht: recorded refactorings\n\nCode changes\namount\napproximation\n\nInput:\nRefactoring efforts\n\nMulti-Objective Suggested\nSearch-based ones\n\n* refactorings\nInput: ; Refactoring\nsource code Semantic\nmeasures\n+ call graphs Outpht: semantic measures\n\nInput:\nList of previous\nprogram versions\n\nSimilarity with\ngood recorded\nrefactorings\n\nOutput: recorded refactorings\nFig. 2: Multi-objective search-based refactoring framework.\n\nof two or more ROs. An LLR is an elementary refactoring consisting of just one basic\nRO (e.g., Create Class, Delete Method, Add Field). The weight w; for each RO is an\ninteger number in the range [1, 2, 3] depending on code fragment complexity, and on\nchange impact. For a refactoring solution consisting of p ROs, the code changes score\nis computed as:\nPp\nCode_changes = > Wi (2)\ni=1\nTable[[|shows how the code change score is calculated for each refactoring operation.\nAs described in the table, to estimate the number of required code changes for a high\nlevel refactoring, our method considers the number of low level refactoring operations\n(atomic changes) needed to actually implement such a refactoring based on the Soot\ntool. For instance, to move a method m from a class c; to a class c2, the required number\nof chance is calculated as follows: 1 add method with a weight w; = 1, 1 delete method\nwith a w; = 1, n redirect method call with a w; = 2, and n redirect field access with\naw; = 2 as described in Table|I| Using appropriate static code analysis, Soot allows\nto easily calculate the value n, by capturing the number of field references/accesses\nfrom a method, the number of calls that should be redirected based on call graph), the\nnumber of return types and parameters of a method, as well as the control flow graph\nof a method, and so on.\n\n3.2.3. Similarity with recorded code changes. We defined the following function to calcu-\nlate the similarity score between a proposed refactoring operation and a recorded code\nchange:\n\nn\nSim_refactoring_history(RO) = > ej (3)\nj=l\nwhere n is the number of recorded refactoring operations applied to the system in the\npast, and ej is a refactoring weight that reflects the similarity between the suggested\nrefactoring operation (RO) and the recorded refactoring operation j. The weight e; is\ncomputed as follows: if the suggested and the recorded refactorings being compared are\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:9\n\nTable I: High and low level refactoring operations and their associated change scores.\n\nLow level refactoring\n\n3 3 7\n$ 2 5\n38 ,. 2 8B\nz Re a me]\n223 64 37 2 222 3 3 &\n3 3s @ ica —o« FI & & & 3\noS Ss 8 F ZF &£ BB E ~¢ %@ 2g 2g\n¢3i323 FPR GEESE\n2s 2s 83s 3 3 = E& & EE\n. . SoS Aa 4 A £4 A Re He 4 HF Fe w\nHight level refactoring\nWeight wi 2 3 3 1 3 2 2 1 2 1 1 1\nMove method 1 1 n n\nMove field 1 1 n\nPull up field 1 1 n n\nPull up method 1 1 n n n\nPush down field 1 1 n n\nPush down method 1 1 n n n\nInline class 1 n\nExtract method 1 n n n n n\nExtract class 1 n n n n n n\nMove class 1 1 n n n\nExtract interface 1 n n n n n n n n\n\nidentical, e.g., Move Method between the same source and target classes, then weight\ne; = 2. If the suggested and the recorded refactorings are similar, then e; = 1. We\nconsider two refactoring operations as similar if one of them is composed of the other or\nif their implementations are similar, using equivalent controlling parameters, i.e., the\nsame code fragments, as described in Table Some complex refactoring operations,\nsuch as Extract Class can be composed of other refactoring operations such as Move\nMethod, Move Field, Create New Class, etc., the weight w; = 1. Otherwise, w; = 0.\nMore details about the similarity scores between refactoring operations can be found\n\nin [Ouni et al. 2013].\n\n3.2.4. Semantics. To the best of our knowledge, there is no consensual way to investi-\ngate whether refactoring can preserve the design semantics of the original program.\nWe formulate semantic coherence using a meta-model in which we describe the con-\ncepts from a perspective that helps in automating the refactoring recommendation\ntask. The aim is to provide a terminology that will be used throughout this paper.\nFigure[3|shows the semantic-based refactoring meta-model. The class Refactoring rep-\nresents the main entity in the meta-model. As mentioned earlier, we classify refactor-\ning operations into two types: low-level ROs (LLR) and high-level ROs (HLR). A LLR\nis an elementary/basic program transformation for adding, removing, and renaming\nprogram elements (e.g., Add Method, Remove Field, Add Relationship). LLRs can be\ncombined to perform more complex refactoring operations (HLRs) (e.g., Move Method,\nExtract Class). A HLR consists of a sequence of two or more LLRs or HLRs; for exam-\nple, to perform Extract Class we need to Create New Empty Class and apply a set of\nMove Method and Move Field operations.\n\nTo apply a refactoring operation we need to specify which actors, i.e., code fragments,\nare involved in this refactoring and which roles they play when performing the refac-\ntoring operation. As illustrated in Figure |3| an actor can be a package, class, field,\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:10 A. Ouni et al.\n\nTable II: Refactoring operations and their involved actors and roles.\n\nRefactoring operation Actors Roles\n\nclass source class, target class\nMove method method moved method\n\nclass source class, target class\nMove field field moved field\n\nclass source class, target class\nPull up field field moved field\n\nclass source class, target class\nPull up method method moved method\n\nclass source class, target class\nPush down field field moved field\n\nclass source class, target class\nPush down method method moved method\n\n| Inline class class source class, target class\n\nclass source class, target class\nExtract method method source method, new method\n\nstatement | moved statements\n\nclass source class, new class\nExtract class field moved fields\n\nmethod moved methods\nMove class package source package, target package\n\nclass moved class\n\nclass source classes, new interface\nExtract interface field moved fields\n\nmethod moved methods\n\nmethod, parameter, statement, or variable. In Table|II} we specify for each refactoring\noperation the involved actors and their roles.\n\nPackage f€\nO*\nLo\ncss (5\nHigh-level Me Low-level\no.* Refactoring [© Refactoring 0."\nField\nWo. 0."\naccess\nActor:\nRefactoring Method\nperforms ‘‘ ¥\nLay oll\ninvolves\n7 Role Statement\nLo\nhas\nConstraints\nT* _satisly\nParameter }€>\noo] Structural Semantic\nConstraints Constraints\n[ I T T 1\nPre-Condition | | Post-Condition Vocabulary ||Shared|| Shared || Implementatio||Feature inheritance ||Cohesion-based|\n1 1." |based similarity || fields || methods usefulness dependancy\n\nFig. 3: Semantics-based refactoring meta-model.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:11\n\n3.3. Design coherence measures\n\n3.3.1. Vocabulary-based similarity (VS). This kind of similarity is interesting to consider\nwhen moving methods, fields, or classes. For example, when a method has to be moved\nfrom one class to another, the refactoring would make sense if both actors (source class\nand target class) use similar vocabularies (Quni ef al. 2012). The vocabulary could be\nused as an indicator of the semantic/textual similarity between different actors that\nare involved when performing a refactoring operation. We start from the assumption\nthat the vocabulary of an actor is borrowed from the domain terminology and therefore\ncan be used to determine which part of the domain semantics an actor encodes. Thus,\ntwo actors are likely to be semantically similar if they use similar vocabularies.\n\nThe vocabulary can be extracted from the names of methods, fields, variables, pa-\nrameters, types, etc. Tokenisation is performed using the Camel Case Splitter\nlet al. 2012}, which is one of the most used techniques in Software Maintenance tools\n\nor the preprocessing of identifiers. A more pertinent vocabulary can also be extracted\nfrom comments, commit information, and documentation. We calculate the semantic\nsimilarity between actors using an information retrieval-based technique, namely co-\nsine similarity, as shown in Equation |4] Each actor is represented as an n-dimensional\nvector, where each dimension corresponds to a vocabulary term. The cosine of the angle\nbetween two vectors is considered as an indicator of similarity. Using cosine similarity,\nthe conceptual similarity between two actors c; and c2 is determined as follows:\n\nC1 + Wi X Wi\nSim(c1,¢2) = Cos(4,) = ~4-? _ = Dien Wi X We (4)\nHeil Heal \\/yre yw, x OL uP,\nwhere ¢j = (wi,1,-..,Wn,1) is the term vector corresponding to actor c; and 6 =\n\n(w1,2,-+-;Wn,2) is the term vector corresponding to c2. The weights wi,j can be com-\nputed using information retrieval based techniques such as the Term Frequency — In-\nverse Term Frequency (TF-IDF) method. We used a method similar to that described\nin to determine the vocabulary and represent the actors as term vectors.\n\n3.3.2. Dependency-based similarity (DS). We approximate domain semantics closeness\nbetween actors starting from their mutual dependencies. The intuition is that actors\nthat are strongly connected (i.e., having dependency links) are semantically related. As\na consequence, refactoring operations requiring semantic closeness between involved\nactors are likely to be successful when these actors are strongly connected. We con-\nsider two types of dependency links based on use the Jaccard similarity coefficient as\n\nthe way you compute the similarity |Jaccard 1901]:\n\n— Shared Field Access (SFA) that can be calculated by capturing all field references\nthat occur using static analysis to identify dependencies based on field accesses\n(read or modify). We assume that two software elements are semantically related\nif they read or modify the same fields. The rate of shared fields (read or modified)\nbetween two actors c; and cz is calculated according to Equation} In this equation,\nfieldRW (c;) computes the number of fields that may be read or modified by each\nmethod of the actor c;. Note that only direct field access is considered (indirect field\naccesses through other methods are not taken into account). By applying a suitable\nstatic program analysis to the whole method body, all field references that occur can\nbe easily computed.\n\n| fieldRW (cx) N fieldRW (c2)\n| fieldRW (cx) U fieldRW (c2)\n\nsharedFieldsRW (ci, c2) (5)\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:12 A. Ouni et al.\n\n— Shared Method Calls (SMC) that can be captured from call graphs derived from\nthe whole program using CHA (Class Hierarchy Analysis) Walleeraiet l. 2000}\nA call graph is a directed graph which represents the different calls (call in and call\nout) among all methods of the entire program. Nodes represent methods, and edges\nrepresent calls between these methods. CHA is a basic call graph that considers\nclass hierarchy information, e.g, for a call c.m/(...) assume that any m(...) is reachable\nthat is declared in a subtype or sometimes supertype of the declared type of c. For\na pair of actors, shared calls are captured through this graph by identifying shared\nneighbours of nodes related to each actor. We consider both, shared call-out and\nshared call-in. Equations Bjand[7Jare used to measure respectively the shared call-\nout and the shared call-in between two actors c; and c2 (two classes, for example).\n\n| callOut(c1) N callOut(c2)\n\nsharedCallOut(c1,¢ >\nsharedCallOut(cr, c2) | callOut(c1) U callOut(c2)\n\n(6)\n\n| callIn(e1) M callIn(c2) |\n| eallIn(cy) U callIn(cg) |\n\nA shared method call is defined as the average of shared call-in and call-out.\n\n(7)\n\nsharedCallIn(c,, c2) =\n\n3.3.3. Implementation-based similarity (IS). For some refactorings like Pull Up Method,\nmethods having similar implementations in all subclasses of a super class should be\nmoved to the super class (Fowler 1999). The implementation similarity of the meth-\nods in the subclasses is investigated at two levels: signature level and body level. To\ncompare the signatures of methods, a semantic comparison algorithm is applied. It\nconsiders the methods names, the parameter lists, and return types. Let Sig(m;) be\nthe signature of method m;. The signature similarity for two methods m, and m2 is\ncomputed as follows:\n\n| Sig(ma) 1 Sig(ma) |\n| Sig(mi) U Sig(ma) |\n\nTo compare method bodies, we use Soot ||Vallée-Rai et al. 2000], a Java optimization\nframework, which compares the statements in the body, the used local variables, the\nexceptions handled, the call-outs, and the field references. Let Body(m) (set of state-\n\nments, local variables, exceptions, call-outs, and field references) be the body of method\nm. The body similarity for two methods m, and mz is computed as follows:\n\n(8)\n\nSig_sim(m,,m2) =\n\n| Body(m1) 1 Body(ma) |\n| Body(m1) U Body(mz) |\n\nThe implementation similarity between two methods is the average of their Sig_Sim\nand Body_Sim values.\n\n(9)\n\nBody_sim(m1, m2)\n\n3.3.4. Feature inheritance usefulness (FIU) . This factor is useful when applying the Push\nDown Method and Push Down Field operations. In general, when method or field is\nused by only few subclasses of a super class, it is better to move it, i.e., push it down,\nfrom the super class to the subclasses using it [Fowler 1999]. To do this for a method,\nwe need to assess the usefulness of the method in the subclasses in which it appears.\n\nWe use a call graph and consider polymorphic calls derived using XTA (Separate Type\nAnalysis) Tip and Palsberg 2000]. XTA is more precise than CHA by giving a more\nlocal view of what types are available. We are using Soot [Vallée-Rai et al. 2000] as a\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:13\n\nstandalone tool to implement and test all the program analysis techniques required in\nour approach. The inheritance usefulness of a method is given by Equation[10}\n\nLiki call(m, i) (10)\n\nn\n\nFIU(m,c) =1-\n\nwhere n is the number of subclasses of the superclass c, m is the method to be pushed\ndown, and call is a function that returns 1 if m is used (called) in the subclass i, and 0\notherwise.\n\nFor the refactoring operation Push Down Field, a suitable field reference analysis is\nused. The inheritance usefulness of a field is given by Equation{I]}\n\nSL, use(f, ci) ay\n\nn\n\nFIU(f,c) =1-\n\nwhere n is the number of subclasses of the superclass c, fis the field to be pushed down,\nand use is a function that return 1 if f is used (read or modified) in the subclass c;, and\n0 otherwise.\n\n3.3.5. Cohesion-based dependency (CD). We use a cohesion-based dependency measure\nfor the Extract Class refactoring operation. The cohesion metric is typically one of the\n\nimportant metrics used to identify and fix design defects |[Moha et al. 2010}|Moha et al.\n- How-\never, the cohesion-based similarity that we propose for code refactoring, in particular\nwhen applying extract class refactoring, is defined to find a cohesive set of methods\nand attributes to be moved to the newly extracted class. A new class can be extracted\nfrom a source class by moving a set of strongly related (cohesive) fields and methods\nfrom the original class to the new class. Extracting this set will improve the cohesion of\nthe original class and minimize the coupling with the new class. Applying the Extract\nClass refactoring operation on a specific class will result in this class being split into\ntwo classes. We need to calculate the semantic similarity between the elements in the\noriginal class to decide how to split the original class into two classes.\n\nWe use vocabulary-based similarity and dependency-based similarity to find the co-\nhesive set of actors (methods and fields). Consider a source class that contains n meth-\nods {mj,...mn,} and m fields {f1,...fm}. We calculate the similarity between each pair\nof elements (method-field and method-method) in a cohesion matrix as shown in Table\naul\n\nThe cohesion matrix is obtained as follows: for the method-method similarity, we\nconsider both vocabulary and dependency-based similarity. For the method-field simi-\nlarity, if the method m; may access (read or write) the field f;, then the similarity value\nis 1. Otherwise, the similarity value is 0. The column “Average”” contains the average\nof similarity values for each line. The suitable set of methods and fields to be moved to\na new class is obtained as follows: we consider the line with the highest average value\nand construct a set that consists of the elements in this line that have a similarity\nvalue that is higher than a threshold equals to 0.5. We used a trial and error strategy\nto find this suitable threshold value after executing our similarity measure more than\n30 times.\n\nOur decision to use such a technique is driven by the computation complexity since\nheavy and complex techniques might affect the whole search process. While cohesion is\none of the strongest metrics which is already used in related work [Fokaefs et al. 2011|\n[Bavotal\n\n0] for identifying extract class refactoring opportunities, we are planning to\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:14 A. Ouni et al.\n\nTable III: Example of a cohesion matrix.\n\nfi | fo +++ fm | mi m2 «ss mn Average\nmy, 1 0 1 1 0.15 0.1 0.42\nm2 0 1 I 1 I 0 0.6\nip I 0 0 0.6 0.2 T 0.32\n\ncombine it with coupling metric, in order to reduce coupling between the extracted\nclass and the original one.\n\n4. NSGA-II FOR SOFTWARE REFACTORING\n\nThis section is dedicated to describing how we encoded the problem of finding a good\nrefactoring sequence as an optimization problem using the non-dominated sorting ge-\n\nnetic algorithm NSGA-II [Deb et al. 2002].\n\n4.1. NSGA-II overview\n\nOne of the most powerful multi-objective search techniques is NSGA-II\nthat has shown good performance in solving several software engineering prob-\nems [Harman et al. 2012].\n\nA high-level view of NSGA-II is depicted in Algorithm[]] NSGA-II starts by randomly\ncreating an initial population P) of individuals encoded using a specific representation\n(line 1). Then, a child population Qo is generated from the population of parents Po\n(line 2) using genetic operators (crossover and mutation). Both populations are merged\ninto an initial population Ro of size N (line 5). Fast-non-dominated-sort\nis the technique used by NSGA-II to classify individual solutions into different\ndominance levels (line 6). Indeed, the concept of non-dominance consists of comparing\neach solution x with every other solution in the population until it is dominated (or\nnot) by one of them. According to Pareto optimality: “A solution x; is said to dominate\nanother solution ro, if x; is no worse than x2 in all objectives and x, is strictly better\nthan £2 in at least one objective”. Formally, if we consider a set of objectives f; , i € 1..n,\nto maximize, a solution x; dominates 72 :\n\niff Vi, fi(w2) < fi(w1) and 33 | fi(w2) < fi (x1)\n\nThe whole population that contains N individuals (solutions) is sorted using the\ndominance principle into several fronts (line 6). Solutions on the first Pareto-front Fo\nget assigned dominance level of 0 Then, after taking these solutions out, fast-non-\ndominated-sort calculates the Pareto-front F, of the remaining population; solutions\non this second front get assigned dominance level of 1, and so on. The dominance level\nbecomes the basis of selection of individual solutions for the next generation. Fronts\nare added successively until the parent population P,; is filled with N solutions (line\n8). When NSGA-II has to cut off a front F; and select a subset of individual solutions\nwith the same dominance level, it relies on the crowding distance\nto make the selection (line 9). This parameter is used to promote diversity within the\npopulation. The crowding distance of a non-dominated solution serves for getting an\nestimate of the density of solutions surrounding it in the population. It is calculated by\nthe size of the largest cuboid enclosing each particle without including any other point.\nHence, the crowding distance mechanism ensures the selection of diversified solutions\nhaving the same dominance level. The front F; to be split, is sorted in descending\norder (line 13), and the first (N- |P,,:|) elements of F; are chosen (line 14). Then a\nnew population Q;+1 is created using selection, crossover and mutation (line 15). This\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:15\n\nAlgorithm 1 High level pseudo code for NSGA-II\n\n1: Create an initial population Po\n\n2: Create an offspring population Qo\n\n3: t=0\n\n4: while stopping criteria not reached do\nH 1=PpUQ\n\n5:\n\n6: F = fast-non-dominated-sort(R;)\n7 Pi =Oandi=1\n\n8: while | Pi41|+ | Fi |< N do\n\n9: Apply crowding-distance-assignment(F;)\n10: Pua = Py UF:\n\n11: t=it+l1\n\n12: end while\n\n13: Sort(Fi, < n)\n\n14.00 Pry = Pry UF [N—- | Pras |]\n15:  Qi41 = create-new-pop(P:+1)\n16: t=t+1\n\n17: end while\n\nprocess will be repeated until reaching the last iteration according to stop criteria (line\n4).\n\n4.2. NSGA-II adaptation\nThis section describes how NSGA-II [Deb et al. 2002] can be used to find refactoring\n\nsolutions with multiple conflicting objectives. To apply NSGA-II to a specific problem,\nthe following elements have to be defined: representation of the individuals, creation of\na population of individuals, evaluation of individuals using a fitness function for each\nobjective to be optimized to determine a quantitative measure of their ability to solve\nthe problem under consideration, selection of the individuals to transmit from one\ngeneration to another, creation of new individuals using genetic operators (crossover\nand mutation) to explore the search space, generation of a new population.\n\nThe next sections explain the adaptation of the design of these elements for the\ngeneration of refactoring solutions using NSGA-II.\n\n4.2.1. Solution representation. To represent a candidate solution (individual), we used\na vector representation. Each vector’s dimension represents a refactoring operation.\nThus, a solution is defined as a sequence of refactorings applied to different parts of\nthe system to fix design defects. When created, the order of applying these refactorings\ncorresponds to their positions in the vector. In addition, for each refactoring, a set of\ncontrolling parameters (stored in the vector), e.g., actors and roles, as illustrated in\nTable are randomly picked from the program to be refactored and stored in the\nsame vector. An example of a solution is presented in Figure/dal\n\nMoreover, when creating a sequence of refactorings (an individual), it is important\nto guarantee that they are feasible and that they can be legally applied. The first work\nin the literature was proposed by who introduced a way of formalizing\nthe preconditions that must be met before a refactoring can be applied and ensure that\nthe behavior of the system is preserved. Opdyke created functions which could be used\nto formalize constraints. These constraints are similar to the Analysis Functions used\nlater by [Cinnéide 2001] and [Roberts and Johnson 1999].\n\nFor each refactoring operation we specify a set of pre- and post-conditions to ensure\nthe feasibility of applying them using a static analysis. For example, to apply the refac-\ntoring operation move method(Person, Employee, getSalary()), a number of necessary\npreconditions should be satisfied, e.g., Person and Employee should exists and should be\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:16 A. Ouni et al.\n\nclasses; getSalary() should exist and should be a method; classes Person and Employee\nshould not be in the same inheritance hierarchy; the method getSalary() should be\nimplemented in Person; the method signature of getSalary() should not be present\nin class Employee. As postconditions, Person, Employee, and getSalary() should ex-\nist; getSalary() declaration should be in class Employee; and getSalary() declaration\nshould not exist in class Person. Figure|4b| describes for each refactoring operation its\npre and post conditions that should be satisfied. To express these conditions we defined\na set of functions. These functions include:\n\n—isClass(c): checks whether c is a class (similarly for areClasses()).\n\n—isInterface(c): checks whether c is an interface (similarly for areInterfaces()).\n\n— isMethod(m): checks whether m is a method.\n\n—Sig(m): returns the signature of the method m.\n\n— isField(f): checks whether f is a field.\n\n—defines(c,e): checks whether the code element e (method or field) is implemented\nin the class/interface c.\n\n—exists(e): checks whether the code element e exists in the current version of the\ncode model (Similarly for exist ()).\n\n— inheritanceHierarchy(ci,c2): checks whether both classes c1 and c2 belong to the\nsame inheritance hierarchy.\n\n— isSuperClassOf (c1,c2): checks whether ci is a superclass of c2.\n\n— isSubClassOf (c1,c2): checks whether ci is a subclass of c2.\n\n—fields(c): returns the list of fields defined in the class or interface c.\n\n—methods(c): returns the list of methods implemented in class or interface c.\n\nFor composite refactorings, such as extract class and inline class, the overall pre\nand post conditions should be checked. For a sequence of refactorings which may be\nof any length, we simplify the computation of its full precondition by analyzing the\nprecondition of each refactoring in the sequence and the corresponding effects on the\ncode model (postconditions).\n\n4.2.2. Fitness functions. After creating a solution, it should be evaluated using fitness\nfunction to ensure its ability to solve the problem under consideration. Since we have\nfour objectives to optimize, we are using four different fitness functions to include in\nour NSGA-II adaptation. We used the four fitness functions described in the previous\nsection:\n\n(1) Quality fitness function. It aims at calculating the number of fixed design de-\nfects after applying the suggested refactorings.\n\n(2) Design coherence fitness function. It aims at approximating the design preser-\nvation after applying the suggested refactorings. In Table |IV| we specify, for each\nrefactoring operation, which measures are taken into account to ensure that the\nrefactoring operation preserves design coherence.\n\n(3) Code changes fitness function. It calculates the amount of code changes re-\nquired to apply the suggested refactorings.\n\n(4) History of changes fitness function. It calculates the consistency of the sug-\ngested refactorings with prior code changes.\n\n4.2.3. Selection. To guide the selection process, NSGA-II uses a binary tournament\nselection based on dominance and crowding distance Deb et al. 2002}. NSGA-II sorts\nthe population using the dominance principle which classifies individual solutions into\ndifferent dominance levels. Then, to construct a new offspring population Q;1, NSGA-\nII uses a comparison operator based on a calculation of the crowding distance\nto select potential individuals having the same dominance level.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:17\n\nmove field (Person, Employee, salary)\n\nextract method (Person, printInfo(), printContactInfo())\n\nmove method (Person, Employee, getSalary())\npush down field (Person, Student, studentId)\ninline class (Car, Vehicle)\n\nmove method (Person, Employee, setSalary())\n\nmove field (Person, Employee, tax)\n\nextract,class(Person, Adress, streetNo, city, zipCode, getAdress(), updateAdress())\n\n(a) Solution representation.\n\nRefactorings Pre and post-conditions\nPre. | €xist(cl,e2, m) AND areClasses(cl,¢2) AND isMethod(m) AND NOT(nheritanceHierarchy(cl,¢2))\nre\nMove Method(cl,c2,m) AND defines(cl,m) AND NOT(defines(c2,sig(m))\n\nPost: | exist(cl,c2,m) AND defines(c2,m) AND NOT(defines(c1,m))\n\nexist(cl, c2,f) AND areClasses(cl,c2) AND isField(f) AND NOT(inheritanceHierarchy(cl,c2))\nMove Field(cl,c2,f) AND defines(c1,f) AND NOT(defines(c2,f))\n\nPost: | exist(cl,c2, f) AND defines(c2,f) AND NOT(defines(cl,f))\n\nexist(cl, c2,f) AND areClasses(cl, c2) AND isField(f) AND isSuperClassOf(c2,c1) AND\nPull Up Field(c1,c2,f) * | defines(c1,f) AND NOT(defines(c2, f))\n\nPost: | exist(cl,c2,f) AND defines(c2,f) AND NOT(defines(c1,f))\n\nexist(cl,c2,m) AND areClasses(c1,c2) AND isMethod(m) AND isSuperClassOf(c2,cl) AND\nPull Up Method(cl,c2,m) | defines(cl,m) AND NOT(defines(c2,sig(m))\n\nPost: | exist(cl,c2,m) AND defines(c2,m) AND NOT(defines(c1,m))\n\nexist(cl,c2, f) AND areClasses (cl,c2) AND isField(f) AND isSubClassOf{c2,c1) AND\nPush Down Field(c1,c2,f) | defines(c1,f) AND NOT(defines(c2,f)\n\nPost: | exist(cl) AND exists(c2) AND exits(m) AND defines(c2,m) AND NOT(defines(c1,m))\nexist(cl,c2,m) AND areClasses (cl,c2) AND isMethod(m) AND isSubClassOf(c2,c1) AND\nPush Down Method(cl,c2,m) |__| defines(c1,m) AND NOT(defines(c2,sig(m))\n\nPost: | exist(cl,c2,m) AND defines(c2,m) AND NOT(defines(c1,m))\n\nPre: | exist(cl,c2) AND areClasses(cl,c2)\n\nPost: | exists(cl) AND NOT¢(exists(c2))\n\nPre: | exists(cl) AND NOT(exists(c2)) AND isClass(cl) AND |methods(cl)|>2\n\nPost: | exist(cl,c2) AND isClass(c2)\n\nPre: | exists(cl) AND NOT(exists(c2)) AND isInterface(cl) AND |methods(c1)| >2\n\nPost: | exist(cl,c2) AND isInterface(c2)\n\nPre: | exists(cl) AND NOT(exists(c2)) AND isClass(cl) AND |methods(cl)|>2\n\nPost: | exist(cl,c2) AND isClass (c2) AND isSuperClass(c1,c2)\n\nPre: | exists(cl) AND NOT(exists(c2)) AND isClass(cl) AND |methods(cl)|>2\n\nPost: | exist(cl,c2) AND isClass(c2) AND isSubClass(cl,c2)\n\nInline Class(c1,c2)\n\nExtract Class(cl,c2)\n\nExtract Interface(c1,c2)\n\nExtract Super Class(cl,c2)\n\nExtract Sub Class(cl,c2)\n\n(b) Pre- and post- conditions of refactorings.\n\nFig. 4: Representation of an NSGA-II individual and used constraints.\n\n4.2.4. Genetic operators. To better explore the search space, crossover and mutation\noperators are defined.\n\nFor crossover, we use a single, random, cut-point crossover. It starts by selecting and\nsplitting at random two parent solutions. Then crossover creates two child solutions by\nputting, for the first child, the first part of the first parent with the second part of the\nsecond parent, and, for the second child, the first part of the second parent with the\nsecond part of the first parent. This operator must ensure that the length limits are re-\nected by eliminating randomly some refactoring operations. As illustrated in Figure\ncrossover splits the parent solutions in the position i = 3 within their representa-\ntive vectors in order to generate new child solutions. Each child combines some of the\nrefactoring operations of the first parent with some ones of the second parent. In any\ngiven generation, each solution will be the parent in at most one crossover operation.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:18 A. Ouni et al.\n\nTable IV: Refactoring operations and their semantic measures.\n\nRefactorings vs DS Is FIU cD\nmove method x x\nmove field x x\npull up field x x x\npull up method x x x\npush down field x x x\npush down method x x x\ninline class x x\nextract class x x x\nmove class x x\nextract interface x x x\nmove field move field\nextract class extract class\nParent 1 | move method Child 1 | move method\npull up field move field\nextract class extract class\ninline class Crossover\nmove method\nmove method (i= 3) inline class\nParent 2 inline class Child 2 push down field\npush down field pull up field\nmove field extract class\nextract class inline class\nBefore crossover After crossover\n\nFig. 5: Crossover operator.\n\nThe mutation operator picks randomly one or more operations from a sequence and\nreplaces them with other ones from the initial list of possible refactorings. An example\nis shown in Figure [6|where a mutation operator is applied with two random positions\nto modify two dimensions of the vector in the third and the fifth dimensions (j = 3 and\nk=5).\n\nmove field move field\nParent Child\n\nextract class Mutati extract class\nmove method uration move field\npull up field pull up field\nextract class (=3, k=5) move method\ninline class inline class\nBefore mutation After mutation\n\nFig. 6: Mutation operator.\n\nAfter applying genetic operators (mutation and crossover), we verify the feasibility\nof the generated sequence of refactoring by checking the pre and post conditions. Each\nrefactoring operation that is not feasible due to unsatisfied preconditions will be re-\nmoved from the generated refactoring sequence. The new sequence is considered valid\nin our NSGA-II adaptation if the number of rejected refactorings is less than 5% of the\ntotal sequence size. We used trial and error to find this threshold value after several\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:19\n\nexecutions of our algorithm. The rejected refactorings will not be considered anymore\nin the solution.\n\n5. VALIDATION AND EXPERIMENTATION DESIGN\n\nIn order to evaluate the feasibility and the efficiency of our approach for generating\ngood refactoring suggestions, we conducted an experiment based on different versions\nof open-source systems. We start by presenting our research questions. Then, we de-\nee discuss the obtained results. All experimentation materials are available\nonlin\n\n5.1. Research questions\n\nIn our study, we assess the performance of our refactoring approach by determining\nwhether it can generate meaningful sequences of refactorings that fix design defects\nwhile minimizing the number of code changes, preserving the semantics of the design,\nand reusing, as much as possible a base of recorded refactoring operations applied\nin the past in similar contexts. Our study aims at addressing the research questions\noutlined below.\n\nThe first four research questions evaluate the ability of our proposal to find a com-\npromise between the four considered objectives that can lead to good refactoring rec-\nommendation solutions.\n\n—RQl1.1: To what extent can the proposed approach fix different types of design de-\nfects?\n\n— RQI1.2: To what extent does the proposed approach preserve design semantics when\nfixing defects?\n\n—RQ1.3: To what extent can the proposed approach minimize code changes when\nfixing defects?\n\n—RQI1.4: To what extent can the use of previously-applied refactorings improve the\neffectiveness of the proposed refactorings?\n\n— RQ2: How does the proposed multi-objective approach based on NSGA-II perform\ncompared to other existing search-based refactoring approaches and other search\nalgorithms?\n\n—RQ3: How does the proposed approach perform compared to existing approaches\nnot based on heuristic search?\n\n—RQz4: Is our multi-objective refactoring approach useful for software engineers in\nreal-world setting?\n\nTo answer RQ1.1, we validate the proposed refactoring operations to fix design de-\nfects by calculating the defect correction ratio (DCR) on a benchmark composed of six\nopen-source systems. DCR is given by Equation |1| which corresponds to the comple-\nment of the ratio of the number of design defects after refactoring (detected using bad\nsmells detection rules) over the total number of defects that are detected before refac-\ntoring.\n\nTo Snswer RQ1.2, we use two different validation methods: manual validation and\nautomatic validation to evaluate the efficiency of the proposed refactorings. For the\nmanual validation, we asked groups of potential users of our refactoring tool to eval-\nuate, manually, whether the suggested refactorings are feasible and make sense se-\nmantically. We define the metric “refactoring precision” (RP), which corresponds to the\nnumber of meaningful refactoring operations (low-level and high-level), in terms of\nsemantics, over the total number of suggested refactoring operations. RP is given by\n\nhttp ://www-personal.umd.umich.edu/~marouane/tosemref.html\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n\n', '39:20 A. Ouni et al.\n\nEquation{12}\n\n___# coherent refactorings\n# suggested refactorings\n\n€ [0,1] (12)\n\nFor the automatic validation we compare the proposed refactorings with the ex-\npected ones using an existing benchmark |Ouni et al. 2012a}|Moha et al. 2010} Moha|\nin terms of recall (Equation and precision ( quation (14). The expected\nrefactorings are those applied by the software development team to the next software\nrelease. To collect these expected refactorings, we use Ref-Finder (Prete et al. 2010),\nan Eclipse plug-in designed to detect refactorings between two program versions. Ref-\n\nFinder allows us to detect the list of refactorings applied to the current version of a\nsystem (see Table [VIp.\n\n| suggested refactorings M expected refactorings |\n| expected refactorings |\n\nRE yecall € [0,1] (13)\n\n| suggested refactorings N expected refactorings |\n| suggested refactorings |\n\nREprecision = € [0,1] (14)\nThe intuition behind this metric is to assess whether the suggested refactorings are\nsimilar to the ones that a programmer would expect and perform.\n\nTo answer RQ1.3, we evaluate, using our benchmark, if the proposed refactorings\nare useful to fix detected defects with low code changes by calculating the code change\nscore. The code change score is calculated using our model described in Section\nWe then compare the obtained code change scores with and without integrating the\ncode change minimization objective in our tool.\n\nTo answer RQ1.4, we use the metric RP to evaluate the usefulness of the recorded\nrefactorings and their impact on the quality of the suggested refactorings in terms of\ndesign coherence (RP). Consequently, we compare the obtained code RP scores with\nand without integrating the reuse of recorded refactorings in our tool. In addition, in\norder to evaluate the importance of reusing recorded refactorings in similar contexts,\nwe define the metric “reused refactoring" (RR) that calculates the percentage of oper-\nations from the base of recorded refactorings used _to generate the optimal refactoring\nsolution by our proposal. RR is given by Equation[15]\n\n#used refactorings from the base of recorded refactorings\n\nRR #refactorings in the base of recorded refactorings\n\n€ [0,1] (15)\n\nTo answer RQ2, we compared our approach to two other existing search-based refac-\n\ntoring approaches: (7) Kessentini et al. (Kessentini et al. 2011), and (ii) Harman et al.\n{Harman and Tratt 2007] that consider the refactoring suggestion task only from the\nquality improvement perspective. Kessentini et al. formulated refactoring suggestion\nas a single objective problem to reduce as much as possible the number design de-\n\nfects, while Harman et al. formulated refactoring recommendation as multi-objective\nto find a trade-off between two quality metrics, CBO (coupling between objects) and\n\nSDMPC (standard deviation of methods per class). Both approaches [Kessentini et al.|\n2011] [Harman and Tratt 2007] did not consider the design coherence, the history o\nchang\n\nes and the required effort when suggesting refactorings. Moreover, we assessed\n\nthe performance of our multi-objective algorithm NSGA-II compared to another multi-\n\nobjective algorithm (i) MOGA, (ii) random search, and (ii) mono-objective genetic algo-\n\nrithm (GA) where one fitness function is used (an average of the four objective func-\ntions).\n\nTo answer RQ3, we compared our refactoring results with a popular design defects\n\ndetection and correction tool JDeodorant (Fokaefs ot al. 2011}|Fokaefs ot al. 2012) that\n\ndoes not use heuristic search techniques in terms of DCR, change score and RP. The\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:21\n\ncurrent version of JDeodorant |Fokaefs et al. 2012] is implemented as an Eclipse plug-\n\nin that identifies some types of design defects using quality metrics and then proposes\na list of refactoring strategies to fix them.\n\nTo answer RQ4, we asked 6 software engineers (2 groups of 3 developers each) to\nrefactor manually some of the design defects, and then compare the results with those\nproposed by our tool. We, thus, define the following precision metric:\n\n| RiARm |\nRn\n\nwhere R; is the set of refactorings suggested by our tool, and R,, is the set of refactor-\nings suggested manually by software engineers. We calculated an exact matching score\nwhen comparing between the parameters (i.e., actors as described in Table [i of the\nrefactoring suggested by our approach and the ones identified by developers. However,\nwe do not consider the order of the parameters in the comparison formula.\n\nPrecision =\n\n€ [0,1] (16)\n\n5.2. Experimental setting and instrumentation\n\nThe goal of the study is to evaluate the usefulness and the effectiveness of our refac-\ntoring tool in practice. We conducted an evaluation with potential users of our tool.\nThus, refactoring operations should not only remove design defects, but should also be\nmeaningful from a developer’s point of view.\n\n5.2.1. Subjects. Our study involved a total number of 24 subjects divided into 8 groups\n(3 subjects each). All the subjects are volunteers and familiar with Java development.\nThe experience of these subjects on Java programming ranged from 2 to 15 years.\nThe participants who evaluated the open source systems have a good knowledge about\nthese systems and they did similar experiments in the past on the same systems. We\nselected also the groups based on their familiarity with the studied systems.\n\nThe first six groups are drawn from several diverse affiliations: the University of\nMichigan (USA), University of Montreal (Canada), Missouri University of Science and\nTechnology (USA), University of Sousse (Tunisia) and a software development and web\ndesign company. The groups include 4 undergraduate students, 7 master students, 8\nPhD students, one faculty member, and 4 junior software developers. The three master\nstudents are working also at General Motors as senior software engineers. Subjects\nwere familiar with the practice of refactoring.\n\n5.2.2. Systems studied and data collection. We applied our approach to a set of six\nwell-known_and well-commented industrial open source Java projects: Xerces-\nJFreeChart? GanttProject¥| Apache Ant} J HotDraw_] and Rind Xerces-J is a fam-\nily of software packages for parsing XML. JFreeChart is a powerful and flexible Java\nlibrary for generating charts. GanttProject is a cross-platform tool for project schedul-\ning. Apache Ant is a build tool and library specifically conceived for Java applications.\nJHotDraw is a GUI framework for drawing editors. Finally, Rhino is a JavaScript in-\nterpreter and compiler written in Java and developed for the Mozilla/Firefox browser.\nWe selected these systems for our validation because they range from medium to large-\nsized open-source projects, which have been actively developed over the past 10 years,\n\nThttp://xerces.apache.org/xerces-j/|\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:22 A. Ouni et al.\n\nTable V: Programs statistics\n\nSystems Release #classes # design defects KLOC\nXerces-J v2.7.0 991 91 240\nJFreeChart v1.0.9 521 72 170\nGanttProject v1.10.2 245 49 41\nApache Ant v1.8.2 1191 112 255\nJHotDraw v6.1 585 25 21\n\nRhino v1.7R1 305 69 42\n\nTable VI: Analysed versions and refactorings collection\n\nExpected refactorings Collected refactorings\nSystems\nNext release #Refactorings Previous releases # Refactorings\nXerces-J v2.8.1 39 v1.4.2 - v2.7.0 70\nJFreeChart -v1.0.11 31 v1.0.6 - v1.0.9 76\nGanttProject v1.11.2 46 v1.7 - v1.10.2 91\nApache Ant v1.8.4 78 v1.2 - v1.8.2 247\nJHotDraw v6.2 27 v5.1 - v6.1 64\nRhino 1.7R4 46 v1.4R3 - 1.7R1 124\n\nand their design has not been responsible for a slowdown of their developments. Table\n[V] provides some descriptive statistics about these six programs.\n\nTo collect refactorings applied in previous program versions, and the expected refac-\ntorings applied to next version of studied systems, we use Ref-Finder [Prete et al.|\n[2010]. Ref-Finder, implemented as an Eclipse plug-in, can identify refactoring opera-\ntions applied between two releases of a software system. Table|VI|reports the analyzed\nversions and the number of refactoring operations, identified by Ref-Finder, between\neach subsequent couple of analyzed versions, after the manual validation. In our study,\nwe consider only refactoring types described in Table[I]]|\n\n5.2.3. Scenarios. We designed the study to answer our research questions. Our exper-\nimental study consists of two main scenarios: (1) the first scenario is to evaluate the\nquality of the suggested refactoring solutions with potential users (RQ1-3), and (2) the\nsecond scenario is to fix manually a set of design defects and compare the manual\nresults with those proposed by our tool (RQ4). All the recommended refactorings are\nexecuted using the Eclipse platform.\n\nAll the software engineers who accepted an invitation to participate in the study, re-\nceived a questionnaire, a manuscript guide that helps to fill the questionnaire, and the\nsource code of the studied systems, in order to evaluate the relevance of the suggested\nrefactorings to fix. The questionnaire is organized in an excel file with hyperlinks to\nvisualize the source code of the affected code elements easily. The participants were\nable to edit and navigate the code through Eclipse.\n\nScenario 1: The groups of subjects were invited to fill a questionnaire that aims\nto evaluate our suggested refactorings. The questionnaires rely on a four-point Lik-\nert scale in which we offered a choice of pre-coded responses for every\nquestion with no ‘neutral’ option. Thereafter, we assigned to each group a set of refac-\ntoring solutions suggested by our tool to evaluate manually. The participants were\nable to edit and navigate the code through the Eclipse IDE. Table [VII] describes the\nset of refactoring solutions to be evaluated for each studied system in order to an-\nswer our research questions. We have three multi-objective algorithms to be tested\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:23\n\nTable VII: Refactoring solutions for each studied system considering each objective:\nquality (Q), Semantic coherence (S), Code changes (CC) , Recorded refactorings (RR),\nCBO (Coupling Between Objects) and SDMPC (Standard Deviation of Methods Per\nClass).\n\nRef. Solution Algorithm/ Approach # objective Functions Objectives considered\n\nSolution 1 NSGA-II 4 Q,8, CC, RR\nSolution 2 MOGA 4 Q,8, CC, RR\nSolution 3 Random Search (RS) 4 Q, 5S, CC, RR\nSolution 4 Genetic Algorithm 1 Q+S+CC+RR\nSolution 5 Kessentini et al. 1 Q\nSolution 6 Harman et al. 2 CBO, SDMPC\nfor the refactoring suggestion task: NSGA-II (Non-dominated Sorting Genetic Algo-\nrithm) (De al. 2003 MOGA (Multi-Objective Genetic Algorithm)\n, and RS (Random Search) [Zitzler and Thiele 1998]. Moreover, we compared our\n\nresults with a mono-objective genetic algorithm to assess the need for a multi-\nobjective formulation. In addition, two refactoring solutions of both state-of-the art\nworks (Kessentini et al and Harman et al.\n) are empirically evaluated in order to compare them to our approach in terms of\ndesign coherence.\n\nAs shown in Table[WI] for each system, 6 refactoring solutions have to be evaluated.\nDue to the large number of refactoring operations to be evaluated (36 solutions in\ntotal, each solution consists of a large set of refactoring operations), we pick at random\na sample of 10 sequential refactorings per solution to be evaluated in our study. In\nTable we summarize how we divided subjects into groups in order to cover the\nevaluation of all refactoring solutions. In addition, as illustrated in Table we are\nusing a cross-validation for the first scenario to reduce the impact of subjects (groups\nA-F) on the evaluation. Each subject evaluates different refactoring solutions for three\ndifferent systems.\n\nSubjects (groups A-F) were aware that they are going to evaluate the design coher-\nence of refactoring operations, but do not know the particular experiment research\nquestions (algorithms used, different objectives used and their combinations). Con-\nsequently, each group of subjects who accepted to participate to the study, received\na questionnaire, a manuscript guide to help them to fill the questionnaire, and the\nsource code of the studied systems, in order to evaluate 6 solutions (10 refactorings\nper solution). The questionnaire is organized within a spreadsheet with hyperlinks to\nvisualize easily the source code of the affected code elements. Subjects are invited to\nselect for each refactoring operation one of the possibilities: “Yes” (coherent change),\n“No” (non-coherent change), or “May be” (if not sure). All the study material is avail-\nable in [Deb et al. 2002). Since the application of refactorings to fix design defects is a\nsubjective process, it is normal that not all the programmers have the same opinion.\nIn our case, we considered the majority of votes to determine if a suggested refactoring\nis correct or not.\n\nScenario 2: The aim of this scenario is to compare our refactoring results for fixing\ndesign defects suggested by our tool with manual refactorings identified by developers.\nThereafter, we asked two groups of subjects (groups G and H) to fix a set of 72 design\ndefect instances that are randomly selected from each subject system (12 defects per\nsystem) covering all the six different defect types considered. Then we compared their\nsequences of refactorings that are suggested manually with those proposed by our\napproach. The more our refactorings are similar to the manual ones, the more our tool\nis assessed to be useful and efficient in practice.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:24 A. Ouni et al.\n\nTable VIII: Survey organization.\n\nScenarios Subject Systems Algorithm / Approach | Solutions\ngroups\n. NSGA-II Solution 1\nGanttProject Genetic Algorithm Solution 4\nGroup A | Xerces MOGA Solution 2\nuP Harman et al. Solution 6\nRS Solution 3\nJFreeChart Kessentini et al. Solution 5\n. MOGA Solution 2\nGanttProject Harman et al. Solution 6\nRS Solution 3\nGroup B | Xerces Kessentini et al. Solution 5\nNSGA-II Solution 1\nJFreeChart Genetic Algorithm Solution 4\n. RS Solution 3\nGanttProject Kessentini et al. Solution 5\nGroup C | Xerces NSGA-II Solution 1\nuP Genetic Algorithm Solution 4\nMOGA Solution 2\nScenario 1 JFreeChart Harman et al. Solution 6\n‘ApacheAnt NSGA-II Solution 1\nP Genetic Algorithm Solution 4\nMOGA Solution 2\nGroup D | JHotDraw Harman et al. Solution 6\nRhino RS Solution 3\nKessentini et al. Solution 5\nMOGA Solution 2\nApacheAnt Harman et al. Solution 6\nRS, Solution 3\nGroup E | JHotDraw Kessentini et al. Solution 5\nRhino NSGA-II Solution 1\n. Genetic Algorithm Solution 4\nRS Solution 3\nApacheAnt Kessentini et al. Solution 5\nNSGA-II Solution 1\nGroup F | JHotDraw Genetic Algorithm Solution 5\nRhino MOGA, Solution 2\nt Harman et al. Solution 6\nManual correction of\n. Group G | All systems design defects NA.\nScenario 2 Manual correction of\nGroup H | All systems design defects NA.\n\n5.2.4. Algorithms configuration. In our experiments, we use and compare different mono\nand multi-objective algorithms. For each algorithm, to generate an initial population,\nwe start by defining the maximum vector length (maximum number of operations per\nsolution). The vector length is proportional to the number of refactorings that are con-\nsidered, the size of the program to be refactored, and the number of detected design\ndefects. A higher number of operations in a solution does not necessarily mean that the\nresults will be better. Ideally, a small number of operations should be sufficient to pro-\nvide a good trade-off between the fitness functions. This parameter can be specified by\nthe user or derived randomly from the sizes of the program and the employed refactor-\ning list. During the creation, the solutions have random sizes inside the allowed range.\nFor all algorithms NSGA-II, MOGA, Random search (RS), and genetic algorithm (GA),\nwe fixed the maximum vector length to 700 refactorings, and the population size to 200\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:25\n\nindividuals (refactoring solutions), and the maximum number of iterations to 6,000 it-\nerations. We also designed our NSGA-II adaptation to be flexible in a way that we can\nconfigure the number of objectives and which objectives to consider in the execution.\n\nWe consider a list of 11 possible refactorings to restructure the design of the original\nprogram by moving code elements (methods, attributes) from classes in the same or\ndifferent packages or inheritance hierarchies or splitting/merging classes/interfaces.\nAlthough we believe that our list of refactorings is sufficient at least to fix these specific\ntypes of code smells, our refactoring tool is developed in a flexible way so that new\nrefactorings and code smell types can be considered in the future. Moreover, our list\nof possible refactoring is significantly larger than those of existing design defect fixing\ntechniques.\n\nAnother element that should be considered when comparing the results of the four\nalgorithms is that NSGA-II does not produce a single solution like GA, but a set of\noptimal solutions (non-dominated solutions). The maintainer can choose a solution\nfrom them depending on their preferences in terms of compromise. However, at least\nfor our evaluation, we need to select only one solution. Thereafter, and in order to fully\nautomate our approach, we proposed to extract and suggest only one best solution from\nthe returned set of solutions. In our case, the ideal solution has the best value of quality\n(equal to 1), of design coherence (equal to 1), and of refactoring reuse (equal to 1), and\ncode changes (normalized value equal to 1). Hence, we select the nearest solution to\n\nthe ideal one in terms of Euclidian distance, as described in |Ouni et al. 2012b].\n\n5.2.5. Inferential Statistical Test Methods Used. Our approach is stochastic by nature, i.e.,\ntwo different executions of the same algorithm with the same parameters on the same\nsystems generally leads to different sets of suggested refactorings. For this reason,\nour experimental study is performed based on 31 independent simulation runs for\neach problem instance, and the obtained results are statistically analyzed by using\nthe Wilcoxon rank sum test with a 95% confidence level (a = 0.05). The Wilcoxon\nsigned-rank test is a non-parametric statistical hypothesis test used when comparing\ntwo related samples to verify whether their population mean-ranks differ or not. In\nthis way, we could decide whether the difference in performance between our approach\nand the other detection algorithms is statistically significant or just a random result.\n\nThe Wilcoxon rank sum test allows verifying whether the results are statistically\ndifferent or not. However, it does not give any idea about the difference magnitude.\nWe, thus, investigate the effect size using the Cliff’s Delta statistic [Cliff 1993]. The\neffect size is considered: (1) negligible if | d |< 0.147, (2) small if 0.147 <] d |< 0.33, (3)\nmedium if 0.33 <| d |< 0.474, or (4) large if | d |> 0.474.\n\n5.3. Empirical study results\nThis section reports the results of our empirical study, which are further discussed\nin the next sections. We first start by answering our research questions. We use two\ndifferent validations: manual and automatic validations to evaluate the efficiency of\nthe proposed refactorings.\n\nResults for RQ1.1: As described in Table after applying the proposed refac-\ntoring operations by our approach (NSGA-I]), we found that, on average, 84% of the\ndetected defects were fixed (DCR) for all the six studied systems. This high score is con-\nsidered significant in terms of improving the quality of the refactored systems by fixing\nthe majority of defects of various types (blob, spaghetti code, functional decomposition,\ndata class, shotgun surgery, and future envy\net al. 2011)). For the different systems, the total number of refactorings generated by\nour approach was between 91 and 119 as described in the refactoring precision (RP)\ncolumn of Table Furthermore, we assessed the required time to implement the\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:26 A. Ouni et al.\n\nsuggested refactorings on three systems. The average time required by 6 of the partic-\nipants in our experiments to implement all the suggested refactorings was 11.5 hours\nper developer for each system, including the time required to understand and inspect\nthe code before and after applying the refactorings. We believe that this required time\nis quite acceptable comparing to the time that the developer may spend to identify\nthese refactoring opportunities manually from hundreds or thousands of classes and\nmillions of lines of code. In addition, while the effect of refactoring is clearly trans-\nlated by fixing the vast majority of design defects (84%) and significantly improving\nquality factors (see Section|6.1}, other effects on the systems quality (maintainability,\nextendibility, etc.) cannot be assessed immediately.\n\nResults for RQ1.2: To answer RQ1.2, we evaluated the correctness/meaningful-\nness of the suggested refactorings from the developers’ point of view. We reported the\nresults of our empirical evaluation in Table (RP column) related to Scenario 1. On\naverage, for all of our six studied systems, 80% of proposed refactoring operations are\nconsidered by potential users to be semantically meaningful and do not generate de-\nsign incoherence. We also automatically evaluated our approach. Thus, we compared\nthe proposed refactorings with the expected ones. The expected refactorings are those\napplied by the software development team for the next software release as described\nin Table We used Ref-Finder to identify refactoring operations\nthat are applied between the program version under analysis and the next version. Ta-\nble [EX] (RP-automatic column) summarizes our results. We found that a considerable\nnumber of proposed refactorings (an average of 36% for all studied systems in terms\nof recall) were already applied to the next version by a software development team.\nOf course, this precision score is low because that not all refactorings applied to next\nversion are related to quality improvement, but also to add new functionalities, in-\ncrease security, fix bugs, etc. Moreover, the obtained results provide evidence that our\napproach is relatively stable through different executions as the standard deviation is\nstill less than 3.23 in terms of DCR, 3.09 in terms of RP-automatic and 123.3 in terms\nof code change:\n\nTo conclude, we found that our approach produces good refactoring suggestions in\nterms of defect-correction ratio, design coherence from the point of view of (1) potential\nusers of our refactoring tool and (2) expected refactorings applied to the next program\nversion.\n\nResults for RQ1.3 and RQ1.4: To answer these two research questions, we need\nto compare different objective combinations (two, three, or four objectives) to ensure\nthe efficiency and the impact of using each of the objectives we defined. We executed\nthe NSGA-II algorithm with different combinations of objectives: maximize quality\n(Q), minimize design incoherence (S), minimize code changes (CC), and maximize the\nreuse of recorded refactorings (RR) as presented in Table|X/and Figure[7|\n\nTo answer RQ1.3, we present in Figure[7a]and Table [X] the code change scores ob-\ntained when the CC objective is considered (Q+S+RC+CC). We found that our approach\nsucceeded in suggesting refactoring solutions that do not require high code changes\n(an average of only 2,937) with a relatively stable standard deviation of while having\nmore than 3,888 as a code change score when the CC objective is not considered in the\nother combinations. At the same time, we found that the DCR score (Figure (Zc) is not\nsignificantly affected with and without considering the CC objective.\n\nTo answer RQ1.4, we present the obtained results in Figure[7b] The best RP scores\nare obtained when the recorded code changes (RC) are considered (Q+S+RC), while\n\n1°Note that only for the RP metric, we did not report the standard deviation as we directly conducted the\nqualitative evaluation with subjects on the suggested refactoring solution having the median DCR score\nfrom 31 independent runs.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:27\n\nTable IX: Empirical study results on 31 runs (Median & STDev). The results was sta-\ntistically significant on 31 independent runs using the Wilcoxon rank sum test with a\n95% confidence level (p — value < 0.05) in terms of defect correction ratio (DCR), code\nchanges score, refactoring precision (RP), and RP-automatic.\n\nSystems Approach DCR Code changes RP RP-automatic\ny PP Median | STDev | Median | STDev Median | STDev\n83% : 81% 26%\nNSGA-II (76191) 1.58 3,843 | 123.3 (74191) | (10139) 2.09\nHarman et al. 07 NA NA 2,669 78.4 41% el 30) 2.09\nXerces K tini et al. 711 89% 2.24 4,998 | 102.8 37 % 13% 2.97\nessentini et al. (81/91) 5 , . ‘0 (5139) .\n86% 82 % 35%\nNSGA-IL (62172) | 244 | 2016 | 898 | (gz) 106)| iis | 2-98\nHarman et al. 07 NA NA 3,269 86.2 36 % 9% 1.51\n(0131)\nJFreeChart 90% 13%\nKessentini et al. 711 (65172) 2.86 3,389 85.82 37 % (4131) 2.65\n85% : 80 % 46%\nNSGA-II (42149) 3.23 2,826 | 73.82 (63178) | (21146) 2.27\nHarman et al. 07 NA NA 4,790 | 83.72 23 % 0% 1.01\n. (0146)\nGanttProject 95% 5%\nKessentini et al. 711 (47149) 2.96 4,697 86.7 27 % (7146) 2.45\n78% 78 % 31% ,\nNSGA-IL (s71112)| 118 | 4690 | 1129 | gsi i199] (24i78) | 2-22\nHarman et al. 07 NA NA 6,987 | 77.63 40 % 04% 0.96\n(3178)\nApacheAnt — ; 30% 0%\nKessentini et al. 711 (901112) 1.89 6,797 83.1 30 % (0178) 17\n84% ‘ ‘ 80 % 44%\nNSGA-II (21125) 3.21 2,231 | 97.65 (79198) | (1814) 3.09\nHarman et al. 07 NA NA 3,654 | 77.63 37 % 10% 2.69\n(4141)\nJHotDraw 34% 7%\nKessentini et al. 711 (21125) 5.32 3,875 90.83 43 % (3141) 2.73\n85% 80 % 33%\nNSGA-IL 9i69) | 269 | L914 | 89.77 | gj 112)| 5146) | 2-92\nHarman et al. 07 NA NA 2,698 | 77.63 37 % ose) 1.003\nRhino 87% 0%\nKessentini et al. 711 (60169) 3,365 717.61 32 % (4146) 2.97\n‘Average NSGA-II 84% 2,937 80 % 36%\n(all s ed) Harman et al. 07 NA 4,011 36 % 4%\ny' Kessentini et al. 11 89% 4,520 34 % 9%\n\nhaving good correction ration DCR (Figure[7c). In addition, we need more quantitative\nevaluation to investigate the effect of the use of recorded refactorings, on the design\ncoherence (RP). To this end, we compare the RP score with and without using recorded\nrefactorings. In most of the systems when recorded refactoring is combined with se-\nmantics, the RP value is improved. For example, for Apache Ant RP is 83% when only\nquality and semantics are considered, however, when recorded refactoring reuse is in-\ncluded the RP is improved to 87% (Figure|7b).\n\nWe notice also that when code changes reduction is included with quality, semantics\nand recorded changes, the RP and DCR scores are not significantly affected. Moreover,\nwe notice in Figure|7c|that there is no significant variation in terms of DCR with all\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n", '39:28 A. Ouni et al.\n\nTable X: Median refactoring results and standard deviation (STDev) of different objec-\ntive combinations with NSGA-II (average of all the systems) on 31 runs in terms of\ndefect correction ratio (DCR), refactoring precision (RP), code changes reduction and\nrecorded refactorings (RR). The results was statistically significant on 31 independent\nruns using the Wilcoxon rank sum test with a 95% confidence level (p — value < 0.05).\n\nObjectives DCR Code changes RR RP (empirical\ncombinations Median STDev Median STDev Median STDev _ evaluation)\nQ+CC 75% 1.84 2591 87.12 NA. NA. 45%\nQ+S 81% 1.93 4355 94.6 NA. NA. 82%\nQ+RC 85% 2.16 3989 89.76 41% 2.87 54%\nQ+S+RC 81% 1.56 3888 106.24 35% 3.21 84%\nQ+S+RC+CC 84% 2.39 2917 97.91 36% 3.82 80%\n\ndifferent objectives combinations. When four objectives are combined the DCR value\ninduces a slight degradation with an average of 82% in all systems which is even\nconsidered as promising results. Thus, the slight loss in the defect-correction ratio is\nlargely compensated by the significant improvement of the design coherence and code\nchanges reduction. Moreover, we found that the optimal refactoring solutions found by\nour approach are obtained with a considerable percentage of reused refactoring history\n(RR) (more than 35% as shown in Table LX). Thus, the obtained results support the\nclaim that recorded refactorings applied in the past are useful to generate coherent and\nmeaningful refactoring solutions and can effectively drive the refactoring suggestion\ntask.\n\nIn conclusion, we found that the best compromise is obtained between the four objec-\ntives using NSGA-II comparing to the use of only two or three objectives. By default,\nthe tool considers the four objectives to find refactoring solutions. Thus, a software en-\ngineer can consider the multi-objective algorithm as a black-box and he do not need\nto configure anything related to the objectives to consider. The four objectives should\nbe considered and there is no need to select the objectives by the user based on our\nexperimentation results.\n\nResults for RQ2: To answer RQ2, we evaluate the efficiency of our approach com-\nparing to two other contributions of Harman et al. [Harman and Tratt 2007] and\nKessentini et al. (Kessentini et al. 2011]. In\nal. proposed a multi-objective approach that uses two quality metrics to improve CBO\n(coupling between objects) and SDMPC (standard deviation of methods per class) after\napplying the refactorings sequence. In (Kessentini ot al. 2013}, a single-objective ge-\nnetic algorithm is used to correct defects by finding the best refactoring sequence that\nreduces the number of defects. The comparison is performed in terms of: (1) defect\ncorrection ratio (DCR) that is calculated using defect detection rules, (2) refactoring\nprecision (RP) that represents the results of the subject judgments (Scenario 1), and\n(3) code changes needed to apply the suggested refactorings. We adapted our technique\nfor calculating code changes scores for both approaches Harman et al. and Kessentini\net al. Table 8 summarizes our findings and reports the median values and standard\ndeviation (STDev) of each of our evaluation metrics obtained for 31 simulation runs of\nall projects.\n\nAs described in Table after applying the proposed refactoring operations, we\nfound that more than 84% of detected defects were fixed (DCR) as an average for all the\nsix studied systems. This score is comparable to the correction score of Kessentini et\nal. (89%), an approach that does not consider design coherence preservation, nor code\nchange reduction nor recorded refactorings reuse (DCR is not considered in Harman\net al. since their aim is to improve only some quality metrics).\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:29\n\n—= Xerces-J\n8000 + °. = JFreeChart\n\noe  GanttProject\n= AntApache\n7000 + ° = THotbraw\nRhino\n\n6000 +\n\n4900 +\n\n3000 +\n\n2000 +\n\nT\narco as arRe arstRe arstRo+e\n\n(a) Code Change score (CC).\n\n= KercesJ\n<= dFreeChart\n= GanttProject\n“= AntApache\n= JHotbraw\n= Rhino\n\nT\narseRcee\n\n“= Xeroes-J\neo J = JFreeChart\n= GantiProject\n<= AntApache\n= WHotbraw\nso | = Rhino\nT T T T T\n\navce as arc arseRe arseRcrc\n\n(c) Defect Correction Ratio (DCR).\n\nFig. 7: Refactoring results of different objectives combination with NSGA-II in terms\nof (a) code changes reduction (CC), (b) design preservation (RP), (c) defects correction\nratio (DCR).\n\nRegarding the semantic coherence, for all of our six studied systems, an average of\n80% of proposed refactoring operations are considered as semantically feasible and do\nnot generate design incoherence. This score is significantly higher than the scores of\nthe two other approaches having respectively only 36% and 34% as RP scores. Thus,\nour approach performs clearly better for RP and code changes score with the cost of a\nslight degradation in DCR compared to Kessentini et al. This slight loss in the DCR is\nlargely compensated by the significant improvement in terms of design coherence and\ncode change reduction.\n\nWe compared the three approaches in terms of automatic RE;ecai, aS depicted in\nFigure|8| We found that a considerable number of proposed refactorings, an average of\n36% for all studied systems in terms of recall, are already applied to the next version\nby the software development team. By comparison, the results for Harman et al. and\nKessentini et al. are only 4% and 9% respectively, as reported in figure [8b] Moreover,\nthis score shows that our approach is useful in practice unlike both other approaches.\nIn fact, the RE}ccai of Harman et al. is not significant, since only the move method\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:30 A. Ouni et al.\n\nrefactoring is considered when searching for refactoring solutions to improve coupling\nand standard deviation of methods per class. Moreover, expected refactorings are not\nrelated only to quality improvement, but also for adding new functionalities, and other\nmaintenance tasks. This is not considered in our approach when we search for the\noptimal refactoring solution that satisfies our four objectives. However, we manually\ninspected expected refactorings and we found that they are mainly related to adding\nnew functionality (related to adding new packages, classes or methods).\n\nIn conclusion, our approach produces good refactoring suggestions in terms of defect-\ncorrection ratio, design coherence, and code change reduction from the point of view of\n(1) potential users of our refactoring tool, and (2) expected refactorings applied to the\nnext program version.\n\n100 — -+- NSGA-II 100\n= Rarman et al.\n= Kessentini etal.\n80 4 80\nod\n§ eo 4 60 4\noO\na) °\n°\n°\n20 | °— n 20 4 —_\na a——— 4—__ =\n: a t\net on : => =\nT T T T\n\n°\nT T T T T\nXerces-J — AntApache JFreeChart GanttProject Rhino JHotDraw Harman et al. Kessentini et al. NSGA-II\n\n(a) RE_recall results for each system. (b) Boxplots for RE_recall.\n\nFig. 8: Automatic refactoring score (RE_recall) comparison between our approach\n(NSGA-II), Harman et al. and Kessentini et al.\n\nFurthermore, to justify the use of NSGA-II, we compared the performance of our\nproposal to two other multi-objective algorithms: MOGA, and a random search and a\nmono-objective algorithm (genetic algorithm). In a random search, the change opera-\ntors (crossover and mutations) are not used, and populations are generated randomly\nand evaluated using the four objective functions. In our mono-objective adaptation, we\nconsidered a single fitness function, which is the normalized average score of the four\nobjectives using a genetic algorithm. Moreover, since in our NSGA-II adaptation, we\nselect a single solution without giving more importance to some objectives, we give\nequal weights for each fitness function value. As shown in Figure |9} NSGA-II out-\nperforms significantly MOGA, random-search, and the mono-objective algorithm in\nterms of defects-correction ratio (DCR), semantic coherence preservation (RP), and\ncode change reduction. For instance, in JFreeChart, NSGA-II performs much better\nthan MOGA, random search, and genetic algorithm in terms of DCR and RP scores\n(respectively Figurealand Figure(9b). In addition, NSGA-II reduces significantly code\nchanges for all studied systems. For example, for Rhino, the number of code changes\nwas reduced to almost the half comparing to random search as shown in Figure\n\nFurthermore, an interesting finding is that the random search (RS) works as we\nthe single-objective GA. Indeed, we used RS with a multi-objective version by switch-\ning off individual selection based on fitness value, in our original framework. The per-\nformance of RS is clearly less than the other multi-objective algorithms being com-\npared (NSGA-II and MOGA). Some of the results of RS can be considered acceptable,\nthis can be explained by the limited number of refactoring types considered in our ex-\nperiments (limited search space). For GA, after 2,000 generations, we noticed that the\nsearch produced entire populations with high DCR and CC values but lower S and RR\nvalues that has resulted in a relative increase in the combined fitness function which\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:31\n\nled to comparable results to the multi-objective RS. The comparable results between\nRS and GA suggest that our formulation to the refactoring recommendation problem\nas a multi-objective formulation is adequate.\n\nAnother interesting observation from the results in figure [B]is that MOGA has less\ncode changes and higher RP value than NSGA-II in Apache Ant. By looking at the\nproduced results, we noticed that none of the blob design defects was fixed in Apache\nAnt using MOGA. Indeed, the blob design defect is known as one of the most difficult\ndesign defects to fix, and typically requires a large number of refactoring operations\nand code changes (several extract class, move method and move field refactorings).\nThis is also explained by the higher RP score, as it also complicated for developers to\napprove such refactorings.\n\n100 5 @ GA @ NSGA-II\n@ MOGA GO RS\n\n80 4\n60 +\n40\n\n20 4\not\n\nAntApache GanttProject. JFreeChart JHotDraw Rhino Xerces-J\n\n(a) Defect Correction Ratio (DCR).\n\nDefect Correction Ratio (DCR)\n\n100 4 @ GA @ NSGA-II\n@ MOGA O RS\n\n& 8074\n§\n2 604\n&\n2\n£ 404\ng\n2 24\noJ\nAntApache GanttProject JFreeChart JHotDraw Rhino Xerces-J\n(b) Refactoring Precision (RP).\n8000 m GA @ NSGA-II\n@ MOGA © RS\n5000\nSs\n8\n‘2 4000\n8\n8, 3000\n© 2000\n3\n3\n8\n- TT\n0\n\nAntApache GanttProject JFreeChart JHotDraw Rhino Xerces-J\n\n(c) Code Change Score (CC).\n\nFig. 9: Refactoring results of different algorithms NSGA-II, MOGA, GA and RS in\nterms of (a) defects correction ratio, (b) refactoring precision and (c) code changes re-\nduction.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:32 A. Ouni et al.\n\nFor all experiments, we obtained a large difference between NSGA-II results and the\nmono-objective approaches (Harman et al., Kessentini et al., GA and random search)\nusing all the evaluation metrics. However, when comparing NSGA-II against MOGA,\nwe have found the following results: a) On small and medium-scale software systems\n(JFreeChart, Rhino and GanttProject) NSGA-II is better than MOGA on most systems\nwith a small and medium effect size; b) On large-scale software systems (Xerces-J,\nApache Ant and JDI-Ford), NSGA-II is better than MOGA on most systems with a\nhigh effect size.\n\nResults for RQ3: JDeodorant uses only structural information to detect and fix\ndesign defects, but does not handle all the six design defect types that we considered\nin our experiments. Thus, to make the comparison fair, we performed our comparison\nusing only two design defects that can be fixed by both tools: blob and feature envy.\nFigure summarizes our findings for the blob (figure and feature envy (fig-\nure t is clear that our proposal outperforms JDeodorant, on average, on all the\nsystems in terms of the number of fixed defects with a minimum number of changes\nand semantically coherent refactorings. The average number of fixed code smells is\ncomparable between both tools. However, our approach is clearly better in terms of\nsemantically coherent refactorings. This can be explained by the fact that JDeodor-\nant uses only structural metrics to evaluate the impact of suggested refactorings on\nthe detected code smells. In addition, our proposal supports more types of refactorings\nthan JDeodorant and this is also explains our outperformance. However, one of the\nadvantages of JDeodorant is that the suggested refactorings are easier to apply than\nthose proposed by our technique as it provides an Eclipse plugin to suggest and then\nautomatically apply a total of 4 types of refactorings, while the current version of our\ntool requires to apply refactorings by the developers using the Eclipse IDE with more\ncomplex types of refactorings.\n\nResults for RQ4: To evaluate the relevance of our suggested refactorings with our\nsubjects, we compared the refactoring strategies proposed by our technique and those\nproposed manually by groups G and H (6 subjects) to fix several defects on the six sys-\ntems. Figure|11}shows that most of the suggested refactorings by NSGA-II are similar\nto those applied by developers with an average of more than 73%. Some defects can be\nfixed by different refactoring strategies, and also the same solution can be expressed\nin different ways (complex and atomic refactorings). Thus we consider that the aver-\nage precision of more than 73% confirms the efficiency of our tool for real developers\nto automate the refactoring recommendation process. We discuss, in the next section,\nin more detail the relevance of our automated refactoring approach for software engi-\nneers.\n\n6. DISCUSSIONS\n\nThe obtained results from Section [5.3]suggest that our approach performs better than\ntwo existing approaches. We also compared different objective combinations and found\nthat the best compromise is obtained between the four objectives using NSGA-II when\ncompared to the use of only two or three objectives. Therefore, our four objectives are\nefficient for providing "good" refactoring suggestions. Moreover, we found that the re-\nsults achieved by NSGA-II outperforms the ones achieved by both multi-objective al-\ngorithms, MOGA and random search, and the mono-objective algorithm, GA.\n\nWe now provide more quantitative and qualitative analyses of our results and dis-\ncuss some observations drawn from our empirical evaluation of our refactoring ap-\nproach. We aim at answering the following research questions:\n\n— RQ5: What is the effect of suggested refactorings on the overall quality of systems?\n— RQ6: What is the effect of multiple executions on the refactoring results?\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering\n\n100 5\n\nao 4\n\neo 4\n\n44\n\na4\n\nDefect Correction Ratio (OCR)\n\nod\n\nRefactoring precision (RP)\n\n6000 5\n\n5000 4\n\n4000 |\n\n000 4\n\n2000\n\nCode change score (CC)\n\n1000 4\n\nB NS\n\n1 Deodorant\nGAS\n\nB NSGArI\n\nAmApache GantiProject JFreeChart JHotDraw Rhino ‘Xerces-J\n\nDeodorant\nGach\n\nAntApache GanttProject JFreeChart JHotDraw Rhino ‘Xerces~J\n\n\'@ Deodorant\nB NSGATI\n\n‘AntApache GanttProject JFreeChart JHotDraw Rhino ——‘Xerces-J\n\n(a) Blob.\n\n100 5\n\nao 4\n\n60\n\n40\n\nDefect Correction Ratio (DCR)\n\n20\n\noJ\n\n100 5\n\nao 4\n\n«4\n\na4\n\nRefactoring precision (RP)\n\n20 4\n\nod\n\n6000\n\n4000\n\n3000\n\nCode change score (CC)\n\n+1000\n\n39:33\n\nm Deodorant\nNSAI\n\n‘AntApache GanitProject JFreeChart JHotDraw Rhino ——-Xerces-~J\n\n sDeodorant\nISGAI\n\non\n\nAmApache GanttProject JFreeChart JHotDraw Rhino Xerces-J\n\nDeodorant\n@ NSGA-I\n\nAntApache GanttProject JFreeChart JHolDraw Rhino —-Xerves-J\n\n(b) Feature envy.\n\nFig. 10: Comparison results of our approach (NSGA-II) with JDeodorant in terms of\ndefects correction ratio (DCR), design coherence (RP) and code changes score (CC) for\neach system. For NSGA-II, we report the average DCR and CC scores and standard\n\ndeviation obtained through 31 independent runs. Note tha\n\nfor RP score, we did not\n\nreport the standard deviation as we directly conducted the qualitative evaluation with\nsubjects on the suggested refactoring solution that have the median DCR score.\n\n= NSGA-II\n100 4 —— Harman et al\n= Kessentini et al.\n= 80 4 =e\nsc o—_—_—_——_e .\n5 so\nB\n8 40-\na\na4.\nA =— _ a a\n0 6g "=e et\nT T T T T T\nXerces-J JFreeChart GanttProject. —AntApache Rhino JHotDraw\n\nFig. 11: Comparison of our refactoring results with manual refactorings in terms of\nPrecision.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', "39:34 A. Ouni et al.\n\n— RQ7: What is the distribution of the suggested refactoring types?\n\nIn the following subsections we answer each of these research questions.\n\n6.1. The refactoring impact (RQ5)\n\nAlthough our primary goal in this work is to demonstrate that design defects can be\nautomatically refactored, it is also important to assess the refactoring impact on design\nquality. The expected benefit from refactoring is to enhance the overall software design\nquality as well as fixing design defects [Fowler 1999]. We use the QMOOD (Quality\nModel for Object-Oriented Design) model [Bansiya and Davis 2002] to estimate the\neffect of the suggested refactoring solutions on quality attributes. We choose QMOOD,\nmainly because 1) it is widely used in the literature |Shatnawi and Li 2011} O'Keeffe\nand Cinnéide 2008} Zibran and Roy 2011} to assess the effect of refactoring, and 2\n\nit has the advantage of defining six high level design quality attributes (reusability,\nflexibility, understandability, functionality, extendibility and effectiveness) that can be\ncalculated using 11 lower level design metrics (Bansiya and Davis 2003}. In our study\nwe consider the following quality attributes:\n\n—Reusability: The degree to which a software module or other work product can be\nused in more than one computer program or software system.\n\n— Flexibility: The ease with which a system or component can be modified for use in\napplications or environments other than those for which it was specifically designed.\n\n—Understandability: The properties of designs that enable it to be easily learned and\ncomprehended. This directly relates to the complexity of design structure.\n\n— Effectiveness: The degree to which a design is able to achieve the desired function-\nality and behavior using OO design concepts and techniques.\n\nWe did not assess the issue of functionality because we assume that, by definition,\nrefactoring does not change the behavior/functionality of systems; instead, it changes\nthe internal structure. We have also excluded the extendibility factor because it is, to\nsome extent, a subjective quality factor and using a model of merely static measures\n\nto evaluate extendibility is inadequate. Tables and summarize the QMOOD\nformulation of these quality attributes [Bansiya and Davis 2002].\nThe improvement in quality can be assessed by comparing the quality before and\n\nafter refactoring independently to the number of fixed design defects. Hence, the total\ngain in quality G for each of the considered QMOOD quality attributes qi before and\nafter refactoring can be easily estimated as:\n\nGa =%-—% (17)\nwhere qj and q; denotes the value of the quality attribute i respectively after and\nbefore refactoring.\n\nIn Figure we show the obtained gain values (in terms of absolute value) that\nwe calculated for each QMOOD quality attribute before and after refactoring for each\nstudied system. We found that the systems quality increase across the four QMOOD\nquality factors much better than existing approaches. Understandability is the quality\nfactor that has the highest gain value; whereas the Effectiveness quality factor has\nthe lowest one. This mainly due to many reasons 1) the majority of fixed design defects\n(blob, spaghetti code) are known to increase the coupling (DCC) within classes, which\nheavily affect the quality index calculation of the Effectiveness factor; 2) the vast ma-\njority of suggested refactoring types were move method, move field, and extract class\n(Figure [12) that are known to have a high impact on coupling (DCC), cohesion (CAM)\nand the design size in classes (DSC) that serves to calculate the understandability\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n", 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:35\n\nTable XI: QMOOD metrics for design properties.\n\nDesign Property Metric Description\n\nDesign size DSC Design size in classes\nComplexity NOM Number of methods\n\nCoupling DCC Direct class coupling\nPolymorphism NOP Number of polymorphic methods\nHierarchies NOH Number of hierarchies\n\nCohesion CAM Cohesion among methods in class\nAbstraction ANA Average number of ancestors\nEncapsulation DAM Data access metric\n\nComposition MOA Measure of aggregation\nInheritance MFA Measure of functional abstraction\nMessaging CIS Class interface size\n\nTable XII: QMOOD quality factors.\n\nQuality attribute Quality Index Calculation\nReusability = -0.25 * DCC + 0.25 * CAM + 0.5 * CIS + 0.5 * DSC\n\nFlexibility = 0.25 * DAM - 0.25 * DCC + 0.5 * MOA +0.5 * NOP\n4, = 70.33 * ANA + 0.33 * DAM - 0.33 * DCC + 0.33 * = CAM -0.33 * NOP + 0.33 * NOM\nUnderstandability .\n- 0.33 * DSC\nEffectiveness = 0.2 *ANA + 0.2 *DAM + 0.2*MOA + 0.2 * MFA + 0.2 *NOP\n\nquality factor. Furthermore, we noticed that JHotDraw produced the lowest quality\nincrease for the four quality factors. This is justified by the fact that JHotDraw is\nknown to be of good design and implementation practices and\nit contains a small number of design defects compared to the five other studied sys-\ntems.\n\nTo sum up, we can conclude that our approach succeeded in improving the code\nquality not only by fixing the majority of detected design defects but also by improving\nthe user understandability, the reusability, the flexibility, as well as the effectiveness\nof the refactored program.\n\nFinally, it is worth to notice that since the application of refactorings to fix design\ndefects is a subjective process, it is normal that not all the programmers have the\nsame opinion. Thus it is important to study the level of agreement between subjects.\nTo address this issue, we evaluated the level of agreement using Cohen’s Kappa coef-\nficient « (Cohen ef al. 1960), which measures to what extent the subjects agree when\nvoting for a recommended refactoring operation. The Kappa coefficient assessments\nwas 0.78, which is characterized as “substantial agreement" by Landis and Koch [|\nidis and Koch 1977]. This obtained score makes us more confident that our suggested\nrefactorings are meaningful from software engineer’s perspective.\n\n6.2. The effect of multiple executions (RQ6)\n\nIt is important to contrast the results of multiple executions with the execution time\nto evaluate the performance and the stability of our approach. The execution time for\nfinding the optimal refactoring solution with a number of iterations (stopping criteria)\nfixed to 6,000 was less than forty-eight minutes as shown in Figure Moreover, we\nevaluate the impact of the number of suggested refactorings on the DCR, RP, RR, and\ncode change scores in five different executions. Drawn for JFreeChart, the results of\nfigure[13]show that the number of suggested refactorings do not affect the refactoring\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:36 A. Ouni et al.\n\n04\n\no4\n: Kessentini eal @ Kessentini el al.\nm Harman eta Harman et al\n03 5 NSGA“I\nos 4\nEy =\ng 8\n< 02 S a4\n$ 8\n2 os 204\n8 | mall 6 aul\n00 oo 4 a)\n~o1 -o1 J\nEffeciveness  Flexblly __Reusablly Understandabilty Effeciveness Flexibility Reusabilly  Understandabilty\n(a) Xerces-J. (b) JFreeChart.\no4 5 o4 5\n@ Kessentini el al. @ Kessentini el al.\n@ Harman et al. @ Harman et al\nos | 5 NSGA“I os J 5 NSGA-II\n© ow © ow\n8 6\n2 os | 2 014\n6 6\noo 4 | _all]) oo | i ia | =\n-o1 J -o1 J\nEffectiveness Flexbilily _-Reusablly Understandablty Effectiveness Flexibilly —-ReusabiltyUnderstandabilty\n(c) GanttProject. (d) AntApache.\n04 5\nos\n= Kessentin| ela & Kessentin lal\nB NOGA @ Harman et al.\n03 4 03 @ NSGA-II\nFoy =\ng 8\n© OFF S 02\n§ 5\n2 o41- 3\n2 go\noe i ° =H\n00 4 oo | y= —== r\nor\n\n-04\n\nEffectiveness Flexibility Reusability_ Understandabilty Effectiveness Flexibility -Reusabilly  Understandabilty\n\n(e) Rhino. (f) JHotDraw.\n\nFig. 12: The impact of best refactoring solutions on QMOOD quality attributes.\n\nresults. Thus, a higher number of operations in a solution does not necessarily mean\nthat the results will be better. Consequently, we could conclude that our approach is\nscalable from the performance standpoint, especially that our technique is executed,\nin general, up front (at night) to find suitable refactorings. In addition, the results’\naccuracy is not affected by the number of suggested refactorings.\n\nFurthermore, it is also important to assess the impact of the number of design de-\nfects on the size of the refactoring solution (number of refactorings). Figure[14|reports\nthe correlation between the number of design defects and the number of refactorings\nfor each system. Our findings confirm that the number of design defects does not affect\nthe number of refactorings due to the low value of correlation (0.04).\n\nIn addition, figure |15|reports the execution time for each of the search algorithms\nNSGA-II, Harman et al., Kessentini et al.. MOGA, GA and RS. As shown in the figure,\nthe execution time of our NSGA-II approach was very similar to MOGA with an aver-\nage of less than 48 minutes per system. However, the execution time of random search\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:37\n\nnumberof umber of refactorings umber of refacorings\n\n(a) Xerces-J. (b) JFreeChart. (c) GanttProject.\n\number of efactorings numberof\n\nof refactorngs\n\n(d) AntApache. (e) Rhino. (f) JHotDraw.\n\nFig. 13: Results of multiple executions on different project in terms of defect correction\nratio (DCR), code changes (CC), reused refactorings (RR), and execution time (Time).\n\n250 4 JFreeChart\ne\n200 4\no Xerces-J\nD e\n2\n5 150 4\ng\n© 100 precrew GantiBroject Rhino\n6\n5 AntApache\na e\n— 50-4\n5\n2\no-4\nT T T T T T T\n20 30 40 50 60 70 80\n\nnumber of design defects\n\nFig. 14: Impact of the number of design defects on the size of the refactoring solution\n(number of refactorings).\n\nwas half of time spent by NSGA-II and MOGA, but the quality of the random search\nsolutions are much lower. The performance of NSGA-II is slightly better than MOGA\nbased on the different evaluation metrics. However, the adaptation of an NSGA-II al-\ngorithm to our refactoring problem is more complex than MOGA_It is expected that the\nexecution time of the remaining mono-objective approach is almost half the NSGA-II\none due to the following reasons: (1) they just considered one objective function, (2) the\ntime consuming for semantics and history functions of our approach are not consid-\nered by existing mono-objective approaches which require additional time processing,\nfiltering and comparing the identifiers within classes, and (3) existing mono-objective\napproaches are limited to few types of refactorings. Since our refactoring problem is\nnot a real time one, the execution time of NSGA-II is considered acceptable by all the\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:38 A. Ouni et al.\n\nprogrammers of our experiments. In fact, they mentioned that it is not required to use\nthe tool daily and they can execute it at the end of the day and check the results the\nnext day.\n\n@GA G MOGA\n60 + @ Harman etal. O NSGA-II\n@ Kessentini et al. # RS\n\n40 4\n\n30 4\n\nExecution Time (minutes)\n\nApache Ant GanttProject JFreeChart JHotDraw Rhino Xerces-J\n\nFig. 15: Comparison of the execution time for each of the search techniques, NSGA-II,\nHarman et al., Kessentini et al.. MOGA, GA and RS.\n\n6.3. The distribution of suggested refactoring types (RQ7)\n\nAnother important consideration is the refactoring operations’ distribution. We con-\ntrast that the most suggested refactorings are move method, move field, and extract\nclass for the majority of studied systems except JHotDraw. For instance, in Xerces-J,\nwe had different distribution of different refactoring types as illustrated in Figure\nWe notice that the most suggested refactorings are related to moving code elements\n(fields, methods) and extract/inline class. This is mainly due to the type of defects\ndetected in Xerces-J (most of the defects are related to the blob defect) that need par-\nticular refactorings to move elements from blob class to other classes in order to reduce\nthe number of functionalities from them. On the other hand, we found for JHotDraw\nless move method, move field, and extract class refactorings. This is mainly because\nJHotDraw contains a small number of blobs (only three blobs were detected), and it is\nknown to be of good quality. Thus, our results in Figure{16] reveal an effect we found:\nrefactorings like move field, move method, and extract class are likely to be more use-\nful to correcting the blob defect. As part of future work, we plan to investigate the\nrelationship between defect types and refactoring types.\n\n7. INDUSTRIAL CASE STUDY\n\nThe goal of this study is to evaluate the efficiency of our refactoring tool in practice.\nWe conducted an evaluation with potential software engineers, who can use our tool,\nrelated to the relevance of our approach for software engineers. One of the advantages\nof this industrial validation is the participation of the original developers of a system\nin the evaluation of recommended refactorings.\n\nWe performed a small industrial case studybased on one industrial project JDI-Ford\nv5.8. JDI-Ford is a Java-based software system that implements 638 classes having\n247 KLOC. This system is used by our industrial partner, the Ford Motor Company,\nto analyze useful information from the past sales of dealerships data and to suggest\nwhich vehicles to order for their dealer inventories in the future. JDI-Ford is the main\nkey software application used by the Ford Motor Company to improve their vehicle\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:39\n\n\\ i Move method\nRhino alls 2 Move field\nJHotDraw | * Extract class\n4 is Inline class\nAntApache mr EN Es +4 Extract interface\n4 . # Move class\nGanttProject = 4 Extract method\need Pull up field\nJFreeChart | WINES = Pull up method\nXerces-J eee BC S8 ill! Push down method\ny = Push down field\n0 50 100 150 200 250 300 350 400\n\nNumber of refactorings % Other refactorings\n\nFig. 16: Suggested refactorings distribution.\n\nsales by selecting the right vehicle configuration to the expectations of customers. Sev-\neral versions of JDI were proposed by software engineers at Ford during the past 10\nyears. Due to the importance of the application and the high number of updates per-\nformed during a period of 10 years, it is critical to make sure that all the JDI releases\nare within a good quality to reduce the time required by developers to introduce new\nfeatures in the future.\n\nThe software engineers from Ford manually evaluated all the recommended refac-\ntorings for JDI by our tool using the RP metric, described in the previous section,\nbased on their knowledge of the system since they are some of the original developers.\nWe also evaluated the relevance of some of the suggested refactoring for the develop-\ners. In addition, we asked 4 out of the 10 software engineers from Ford to manually\nrefactor some code fragments with a poor quality then we compared their suggested\nrefactorings with the recommended ones by our approach. To decide about the quality\nof a code fragment, we used the domain knowledge of the 10 programmers from Ford\n(since they are part of the original developers of the systems), the quality metrics and\ndetected design defects (to guide developers to identify a list of refactoring opportuni-\nties). Thus, we defined a metric called ER that represents the ratio of the number of\ngood refactoring recommendation over the number of expected refactorings. The four\nselected software engineers are part of the original developers of the JDI system thus\nthey easily provided different refactoring suggestions.\n\nIn this section, we aim at answering to the following two questions:\n\n(1) To what extent can our approach propose correct refactoring recommendations?\n(2) To what extent the suggested refactorings are relevant and useful for software\nengineers?\n\nWe describe, first, in this section the subjects participated in our study. Second, we\ngive details about the questionnaire, instructions, and the conducted pilot study. Fi-\nnally, we describe and discuss the obtained results.\n\n7.1. Subjects\n\nOur study involved 10 software engineers from the Ford Motor Company. All the sub-\njects are familiar with Java development, software maintenance activities including\nrefactoring. The experience of these subjects on Java programming ranged from 4 to\n17 years. They were selected, as part of a project funded by Ford, based on having\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:40 A. Ouni et al.\n\nsimilar development skills, their motivations to participate in the project and their\navailability. They are part of the original developers’ team of the JDI system.\n\n7.2. Pilot Study\n\nSubjects were first asked to fill out a pre-study questionnaire containing six questions.\nThe questionnaire helped to collect background information such as their role within\nthe company, their contribution in the development of JDI, their programming experi-\nence, their familiarity with quality assurance and software refactoring.\n\nWe divided the subjects into 5 groups (two developers per group) to evaluate the\ncorrectness and the relevance of the recommended refactorings according to the num-\nber of refactorings to evaluate, and the results of the pre-study questionnaire. All the\ngroups are similar, in average, in terms of programming experience, familiarity with\nthe system and used tools, and have almost the same refactoring and code smells back-\nground. The study consists of two parts:\n\n(1) The first part of the questionnaire includes questions to evaluate the correctness of\nthe recommended refactoring using the following options: 1. Not correct; 2. Maybe\nCorrect; and 3. Correct.\n\n(2) The second part of the questionnaire includes questions around the relevance of\nthe recommended refactorings using the following scale: 1. Not at all relevant; 2.\nSlightly relevant; 3. Moderately relevant; and 4. Extremely relevant.\n\nThe questionnaire is completed anonymously thus ensuring confidentiality and this\nstudy was approved by the IRB at the University of Michigan: “Research involving\nthe collection or study of existing data, documents, records, pathological specimens, or\ndiagnostic specimens, if these sources are publicly available or if the information is\nrecorded by the investigator in such a manner that participants cannot be identified,\ndirectly or through identifiers linked to the participants”.\n\nThe different programmers from the Ford Motor Company were asked not only to\nevaluate the generated refactoring solutions by our tool but they also used the tool\nto generate the refactoring solutions for the industrial system to evaluate. Thus, they\nperformed all the required steps from the configuration of the multi-objective algo-\nrithm to the generation and analysis of the results. The programmers agreed that the\ntool was very easy to use due to the friendly graphical interface provided by the tool.\nAll the programmers successfully executed the tool without any help from the supervi-\nsors of the experiments. During the entire process, subjects were encouraged to think\naloud and to share their opinions, issues, detailed explanations, and ideas with the\norganizers of the study (one graduate student and one faculty from the University of\nMichigan) and not only answering the questions.\n\nA brief tutorial session was organized for every participant around refactoring to\nmake sure that all of them have a minimum background to participate in the study.\nAll the developers performed the experiments in a similar environment: similar con-\nfiguration of the computers, tools (Eclipse, Excel, etc.) and facilitators of the study.\nBecause some support was needed for the installation of our Eclipse plug-in and the\nother detection techniques considered in our experiments, we added a short descrip-\ntion of this instruction for the participants. These sessions were also recorded as audio\nand the average time required to finish all the questions was 3.5 hours. Thus, the max-\nimum time spent by the developers was 8.5 hours (including the refactoring execution\nand inspection) however the total average time was 4 hours.\n\nPrior to the actual experiment, we did a pilot run of the entire experiment with\none software engineer from Ford. We performed this pilot study to verify whether the\nassignments were clear and if our estimation of the required time to finalize the exper-\niments evaluation were realistic thus all the assignments could be completed in two\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:41\n\nsessions (one day) by the subjects. The pilot study pointed out that the assignments\nand the questions in the questionnaire form were clear and relevant, and that they\ncould be executed as offered by the subjects of the pilot study within a maximum of\n5 hours. The pilot study also pointed out that the description of refactorings and the\nexamples were clear and sufficient to understand the different types of refactorings\nconsidered in our experiments. Note that the engineer who participated in the pilot\nstudy was not involved for the rest of experiment reported in the paper, and was in-\nstructed not to share information about the experience prior to the study.\n\n7.3. Results of the Industrial Case Study\n\nIn this section, we evaluate the performance of our multi-objective refactoring tech-\nnique in an industrial setting.\n\nOur first experiment was to assess to correctness of the suggested refactorings. From\nthe set of suggested refactoring, 87 out of 104 refactoring was accepted by Ford devel-\nopers suggesting that our approach was correct with a precision higher of 84%. For\nmore details, figure [17] reports the different types of refactorings that was correctly\nsuggested by our approach and approved by the majority of software engineers.\n\nSimilar facts were found when analyzing the similarity between the refactorings\nrecommended by our approach and those manually proposed by developers for several\ncode fragments. Most of the fixed code fragments by the software engineers were re-\nlated to the most severe and urgent ones based on their knowledge of the system. A\nnumber of 34 out of the 42 refactorings suggested by the developers were also proposed\nby our technique resulting to a precision of more than 80%. Only 5% of recommended\nrefactorings were considered as not correct and 11% as maybe correct.\n\n100 5\n\n80 4\n\n40 4\n\nRefactoring precision (RP)\n\n204\n\neS\noF yo*\nye\n\nFig. 17: Correctness of the different types of suggested refactorings.\n\nIn fact, the incorrect refactorings are due to some generated conflicts related to the\nfact that we are combining both complex and atomic refactorings in our solution. Al-\nthough our repair operator eliminates the detected identical redundant refactorings\nwithin one solution, it is challenging to detect such issue when dealing with complex\nand atomic refactorings. For example, an extract class is composed by several atomic\nrefactorings such create new class, move methods, move attributes, redirect method\ncalls, etc. Thus, it is challenging to eliminate some conflicts between atomic and com-\nplex refactorings when it is a redundancy issue. A possible solution is to convert all\ncomplex refactorings to atomic ones then we can perform the comparison to detect the\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:42 A. Ouni et al.\n\n@ Recommended refactorings (104)\n1B Applied refactorings (87)\n04\n\n0.3\n\n0.2\n\nImo i\n|\n\nEffectiveness Flexibility Reusability  Understandability\n\nQuality Improvements\n\nFig. 18: Quality improvements on JDI-Ford after applying the recommended refactor-\nings.\n\nredundancy. However, the conversion process is not straightforward since one complex\nrefactoring can be translated in different ways in terms of atomic refactorings.\n\nTo better investigate the relevance of the recommended refactorings, we evaluated\ntheir impact on the quality of JDI-Ford based on QMOOD. Figure}18/depicts the quality\nattributes improvements of the JDI system after applying (i) all recommended refac-\ntorings (104 refactorings), and (ii) only the selected refactorings by the developers (87\nrefactorings). The obtained results shows that our approach succeeded in improving\ndifferent aspect of software quality including reusability (0.11 of improvement), flex-\nibility (0.13), understandability (0.34), and effectiveness (0.09). An interesting point\nhere is that the results achieved by the selected refactorings (87 out of 104) outperform\nthe ones achieved by all the recommended refactorings (104) in terms of understand-\nability and reusability. This finding provides evidence that although developers seek\nto improve the overall quality of their code, they are prioritizing the understandability\nand reusability than other quality aspects. Indeed, we expected that developers will\nmainly apply refactorings that improve the readability and understandability of their\ncode.\n\nMoreover, we asked the developers to evaluate the relevance of the recommended\nrefactorings for the JDI-Ford system. Only less than 5% of recommended refactor-\nings are considered not at all relevant by the software engineers, 7% are considered\nas slightly relevant, 19% are moderately relevant, while 69% are considered as ex-\ntremely relevant. Moreover, the assessment of the Cohen’s Kappa coefficient «\n\net al. 1960], which measures to what extent the developers agree when voting for a\nrecommended refactoring, indicates a score of « = 0.79. This significant score indi-\n\ncates “substantial agreement” as characterized by Landis and Koch\n[1977]. This confirms the importance of the recommended refactorings for developers\nthat they need to apply them for a better quality of their systems.\n\nTo get more insights about the 5% of refactorings that are voted as “not at all rele-\nvant”, we asked the developers to comment on some particular cases. We noticed that\nmost of these rejected refactoring were related to utility classes in JDI, where move\nmethod refactorings are suggested to move some utility methods to the classes that are\ncalling them. Developers mentioned that this kind of refactorings tends to be mean-\ningless.\n\nTo better evaluate the relevance of the recommended refactorings, we investigated\nthe types of refactorings that developers may consider them more or less important\nthan others. Figure [19| shows that move method is considered as one of the most ex-\ntremely relevant refactorings. In addition, extract method is also considered as another\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:43\n\nvery important and useful refactoring. This can be explained by the fact that the de-\nvelopers are more interested to fix quality issues that are related to the size of classes\nor methods. Overall, the different types of refactorings are considered relevant. One\nreason can be that our approach provides a sequence of refactorings.\n\n= Extremely relevant Moderately relevant Slightly relevant = Not at all relevant\n\nPush dow eld SS Eee\nPush down method = ———————\nlpr SS ——————_—_—_—_—_—_—_—_—_——>-[-—-=\nPull Up field\nExtract Method ———eEe=~-~~'»eF»'__—— DOS\noe ls _—>E>E>E————E—_——_—_—_——_—_-_—-=-*“*“>~>~-=~=\natectinteree = ————————--““_tbre_\nine ass EE —————ee—e—eee—e—e\ntls EE — ee —__—_—_—_—_——esss——\nove fd EE —$———$————-=se*r°xX  _,_:\nMove rethed eee\n\n0 10 20 30 40 50 60 70\n\nFig. 19: The relevance of different types of recommended refactorings on JDI-Ford.\n\nIt was clear for our participants that our tool can provide faster and similar results\nthat they can manually suggest. The refactoring of large scale system can be time\nconsuming to fix several quality issues. The participants provided some suggestions\nto make our refactoring better and more efficient. First, the tool does not provide any\nranking to prioritize the suggested refactorings. In fact, the developers do not have\nenough time to apply all the suggested refactorings but they prefer to fix the most se-\nvere quality issues. Second, our technique does not provide a support to fix refactoring\nsolutions when the developers did not approve part of the suggested refactorings. Fi-\nnally, the software engineers prefer that our tool provides a feature to automatically\napply some regression testing techniques to generate test cases for the modified code\nfragments after refactoring. Such a feature is very interesting to include in our tool to\nautomatically test the Java refactoring engine similarly to SafeRefactor\n.\n\n8. THREATS TO VALIDITY\n\nSome potential threats can affect the validity of our experiments. We now discuss these\npotential threats and how we deal with them.\n\nConstruct validity concerns the relation between the theory and the observation.\nIn our experiments, the design defect detection rules we use to\nmeasure DCR could be questionable. To mitigate this threat, we manually inspect and\nvalidate each detected defect. Moreover, our refactoring tool configuration is flexible\nand can support other state-of-the-art detection rules. In addition, different threshold\nvalues were used in our experiments based on trial-and-error, however these values\ncan be configured once then used independently from the system to evaluate. Another\nthreat concerns the data about the actual refactorings of the studied systems. In ad-\ndition to the documented refactorings, we are using Ref-Finder, which is known to\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n", '39:44 A. Ouni et al.\n\nbe efficient [Prete et al. 2010]. Indeed, Ref-Finder was able to detect refactoring op-\nerations with an average recall of 95% and an average precision of 79% [Prete et al.|\n\n2010]. To ensure the precision, we manually inspect the refactorings found by Ref-\n‘inder. We identify three threats to internal validity: selection, learning and fatigue,\nand diffusion.\n\nFor the selection threat, the subject diversity in terms of profile and experience could\naffect our study. First, all subjects were volunteers. We also mitigated the selection\nthreat by giving written guidelines and examples of refactorings already evaluated\nwith arguments and justification. Additionally, each group of subjects evaluated differ-\nent refactorings from different systems for different techniques/algorithms. Further-\nmore, the selection refactorings to be evaluated for each refactoring solution was com-\npletely random.\n\nRandomization also helps to prevent the learning and fatigue threats. For the fatigue\nthreat, specifically, we did not limit the time to fill the questionnaire for the open source\nsystems. Consequently, we sent the questionnaires to the subjects by email and gave\nthem enough time to complete the tasks. Finally, only ten refactorings per system was\nrandomly picked for the evaluation. However, all refactoring solutions were evaluated\nfor the industrial system.\n\nDiffusion threat is limited in our study because most of the subjects are geograph-\nically located in three different universities and a company, and the majority do not\nknow each other. For the few ones who are in the same location, they were instructed\nnot to share information about the experience prior to the study.\n\nConclusion validity deals with the relation between the treatment and the out-\ncome. Thus, to ensure the heterogeneity of subjects and their differences, we took spe-\ncial care to diversify them in terms of professional status, university/company affilia-\ntions, gender, and years of experience. In addition, we organized subjects into balanced\ngroups. Having said that, we plan to test our tool with Java development companies,\nto draw better conclusions. Moreover, the automatic evaluation is also a way to limit\nthe threats related to subjects as it helps to ensure that our approach is efficient and\nuseful in practice. Indeed, we compare our suggested refactorings with the expected\nones that are already applied to the next releases and detected using Ref-Finder.\n\nAnother potential threat can be related to parameters selection. We selected differ-\nent parameters of our NSGA-II algorithm, such as the population size, the maximum\nnumber of iterations, mutation and crossover probabilities, and the solution length,\nbased on the trial-and-error method and depending on the size of the evaluated sys-\ntems, the initial number of design defect instances detected, and the number of refac-\ntoring types implemented in our tool (11 types, table[II). However, as these parameters\nare independent each other, they can be easily configured according to the preferences\nof the developers, for example if they want to reduce the execution time (e.g., reduce\nthe number of iterations) and maybe sacrifice a bit on the quality of the solutions.\n\nAlso when comparing the different approaches, some of them are using less types of\nrefactorings. We believe that this is one of the limitations of these approaches thus it is\ninteresting to show that considering the 11 types of refactorings of our approach may\nimprove the results (even if programmers may apply them less frequently). Further-\nmore, when comparing the different approaches from the effort perspective, the code\nchanges score is relative to DCR level. Not all design defects require the same amount\nof code changes. The process prioritizes the correction of design defects that require\nless changes to have higher DCR score. In addition, our results were consistent on all\nthe different DCR levels for all the systems.\n\nExternal validity refers to the generalizability of our findings. In this study, we\nperformed our experiments on different open-source and industrial Java systems be-\nlonging to different application domains and with different sizes. However, we cannot\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:45\n\nassert that our results can be generalized to other programming languages, and to\nother practitioners.\n\nThe industrial validation section was checked by the Ford Motor Company. Our in-\ndustrial partner accepted to only include the results mentioned in the current valida-\ntion section for several reasons. Similar to most collaborations with industry, we are\nnot allowed to mention the name of code elements or providing any example from the\nsource code. One of our motivations to use open source systems in our validation is the\nhard constraint to not share the industrial data. Thus, the readers can at least check\ndifferent examples of suggested refactorings on the open source system in the website\nprovided with this paper.\n\n9. RELATED WORK\n\nSeveral studies have been focused on software refactoring in recent years. In this sec-\ntion, we survey those works that can be classified into three broad categories: (i) man-\nual and semi-automated approaches, (ii) search-based approaches, and (iii) semantics-\nbased approaches.\n\n9.1. Manual and semi-automated approaches\n\nThe first book in the literature was written by Fowler and provides a\nnon-exhaustive list of low-level design problems in source code have been defined. For\neach design problem (i.e., design defect), a particular list of possible refactorings are\nsuggested to be applied by software maintainers manually. After Fowler’s book sev-\neral approaches have merged with the goal of taking advantage from refactoring to\nimprove quality metrics of software systems. In (Sahraout etal. 2000), Sahraoui et al.\nproposed an approach to detect opportunities of code transformations (i.e., refactor-\nings) based on the study of the correlation between certain quality metrics and refac-\ntoring changes. Consequently, different rules are manually defined as a combination\nof metrics/thresholds to be used as indicators for detecting refactoring opportunities.\nFor each code smell a pre-defined and standard list of transformations should be ap-\nplied. In contrast to our approach, we do not have a pre-defined list of refactorings to\napply, instead, our approach automatically recommends refactorings depending on the\ncontext.\n\nAnother similar work is proposed by Du Bois et al. who starts\nfrom the hypothesis that refactoring opportunities correspond of those which improves\ncohesion and coupling metrics to perform an optimal distribution of features over\nclasses. Anquetil et al. analyze how refactorings manipulate coupling and cohesion\nmetrics, and how to identify refactoring opportunities that improve these metrics (An\niquetil and Laval 2011}. The authors reported that refactorings manually performed by\ndevelopers do not necessarily improve the modularity in terms of cohesion/coupling.\nThis suggests that goal-oriented refactoring recommendation is useful to improve spe-\ncific aspects of the system, which is one of the motivations of our approach. However,\nthese two approaches are limited to only some possible refactoring operations with few\nnumber of quality metrics. In addition, the proposed refactoring strategies cannot be\napplied for the problem of correcting design defects. In our approach, we are taking as\ninput the set of code smells that could be detected using the above studies but we did\nnot address the problem of using quality metrics to identify design defects.\n\nMoha et al. proposed an approach that suggests refactorings using\nFormal Concept Analysis A) to fix god class design defect. This work combines the\nefficiency of cohesion/coupling metrics with FCA to suggest refactoring opportunities.\nHowever, the link between defect detection and correction is not obvious, which make\nthe inspection difficult for the maintainers. Similarly, Joshi et al.\nhave presented an approach based on concept analysis aimed at identifying less\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:46 A. Ouni et al.\n\ncohesive classes. It also helps identify less cohesive methods, attributes and classes\nat the same time. Furthermore, the approach guides refactoring opportunities iden-\ntification such as extract class, move method, localize attributes and remove unused\nattributes. In addition, Tahvildari et al. proposed\na framework of object-oriented metrics used to suggest refactoring opportunities to\nimprove the quality of object-oriented legacy systems. In contrast, our approach is not\nbased explicitly on quality metrics as indicator for quality improvements, instead, we\nare based on the number of fixed design defects. Indeed, improving quality does not\nnecessarily mean that actual design defects are fixed.\n\nAnother generation of semi-automated refactoring techniques have emerged.\n\nMurphy-Hill et al. [Murphy-Hill and Black 2008}|Murphy-Hill et al. 2012} Murphy-Hill\npropose several techniques and empirical studies to support refactor-\ning activities. Tn (Murphy-Hill and Black 2008) and (Murphy-Hill and Black 2012), the\nauthors propose new tools to assist software engineers in applying refactoring such as\n\nselection assistant, box view, and refactoring annotation based on structural informa-\ntion and program analysis techniques. Recently, Ge and Murphy-Hill have proposed\nnew refactoring tool called GhostFactor that allows the\ndeveloper to transform code manually, but check the correctness of the transforma-\ntions automatically. However, the correction is based mainly on the structure of the\ncode and does not consider the issue of design coherence as our proposal does. Other\ncontributions are based on rules that can be expressed as assertions (invariants, pre\nand post-condition). The use of invariants has been proposed to detect parts of pro-\ngram that require refactoring by (Kataoka et al. 2001), In addition, Opdyke\n[1992] proposed the definition and the use of pre- and post-condition with invariants\nto preserve the behavior of the software when applying refactoring. Hence, behavior\npreservation is based on the verification/satisfaction of a set of pre and post-condition.\nAll these conditions are expressed in terms of rules. However, unlike our approach,\nthese approaches focus only on behavior preservation and do not consider the design\ncoherence of the program.\n\nTsantalis et al. |Tsantalis and Chatzigeorgiou 2009] and Sales et al. [Sales et al.\n\nproposed techniques to identify move methods opportunities by studying the ex-\nisting dependencies between classes. A similar technique was suggested by Fokaefs\net al. to detect extract class possibilities by analyzing dependen-\ncies between methods and classes. However, such approaches are local, i.e., they focus\non a specific code fragment. In contrast to our approach, we are providing a generic\n\nrefactoring approach that consider the effect on the whole system being refactored.\nFurthermore, several empirical studies [Kim et al. 2014} Negara et al. 2013}\nFranklin et al. 2013} [Alves et al. 2014] was performed recently to understand the ben-\nefits and risk of refactoring. Thee studies show that the main risk that refactorings\ncould introduce is the creation of bugs after refactoring but several benefits could be\nobtained such as reducing the time that programmers spent to understand existing\n\nimplemented features.\nMore details about current literature related to manual or semi-automated software\n\nrefactoring can be found in the following two recent surveys [Bavota et al. 2014b\nAl Dallal 2015).\n\n9.2. Search-based approaches\n\nTo automate refactoring activities, new approaches have emerged where search-based\ntechniques have been used. These approaches cast the refactoring problem as an opti-\nmization problem, where the goal is to improve the design quality of a system based\nmainly on a set of software metrics. After formulating the refactoring as an optimiza-\ntion problem, several techniques can be applied for automating refactoring, e.g., ge-\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:47\n\nnetic algorithms, simulated annealing, and Pareto optimality, etc. Hence, we classify\nthose approaches into two main categories: (1) mono-objective and (2) multi-objective\noptimization approaches.\n\nIn the first category, the majority of existing work combines several metrics in a\nsingle fitness function to find the best sequence of refactorings. Seng et al.\n[2006] propose a single-objective search-based approach using genetic algorithm to sug-\ngest a list of refactorings to improve software quality. The search process uses a single\nfitness function to maximize a weighted sum of several quality metrics. The employed\nmetrics are mainly related to various class level properties such as coupling, cohesion,\ncomplexity and stability while satisfying a set of pre-conditions for each refactoring.\nThese conditions serve at preserving the program behavior (refactoring feasibility).\nHowever, in contrast to our approach, this approach does not consider the design co-\nherence of the refactored program and limited only to move method refactoring. An-\nother similar work of O’Keeffe et al. |O’Keeffe and Cinnéide 2008] that uses different\nlocal search-based techniques such as hill climbing and simulated annealing to provide\nan automated refactoring support based on the QMOOD metrics suite. Interestingly,\nthey also found that the understandability function yielded the greatest quality gain,\n\nin keeping with our observation in Section\nQayum et al. |Qayum and Heckel 2009] considered the problem of refactoring\nscheduling as a graph transformation problem. They expressed refactorings as a\n\nsearch for an optimal path, using Ant Colony Optimization, in the graph where nodes\nand edges represent respectively refactoring candidates and dependencies between\nthem. However the use of graphs is limited only on structural and syntactical informa-\ntion and does not consider the design semantics neither its runtime behavior. Fatire-\ngun et al. [Fatiregun et al. 2004] show how search-based transformations could be\nused to reduce code size and construct amorphous program slices. However, they have\nused small atomic level transformations in their approach. However, their aim was to\nreduce program size rather than improving its structure/quality.\n\nOtero et al. [Otero et al. 2010] introduced an approach to explore the addition of a\nrefactoring step into the genetic programming iteration. It consists of an additional\nloop in which refactoring steps, drawn from a catalog, will be applied to individuals\nof the population. Jensen et al. (Jensen and Cheng 2010) and Cheng 2010] have proposed an approach\nthat supports composition of design changes and makes the introduction of design\npatterns a primary goal of the refactoring process. They used genetic programming\nand software metrics to identify the most suitable set of refactorings to apply to a\nsoftware design. Furthermore, Kilic et al. explore the use of a variety\nof population-based approaches to search-based parallel refactoring, finding that local\nbeam search could find the best solutions. However, still these approach focusing on\nspecific refctoring types and not not consider the design semantics.\n\nZibran et al. formulated the problem of scheduling of code\nclone refactoring activities as a constraint satisfaction optimization problem (CSOP)\nto fix known duplicate code code-smells. The proposed approach consists of applying\nconstraint programming (CP) technique that aims to maximize benefits while mini-\nmizing refactoring efforts. An effort model is used for estimating the effort required\nto refactor code clones in object-oriented codebase. Although there is a slight similar-\nity between the proposed effort model and our code changes score model\nd72a), the proposed approach does not ensure the design coherence of the refactored\nprogram.\n\nIn the second category, the first multi-objective approach was introduce by Harman\net al. [Harman and Tratt 2007] as described earlier. Recently, O Cinneide et al. [0 Cin,\nnéide et al. 201 ave proposed a multi-objective search-based refactoring to conduct\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n\n', '39:48 A. Ouni et al.\n\nan empirical investigation to assess some structural metrics and to explore relation-\nships between them. To this end, they have used a variety of search techniques (Pareto-\noptimal search, semi-random search) guided by a set of cohesion metrics. The main\nweakness in all of these approaches is that the design preservation have not been ad-\ndressed to obtain correct and meaningful refactorings, neither the effort required to\napply refactoring which addressed in our approach.\n\n9.3. Semantic coherence for refactoring\nThere exists few works focusing on refactorings that involves semantic coherence. In\n\ntheir approach, Fatiregun et al. have\napplied a number of simple atomic transformation rules called axioms. The authors\npresume that if each axiom preserves semantics then a whole sequence of axioms ought\nto preserve semantics equivalence. However, semantics equivalence depends on the\nprogram and the context and therefore it could not be always proved. Indeed, their\nproposed semantic equivalence is based only on structural rules related to the axioms,\nrather than a semantic analysis of the code.\n\nLater, Bavota et al. have proposed an approach for automating\nthe refactoring extract class based on graph theory that exploits structural and se-\nmantic relationships between methods. The proposed approach uses a weighted graph\nto represent a class to be refactored, where each node represents a method of that\nclass. The weight of an edge that connects two nodes (representing methods) is a mea-\nsure of the structural and semantic relationship between two methods that contribute\nto class cohesion. After that, they split the built graph in two sub-graphs, to be used\nlater to build two new classes having higher cohesion than the original class. Similarly,\nBavota et al. used cohesion metrics in order\nto identify opportunities of extract class.\n\nBy exploiting semantic information, Bavota et al. proposed a\ntechnique for the recommendation of move method using semantic information and\nrelational topic models. Other studies tried to formulate semantic information based\non cohesion for software clustering and remodularization ||Corazza et al. 2011}|Bavota\nSeanniello et al. 2010), Mkaouer et al. [Mkaouer et al. 2015] formulate:\nsoftware remodularization as many-objective problem however they used very basic\nsemantic measures and it was limited to few refactorings applied at the package level.\nMoreover, Bavota et al. suggested an approach to recommend\nappropriate refactoring operations to adjust the design according to the teams’ activity\npatterns.\n\nIn (Baar and Markovié 2007), Baar et al. have presented a simple criterion and a\nproof technique for the preservation of the design coherence of refactoring rules that\nare defined for UML class diagrams and OCL constraints. Their approach is based on\nformalization of the OCL semantics taking the form of graph transformation rules.\nHowever, their approach does not provide a concrete design preservation since there\nis no explicit differentiation between behaviour and design preservation. In fact, they\nconsider that the semantic coherence “means that the observable behaviors of original\nand refactored programs coincide”. In addition, in contrast to our approach, they par-\ntially address the design preservation in the model level with a high level of abstrac-\ntion without considering the code/implementation level. In addition, this approach\nuses only the refactoring move attribute and do not consider popular refactoringg"|\n\nAnother semantics-based framework was introduced by Logozzo [Logozzo and|\nCortesi 2006] for the definition and manipulation of class hierarchies-based refactor-\nings. The framework is based on the notion of the observable part of a class, i.e., an\n\n1 Thttp //www.refactoring.com/catalog/,\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:49\n\nabstraction of its semantics when focusing on a behavioral property of interest. They\ndefine a semantic subclass relation, capturing the fact that a subclass preserves the\nbehavior of its superclass up to a given observed property.\n\nFurthermore, it is worth to note that most of the existing techniques are limited\nto a small number of refactorings (single refactoring based approaches). For instance,\n\nHarman et al. [Harman and Tratt 2007], Bavota et al. |[Bavota et al. 2014a], Tsantalis\n\nand Chatzigeorgiou [Tsantalis and Chatzigeorgiou 2009), and Sales et al. [Sales et al)\n2013] recommend only move method refactoring. Bavota et al. [Bavota et al. 2014a)\n\nBavota ot a. 2011} |Bavota et al. 2014b}/Bavota et al. 2010] and Fokaefs et al. [Fokaefs|\n[Fokaefs et al. 2012] address the recommendation of the extract class refac-\ntoring. On i e other and, ilva et al., [Silva et al. 2014], Tsantalis and Chatzige-\norgiou |Tsantalis and Chatzigeorgiou 2011] introduce an approach for recommending\nextract method refactorings, while} Krishnan and Tsantalis focus on code clone refactor-\nings [Krishnan and Tsantalis 2014]. To help devele pers with efficient refactoring rec-\nommendations, JDendorant [Fokaefs etal. 2011\nTsantalis and ae 009] unifies different techniques\nin one tool t. at oe ive refactorings (move method, extract class, extract method,\nreplace conditional with polymorphism, and replace type code with state/strategy) to\nfix four types of code smells (god class, feature envy, type checking, and long method).\n\nMoreover, Seng et al. implemented five refactorings (move method,\npull up attribute, push down attribute, pull up method, and push down method) but\nthey only focus on move method refactoring in their paper (Seng et al. 2006) In con-\ntrast, one of the strengths of our approach is that it addresses several refactoring types\n(11 refactorings) at the same time as listed in Table|II}\n\n10. CONCLUSIONS AND FUTURE WORK\n\nThis paper presented a novel search-based approach taking into consideration multiple\ncriteria to suggest “good” refactoring solutions to improve software quality. The process\naims at finding the sequence of refactorings that (i) improves design quality, (ii) pre-\nserves the design coherence of the refactored program, (iii) minimizes code changes,\nand (iv) maximizes the consistency with development change history. We, thus, formu-\nlated our problem as a multi-objective search problem to find a trade-off between all\nthese objectives using NSGA-II. Moreover, we defined different measures to estimate\nthe design coherence of a code after refactoring and we also used the similarity with\nprevious code changes as an indicator of the design consistency.\n\nTo evaluate our approach, we conducted an empirical study from both quantitative\nand qualitative perspectives on open-source and industrial projects. The open-source\nevaluation involved six medium and large size open-source systems with a comparison\n\nagainst three existing approaches [Harman and Tratt 2007; Kessentini et al. 2011}\nFokaefs et al. 2011].\n\nOur empirical study shows the efficiency of our approach in im-\nproving the quality of the studied systems while successfully fixing an average of 84%\nof design defects with low code change score (an average of 2,937 of low level changes).\nThe qualitative evaluation shows that most of the suggested refactorings (an aver-\nage of 80%) are considered as relevant and meaningful from developer’s point of view.\nMoreover, unlike existing approaches, the obtained results show that our approach\nis efficient in suggesting a significant number of expected refactorings that was per-\nformed in the next release of the systems being studied which provides evidence that\nour approach is more efficient and useful in practice.\n\nIn addition, we conducted an industrial validation of our approach on a large-scale\nproject and the results was manually evaluated by 10 active software engineers to as-\nsess the relevance and usefulness of our refactoring suggestions. The obtained results\nprovide evidence that our approach succeeded in improving different aspect of software\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:50 A. Ouni et al.\n\nquality including reusability (0.11 of improvement), flexibility (0.13), understandabil-\nity (0.34), and effectiveness (0.09). Moreover, 84% of the recommended (87 out of 104)\nwas meaningful and useful from developer’s point of view.\n\nIn future work, we are planning to conduct an empirical study to understand the\ncorrelation between correcting design defects and introducing new ones or fixing\nother design defects implicitly. We also plan to adapt our multi-objective approach\nto fix other types of defects that can occur in new emergent service-based applica-\ntions (Ratem-Gal-Or et al. 2012) Future replications of our study with additional\nsystems and design defect types are necessary to confirm our findings. Another lim-\nitation of our current approach is the selection of the best solution from the Pareto\nfront. We used the technique of selecting the closest solution to the ideal point. How-\never, we plan in our future work to integrate the developers preferences to select the\nbest solution from the set of non-dominated solutions. Moreover, one limitation of our\napproach is that one input is a base of recorded/collected code changes on previous\nversions. We believe that this data is not always available, especially in the begin-\nning of the projects. As a future work, we plan to reuse refactorings recorded/col-\nlected for other similar contexts can be used instead. This can be done by calculat-\ning the similarity with not only the refactoring type but also between the contexts\n(code fragments). Furthermore, we are planning to include more criteria and con-\nstraints to improve the meaningfulness of the suggested refactorings, an interesting\none is to identify refactorings related to utility classes and prevent moving method-\ns/fields between utility and functional classes, as these refactoring are unlikely to be\nmeaningful. Finally, we are planning to include other fine-grained refactoring opera-\ntions such as Decompose Conditional, Replace Conditional with Polymorphism, and\nReplace Type Code with State/Strategy to improve the quality of the code.\n\nACKNOWLEDGMENTS\n\nThe authors would like to thank the anonymous reviewers for their relevant feedback and useful comments\nthat helped them to improve this work. Also, the authors are grateful to the subject students from the\nUniversity of Michigan-Dearborn and developers who participated in their empirical study. This work was\nsupported by the Ford-University of Michigan alliance Program, Japan Society for the Promotion of Science,\nGrant-in-Aid for Scientific Research (S) (No.25220003) and by Osaka University Program for Promoting\nInternational Joint Research.\n\nREFERENCES\n\nJehad Al Dallal. 2015. Identifying refactoring opportunities in object-oriented code: A systematic literature\nreview. Information and Software Technology 58 (2015), 231-249.\n\nEverton LG Alves, Myoungkyu Song, and Miryung Kim. 2014. RefDistiller: a refactoring aware code re-\nview tool for inspecting manual refactoring edits. In 22nd ACM SIGSOFT International Symposium on\nFoundations of Software Engineering (FSE). 751-754.\n\nRaquel Amaro, Rui Pedro Chaves, Palmira Marrafa, and Sara Mendes. 2006. Enriching wordnets with new\nrelations and with event and argument structures. In Computational Linguistics and Intelligent Text\nProcessing. Springer, 28-40.\n\nNicolas Anquetil and Jannik Laval. 2011. Legacy software restructuring: Analyzing a concrete case. In 15th\nEuropean Conference on Software Maintenance and Reengineering (CSMR). 279-286.\n\nThomas Baar and Slavisa Markovié. 2007. A graphical approach to prove the semantic preservation of\nUML/OCL refactoring rules. In Perspectives of systems informatics. Springer, 70-83.\n\nJagdish Bansiya and Carl G Davis. 2002. A hierarchical model for object-oriented design quality assessment.\nIEEE Transactions on Software Engineering 28, 1 (2002), 4-17.\n\nGabriele Bavota, Andrea De Lucia, Andrian Marcus, and Rocco Oliveto. 2014a. Automating extract class\nrefactoring: an improved method and its evaluation. Empirical Software Engineering 19, 6 (2014), 1617-\n1664.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:51\n\nGabriele Bavota, Andrea De Lucia, Andrian Marcus, and Rocco Oliveto. 2014b. Recommending Refactoring\nOperations in Large Software Systems. In Recommendation Systems in Software Engineering. Springer,\n387-419.\n\nGabriele Bavota, Andrea De Lucia, Andrian Marcus, Rocco Oliveto, and Fabio Palomba. 2012. Supporting\nextract class refactoring in Eclipse: the ARIES project. In 34th International Conference on Software\nEngineering (ICSE). IEEE Press, 1419-1422.\n\nGabriele Bavota, Andrea De Lucia, and Rocco Oliveto. 2011. Identifying extract class refactoring opportu-\nnities using structural and semantic cohesion measures. Journal of Systems and Software 84, 3 (2011),\n397-414.\n\nGabriele Bavota, Malcom Gethers, Rocco Oliveto, Denys Poshyvanyk, and Andrea de Lucia. 2014. Improving\nsoftware modularization via automated analysis of latent topics and dependencies. ACM Transactions\non Software Engineering and Methodology 23, 1 (2014), 4.\n\nGabriele Bavota, Rocco Oliveto, Andrea De Lucia, Giuliano Antoniol, and Yann-Gaél Guéhéneuc. 2010. Play-\ning with refactoring: Identifying extract class opportunities through game theory. In 26th International\nConference on Software Maintenance (ICSM). 1-5.\n\nGabriele Bavota, Rocco Oliveto, Malcom Gethers, Denys Poshyvanyk, and Andrea De Lucia. 2014a. Method-\nbook: Recommending move method refactorings via relational topic models. IEEE Transactions on Soft-\nware Engineering 40, 7 (2014), 671-694.\n\nGabriele Bavota, Sebastiano Panichella, Nikolaos Tsantalis, Massimiliano Di Penta, Rocco Oliveto, and\nGerardo Canfora. 2014b. Recommending refactorings based on team co-maintenance patterns. In 29th\nInternational Conference on Automated software engineering (ASE). 337-342.\n\nWilliam H Brown, Raphael C Malveau, and Thomas J Mowbray. 1998. AntiPatterns: refactoring software,\narchitectures, and projects in crisis. (1998).\n\nMel O Cinnéide. 2001. Automated application of design patterns: a refactoring approach. Ph.D. Dissertation.\nTrinity College Dublin.\n\nNorman Cliff. 1993. Dominance statistics: Ordinal analyses to answer ordinal questions. Psychological Bul-\nletin 114, 3 (1993), 494.\n\nJacob Cohen and others. 1960. A coefficient of agreement for nominal scales. Educational and psychological\nmeasurement 20, 1 (1960), 37-46.\n\nAnna Corazza, Sergio Di Martino, and Valerio Maggio. 2012. LINSEN: An efficient approach to split iden-\ntifiers and expand abbreviations. In 28th International Conference on Software Maintenance (ICSM).\n233-242.\n\nAnna Corazza, Sergio Di Martino, Valerio Maggio, and Giuseppe Scanniello. 2011. Investigating the use of\nlexical information for software system clustering. In 15th European Conference on Software Mainte-\nnance and Reengineering (CSMR). 35-44.\n\nMarco D’Ambros, Alberto Bacchelli, and Michele Lanza. 2010. On the impact of design flaws on software\ndefects. In 10th International Conference on Quality Software (QSIC). 23-31.\n\nKalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. 2002. A fast and elitist multiobjec-\ntive genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation 6, 2 (2002), 182-197.\n\nIgnatios Deligiannis, Martin Shepperd, Manos Roumeliotis, and Ioannis Stamelos. 2003. An empirical in-\nvestigation of an object-oriented design heuristic for maintainability. Journal of Systems and Software\n65, 2 (2003), 127-139.\n\nK. Dhambri, H. Sahraoui, and P. Poulin. 2008. Visual Detection of Design Anomalies. In 12th European\nConference on Software Maintenance and Reengineering (CSMR). 279-283.\n\nBart Du Bois, Serge Demeyer, and Jan Verelst. 2004. Refactoring-improving coupling and cohesion of exist-\ning code. In 11th Working Conference on Reverse Engineering (WCRE). 144-151.\n\nLen Erlikh. 2000. Leveraging legacy system dollars for e-business. IT professional 2, 3 (2000), 17-23.\n\nDeji Fatiregun, Mark Harman, and Robert M Hierons. 2004. Evolving transformation sequences using ge-\nnetic algorithms. In 4th International Workshop on Source Code Analysis and Manipulation (SCAM).\n65-74.\n\nDeji Fatiregun, Mark Harman, and Robert M Hierons. 2005. Search-based amorphous slicing. In 12th Work-\ning Conference on Reverse Engineering (WCRE). IEEE, 10—pp.\n\nNorman E. Fenton and Shari Lawrence Pfleeger. 1998. Software Metrics: A Rigorous and Practical Approach\n(2nd ed.). PWS Publishing Co., Boston, MA, USA.\n\nMarios Fokaefs, Nikolaos Tsantalis, Eleni Stroulia, and Alexander Chatzigeorgiou. 2011. JDeodorant: iden-\ntification and application of extract class refactorings. In 33rd International Conference on Software\nEngineering (ICSE). 1037-1039.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:52 A. Ouni et al.\n\nMarios Fokaefs, Nikolaos Tsantalis, Eleni Stroulia, and Alexander Chatzigeorgiou. 2012. Identification and\napplication of extract class refactorings in object-oriented systems. Journal of Systems and Software 85,\n10 (2012), 2241-2260.\n\nCarlos M Fonseca, Peter J Fleming, and others. 1993. Genetic Algorithms for Multiobjective Optimization:\nFormulation, Discussion and Generalization.. In 5th International Conference on Genetic Algorithms,\nVol. 93. 416-423.\n\nMartin Fowler. 1999. Refactoring: Improving the Design of Existing Code. Addison-Wesley Longman Pub-\nlishing Co., Inc., Boston, MA, USA.\n\nLyle Franklin, Alex Gyori, Jan Lahoda, and Danny Dig. 2013. LAMBDAFICATOR: from imperative to func-\ntional programming through automated refactoring. In 35th International Conference on Software En-\ngineering (ICSE). 1287-1290.\n\nXi Ge and Emerson Murphy-Hill. 2014. Manual refactoring changes with automated refactoring validation.\n36th International Conference on Software Engineering (ICSE) 36 (2014), 1095-1105.\n\nMohamed Salah Hamdi. 2011. SOMSE: A semantic map based meta-search engine for the purpose of web\ninformation customization. Applied Soft Computing 11, 1 (2011), 1310-1321.\n\nMark Harman, S Afshin Mansouri, and Yuanyuan Zhang. 2012. Search-based software engineering: Trends,\ntechniques and applications. ACM Computing Surveys (CSUR) 45, 1 (2012), 11.\n\nMark Harman and Laurence Tratt. 2007. Pareto optimal search based refactoring at the design level. In 9th\nannual conference on Genetic and evolutionary computation (GECCO). 1106-1113.\n\nPaul Jaccard. 1901. Etude comparative de la distribution florale dans une portion des Alpes et des Jura.\nBulletin del la Société Vaudoise des Sciences Naturelles 37 (1901), 547-579.\n\nAdam C. Jensen and Betty H.C. Cheng. 2010. On the Use of Genetic Programming for Automated Refactor-\ning and the Introduction of Design Patterns. In 12th Annual Conference on Genetic and Evolutionary\nComputation (GECCO). 1341-1348.\n\nPadmaja Joshi and Rushikesh K. Joshi. 2009. Concept Analysis for Class Cohesion. In 13th European Con-\nference on Software Maintenance and Reengineering (CSMR). Washington, DC, USA, 237-240.\n\nYoshio Kataoka, David Notkin, Michael D Ernst, and William G Griswold. 2001. Automated support for\nprogram refactoring using invariants. In International Conference on Software Maintenance (ICSM).\nIEEE Computer Society, 736.\n\nMarouane Kessentini, Wael Kessentini, Houari Sahraoui, Mounir Boukadoum, and Ali Ouni. 2011. Design\ndefects detection and correction by example. In 19th International Conference on Program Comprehen-\nsion (ICPC). 81-90.\n\nMarouane Kessentini, Stéphane Vaucher, and Houari Sahraoui. 2010. Deviance from perfection is a bet-\nter criterion than closeness to evil when identifying risky code. In 25th International Conference on\nAutomated Software engineering (ASE). 113-122.\n\nHurevren Kilic, Ekin Koc, and Ibrahim Cereci. 2011. Search-based parallel refactoring using population-\nbased direct approaches. In 3rd International Symposium on Search Based Software Engineering (SS-\nBSE). Springer, 271-272.\n\nMiryung Kim, Thomas Zimmermann, and Nachiappan Nagappan. 2014. An Empirical Study of Refactor-\ningChallenges and Benefits at Microsoft. IEEE Transactions on Software Engineering 40, 7 (2014),\n633-649.\n\nGiri Panamoottil Krishnan and Nikolaos Tsantalis. 2014. Unification and refactoring of clones. In Software\nEvolution Week - IEEE Conference on Software Maintenance, Reengineering and Reverse Engineering\n(CSMR-WCRE). 104-113.\n\nJ Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data.\nbiometrics (1977), 159-174.\n\nWei Li and Raed Shatnawi. 2007. An empirical study of the bad smells and class error probability in the\npost-release object-oriented system evolution. Journal of systems and software 80, 7 (2007), 1120-1128.\n\nRensis Likert. 1932. A technique for the measurement of attitudes. Archives of psychology (1932).\n\nFrancesco Logozzo and Agostino Cortesi. 2006. Semantic hierarchy refactoring by abstract interpretation.\nIn Verification, Model Checking, and Abstract Interpretation. Springer, 313-331.\n\nMika Mantyla, Jari Vanhanen, and Casper Lassenius. 2003. A taxonomy and an initial empirical study of\nbad smells in code. In International Conference on Software Maintenance (ICSM). IEEE, 381-384.\n\nRadu Marinescu. 2004. Detection strategies: metrics-based rules for detecting design flaws. In 20th Interna-\ntional Conference on Software Maintenance (ICSM). 350-359.\n\nTom Mens and Tom Tourwé. 2004. A survey of software refactoring. IEEE Transactions on Software Engi-\nneering 30, 2 (2004), 126-139.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', 'Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:53\n\nWiem Mkaouer, Marouane Kessentini, Adnan Shaout, Patrice Koligheu, Slim Bechikh, Kalyanmoy Deb,\nand Ali Ouni. 2015. Many-Objective Software Remodularization Using NSGA-III. ACM Transactions\non Software Engineering and Methodology 24, 3 (2015), 17.\n\nNaouel Moha, Yann-Gaél Guéhéneuc, Laurence Duchien, and Anne-Francoise Le Meur. 2010. DECOR: A\nMethod for the Specification and Detection of Code and Design Smells. IEEE Transactions on Software\nEngineering 36, 1 (Jan 2010), 20-36.\n\nNaouel Moha, Amine Mohamed Rouane Hacene, Petko Valtchev, and Yann-Gaél Guéhéneuc. 2008. Refactor-\nings of Design Defects Using Relational Concept Analysis. In Formal Concept Analysis, Raoul Medina\nand Sergei Obiedkov (Eds.). Lecture Notes in Computer Science, Vol. 4933. Springer Berlin Heidelberg,\n289-304.\n\nEmerson Murphy-Hill and Andrew P Black. 2008. Breaking the barriers to successful refactoring. In 30th\nInternational Conference on Software Engineering (ICSE). 421-430.\n\nEmerson Murphy-Hill and Andrew P Black. 2010. An interactive ambient visualization for code smells. In\n5th international symposium on Software visualization (VISSOFT). ACM, 5-14.\n\nEmerson Murphy-Hill and Andrew P Black. 2012. Programmer-friendly refactoring errors. IEEE Transac-\ntions on Software Engineering 38, 6 (2012), 1417-1431.\n\nEmerson Murphy-Hill, Chris Parnin, and Andrew P Black. 2012. How we refactor, and how we know it.\nIEEE Transactions on Software Engineering 38, 1 (2012), 5-18.\n\nStas Negara, Nicholas Chen, Mohsen Vakilian, Ralph E. Johnson, and Danny Dig. 2013. A Comparative\nStudy of Manual and Automated Refactorings. In 27th European Conference on Object-Oriented Pro-\ngramming (ECOOP). 552-576.\n\nMel O Cinnéide, Laurence Tratt, Mark Harman, Steve Counsell, and Iman Hemati Moghadam. 2012. Ex-\nperimental assessment of software metrics using automated refactoring. In International Symposium\non Empirical Software Engineering and Measurement (ESEM). 49-58.\n\nMark O’Keeffe and Mel O Cinnéide. 2008. Search-based refactoring for software maintenance. Journal of\nSystems and Software 81, 4 (2008), 502-516.\n\nWilliam F Opdyke. 1992. Refactoring: A program restructuring aid in designing object-oriented application\nframeworks. Ph.D. Dissertation. PhD thesis, University of Illinois at Urbana-Champaign.\n\nFernando EB Otero, Colin G Johnson, Alex A Freitas, and Simon J Thompson. 2010. Refactoring in auto-\nmatically generated programs. In 2nd International Symposium on Search Based Software Engineering\n(SSBSE), Massimiliano Di Penta, Simon Poulding, Lionel Briand, and John Clark (Eds.). Benevento,\nItaly.\n\nAli Ouni, Marouane Kessentini, and Houari Sahraoui. 2013. Search-Based Refactoring Using Recorded Code\nChanges. In 17th European Conference on Software Maintenance and Reengineering (CSMR). 221-230.\n\nAli Ouni, Marouane Kessentini, Houari Sahraoui, and Mounir Boukadoum. 2012a. Maintainability defects\ndetection and correction: a multi-objective approach. Automated Software Engineering 20, 1 (2012), 47—\n79.\n\nAli Ouni, Marouane Kessentini, Houari Sahraoui, and Mohamed Salah Hamdi. 2012b. Search-based refac-\ntoring: Towards semantics preservation. In 28th International Conference on Software Maintenance\n(ICSM). 347-356.\n\nAli Ouni, Marouane Kessentini, Houari Sahraoui, and Mohamed Salah Hamdi. 2013. The use of develop-\nment history in software refactoring using a multi-objective evolutionary algorithm. In 15th annual\nconference on Genetic and Evolutionary Computation (GECCO). 1461-1468.\n\nKyle Prete, Napol Rachatasumrit, Nikita Sudan, and Miryung Kim. 2010. Template-based reconstruction of\ncomplex refactorings. In 26th International Conference on Software Maintenance (ICSM). 1-10.\n\nFawad Qayum and Reiko Heckel. 2009. Local Search-Based Refactoring as Graph Transformation. In Ist\nInternational Symposium on Search Based Software Engineering (SSBSE). 43-46.\n\nDonald Bradley Roberts and Ralph Johnson. 1999. Practical analysis for refactoring. Ph.D. Dissertation.\n\nArnon Rotem-Gal-Oz, Eric Bruno, and Udi Dahan. 2012. SOA patterns. Manning.\n\nHouari Sahraoui, Robert Godin, Thieny Miceli, and others. 2000. Can metrics help to bridge the gap between\nthe improvement of 00 design quality and its automation?. In International Conference on Software\nMaintenance (ICSM). 154-162.\n\nVicent Sales, Ricardo Terra, Luis Fernando Miranda, and Marco Tulio Valente. 2013. Recommending\nmove method refactorings using dependency sets. In 20th Working Conference on Reverse Engineering\n(WCRE). 232-241.\n\nGiuseppe Scanniello, Anna D’Amico, Carmela D’Amico, and Teodora D’Amico. 2010. Using the kleinberg\nalgorithm and vector space model for software system clustering. In 18th International Conference on\nProgram Comprehension (ICPC). 180-189.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n', '39:54 A. Ouni et al.\n\nOlaf Seng, Johannes Stammel, and David Burkhart. 2006. Search-based determination of refactorings for\nimproving the class structure of object-oriented systems. In 8th annual Conference on Genetic and Evo-\nlutionary Computation (GECCO). ACM, 1909-1916.\n\nRaed Shatnawi and Wei Li. 2011. An empirical assessment of refactoring impact on software quality using\na hierarchical quality model. International Journal of Software Engineering and Its Applications 5, 4\n(2011), 127-149.\n\nDanilo Silva, Ricardo Terra, and Marco Tulio Valente. 2014. Recommending automated extract method\nrefactorings. In 22nd International Conference on Program Comprehension (ICPC). 146-156.\n\nGustavo Soares, Rohit Gheyi, and Tiago Massoni. 2013. Automated behavioral testing of refactoring engines.\nIEEE Transactions on Software Engineering 39, 2 (2013), 147-162.\n\nLadan Tahvildari and Kostas Kontogiannis. 2003. A metric-based approach to enhance design quality\nthrough meta-pattern transformations. In 7th European Conference on Software Maintenance and\nReengineering (CSMR). 183-192.\n\nFrank Tip and Jens Palsberg. 2000. Scalable Propagation-based Call Graph Construction Algorithms. In\n15th Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA).\n281-293.\n\nNikolaos Tsantalis and Alexander Chatzigeorgiou. 2009. Identification of move method refactoring opportu-\nnities. IEEE Transactions on Software Engineering 35, 3 (2009), 347-367.\n\nNikolaos Tsantalis and Alexander Chatzigeorgiou. 2011. Identification of extract method refactoring oppor-\ntunities for the decomposition of methods. Journal of Systems and Software 84, 10 (2011), 1757-1782.\n\nRaja Vallée-Rai, Etienne Gagnon, Laurie Hendren, Patrick Lam, Patrice Pominville, and Vijay Sundaresan.\n2000. Optimizing Java Bytecode Using the Soot Framework: Is It Feasible? In Compiler Construction,\nDavid A. Watt (Ed.). Lecture Notes in Computer Science, Vol. 1781. Springer Berlin Heidelberg, 18-34.\n\nAtsushi Yamashita and Leon Moonen. 2013. Do developers care about code smells? An exploratory survey.\nIn 20th Working Conference on Reverse Engineering (WCRE). IEEE, 242-251.\n\nMinhaz F Zibran and Chanchal K Roy. 2011. A constraint programming approach to conflict-aware optimal\nscheduling of prioritized code clone refactoring. In 11th International Working Conference on Source\nCode Analysis and Manipulation (SCAM). 105-114.\n\nEckart Zitzler and Lothar Thiele. 1998. Multiobjective optimization using evolutionary algorithms—a com-\nparative case study. In Parallel problem solving from nature-PPSN V. Springer, 292-301.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n']}


---

#### JSON Representation ####
```json
{
    "name": "Example Project",
    "file": "~auto-sgp-refactoring-1.0.1.md",
    "type": "Text/Markdown",
    "version": "1.0.0",
    "date": "2024-09-24",
    "author": "ScriptGPT",
    "goal": "Track project file parsing activities and output refactoring details.",
    "focus": "Refactoring, Analysis, Documentation",
    "tags": [
        "Refactoring",
        "Analysis",
        "Documentation"
    ],
    "commands": [
        {
            "name": "Initialize Project"
        },
        {
            "name": "Run Parsing"
        },
        {
            "name": "Generate Report"
        },
        {
            "name": "Export Data"
        }
    ],
    "instructions": [
        {
            "step": 1,
            "description": "Run the initial analysis on the source folder."
        },
        {
            "step": 2,
            "description": "Process the files and extract data summaries."
        },
        {
            "step": 3,
            "description": "Generate a final report in markdown format."
        }
    ],
    "zen": [
        "Simplicity is the ultimate sophistication.",
        "Code should be written for humans first, then for machines.",
        "Continuous improvement leads to mastery."
    ],
    "articles": {
        "Project": [
            "Introduction to Refactoring",
            "Best Practices for Code Analysis"
        ],
        "Analysis": [
            "Understanding Code Complexity",
            "Improving Software Performance"
        ]
    },
    "parsed_files": [
        {
            "file_name": "S:\\OneDrive\\@Dev\\!GPT\\ScriptGPT\\library\\Refactoring\\Source\\MooijKetemaKlusenerSchuts2020.pdf",
            "time_taken": 13.310996770858765,
            "data_extracted": {
                "text": [
                    "ResearchGate\n\nSee discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/340412930\n\nReducing Code Complexity through Code Refactoring and Model-Based\nRejuvenation\n\nConference Paper - February 2020\n\nDOI: 10.1109/SANER48275,2020.9054823\n\nCITATIONS READS\n6 213\n\n4 authors, including:\n\nSteven Klusener Mathijs Schuts\n\nKlusener Consultancy TNO\n\n8 PUBLICATIONS 22 CITATIONS 27 PUBLICATIONS 176 CITATIONS\nSEE PROFILE SEE PROFILE\n\nAll content following this page was uploaded by Mathijs Schuts on 27 April 2022.\n\nThe user has requested enhancement of the downloaded file.\n",
                    "Reducing Code Complexity through\nCode Refactoring and Model-Based Rejuvenation\n\nArjan J. Mooij, Jeroen Ketema, Steven Klusener\n\nESI (TNO), Eindhoven, The Netherlands\n\nAbstract\u2014Over time, software tends to grow more complex,\nhampering understandability and further development. To reduce\naccidental complexity, model-based rejuvenation techniques have\nbeen proposed. These techniques combine reverse engineering\n(extracting models) with forward engineering (generating code).\nUnfortunately, model extraction can be error-prone, and vali-\ndation can often only be performed at a late stage by testing\nthe generated code. We intend to mitigate the aforementioned\nchallenges, making model-based rejuvenation more controlled.\n\nWe describe an exploratory case study that aims to rejuvenate\nan industrial embedded software component implementing a\nnested state machine. We combine two techniques. First, we\ndevelop and apply a series of small, automated, case-specific\ncode refactorings that ensure the code (a) uses well-known\nprogramming idioms, and (b) easily maps onto the type of\nmodel we intend to extract. Second, we perform model-based\nrejuvenation focusing on the high-level structure of the code.\n\nThe above combination of techniques gives ample opportunity\nfor early validation, in the form of code reviews and testing,\nas each refactoring is performed directly on the existing code.\nMoreover, aligning the code with the type of model we intend to\nextract significantly simplifies the extraction, making the process\nless error-prone. Hence, we consider code refactoring to be a\nuseful stepping stone towards model-based rejuvenation.\n\nI. INTRODUCTION\n\nEmbedded software is typically reused in multiple product\ngenerations, with changes being made for each generation.\nUnfortunately, as observed by Lehman [1], \u201cas an evolving\nprogram is continually changed, its complexity ...\nunless work is done to maintain or reduce it.\u2019 In other words,\nsoftware modernization is required. However, this turns out\nto be challenging. Industrial practitioners [2] indicate hurdles\nsuch as limited knowledge of the software to be modernized,\nwhich, e.g., makes it hard to identify business logic.\n\nMultiple techniques have been proposed to address the\nchallenges related to software modernization, such as code\nrefactoring [3], [4] and model-based rejuvenation [5]-[8], with\nthe latter combining reverse engineering (extracting abstract\nmodels) with forward engineering (generating new code). In\nspite of this, challenges remain. Pizka [9] observes that \u201cthe\nimpact of refactoring is limited if the code base has gone\nastray for a longer period of time.\u201d The risk of introducing\nbugs is also widely recognized [10], and validating correctness\ncan often only be done at a late stage by testing of the changed\nor newly generated code [3]-[5], [7], [8].\n\nIn this paper, we propose a mitigation strategy for the\nchallenges sketched above. The strategy (see Fig. 1) com-\n\nincreases\n\nMathijs Schuts\nPhilips, Best, The Netherlands\n\nbines code refactoring with model-based rejuvenation (Extract-\nTransform-Generate). Specifically, to ensure that the code\n\ne uses well-known programming idioms, and\n\ne easily maps onto the type of model we intend to extract,\nwe first refactor the code in a number of small, case-specific\nsteps. Once done, we apply model-based rejuvenation.\n\nTransform\nModel\nExtact nae Extract Generates Generate\n(askew) 2-7 (aligned) (askew) 9 (aligned)\n= Refactor Refactor ys,\nCode +, Code +, Code % Code\n\nFigure 1. Code refactoring and model-based rejuvenation. The solid arrows\nrepresent our proposed strategy, which combines refactoring and rejuvenation.\nThe dashed arrows represent a more traditional rejuvenation approach that\nlacks explicit refactoring and transformation steps.\n\nAlthough performing a number of small refactoring steps up\nfront may sound counter-intuitive, as we eventually generate\nnew code, the benefits of this approach are four-fold:\n\n1) The refactoring steps give ample opportunity for early\n\nvalidation, as changes can be easily reviewed and tested.\n\n2) Ensuring that the code easily maps onto the type of\nmodel we intend to extract simplifies model extraction.\n\n3) As low-level code fragments have been tested and follow\nan idiomatic style after refactoring, we can re-use them\nas part of the rejuvenated code, allowing us to abstract\nfrom these fragments and focus on the high-level code\nstructure during model extraction.\n\n4) Aligning the refactored code and the extracted model\nmeans that the extracted model has clearly defined\nsemantics in terms of the code.\n\nResearch Method and Contributions: We conducted the\nreported research as an exploratory case study that aimed to\nmodernize an industrial software component (see Sect. II).\nOur main contribution is the formulation of a strategy making\nmodel-based rejuvenation more controlled. We report on the\nstrategy in Sects. III and IV in the context of our case study.\n\nIl. INDUSTRIAL CASE STUDY\n\nOur case study concerns a Controller Area Network (CAN)\nadapter from a large embedded system. The adapter, written in\nC++, was hand coded and implements a nested state machine,\nwith states and substates, that handles incoming messages and\n",
                    "StateEnum StateInit::ReceiveResp(Resp+ pCanMsg) {\nCanModuleStateEnum state = GetState();\nm_iReceivedMsg++;\nif (m_iReceivedMsg == MODULE_STATUS) {\n\nVersionRequest *pNewCanMsg = NULL;\nif (m_bPost) {\n\nm_bPost = ! (pCanMsg->GetPostError ());\n\nt\n9 if (pCanMsg->GetRuntimeError()) {\n10 AddDefectiveControl (pCanMsg->GetControlID());\nli state = FATAL;\n12 } else {\n13 pNewCanMsg = new VersionRequest;\n14 pNewCanMsg->Set ID (m_byID) ;\n15 SendMsg (pNewCanMsg) ;\n\n}\n17. } else {\n\n18 m_iReceivedMsg--;\n19}\n\n20 return (state);\n\n21 }\n\nFigure 2. Coding style of the original code\n\nperforms some data processing. The goal is to modernize\nthe adapter to facilitate the implementation of envisioned\narchitectural changes in future product generations.\n\nThe state machine consists of 14 states implemented using\nthe state pattern [11], with two states inheriting from a single\nabstract, partial state. Each state uses up to 13 variables to\nstore state-specific data. The substates, of which there are 69,\nare distributed over eight states, with the maximum number of\nsubstates of a state being 13. The substates are implemented\nusing enumerated types and their C++ integer representation.\nThere are 27 incoming message types.\n\nThe adapter has evolved over a long period and functions\nreliably, but is considered difficult to understand, maintain, and\nextend. Inspection of the code indicated the use of a number\nof unconventional programming idioms:\n\ne the integer variables representing substates are often\nchanged multiple times during the handling of a single\nmessage and generally hold values that are off-by-one (an\nincrement operation occurs just before every use);\n\ne large non-encapsulated code fragments are used to im-\nplement elementary behavior such as sending messages.\n\nFigure 2 illustrates the coding style. The code handles incom-\ning messages of type Resp in state StateInit and returns\nthe next state. The integer variable m_iReceivedMsg rep-\nresents a substate. Lines 5-16 encode the behavior in substate\nMODULE_STATUS for message type Resp.\n\nIntended Model and Extraction Approach: As the adapter\nimplements a state machine, we would like to obtain a state\nmachine model as part of our rejuvenation effort. To this end,\nwe can either apply automata learning techniques or static code\nanalysis techniques. As the adapter performs some amount of\ndata processing, which automata learning techniques do not\nhandle well, we opted for static code analysis techniques.\n\nIII. IMPROVING PROGRAMMING IDIOMS\n\nAs highlighted above, the CAN adapter uses some un-\nconventional programming idioms, hindering understandabil-\nity. To improve understandability, we began by applying a\nnumber of refactorings that substituted the idioms by more\n\n1 StateEnum $class::$method($$args) {\n\n2 aif (m_iReceivedMsg == $substate) {\n\n3 // REPLACE: S$boolVar = SboolVar && $boolExp;\n4 if (Sboolvar) {\n\n5 Sboolvar = $boolExp;\n\n6 t\n\n7 43\n\n8}\n\nFigure 3. An example refactoring specified in our refactoring language\n\nconventional ones. The applied refactorings were selected\nby hand, and were mostly case specific (e.g., because no\nstandard refactorings exist that can turn an off-by-one substate\nrepresentation into a non-off-by-one representation).\n\nA. Refactoring Technique\n\nAfter manually selecting appropriate refactorings, we want\nto apply them automatically, as this allows for repeatability\nover, e.g., multiple development branches. Moreover, to not\ninterfere with active development, we do not want annotate\nthe source code that is to be refactored to indicate where\nrefactorings should be applied.\n\n1) Refactoring Language: To satisfy the above constraints,\nwe specify our refactorings in a separate language. We do\nnot aim for the language to be fully generic; the language\nshould simply suffice for our industrial case. As the software\ndevelopers we work with are already familiar with C++, the\nlanguage is defined as an extension of C++. This allows the\ndevelopers to review proposed refactorings without having to\nlearn a completely new language.\n\nThe refactoring language consists of several elements that\nexpress which refactoring operations to apply, to which code\nfragments in which contexts, and in which order and how\noften. Figure 3 showcases some of these elements. The refac-\ntoring operation is specified via an annotation (line 3) that\noccurs immediately above the code pattern to which it should\nbe applied (lines 4-6). The context in which the operation\nmay be applied surrounds the operation and pattern (lines 1-\n2 and 7-8). As a convention [8], an identifier starting with a\nsingle $ denotes a placeholder for a single syntactical element\n(expression, function argument, ...), and an identifier starting\nwith $$ denotes a list of placeholders. Although not shown,\nwe use annotations similar to those for refactoring operations\nto order refactorings and express how often they should be\napplied.\n\n2) Refactoring Operations: A refactoring operation either\nspecifies how a certain code fragment should be changed, or\ninstructs the refactoring engine to extract certain values for\nlater use. We can also specify side-conditions, e.g., to indicate\nwhether an operation should be applied when the code being\nrefactored has side-effects.\n\nCode-changing refactoring operations come in two varieties:\n\n\u00ab replacement of concrete syntax patterns (see Fig. 3), and\n\n\u00a2 direct manipulation of the abstract syntax tree (AST).\nIn both cases we build on the C++ parser that is part of the\nEclipse C/C++ Development Tools (CDT)!. Building on this\n\n'http://www.eclipse.org/cdt/\n",
                    "1 StateEnum StateInit::ReceiveResp(Resp* pCanMsg) {\n2  CanModuleStateEnum state = GetState();\n\n3 if (InSubState(MODULE_STATUS)) {\n\n4 m_bPost = m_bPost && !pCanMsg->GetPostError ();\n5 if (pCanMsg->GetRuntimeError()) {\n\n6 AddDefectiveControl (pCanMsg->GetControlID());\n7 state = FATAL;\n\n8\n\n} else {\n9 SendMsg (new VersionRequest (m_byID));\n10 ChangeSubSt ate (VERSION_REQUESTED) ;\nli }\n\n12. }\n\n13 return state;\n\n14 }\n\nFigure 4. Coding style after refactoring\n\nparser side-steps the need to write our own, which is a non-\ntrivial task.\n\nInitially, we focused only on replacement of concrete syntax\npatterns, which software developers find intuitive. However,\nwe observed that some operations are more conveniently\nexpressed at the AST-level, such as:\n\ne inlining methods, to avoid having to specify complete\nmethod bodies;\n\n\u00a2 extracting methods, to avoid having to specify method\nbodies twice (once for creating methods, and once for\nidentifying locations where calls should be introduced);\n\n\u00ab removing unused variables, to avoid having to specify\nwhat \u201cunused\u201d means using concrete code patterns;\n\n\u00ab combining series of assignments to the same variable,\nto avoid having to explicitly specify the multitude of\npossible concrete assignment patterns;\n\n\u00ab moving statements backwards and forwards by swapping\nthem with independent predecessor and successor state-\nments, again to avoid the multitude of possible patterns.\n\nB. Industrial Case Study\n\nAlthough all states and substates of the CAN adapter all\nhave different functionality, their original coding patterns were\nall very similar, and hence they could be refactored in similar\nways. We applied the following refactoring steps in order,\nwhere steps 3-5 depend on steps 1-2:\n\n1) removing dead code, and declaring local variables as late\nas possible in the closest encompassing scope;\n\n2) ensuring that updates to (sub)states occur as late as pos-\nsible and are not mixed with the sending of messages;\n\n3) replacing updates to substates via integer operations\nwith direct assignments of the members of the relevant\nenumerated types, and ensuring these are not off-by-one;\n\n4) encapsulating all operations required for message cre-\nation in message class constructors;\n\n5) making logging homogeneous, and introducing auxiliary\nmethods encapsulating data processing.\n\nThe result of applying the refactorings to the code of Fig. 2\ncan be found in Fig. 4. The first refactoring is obviously\ngeneric, while the others are case specific. All refactorings\naim to improve the understandability of the code.\n\nThe individual refactorings were easily validated (by means\nof code reviews and existing test suites), and integrating the\n\nresults into the code base went smoothly. The refactorings\ncould also easily be adapted to other development branches,\nand the developers of the adapter considered the refactored\ncode to be much easier to understand.\n\nC. What We Learned\n\nWe learned the following while refactoring the adapter:\n\n\u00a2 specifying case-specific refactorings is both about the\noperations and the context in which they are applied;\n\ne specifying refactorings is sometimes done best at the\nconcrete syntax-level, and sometimes at the AST-level;\n\ne case-specific refactorings enable code changes that are\nonly valid for the specific case;\n\ne incrementally applying small, case-specific refactorings\nfacilitates early validation.\n\nIV. REDESIGNING THE HIGH-LEVEL STRUCTURE\n\nThe refactoring steps from the previous section addressed\nthe main issues with the CAN adapter identified in Sect. II.\nHowever, the steps did not improve insight into the overall\nbehavior of the implemented state machine. To improve in-\nsight, we next visualized the state machine by extracting its\nhigh-level structure (again using CDT\u2019s C++ parser), while\nignoring low-level details. The visualization provided some\nnew insights and could be kept up-to-date by regenerating it,\nbut its size was substantial given the number of states and\nsubstates, which still made it difficult to fully comprehend the\nstate machine.\n\nWe next investigated whether we could improve the high-\nlevel structure of the code to reduce the need for a separate\nvisualization. The key observation we made was that the code\nwas structured from the top down (i.e., viewed from class\ndefinitions, via method definitions, to control statements) as\n\nstate + message type \u2014 substate > logic\nwhereas the visualization was structured as\n\nstate + substate > message type \u2014 logic.\nAlthough the top-down structure of the code reduces code\nduplication when incoming messages are handled similarly\nacross multiple substates, the top-down structure of the visu-\nalization is more useful when trying to understand the code\u2019s\nbehavior in terms of the order of operations from entering a\ncertain substate, via processing of various incoming messages,\nto exiting the substate.\n\nTo transform the top-down code structure to match the one\nused in the visualization, we first created a model by extracting\nthe high-level structure of the code. Thereafter, we transformed\nthe model to give it the appropriate top-down structure, and\nwe generated new code by combining the transformed model\nwith low-level code fragments that we had retained.\n\nA. Rejuvenation Technique\n\nBuilding again on CDT\u2019s C++ parser, we parse the relevant\nsource files without expanding any #include directives. We\nthen extract a high-level model that is close to the structure\nof the original source code, using pattern matching on both\nconcrete syntax patterns and AST patterns. We retain low-level\n",
                    "code details by saving relevant code fragments along with the\nhigh-level model, as proposed by [8].\n\nTo validate the extraction and give the extracted model\nsemantics, we develop a code generator in Xtend? that is able\nto regenerate the original code (apart from formatting) from\nthe high-level model and the retained low-level fragments.\nThereafter, we transform the model to obtain the desired high-\nlevel structure, and use Xtend to generate new code from\nthe transformed model and the retained low-level fragments.\nLastly, we use CDT\u2019s code formatting capabilities to format\nthe new code (to nicely integrate the retained fragments).\n\nB. Industrial Case Study\n\nOnce we started to extract a state machine model from the\nCAN adapter code, we realized that although we did improve\nthe structure of the code while refactoring, extraction was not\nas straightforward as it could be. Therefore, before proceeding,\nwe applied a number of additional refactorings:\n\ne introducing explicit state assignments in simple cases\n\nwhere the state does not change (for homogeneity);\n\n\u00a2 replacing if-statements with guards containing a disjunct\n\nwith an InSubState call by a chain of if-then-else\nstatements (to isolate InSubState);\n\n\u00a2 replacing if-statements with guards containing a conjunct\n\nwith an InSubState call by nested if-statements (again\nto isolate InSubState);\n\n\u00a2 moving calls to the InSubState method out of helper\n\nfunctions, by inlining fragments of these functions.\nThe above refactorings were geared towards making the state\nmachine, and hence the high-level structure, more explicit. We\ndid not perform these steps earlier, as they somewhat increased\nthe number of lines of code.\n\nOnce we applied the above refactorings, model extraction\nwas straightforward, with the inheritance from partial states\nbeing the only source of complexity. After extraction we\ntransformed the model to match the top-down structure of our\nvisualization, and generated new code.\n\nFigure 5 presents the result of applying the above steps to\nthe code of Fig. 4. The depicted method handles all messages\nthat can be received in substate MODULE_STATUS of state\nStateInit. Of course, as the steps changed the high-\nlevel structure, their impact is difficult to gauge from code\nfragments only. However, the adapter\u2019s developers considered\nthe code much better to understand even without a separate\nvisualization, although code size increased slightly.\n\nC. What We Learned\n\nWe learned the following while rejuvenating the adapter:\n\ne aligning the original code with the type of model we in-\ntend to extract simplifies the extraction, and also enables\nearly validation through easy regeneration of the code;\n\n\u00a2 retaining low-level code fragments and linking these to\nthe extracted model allows us to focus on the high-level\nbehavior in the model;\n\n?https://www.eclipse.org/xtend/\n\n1 StateEnum StateInit::ModuleStatus (Msg+ pMsg) {\n\n2  CanModuleStateEnum state = GetState();\n\n3 if (dynamic_cast<Resp*>(pMsg) != NULL) {\n\n4 Resp* pCanMsg = dynamic_cast<Resp*>(pMsg) ;\n\n5 m_bPost = m_bPost && !pCanMsg->GetPostError ();\n6 if (pCanMsg->GetRuntimeError()) {\n\n7 AddDefect iveCont rol (pCanMsg->GetCont rolID());\n8 state = FATAL;\n\n9 } else {\n\n10 SendMsg (new VersionRequest (m_byImageID) );\n\n11 ChangeSubState (VERSION_REQUESTED) ;\n\n12 }\n\n13}\n\n14 return state;\n\n15 }\n\nFigure 5. Coding style after refactoring and rejuvenation\n\ne code refactoring can act as a useful stepping stone to-\nwards model-based rejuvenation;\n\ne changing the high-level code structure can reduce the\nneed for separate visualizations.\n\nV. THREATS TO VALIDITY\n\nThreats to internal validity come from the way in which we\ncarried out our case study. Our study focused on qualitative\naspects and ignored quantitative aspects. The time to create\nthe case-specific refactoring steps was not considered.\n\nThreats to construct validity come from the way in which\nwe evaluated our case study. We did not attempt to transform\nthe considered adapter using different approaches.\n\nThreats to external validity come from the way in which\nour results will generalize to other software components. Our\ncase study only considered a single adapter. We believe that\nthe presented approach could have helped in case studies such\nas [8], but more research is needed to determine its generality.\n\nVI. RELATED WORK\n\nWe discuss various avenues of related work.\n\nA. Model-Based Methods for Software Modernization\n\nThe literature review from [12] compares in detail fifteen\ndifferent model-driven reverse engineering approaches, and\nobserves that the approaches and applications are diverse.\n\nIndustrial case studies on model-based rejuvenation are\npresented in [5], [8]. The study from [5] employs models\nbased on generic concepts such as data structures, algorithms,\nand GUI elements. Any source element that does not easily\nfit the model is reported to the user. The study from [8]\nemploys domain-specific models, and inspired us to retain low-\nlevel code fragments during rejuvenation. Contrary to us, the\nauthors of [8] do not first attempt to align the code with the\ntype of model they intend to extract.\n\nAn industrial case that combines model extraction and\ncode refactoring is presented in [13]. Architectural models\nare extracted to give insight into code structure, after which\ndesired model changes are specified. Next, case-specific code\nrefactorings are derived, but no rejuvenation is performed.\n",
                    "B. Code Refactoring Tools\n\nMost refactorings are performed in batches [14], and\ndeveloper-specified refactorings are sometimes seen as the\nHoly Grail of refactoring [15]. Unfortunately, tooling for\ndeveloper-specified refactorings is currently lacking [10].\n\nFrameworks for specifying refactorings generally only allow\nrefactorings to be specified at the AST-level. Examples of\nsuch frameworks are Clang\u2019s C++ refactoring engine*, the\nJava refactoring engine presented in [16], and the MoDisco\nframework [17], which has been used to perform large-scale\nJava refactorings.\n\nSpecifying C/C++ refactorings using concrete syntax pat-\nterns is supported by a limited number of tools. Coccinelle [18]\nenables developers to define C refactorings, and Rascal [19]\nsupports C/C++ via its ClaiR* module [20]. DMS [21] and\nTXL [22], which focus on program transformations including\nrestructuring, also support concrete C/C++ syntax patterns, and\nhave been used in commercial applications.\n\nC. Extracting State Machine Models from Code\n\nTo enhance insight in software, [23] describes a method\nfor extracting visual representations of state machines imple-\nmented in C. The method is based on matching specific im-\nplementation patterns. Nested state machines and conditional\nstate transitions are left as future work, although the authors do\ngive examples that exhibit a similar kind of top-down structure\nas the state machine from our case study (see Sect. IV).\n\nAn industrial case study on extracting flat state machine\nmodels from C code is described in [24]. The approach taken\nin the study is similar to ours, namely, recognizing occurrences\nof specific implementation patterns that are used consistently\nin the considered code base. Semi-automated techniques for\nextracting state machines from C code that does not follow\nspecific implementation patterns are presented in [25] in the\ncontext of embedded control software.\n\nVII. CONCLUSIONS AND FUTURE WORK\n\nAs we have shown, code refactoring can be used to make\nmodel-based rejuvenation a more controlled software modern-\nization technique. Code refactoring allows for early validation,\nand can be used to simplify the extraction process.\n\nIn future work we would like to develop more sophisti-\ncated techniques for specifying, applying, and validating case-\nspecific refactoring operations, to allow for easier refactoring\nof code. We would also like to establish whether refactoring\nafter rejuvenation could be beneficial.\n\nACKNOWLEDGMENTS\n\nThis research was carried out as part of the Vivace program\nunder the responsibility of ESI (TNO) with Royal Philips as\ncarrying industrial partner. The Vivace program is supported\nby the Netherlands Organisation for Applied Scientific Re-\nsearch TNO.\n\n3https://clang.Ilvm.org/docs/RefactoringEngine.html\n4https://github.com/ewi-swat/clair\n\nWe would like to thank Peter Blom and Roel Kolman of\nPhilips for their technical support and their help integrating\nthe modernized code.\n\nREFERENCES\n\n1] M. M. Lehman, \u201cPrograms, life cycles, and laws of software evolution,\u201d\nProceedings of the IEEE, vol. 68, no. 9, pp. 1060-1076, 1980.\n\n2] R. Khadka, B. V. Batlajery, A. Saeidi, S. Jansen, and J. Hage, \u201cHow do\nprofessionals perceive legacy systems and software modernization?\u201d in\nICSE\u201914, 2014, pp. 36-47.\n\n3] M. Fowler, Refactoring: Improving the Design of Existing Code.\nAddison-Wesley Professional, 1999.\n\n4] M. C. Feathers, Working Effectively with Legacy Code.\n2004.\n\n5] F. Fleurey, E. Breton, B. Baudry, A. Nicolas, and J.-M. J\u00e9z\u00e9quel,\n\u201cModel-driven engineering for software migration in a large industrial\ncontext,\u201d in MoDELS\u201907, 2007, pp. 482-497.\n\n6] A. J. Mooij, G. Eggen, J. Hooman, and H. van Wezep, \u201cCost-\neffective industrial software rejuvenation using domain-specific models,\u201d\nin ICMT\u2019I5, 2015, pp. 66-81.\n\n7) A. J. Mooij, M. M. Joy, G. Eggen, P. Janson, and A. Radulescu, \u201cIn-\ndustrial software rejuvenation using open-source parsers,\u201d in JCMT\u201916,\n2016, pp. 157-172.\n\n8] S. Klusener, A. J. Mooij, J. Ketema, and H. van Wezep, \u201cReducing code\nduplication by identifying fresh domain abstractions,\u201d in JCSME\u201918,\n2018, pp. 569-578.\n\n9] M. Pizka, \u201cStraightening spaghetti-code with refactoring?\u201d in SERP\u201904,\n2004, pp. 846-852.\n\n10] M. Kim, T. Zimmermann, and N. Nagappan, \u201cAn empirical study of\nrefactoring challenges and benefits at Microsoft,\u201d JEEE Trans. Software\nEng., vol. 40, no. 7, pp. 633-649, 2014.\n\n11] E. Gamma, R. Helm, R. Johnson, and J. Vlissides, Design Patterns:\nElements of Reusable Object-Oriented Software. Addison-Wesley,\n1995.\n\n12] C. Raibulet, F. A. Fontana, and M. Zanoni, \u201cModel-driven reverse\nengineering approaches: A systematic literature review,\u201d IEEE Access,\nvol. 5, pp. 14516-14542, 2017.\n\n13] D. Dams, A. J. Mooij, P. Kramer, A. Radulescu, and J. Vanhara, \u201cModel-\nbased software restructuring: Lessons from cleaning up COM interfaces\nin industrial legacy code,\u201d in SANER\u201918, 2018, pp. 552-556.\n\n14] E. R. Murphy-Hill, C. Parnin, and A. P. Black, \u201cHow we refactor, and\nhow we know it,\u201d JEEE Trans. Software Eng., vol. 38, no. 1, pp. 5-18,\n2012.\n\n15] R. M. Fuhrer, M. Keller, and A. Kiezun, \u201cAdvanced refactoring in the\nEclipse JDT: past, present, and future,\u201d in WRT\u201907, 2007, pp. 30-31.\n16] M. Schiafer and O. de Moor, \u201cSpecifying and implementing refactor-\nings,\u201d in OOPSLA\u201910, 2010, pp. 286-301.\n\n17] H. Bruneli\u00e9re, J. Cabot, G. Dup\u00e9, and F. Madiot, \u201cMoDisco: A model\ndriven reverse engineering framework,\u201d Information & Software Tech-\nnology, vol. 56, no. 8, pp. 1012-1032, 2014.\n\n18] Y. Padioleau, J. L. Lawall, and G. Muller, \u201cUnderstanding collateral\nevolution in Linux device drivers,\u201d in EuroSys\u201906, 2006, pp. 59-71.\n\n19] P. Klint, T. van der Storm, and J. J. Vinju, \u201cRASCAL: A domain specific\nlanguage for source code analysis and manipulation,\u201d in SCAM\u201909, 2009,\npp. 168-177.\n\n20] R. Aarssen, J. J. Vinju, and T. van der Storm, \u201cConcrete syntax with\nblack box parsers,\u201d The Art, Science, and Engineering of Programming,\nvol. 3, no. 3, 2019.\n\n21) I. D. Baxter, C. W. Pidgeon, and M. Mehlich, \u201cDMS\u00ae: Program\ntransformations for practical scalable software evolution,\u201d in ICSE\u201904,\n2004, pp. 625-634.\n\n22] J. R. Cordy, T. R. Dean, A. J. Malton, and K. A. Schneider, \u201cSource\ntransformation in software engineering using the TXL transformation\nsystem,\u201d Information & Software Technology, vol. 44, no. 13, pp. 827\u2014\n837, 2002.\n\n23] S.S. Som\u00e9 and T. Lethbridge, \u201cEnhancing program comprehension with\nrecovered state models,\u201d in JWPC\u201902, 2002, pp. 85-93.\n\n24] M.G. J. van den Brand, A. Serebrenik, and D. van Zeeland, \u201cExtraction\nof state machines of legacy C code with Cpp2XMI,\u201d in BENEVOL\u201908,\n2008, pp. 28-30.\n\n25] W. Said, J. Quante, and R. Koschke, \u201cOn state machine mining from\nembedded control software,\u201d in ICSME\u201918, 2018, pp. 138-148.\n\nPrentice Hall,\n\n"
                ]
            }
        },
        {
            "file_name": "S:\\OneDrive\\@Dev\\!GPT\\ScriptGPT\\library\\Refactoring\\Source\\Analysis_of_Code_Refactoring_Impact_on_Software_Qu.pdf",
            "time_taken": 13.480034351348877,
            "data_extracted": {
                "text": [
                    "MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\nAnalysis of Code Refactoring Impact on Software Quality\n\nAmandeep Kaur\u2018 and Manpreet Kaur*\n\n\u2018Computer Science and Engg. Department, Punjab Technical University, Jalandhar, India, amansidhu1092@gmail.com\n2Computer Science and Engg. Department, Punjab Technical University, Jalandhar, India, manpreet.kaur@bbsbec.ac.in\n\nAbstract. Code refactoring is a \u201cTechnique used for restructuring an existing source code, improving its internal\nstructure without changing its external behaviour\u201d. It is the process of changing a source code in such a way that it\ndoes not alter the external behaviour of the code yet improves its internal structure. It is a way to clean up code that\nminimizes the chances of introducing bugs. Refactoring is a change made to the internal structure of a software\ncomponent to make it easier to understand and cheaper to modify, without changing the observable behaviour of that\nsoftware component. Bad smells indicate that there is something wrong in the code that have to refactor. There are\ndifferent tools that are available to identify and remove these bad smells. It is a technique that change our source code\nin a more readable and maintainable form by removing the bad smells from the code. Refactoring is used to improve\nthe quality of software by reducing the complexity. In this paper bad smells are found and perform the refactoring\nbased on these bad smell and then find the complexity of program and compare with initial complexity. This paper\nshows that when refactoring is performed the complexity of software decrease and easily understandable.\n\n1 Introduction\n\n1.1 Refactoring\n\nRefactoring is a change made to the internal structure of a\nsoftware component to make it easier to understand and\ncheaper to modify, without changing the observable\nbehaviour of that software component [1]. This definition\nis attractive because it not only defines what refactoring\nis, but also why refactoring are performed. Refactoring\nare performed to improve the functional factorings of a\nsoftware system in order to increase understand ability\nand modifiability.\n\nRefactoring is the process of changing a software system\nin such a way that it does not alter the external behaviour\nof the code yet improves its internal structure. It is a way\nto clean up code that minimizes the chances of\nintroducing bugs.\n\nCode refactoring is a \u201cdisciplined technique for\nrestructuring an existing body of code, altering its\ninternal structure without changing its external\nbehaviour\u201d, undertaken in order to improve some of the\nnon-functional attributes of the software. Typically, this\nis done by applying a series of \u201cRefactoring\u201d, each of\nwhich is a (usually) tiny change in a computer program\u2019s\nsource code that does not modify its conformance to\nfunctional requirements. Advantages include improved\ncode readability and reduced complexity to improve the\nmaintainability of the source code, as well as a more\n\nAmandeep Kaur: amansidhu1092@gmail.com\n\nexpressive internal architecture or object model to\nimprove extensibility.\n\nRefactoring is the process of taking existing code and\nchanging it in some way to make it more readable and\nperhaps less complex. The key thing to note, is that when\nthe code is changed, is does not affect the underlying\nfunctionality of the code itself. So the changes you\u2019re\nmaking are literally just to make the code easier.\n\nManual refactoring are often error-prone and time-\nconsuming [2]. For instance, renaming a method requires\nchecking that the method\u2019s new name is not yet in use as\nwell as updating all invocations. Besides being obviously\ntime-consuming this operation is also error-prone because\npolymorphism may cause a forgotten update to compile\ncorrectly but change the software\u2019s behaviour\ninadvertently. This requires the maintainer to manually\nlocate the changed functionality and update the omitted\ninvocation. As a result, these refactoring are often not\nperformed and the software\u2019s structure deteriorates as a\nresult of functional changes.\n\nAutomated refactoring tools can reduce these problems.\nIf performed by a tool that can guarantee that the\nrefactoring it performs are behaviour-preserving, the\nerror-prone aspect is mitigated. Furthermore, performing\nthe required analysis automatically can drastically reduce\nthe time required to perform refactoring. Automated\nrefactoring is made possible through the use of\npreconditions that if satisfied guarantee that a refactoring\nis behaviour-preserving [3]. Also, composing larger\n\n\u00a9 The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons Attribution\n\nLicense 4.0 (http://creativecommons.org/licenses/by/4.0/).\n",
                    "MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\nrefactoring from smaller ones means that a relatively\nsmall set of automated refactoring can be used to perform\na large set of refactoring.\n\nWhen performed by a tool, refactoring consists of at least\ntwo steps [2]. The first is analysis, where the program to\nbe refactored is analyzed in order to determine whether\nthe desired refactoring\u2019s preconditions are satisfied. If\nthis is the case, the second step is executed, the actual\ntransformation of the program source code. Both steps\nmust take both the syntax and semantics of the\nprogramming language the tool supports into\nconsideration, making it a considerable effort to\nimplement a refactoring tool from scratch. This may\nexplain why refactoring tools are often integrated with\nother development tools such as IDEs, since these\ntypically expose a large part of this required functionality\nwhich can be reused by the refactoring tool. No matter\nhow complex a refactoring tool is, in theory it only needs\nto be implemented once for each programming language\nwhere automated refactoring functionality is desired and\nthen evolved along with the language it supports. Given\nthe large developer communities and companies backing\nthe most popular programming languages, several\nrefactoring tools are readily available.\n\n1.2 Bad smell\n\nBad smells indicate that there is something wrong in the\ncode that we have to refactor. Bad smells are design\nflaws in the code. There are many tools that are available\nto identify the bad smells and remove these bad smells by\nusing refactoring tools and by using refactoring\ntechnique. Refactoring is a technique that restrict our\nsource code in a more readable and maintainable form by\nremoving the bad smells from the code. Refactoring does\nnot change the external behaviour of software[7].\n\nBad smells are potential problems that can be remove\nthrough refactoring. There are various kind of bad smells\nthat make our source code difficult to understand and\nmodify. Bad in a code is not any problem but may lead to\nany mistake in future. We can detect and refactor this bad\nsmells through various tools. Jdeodorant are such kind of\ntools that are used to detect the bad smells [7]. Bad smells\nare of following types:\n\n* LARGE CLASS- means a class that is too large. Size\nof the large class is too much large. This type of class is\ndifficult to understand and it is too much hard to\nunderstand that which functioning is performed by this\nclass.\n\n* LONG METHOD- is a method that is too long. Long\nmethods are same as large classes. Long methods also\nleads to confusion for the new developer and these are\nalso very much difficult to understand.\n\n* DUPLICATE CODE- is a code that is repeated at too\nmany places in a same source code. The problem arises\nwhen we try to update this code. We have updated the\nduplicate code at all the places in which this code is\nplaced. If we forget to update the code on single place, it\nmay create problem. Hence duplicate code is difficult to\nmaintain.\n\n* FEATURE ENVY - is a bad smell that violates the\nprinciple of its class in a source code. From many\ndiscussion we found that this a bad smell that is not\ninterested to use its own source class but interested to use\nany another source class.\n\nAll these bad smells can be clean up by using the\nrefactoring.\n\n1.3 Refactoring Techniques\n\nRefactoring Technique is used to remove the bad smells\nfrom code and make code clean. Refactoring is a set of\ntechniques, procedures and steps to keep your source\ncode as clean as possible. Clean code, on the other hand,\ndescribes how well-written code should look in ideal\nconditions. In a pragmatic sense, all refactoring\nrepresents simple steps toward clean code. There are\nsome basic techniques that are used for refactoring[7]-\n\n* EXTRACT METHOD:- Extract method means\nextract a method that appear at many places in the source\ncode and place it in a different method.\n\n* INLINE METHOD:-Its working is totally opposite to\nthe extract method. A method of body is as clear as its\nname. Put the body of the method into the body of its\ncaller. Then remove the method.\n\n* MOVE METHOD:-Move method means moving the\nmethod from one class to another when a class has too\nmany functions to do.\n\n+ INLINE CLASS:-This method is applicable to those\nclasses which are not doing too much work. With the\nhelp of inline class we can move the functionality of this\nclass to another class and remove the class.\n\n* RENAME METHOD:-Rename method simply\nmeans renames any method. We can rename the method\naccording to the functionality of the method. It makes our\ncode easier to understand.\n\n* REPLACE ARRAY WITH OBJECT:-If we have\nmany or different elements. We can replace this array\nwith an object, in this object each element have specific\nfield.\n\nRefactoring technique is used to make code clean . After\nfinding the bad smell in code then we apply the respected\nrefactoring to remove the bad smell in code. After\nperforming the refactoring the code complexity is\ndecrease and it is easily understandable by anyone.\n\n1.4 Refactoring Tool\n\nECLIPSE:- Eclipse is an IDE(Integrated Development\nEnvironment). The Eclipse platform which provides the\nfoundation for the Eclipse IDE is composed of plug-ins\nand is designed to be extensible using additional plug-ins.\nDeveloped using Java, the Eclipse platform can be used\nto develop rich client applications, integrated\ndevelopment environments and other tools. Eclipse can\nbe used as an IDE for any programming language for\nwhich a plug-in is available. In it there is a workspace\n",
                    "MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\nand an extensible plug-in for customizing the\nenvironment. Written in java language it is used to\ndevelop application. Eclipse may also helpful to create\nthe application in C, C++, PHP and COBOL and many\nmore languages. Eclipse also provide an option to\nrefactor the source code. There are many plug-in for\neclipse that can used to detect the bad smells in the code.\nTo refactor the bad smells it provides various refactoring\ntechniques.\n\nIn order to increase the availability of refactoring tools\nfor DSLs it is worthwhile to investigate whether\nrefactoring tool implementation can be simplified in\norder to be able to develop refactoring tools (for DSLs)\nfaster or even generate them. To this end, it is useful to\nexamine the implementation of existing refactoring tools\nin order to gain insight into the complexities of their\ndevelopment. Eclipse is a suitable candidate for such an\nexamination, since it is a very widely used IDE and it\nsupports a large set of refactoring out-of-the-box,\nespecially in the JDT, the Java Development Tooling\nplug-in[18].\n\nPlugins-\n\nJDeodorant is an Eclipse plugin that are used to detect the\nbad smells. JDeodorant was simplistic in both design and\nusage. It support 4 type of bad smells-God class, Long\nmethod, Type checking, Feature envy. JDeodorant that\nautomatically identifies the Feature Envy, God Class,\nLong Method and Switch Statement (in its Type\nChecking variant) code smells in Java programs. This\nplugin is used to find the bad smells in the software.\n\nEMF Refactor are such kind of tool that are used to\nperform the refactoring on code. It support various type\nof refactoring like Rename Method/Class, Extract\nMethod/Class, Inline Method , Move ,Replace etc. This\nplugin is used to remove the bad smells in the software.\n\nEclipse Metrics Plugin are such kind of tool that are used\nto calculate the various type of metrics in the program. It\ncalculate Lines of code, No. of attributes, No. of Classes,\nNo. of Methods, Cohesion, Coupling, McCabe\nCyclomatic Complexity, Weighted methods per class.\nThis plugin is used to find the complexity of software to\ndetect the quality of software.\n\n2 Literature Survey\n\nFowler et al., [1] say that, Refactoring is basically the\nobject-oriented variant of restructuring: \u201cthe process of\nchanging a software system in such a way that it does not\nalter the external behavior of the code, yet improves its\ninternal structure\u201d. They outlines four different occasions\nwhen a programmer should refactor. First, when code is\nduplicated for the second time, a programmer should\nfactor out the duplication. Second, they should refactor\nwhen functionality needs to be added, but the existing\ncode is hard to understand or the addition is not easy to\nmake because of the existing design. Third, they should\nrefactor when a bug needs to be fixed and refactoring the\ncode will help make the code clearer and expose the bug.\n\nFinally, the should refactor when programmers are doing\na code review and refactoring will immediately produce\ncode that everyone understands.\n\nMartin Fowler[1] discusses in his paper that how\nrefactoring improve the existing design. He introduce\ndeeply about refactoring in his paper. This paper also\ndiscuss more about the bad smells. How to detect these\nsmells and tells that which refactoring technique is\napplicable to remove this bad smell. Martin also\nintroduce that a code that have bad smells are hard to\nmaintain and hard to modify. A bad smell is an indication\nof some problem in the code, which requires refactoring\nto deal with. Many tools are available here for detection\nand refactoring of these code smells. These tools vary\ngreatly in detection methodologies and acquire different\ncompetencies. In this work, we studied different code\nsmell detection tools minutely and try to comprehend our\nanalysis by forming a comparative of their features and\nworking scenario. We also extracted some suggestions on\nthe bases of variations found in the results of both\ndetection tools. This helps us to select the tool to refactor\nthe code.\n\nSandeep Kaur[7], says that, Bad smells indicate that there\nis something wrong in the code that we have to refactor.\nBad smells are design flaws in the code. There are many\ntools that are available to identify the bad smells and\nremove these bad smells by using refactoring tools and\nby using refactoring technique. Refactoring is a technique\nthat restrict our source code in a more readable and\nmaintainable form by removing the bad smells from the\ncode. Refactoring does not change the external behaviour\nof software. In this paper we discussed about tools and\ntechniques to refactor the source code.\n\nRoberts [2] says that, Refactoring consists of at least two\nsteps, The first is analysis, where the program to be\nrefactored is analyzed in order to determine whether the\ndesired refactoring\u2019s preconditions are satisfied. If this is\nthe case, the second step is executed, the actual\ntransformation of the program source code. Both steps\nmust take both the syntax and semantics of the\nprogramming language the tool supports into\nconsideration, making it a considerable effort to\nimplement a refactoring tool from scratch. This may\nexplain why refactoring tools are often integrated with\nother development tools such as IDEs, since these\ntypically expose a large part of this required functionality\nwhich can be reused by the refactoring tool.\n\nMealy and Strooper [4] say that, Refactoring is a method\nwhich used to rearrange and modify the existing code in a\nway that the intentional behaviour of code stays the same.\nRefactoring allows to simplify and improve both\nperformance and readability of your code. One of the key\nissue in software refactoring is source code that should be\nrefactored. It should be implemented by the object\noriented programs. Kent beck and Martin fowler calls\nthem Bad smells, indicating that some part of the source\ncode are terrible. Sometimes in other words bad smells\nare assigned as the duplicate code. Duplicate code is a\ncomputer program sequence of source code that occurs\n",
                    "MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\nmore than once. Improving the design that often includes\nremoving duplicate code .\n\nRoberts D. Brant [2] says that refactoring is provided as a\nprogram transformation that has a precondition and a\npost-condition that a program must satisfy for the\nrefactoring to be easily applied. Each program is thought\nto have a specification for it and that specification is\nsatisfied (or unsatisfied) by a test suite. A refactoring is\ntherefore behaviour preserving if it satisfies the original\ntest suite. If a new component is added to the program,\nthe program must satisfy the original test suite plus any\nadditional tests. When satisfying the original test suite,\none must recognize that this is the conceptual original test\nsuite that is satisfied.\n\nKumar and Chanaky[20] say that A, Code and design\nsmells are the indicators of potential problems in code.\nThey may obstruct the development of a system by\ncreating difficulties for developers to fulfil the changes.\nDetecting and resolving code smells, is time-consuming\nprocess. Many number of code smells have been\nidentified and the sequences through which the detection\nand resolution of code smells are operated rarely because\ndevelopers do not know how to rectify the sequences of\ncode smells. Refactoring tools are used to facilitate\nsoftware refactoring and helps the developers to\nrestructure the code. Refactoring tools are passive and\nused for code smell detection. Few refactoring tools\nmight result in poor software quality and delayed\nrefactoring may lead to higher refactoring cost. A\nRefactoring Framework is proposed which instantly\ndetects the code smells and changes in the source code\nare analyzed by running a monitor at the background. The\nproposed framework is evaluated on different non trivial\nopen source applications and the evaluation results\nsuggest that the refactoring framework would help to\navoid code smells and average life span of resolved\nsmells can be reduced.\n\nElish [22] says that, Refactoring to patterns allows\nsoftware designers to safely move their designs towards\nspecific design patterns by applying multiple low-level\nrefactorings. There are many different refactoring to\npattern techniques, each with a particular purpose and a\nvarying effect on software quality attributes. Thus far,\nsoftware designers do not have a clear means to choose\nrefactoring to pattern techniques to improve certain\nquality attributes. This paper takes the first step towards a\nclassification of refactoring to pattern techniques based\non their measurable effect on software quality attributes.\nThis classification helps software designers in selecting\nthe appropriate refactoring to pattern techniques that will\nimprove the quality of their design based on their design\nobjectives. It also enables them to predict the quality drift\ncaused by using specific refactoring to pattern techniques.\n\n3 Problem Formulation\n\nProducing software is a very complex process that takes a\nconsiderable time to evolve. Poorly designed software\nsystems are difficult to understand and maintain.\nSoftware maintenance can take up to 50% of the overall\n\ndevelopment costs of producing software. One of the\nmain attributes to these high costs is poorly designed\ncode, which makes it difficult for developers to\nunderstand the system even before considering\nimplementing new code. In the context of software\nengineering process, Software Refactoring has a direct\ninfluence on reducing the cost of software maintenance\nthrough changing the internal structure of the code,\nwithout changing it external behaviour. Refactoring is a\ntechnique for restructuring an existing body of code.\n\n. Code is not easily maintainable and difficult to\nunderstand.\n\n. Design of program is more complex and difficult to\nfind bugs in the program.\n\n. Code review is more time consuming.\n\nTo remove these problems in our java program we\nperform refactoring using Eclipse Tool. After performing\nrefactoring, the internal structure of program is modified\nand its external behaviour remains same.\n\nModifications and Improvements\n. After refactoring, code is easy to understand.\n\n. Refactored code is easily maintainable and reduce\nthe code maintenance cost.\n\n. Refactored code is of better quality and reduce the\nchance of introducing bugs.\n\nObjectives\n\n. To find the code smells.\n\n. To remove bad smells in code.\n\n. To improve the quality of code.\n\n. To improve code readability and reduced\ncomplexity.\n\n. To make software easy to understand.\n\n. To reduce the maintenance cost.\n\n4 Research Methodology\n\nThis section gives description of step involved for\nRefactoring-\n\n1. Run the program to check its external behaviour\nand ensure that it is unchanged after refactoring.\n\n2. Before applying any single refactoring, calculate the\ncomplexity of program.\n\n3. Identify where the code should be refactored-\nDetermine which refactoring should be applied to the\nidentified places.\n",
                    "MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\n4. Make a small change- a single refactoring without\nchanging the outer behaviour of the code.\n\n5. Test Refactored code if everything work\u2019s, move on\nto the next refactoring.\n\n6. If fails rollback the last smaller change and repeat\nthe refactoring in a different way.\n\n7. After applying all the refactoring technique,\nCalculate the complexity to determine the impact of\nrefactoring on Quality.\n\n5 Experiment\n\nThe experiment is done by using the eclipse tool and\njdeodrant plugin as bad smell detection tool and eclipse\nmetrics plugin as to calculate the complexity of source\ncode. We take an source code of library software which is\nwritten in Java.\n\nBad Jdeodoran | Refactoring technique that\nsmells t can be applied\n\nFeature | Yes Move method\n\nenvy\n\nType Yes Replace\n\ncheckin replace type code with state\ng\n\nLarge Yes Move Method\n\nclass\n\nLong Yes Extract Method\n\nMethod\n\nComparison between Before and After Refactoring\n\nBefore refactoring, we can find two types of bad smells\nin our project and the initial complexity is calculate.\nWhen bad smells find in our project, then we remove\nthese bad smells by applying the appropriate techniques\nof refactoring. After applying all type of refactoring then\nagain we calculate the complexity and compare it with\ninitial complexity and check the difference between these\ntwo. Complexity can also be measure in the form of\nMcCabe Cyclomatic Complexity, Weighted Methods per\n\nclass(WMC), Lack of Cohesion of Methods(LCOM),\nDepth of Inheritance Tree(DIT). A low number for DIT\nand WMC implies less complexity and a high number for\nDIT and WMC implies higher complexity with a higher\nprobability of errors in the code. If LCOM value is small\nthen there is more cohesion between the methods and if\nthe value is large then there is lack of cohesion between\nthe methods. The complexity of our project is reduced\nafter apply the refactoring technique. We can say that\nrefactoring improve the internal structure of our project\nby decreasing the complexity and improve the quality of\nour project.\n\nMeasures Before After\nRefacto | Refact\nring oring\n\nLong Method yes No\nBad Feature Envy No No\nSmells | Long Class Yes No\nType Checking No No\nMcCabe 3.7 2.701()\nCompl | Cyclomatic )\ne Complexity\n-xity -\nWeighted 13.45 12.25(|\nMethods Per )\nwnc | Class(WMC)\nLCO Lack of Cohesion | 0.43 0.397()\nM of )\nMethods(LCOM)\nDIT Depth of | 3.864 3.25(|)\nInheritance Tree\n\n6 Conclusion\n\nRefactoring is an important and easy activity to refactor\nthe source code in a well manner. Refactoring makes a\ncode easier to understand and improve the quality of\ncode. We take a source code of library software which is\nwritten in Java. We can detect bad smells by JDeodorant\nand find the complexity by Metrics plugin. Bad smells\nmake our source code more difficult to manage. By\napplying refactoring using Eclipse, we can refactor these\nbad smells and make our source code essay to\nunderstand. After applying the refactoring, calculate the\ncomplexity of project and compare it with initial\ncomplexity and then check the result. The complexity of\nour project is reduced after apply the refactoring and\nimprove the quality of our project. Also we can say that\nrefactoring reduces the maintenance cost because the\ncomplexity of software is decrease and it is easily\nunderstandable.\n\nReferences\n\n1. Martin Fowler, Kent, John Brant, William Opdyke,\nDon Roberts, D. B. \u201cRefactoring: Improving the\nDesign of Existing Code\u201d, Addison-Wesley, New\nYork, (1999).\n",
                    "MATEC Web of Conferences 57, 02012 (2016)\nICAET-2016\n\nDOI: 10.1051/matecconf/20165702012\n\n10.\n\n11.\n\n12.\n\n13,\n\n14.\n\n15.\n\n16.\n\n17.\n\n18.\n\nRoberts, D. B \u201cPractical Analysis for Refactoring\u201d,\nPhD thesis, Department of Computer Science ,\nUniversity of Illinois at Urbana-Champaign, (1999).\nOpdyke, W. F , \u201cRefactoring Object-Oriented\nFrameworks\u201d, PhD thesis, University of Illinois at\nUrbana-Champaign,(1992).\n\nE.Mealy and P.Strooper,\u2019Evaluating \u2014 software\nRefactoring Tool support\u201d, Proceeding of Australian\nSoftware Engineering Conference, pp. 331-340,\n(2006).\n\nE.Mealy, D.Carrington, P.Strooper, and P.Wyeth,\n\u201cImproving usability of software refactoring tools\u201d,\nProceeding of Australian Software Engineering\nConference , pp.307-318, (Apr.2007).\n\nTom Mens and Tom Touwe, \u201cA survey of software\nrefactoring\u2019 IEEE Transactions on _ software\nEngineering ,vol.30, no.2, pp. 126-139, (Feb 2004).\nSandeep kaur , \u201c Review on Identification and\nRefactoring of Bad Smells using Eclipse\u201d,\nInternational Journal For Technological Research In\nEngineering (IJTRE) Volume 2, (March-2015).\n\nR. Fanta and V. Rajlich, \u201cReengineering object-\noriented code,\u201d in Proceeding of International\nConference on Software Maintenance, pp. 238-246,\n1998, IEEE Computer Society.\nEclipse Modeling Framework\nhttp://www.eclipse.org/emf/\n\nEMF Refactor -\nhttp://www.eclipse.org/emf/refactor/.\n\nJDeodorant -\nhttps://marketplace.eclipse.org/content/jdeodorant\nEMF Metrics Plugin -\nhttp://sourceforge.net/projects/metrics/\n\nMika V. Mantyl\u00e9, Jari Vanhanen, and Casper\nLassenius,\u201d A taxonomy and an initial empirical\nstudy of bad smells in code\u201d, In Proceedings of\nInternational Conference on Software Maintenance\n(ICSM 2003), IEEE Computer Society pages 381\u2014\n384, Amsterdam, The Netherlands, (September\n2003).\n\nNikolaos Tsantalis and Alexander Chatzigeorgiou,\n\u201cIdentification of move method refactoring\nopportunities\u201d, IEEE Transactions on Software\nEngineering, 35(3):347\u2014367, (2009).\n\nNikolaos Tsantalis, \u201cIdentification Of Move Method\nRefactoring Opportunities\u201d, IEEE Transactions On\nSoftware Engineering, Vol. 35, No. 3, (May/June\n2009).\n\nS.H. Kannangara, \u201cAn Empirical Evaluation Of\nImpact Of Refactoring On Internal And External\nMeasures Of Code Quality\u201d, International Journal Of\nSoftware Engineering & Applications (Ijsea), Vol.6,\nNo.1, (January 2015).\n\nJ. van den Bos, \u201cRefactoring (in) Eclipse\u201d, Master\nSoftware Engineering, Universiteit van Amsterdam,\nMaster\u2019s thesis , (August 2008).\n\nMesfin Abebe and Cheol-Jung Yoo, \u201c Trends,\nOpportunities and Challenges of Software\nRefactoring: A Systematic Literature Review\u201d,\nInternational Journal of Software Engineering and Its\nApplications Vol.8, No.6 ,pp.299-318,( 2014).\n\n(EMF) -\n\n19.\n\n20.\n\n21.\n\n22.\n\nEmerson Murphy-Hill and Andrew P. Black,\n\u201cRefactoring Tools: Fitness for Purpose\u201d,\nDepartment of Computer Science, Portland State\nUniversity Portland, Oregon, (May 7, 2008).\n\nD. Raj Kumar, G.M. Chanakya, \u201cRefactoring\nFramework for Instance Code Smell Detection\u201d,\nInternational Journal of Advanced Research in\nComputer Engineering & Technology (IJARCET)\nVolume 3 Issue 9, (September 2014).\n\nEmerson Murphy-Hill, Chris Parnin, And Andrew P.\nBlack, \u201cHow We Refactor, And How We Know It\u201d,\nIEEE Transactions On Software Engineering, Vol.\n38, No. 1, (January/February 2012).\n\nKarim O. Elish, \u201cUsing Software Quality Attributes\nto Classify Refactoring to Patterns\u201d, Journal Of\nSoftware, Vol. 7, No. 2, (February 2012).\n"
                ]
            }
        },
        {
            "file_name": "S:\\OneDrive\\@Dev\\!GPT\\ScriptGPT\\library\\Refactoring\\Source\\Refactoring-vs-Refuctoring-Advancing-the-state-of-AI-automated-code-improvements.pdf",
            "time_taken": 13.813110828399658,
            "data_extracted": {
                "text": [
                    "9 January 2024\n\nRefactoring vs\nRefuctoring:\n\nAdvancing the state of Al-\n\nautomated code improvements\nBy Adam Tornhill, Markus Borg, PhD & Enys Mones, P\n\nSummary\n\nThis report is the conclusion of a benchmark study of the mos\nLanguage Models (LLMs) and their ability to generate code fo!\ntasks. We aim to illustrate the current standards and limitati\nshow new methodologies with higher confidence results.\n\nC) CodeScene\n",
                    "Introduction\n\nThe remarkable advances in Al promised a coding revolution, spawning tools to help us\nwrite code faster. Yet the true gains elude us. The crux? The majority of a developer's time\nisn't writing but understanding and maintaining existing code.\n\nThis whitepaper explores this new frontier by investigating Al support for improving existing\ncode. We do that via two important contributions:\n\n\u00ab First, we benchmark the performance of the most popular Large-Language Models\n(LLM) on refactoring tasks for improving real-world code. We find that existing Al\nsolutions only deliver functionally correct refactorings in 37% of the cases.\n\n\u00ab Second, as a response to the poor performance of LLMs, we introduce a novel\ninnovation for fact-checking the Al output and augmenting the proposed refactorings\nwith a confidence level. By rejecting incorrect solutions, 98% of the remaining Al-\ngenerated refactorings improve the code while retaining the original behavior.\n\nThis level of precision exceeds that of even human experts, highlighting the utility of fact-\nchecked Al. By applying this innovation, software organizations get a viable way forward for\nautomating improvements to existing code, including auto-mitigations of technical debt.\n\n2. Improved performance:\nFact-checked Al refactoring\n\n98%\n\n1. Al refactoring performance\n\nHigh confidence 70% Adding a fact-checking validation layer allows\n\nyou to refactor without breaking the code.\n\n100%\nLLMs are more likely than not to break existing \u00abx\ncode during a refactoring attempt.\n70%\n\n2%\n\n63% ;\n- Vad Validate\n40% 7 w? refactorings\n37% Correct refactorin Incorrect refactoring\nwhich preserves the which introduces bugs\nLow confidence behavior of the code\n\nCorrect refactorin Incorrect refactoring Rejected refactoring proposals\nwhich preserves the which introduces bugs\n\nbehavior of the code TT\n\nFigure 1: State-of-the-art generative Al breaks the code in 63% of all refactoring attempts (left). Fact-checking the\nAl allows us to reject the majority of all broken refactoring attempts.\n\nCodeScene 1\n",
                    "Benchmark: Al\nperformance on code\nrefactoring\n\nAs exciting as the Al revolution is, we are far\nfrom realizing the claimed productivity\nbreakthrough. At least for non-trivial coding\ntasks. (See our Forbes article for why Al-\nassisted coding is still in its infancy).\n\nSpecifically, two main barriers remain to be\nconquered before Al can truly disrupt the\nway we work with source code:\n\n1. Optimize for software maintenance\nwhich accounts for more than 90% of a\nsoftware product's life cycle costs.\nSpecifically, 70% of developers\u2019 time is\nspent on program understanding,\nmeaning that any improvements that\nmake the existing code easier to grasp\nwill have a high return on investment.\n\nWriting/Editing code\n5%\n\n2.Improve Al precision to the level of a\nhuman expert. The disappointing 37%\ncorrectness score of today\u2019s Al solutions\nsimply isn\u2019t good enough for refactoring\nproduction code. Rather, the hit-or-miss\nsuccess ratio adds to the problem by\nincreasing developers\u2019 cognitive load as\nwe have to scrutinize all Al refactorings\nwith great care to sort out the good\nfrom the bad. Reviewing code _ is\narguably a harder task than writing it.\n\nThese two factors indicate that innovation\nin tooling to support improving existing\ncode - without breaking it - is a more\nimportant direction than focusing on\noptimizing the less significant code-writing\nprocess. Let\u2019s discuss why.\n\nAre we refactoring or just\nbreaking code?\n\nRefactoring is defined as improving the\ndesign of existing code without changing its\nbehavior. It\u2019s a simple definition, but with\nsome important implications:\n\nOther activities\n(e.g. meetings,\nnavigating code)\n\n25%\n\n\u00ab It\u2019s not a refactoring unless we improve\nthe design. \u201cImprove\u201d has been largely\nsubjective. To automate refactoring, we\nneed a gold standard to make\nimprovements objectively measurable.\n\n\u00ab It\u2019s not a refactoring if we fail to\npreserve the behavior of the original\ncode, e.g. we introduce a bug. To\n\nUnderstanding automate refactoring, we need\n\n70% confidence that the machine adheres to\nthis assumption.\n\nFigure 2: The majority of a developer's time is spent trying\n\nto understand the existing system (data from Minelli, et. Unless these two conditions are met, a code\n\nal., 2015) 1 change is simply not a refactoring. For the\npurpose of this article, we will use the term\nrefuctoring to refer to the process of\nchanging existing code while - involuntarily\n\n- altering the program\u2019s behavior.\n\n1. https://ieeexplore.ieee.org/abstract/document/7181430\n\nCodeScene 2\n",
                    "Benchmarking criteria: a gold standard for code\nimprovements\n\nThis study uses the Code Health metric as a proxy for code quality. Code Health is the only\ncode-level metric with a proven business impact in terms of development velocity and post-\nrelease defects. (See the Code Red paper for details).\n\nThe Code Health metric is a particularly good fit when refactoring code as the measure is\nbased on factors known to make code harder to understand and riskier to maintain in terms\nof defect introduction:\n\nCode health categories:\n\nModule/Class level smells tomate\n\nSource code \u2014 (e.g. Low Cohesion, God Classes) \u2014\u2122,\nFunction level smells Score, aggregate,\n\ne.g. conv pasted lic, God Functions, and categorize\n> rimitive Obsession,\nNL Implementation smells __ 7\u201d\nee (e.g. deep nested logic, complex\n\nconditionals)\n\nFigure 3: Code Health is a language-neutral, aggregated code quality metric based on a combination of 25 code smells.\n\nThe detailed Code Health scores used in this study go from 10.0 (healthy code) all the way\ndown to 1.0 (a maintenance nightmare / spaghetti code / high technical debt). As such, the\nCode Health metric offers an objective assessment of any code changes: did the Code\nHealth improve - a refactoring that makes the code easier to understand - or was the code\nmerely changed without getting objectively better?\n\nAl performance on code refactoring\n\nTo evaluate how well current Al platforms perform, we collected more than 100,000 real-\nworld code smells, and pointed state-of-the-art Al models at these targets to refactor the\ncode. We used the CodeScene tool to identify Code Health issues in codebases. We then\nevaluated the correctness of the attempted refactoring by running the code\u2019s automated\ntests, as well as making sure the code quality improved by re-assessing the Code Health.\n\nFor this benchmarking study, we focused on code in JavaScript and TypeScript. LLM\nperformance varies across programming languages, so we chose to start with two popular\nand well-supported languages. We also centered the refactorings on four common Code\nHealth issues: Complex Conditionals, Deep Nested Logic, Bumpy Road, and Complex\nMethod. (See the docs for descriptions of these code smells).\n\nCodeScene\n",
                    "Using this data, we measure the ratio of refactoring vs refuctoring for a series of popular Al\nmodels:\n\nValid code? rode ren\nP . Valid refactoring?\nAl model (check the (did the code ;\nsyntax of the (do the tests still pass after the\nrefactored change by the Al changed the code?)\nAl mitigate the 9 .\ncode smell?)\n\nPaLM 2 code\n\nPaLM 2t\nphind-codellama-34B-v2*\n\nTable 1: Benchmarking of refactoring correctness for a series of popular Large Language Models. *A fine-tuned model\n\nbased on CodeLlama 34B.\n\nAs the preceding table shows, using an out-of-the-box Al is very much a hit-or-miss situation.\nIn fact, with the best-performing model only giving a 37% probability of success, it\u2019s more\nlikely that the attempted refactoring will break your code than not.\n\nNotes on GPT4\nPerformance\n\nDuring our research, we also\nmade some benchmarks using\nGPT4. These tentative studies\nindicate that GPT4 seems to\nperform marginally better.\nHowever, those potential\n\nimprovements are offset by GPT4\nbeing significantly slower and an\norder of magnitude more\nexpensive. Without a drastic gain,\nGPT4-based refactoring doesn\u2019t\nseem to be a viable alternative\neither.\n\nIs fragile code an acceptable\nnew normal?\n\nOur research findings align with a 2023 study\nwhich found that popular Al-assistants like\nCopilot and CodeWhisperer only deliver\nfunctionally correct code in 31% - 65% of the\ncases. Generating new code is arguably a\nsimpler task than refactoring complex code,\nwhich explains the higher-end of those\nnumbers. However, those research findings\nare within the same ballpark; the performance\nof those Al-assistants reflects the\nperformance of the underlying LLM.\n\nIt's safe to assume that a human developer\nshipping code which breaks 60-80% of the\ntime would be asked to look for new\nchallenges. Promptly.\n\nCodeScene 4\n",
                    "How does an Al err?\n\nStrictly speaking, an Al based on LLMs doesn\u2019t have a concept of \u201ccorrectness\u201d;\nsome tokens (e.g. output) are more probable than others, but the Al itself doesn\u2019t\nexecute the code, nor does it \u201cprove\u201d its response in any meaningful way.\n\nThis spells trouble as the nature of programming is a much more constrained\nenvironment than a conversation between humans. For example, when\ngenerating a natural text, choosing one synonym over another usually doesn\u2019t\nmake or break a text. (e.g. consider \u201cgear\u201d and \u201cequipment\u201d). Not so in code - a\nsingle character can alter the behavior of any program.\n\nDuring our research we inspected a lot of Al-generated code. Some common\npatterns are that the Al:\n\n- Drops entire branches, e.g. throwing away an if-block. Note that these bugs\ncan easily become a cybersecurity threat in case important input validation\ngets removed.\n\n- Inverts boolean logic, e.g. a && b becomes !(a && b).\n\n\u00ab In JavaScript - which is notorious for its complex rules for the this-keyword \u2014\nthe Al often mistreats the function-level this by extracting the literal\nexpression to a new function, a classic refuctoring pitfall.\n\nAll of these failures are subtle and not obvious to the human eye during a code\ninspection.\n\ndeep nesting constructs: the Al introduced a nasty and subtle bug:\n\nOriginal code with complex, o> Al refactored code. Looks much cleaner, but\n\nexport function maxDepth(ary) { export function maxDepth(ary) {\nvar stack = (J; var stack = [];\nvar max_depth = @; var max_depth = @;\nfor (var i = @; i < ary.length; i++) {\nstack. push(arylil); for (var i = @; i < ary.length; i++) { Bug: max_depth is\nstack. push(ary[il); sed by value...\nif (stack. length > 1) { p y passed by value...\n\nvar top = stack[stack. length - 1] uadarestack(atack)s\nzs 4\n\nvar bottom_idx = -1; updateMaxDepth(stack, max_depth) ; Vi\nfor (var j = @; j < stack.length ~ 1; j++) {\n\ni Fase ecel 7 ..-and, consequently, the function will\n\ni return max_depth; always return 0 (zero)..\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nif (bottom_idx >= 0) { -\nstack = stack.slice(bottom_idx + 1); _\u2014___ function updateMaxDepth(stack, max_depth) {\n; , ee if (stack. length > max_depth) {\nif (stack. length > maxdepth) ( \u2014\u2014 fax_depth = stack. length; because this will only be a\nmax_depth = stack. length; 7 } local modification!\n}\n}\nfunction updateStack(stack) {\nif (stack.length > 1) {\nvar top = stack[stack. length - 1];\nvar bottom_idx = findBottomIndex(stack, top[@]);\nif (bottom_idx >= @) {\n\nreturn max_depth;\n\nFigure 4: Example on an Al-introduced, subtle bug that takes a great deal of effort to track down.\n\n",
                    "These findings also indicate that we should be more cautious about how and when we apply\nAl-assisted coding. What if we could add a safety net around the Al? That would let us reap\nthe benefits of automation by outsourcing the mechanics to an Al while still giving us - as\nsoftware people - some guarantees that we are refactoring, not refuctoring. Welcome to the\nfuture \u2014 there\u2019s a light on the horizon.\n\nTools for inspiration...but use with care!\n\nThe Al-assistance of today can still be helpful, despite their frequent errors. In particular, an\nAl-assistant like Copilot or CodeWhisperer can be useful as the starting point for new code,\nserving as an inspiration and a coach. However, the burden is still on you to verify that the\ncode is correct and \u2014 just as important - that it\u2019s code you and your team can maintain going\nforward.\n\nInto the future: improving automated refactoring\nby fact-checking the Al\n\nGiven the low correctness of the stochastic Al models, it becomes strikingly clear that we\ncannot use out-of-the-box Al models or tools that merely wrap an LLM API. Instead, a more\npromising approach is to use generative Al to come up with a pool of potential solutions and\nthen add a fact-checking layer around the Al. That way, we get the benefits of automation\nwhile retaining a certain level of guarantee that a proposed solution is a refactoring rather\nthan a refuctoring.\n\nCodeScene\u2019s research team took on this challenge by creating a layered fact-checking\nmodel:\n\nAl-refactored code to be\nfact-checked.\n\ncode?\n\n(x) \u00a9 No Confidence\n\n\u2014\u2014\u2014>> | Reject the \u201csolution\u201d, never\n\nshow to users.\n\nCode Health review:\nimproved code quality?\n\nSemantic comparison: g -\u00bb High Confidence ae fraction\nn r? \u2014_> :\nTragedies The code is likely to be a valid\nrefactoring.\n| \u00a9 Mid Confidence\n\nMinimal refactoring: small g 80% Precision\nand focused code change? \u2014_ Likely to be OK, but requires a\nreview before the refactoring is\napplied.\n\n(x) \u2018\u00a9 Low Confidence\n\nUseful as an inspiration, but\nshouldn't be applied.\n\nFigure 5: A schematic overview of the layered model for fact-\n\nchecking Al-refactored code. CodeScene\n",
                    "Benchmarking: improving Al correctness with a fact-\nchecking model\n\nTo evaluate the fact-checking model, we re-ran the benchmarking study described in Table 1\n\nabove. This makes it possible to compare the correctness gained from the fact-checking\nmodel:\n\nCorrect code smell refactorings\n\nDeep\nComplex Nested Complex\n\nConditional . Method\nLogic\n\nAl with CodeScene\u2019s fact-\nchecking\n\nTable 2: Benchmarking data showing how the confidence in refactoring can be improved to 98%. GPT-3.5 performance\nadded for comparison.\n\nTable 2 shows that the layered fact-\nchecking model is a massive\nimprovement over GPT-3.5 - and any\nother commercially available LLM - with\nrespect to correctness. An LLM without\nfact-checking will always give you an\nanswer, be it correct or not. CodeScene\u2019s\nfact-checking model is able to validate\nthe proposed code changes, and reject\n98% of the incorrect refactorings.\n\nBefore we discuss the disruptive\npotential that this level of Al performance\nenables, we need to look behind the\nmodel to understand how the Al fact-\nchecking is possible at all.\n\nData as the secret sauce\n\nThe main challenge in the fact-checking\nis to ensure semantic equivalence\nbetween the original code and the\nrefactored code. This is a largely\nunsolved research problem\n\nin academia where the problem is studied\nin the area of Automatic Program Repair\n(APR).\n\nPotential solutions like formal methods\nand code similarity metrics haven't been\nable to reliably verify semantic\ncorrectness between a given piece of\ncode and its fixed/refactored counterpart.\nSo how did the CodeScene team pull this\noff? There are two unfair advantages at\nour disposal plus one critical constraint\nthat we chose:\n\nFirst, when building the fact-checking\nmodel we had access to our data lake\nconsisting of +100,000 real-world\nJavaScript refactoring samples with a\nknown ground truth (i.e. semantically\nequivalent or not). This made it possible\nfor our algorithms to observe and learn\npatterns in successfully refactored code.\n\nCodeScene\n",
                    "Second - and in fact a basis and pre-requisite for #1 - the data lake was built up using the\nautomated code review capabilities of the CodeScene tool. This automated code review is\ndeterministic and driven by the Code Health metric. This step is crucial as it directly\ninfluences the quality of the data; poor sample quality, and it won't be possible to achieve\nthese levels of accuracy.\n\nThird, it\u2019s important to point out that we didn\u2019t attempt to solve semantic equivalence in\ngeneral. Doing so would be futile at best. Instead, we limited the fact-checking to the set of\ncode smells identified via the Code Health metric. That way, we could be more specific in our\nAl prompts as well as constraining the fact-checking model to a finite number of structural\nchanges that can be learned by our in-house models.\n\nSummary\n\nThis benchmarking study shows that Al is nowhere near replacing humans in a coding\ncontext; today\u2019s Al is simply too error-prone, and far from a point where it is able to securely\nmodify existing code. However, by introducing a novel fact-checking model for the Al output,\nwe can elevate generative Al to a point where it is genuinely useful as several complex code\nsmells can be mitigated safely. This allows us to optimize for understanding - the dominant\nand most human-intensive aspect \u2014- not just the narrow task of writing new code.\n\nPerhaps the most intriguing possibility is the progress in technical debt mitigation made\npossible via this innovation. Every business manager is aware of technical debt, but few\nprioritize it - and even fewer manage it actively. Traditionally, there\u2019s been a hard trade-off\nbetween improving existing code vs. adding the next big feature. Predictably, improvements\nget the back seat despite hard numbers showing how a healthy codebase is a competitive\nadvantage. Now that the process can be automated to a large degree, companies can finally\nstart to reap these benefits without having to put feature development on hold.\n\nDuring our research, we also couldn't help to reflect on the fact that these benchmarks on Al-\nassisted programming re-emphasize solid engineering practices like unit testing, code quality\ngates, and continuous code reviews. Those practices were always important, but perhaps\neven more so in the age of Al where humans need to understand and verify machine-\ngenerated code.\n\nIn our study, we too used commonly available Al models which we augmented with specific\ndomain data to improve their refactoring performance. Yet, the key to our breakthrough\nwasn\u2019t Al augmentation or magic prompt engineering, but rather the ability to provide a\nconfidence indication for each refactoring with respect to its semantic equivalence to the\noriginal code. Knowing the confidence of a proposed refactoring is a time saver.\n\nCodeScene\n\n8\n",
                    "Try the Automated Refactoring on your own code\n\nThe fact-checking innovation described in this whitepaper will be available to the general\npublic via CodeScene. Sign-up for the beta testing waitlist at https://codescene.com/ai.\n\n25 VectorLayerjs 9+ JS indexjs 8 CodeScene AI Refactor x\n\n}\n\n} else\n\nThe automated code review highlights a\n\n>\n(e) Refactoring recommendation\n\nThe proposed change is \u201cHigh Confidence\u201d,\n\nLt} code smell, Complex Conditional \u2014\u2014=- and refactors the code by extracting a\n\nreturn targe variable to clarify the intent of the code\n* ! Proposed change\nfunction propagationHandler: (qi, options) { 4\n\nreturn function( \u2019 4 function propagationHandler (qi, options) {\n\nreturn function( \u2019 \u00a2\nit const = options. stopPropagation &&\nloptions. stopPropagation && tqi- options. stopPropagation &&\n\n|onnust@ ds options. stopPropagation 66 Gi. options. parent;\n\nqi.options. parent I\nMt if ( 4\nreturn qi.options.parent. $query (options); - return qi.options. parent. $query (options) ;\n\nfunction findAverageatSite(site) {\nconst nea = fetchMeasurenents();\n\nAbout the authors\n\nAdam Tornhill is the founder and CTO of CodeScene. Adam is a programmer who combines\ndegrees in engineering and psychology. He\u2019s also the author of the best-selling Your Code as\na Crime Scene as well as multiple other technical books.\n\nMarkus Borg, PhD, is a senior researcher at the intersection of software engineering and\napplied Al. He is a principal researcher at CodeScene and an adjunct associate professor at\nLund University, Sweden.\n\nEnys Mones, PhD, is the Lead Data Scientist at CodeScene who also enjoys doing basic\n\nresearch. A theoretical physicist by training, his focus is applying mathematical models to\nunderstand human-computer interaction.\n\n(eo) CodeScene\n\nNext generation code analysis\n\nwww.codescene.com\n\n"
                ]
            }
        },
        {
            "file_name": "S:\\OneDrive\\@Dev\\!GPT\\ScriptGPT\\library\\Refactoring\\Source\\Refactoring Practices in the Context of Modern Code Review.pdf",
            "time_taken": 25.187671184539795,
            "data_extracted": {
                "text": [
                    "2102.05201v1 [cs.SE] 10 Feb 2021\n\narXiv\n\nRefactoring Practices in the Context of Modern Code Review:\nAn Industrial Case Study at Xerox\n\nEman Abdullah AlOmar*, Hussein AlRubaye', Mohamed Wiem Mkaouer*, Ali Ouni?, Marouane Kessentini!\n\u201cRochester Institute of Technology, Rochester, NY, USA\ntXerox Corporation, Rochester, NY, USA\nSETS Montreal, University of Quebec, Montreal, QC, Canada\nSUniversity of Michigan, Dearborn, MI, USA\neman.alomar@maiL.rit.edu, hussein.alrubaye@xerox.com, mwmvse @rit.edu, ali.ouni@etsmtl.ca, marouane@umich.edu\n\nAbstract\u2014Modern code review is a common and essential\npractice employed in both industrial and open-source projects\nto improve software quality, share knowledge, and ensure con-\nformance with coding standards. During code review, developers\nmay inspect and discuss various changes including refactoring\nactivities before merging code changes in the code base. To date,\ncode review has been extensively studied to explore its general\nchallenges, best practices and outcomes, and _ socio-technical\naspects. However, little is known about how refactoring activities\nare being reviewed, perceived, and practiced.\n\nThis study aims to reveal insights into how reviewers develop\na decision about accepting or rejecting a submitted refactoring\nrequest, and what makes such review challenging. We present an\nindustrial case study with 24 professional developers at Xerox.\nParticularly, we study the motivations, documentation practices,\nchallenges, verification, and implications of refactoring activities\nduring code review.\n\nOur study delivers several important findings. Our results\nreport the lack of a proper procedure to follow by developers\nwhen documenting their refactorings for review. Our survey\nwith reviewers has also revealed several difficulties related to\nunderstanding the refactoring intent and implications on the\nfunctional and non-functional aspects of the software. In light of\nour findings, we recommended a procedure to properly document\nrefactoring activities, as part of our survey feedback.\n\nIndex Terms\u2014Refactoring, Code Review, Software Quality\n\nI. INTRODUCTION\n\nThe role of refactoring has been growing in practice beyond\nsimply improving the internal structure of the code without\naltering its external behavior to become a widespread\nconcept for the agile methodologies, and a de-facto practice to\nreduce technical debt [2]. In parallel, contemporary software\nprojects adopt code review, a well-established practice for\nmaintaining software quality and sharing knowledge about\nthe project By}. (4). Code review is the process of manually\ninspecting new code changes to verify their adherence to\nstandards and its freedom from faults Bi. Modern code review\nhas emerged as a lightweight, asynchronous, and tool-based\nprocess with reliance on a documentation of the inspection\nprocess, in the form of a discussion between the code change\nauthor and the reviewer(s)\n\nRefactoring, just like any code change, has to be reviewed,\nbefore being merged into the code base. However, little is\nknown about how developers perceive and practice refactoring\nduring the code review process, especially that refactoring, by\n\ndefinition, is not intended to alter to the system\u2019s behavior, but\nto improve its structure, so its review may differ from other\ncode changes. Yet, there is not much research investigating\nhow developers review code refactoring. The research on\nrefactoring has been focused on its automation by identifying\nrefactoring opportunities in the source code, and recommend-\ning the adequate refactoring operations to perform (6)-I8}.\nMoreover, the research on code reviews has been focused on\nautomating it by recommending the most appropriate reviewer\nfor a given code change (3). However, despite the critical role\nof refactoring and code review, the innate relationship between\nthem is still largely unexplored in practice.\n\nThe goal of this paper is to understand how developers\nreview code refactoring, i.e., what criteria developers rely on\nto develop a decision about accepting or rejecting a submitted\nrefactoring change, and what makes this process challenging.\nThis paper seeks to gain practical insights from the existing\nrelationship between refactoring and code review through the\ninvestigation of five main research questions:\n\nRQ1. What motivates developers to apply refactorings in the\ncontext of modern code review?\n\nRQ2. How do developers document their refactorings for code\nreview?\n\nRQ3. What challenges do reviewers face when reviewing\nrefactoring changes?\n\nRQ4. What mechanisms are used by developers and reviewers\nto ensure the correctness after refactoring?\n\nRQS5. How do developers and reviewers assess and perceive\nthe impact of refactoring on the source code quality?\n\nTo address these research questions, we surveyed 24 pro-\nfessional software developers, from the research and develop-\nment team, at Xerox. Our survey questions were designed to\ngather the necessary information that can answer the above-\nmentioned research questions and insights into the review\npractices of refactoring activities in an industrial setting.\nMoreover, we perform a pilot study by comparing between\ncode reviews related to refactoring, and the remaining code\nreviews, in terms of time to resolution and number of ex-\nchanged responses. Our findings indicate that refactoring-\nspecific code reviews take longer to be resolved and typically\n",
                    "triggers more discussions between developers and reviewers\nto reach a consensus. The survey with reviewers, has revealed\nmany challenges they are facing when they review refactored\ncode. We report them as part of our survey results, and we\nprovide some guidelines for developers to follow in order to\nfacilitate the review of their refactorings.\n\nIl. RELATED WORK\nA. Surveys & Case Studies on Refactoring\n\nPrior works have conducted literature surveys on refactoring\nfrom different aspects. The focus of these surveys ranges\nbetween investigating the impact of refactoring on software\nquality (13). to comparing refactoring tools [9], and exploring\nrefactoring challenges and _ practices a (14), (15).\nThese studies are depicted in Table\n\nMurphy-Hill & Black i) surveyed 112 Agile Open North-\nwest conference attendees and found that refactoring tools are\nunderused by professional programmers. In an explanatory\nsurvey involving 33 developers, Arcoverde et al. [1\nhow developers react to the presence of design defects in\nthe code. Their primary finding indicates that design defects\ntend to live longer due to the fact that developers avoid\nperforming refactoring to prevent unexpected consequences.\nYamashita & Moonen performed an empirical study in\ncommercial software to evaluate the severity of code smells\nand the usefulness of code smell-related tooling. The authors\nfound that 32% of the interviewed developers are unaware\nof code smells, and refactoring tools should provide better\nsupport for refactoring suggestions. Kim et al. surveyed\n328 professional software engineers at Microsoft to investigate\nwhen and how they do refactoring. When surveyed, the de-\nvelopers cited the main benefits of refactoring to be: improved\nreadability (43%), improved maintainability (30%), improved\nextensibility (27%) and fewer bugs (27%). When asked what\nprovokes them to refactor, the main reason provided was poor\nreadability (22%). Only one code smell, i.e., code duplication,\nwas reported (13%). Szoke et al. conducted 5 large-scale\nindustrial case studies on the application of refactoring while\nfixing coding issues; they have shown that developers tend\nto apply refactorings manually at the expense of a large time\noverhead. Sharma et al. surveyed 39 software architects\nasking about the problems they faced during refactoring tasks\nand the limitations of existing refactoring tools. Their main\nfindings are: (1) fear of breaking code restricts developers\nto adopt refactoring techniques; and (2) refactoring tools\nneed to provide better support for refactoring suggestions.\nNewman et al. conducted a survey of 50 developers\nto understand their familiarity with transformation languages\nfor refactoring. They found that there is a need to increase\ndeveloper confidence in refactoring and transformation tools.\n\nB. Refactoring Awareness & Code Review\n\nResearch on modern code review topics has been of import-\nance to practitioners and researchers. A considerable effort is\nspent by the research community in studying traditional and\nmodern code review practices and challenges. This literature\n\nhas been includes case studies (e.g., ), user studies\n]), and surveys (e.g., By}. . However, most of the\nudies focus on studying the effectiveness of modern\ncode review in general, as opposed to our work that focuses on\nunderstanding developers\u2019 perception of code review involving\nrefactoring. In this section, we are only interested in research\nrelated to refactoring-aware code review.\n\nIn a study performed at Microsoft, Bacchelli and Bird\nobserved, and surveyed developers to understand the chal-\nlenges faced during code review. They pointed out purposes for\ncode review (e.g., improving team awareness and transferring\nknowledge among teams) along with the actual outcomes\n(e.g., creating awareness and gaining code understanding). In\na similar context, MacLeod et al. interviewed several\nteams at Microsoft and conducted a survey to investigate the\nhuman and social factors that influence developers\u2019 experi-\nences with code review. Both studies found the following\ngeneral code reviewing challenges: (1) finding defects, (2)\nimproving the code, and (3) increasing knowledge transfer.\nGe et al. developed a refactoring-aware code review tool,\ncalled ReviewFactor, that automatically detects refactoring\nedits and separates refactoring from non-refactoring changes\nwith the focus on five refactoring types. The tool was inten-\nded to support developers\u2019 review process by distinguishing\nbetween refactoring and non-refactoring changes, but it does\nnot provide any insights on the quality of the performed\nrefactoring. Inspired by the work of [16], Alves et al.\nproposed a static analysis tool, called RefDistiller, that helps\ndevelopers inspect manual refactoring edits. The tool compares\ntwo program versions to detect refactoring anomalies\u2019 type\nand location. It supports six refactoring operations, detects\nincomplete refactorings, and provides inspection for manual\nrefactorings.\n\nTo summarize, existing studies mainly focus on proposing\nand evaluating refactoring tools that can be useful to support\nmodern code review, but the perception of refactoring in\ncode review remains largely unexplored. To the best of our\nknowledge, no prior studies have conducted case studies in\nan industrial setting to explore the following five dimensions:\n(1) developers motivations to refactor their code, (2) how\ndevelopers document their refactoring for code review, (3)\nthe challenges faced by reviewers when reviewing refactoring\nchanges, (4) the mechanisms used by reviewers to ensure the\ncorrectness after refactoring, and (5) developers and reviewers\nassessment of refactoring impact on the source code\u2019s quality.\nPrevious studies, however, discussed code review motivations\nand challenges in general . To gain more in-depth\nunderstanding of the above-mentioned five dimensions, in this\npaper, we surveyed several developers at Xerox.\n\nIII. STUDY DESIGN\nA. Research Questions\nRQI1. What motivates developers to apply refactorings\n\nin the context of modern code review? Several motivations\nbehind refactoring have been reported in the literature (i).\n",
                    "Table (1) Related work in industrial case study & survey on refactoring.\n\nStudy Year \u2014 Research Method Focus Single/Multi Company Subject/Object Selection Criteria # Participants\nMurphy-Hill & 2008 Survey Refactoring tools Yes/No programmers 112\nArcoverde et al 2011 Survey Longevity of code smells No/Yes belongs to development team 33\n\u2018Yamashita & M 2013 Survey Developer perception of code smells NolYes developers 35\n\nm eta 2014 Survey & Interview Refactoring challenges & benefits YesINo has change messages including \u201cretactor\u2122 328\n\nwithin last 2 years\n\nSzoke etal. [13 2014 __Case Study & Survey Impact of refactoring on quality Nolves developers a\n\u2018Sharma et al 2015 Survey Challenges & solutions for refactoring adoption YesINo 3\nNewman et al 2018 Survey Developer familiarity of transformation NolYes development\u201d in job title & not students 50\n\nlanguages for refactoring\n\nor faculty members\n\nCreate Review\nRequest\n\nview Request\nInder Review\n\nRevisions Requested\n\nReviewer Assigned\n\nRevisions Completed\n\n<>\n\nChanges Approved\n\nFigure (1)\n\nReview process overview.\n\n(}. Our first research question seeks to understand\n\nwhat motivations drive code review involving refactoring in\nvarious development contexts to augment our understanding\nof refactorings in theory versus in practice.\n\nRQ2. How do developers document their refactorings\nfor code review? Since there is no consensus on how to\nformally document refactoring activities (22) , we aim in\nthis research question to explore what information developers\nhave explicitly provided, and what keywords developers have\nused when documenting refactoring changes for a review.\nThis question aims to capture the taxonomy used and observe\nwhether it is currently helpful in providing enough insights for\nreviewers to be able to adequately assess the proposed changes\nto the software design.\n\nRQ3. What challenges do reviewers face when reviewing\nrefactoring changes? We investigate the challenges associated\nwith refactoring, as well as the bad refactoring practices that\ndevelopers catch when reviewing refactoring changes. This\nsheds light on how developers should mitigate some of these\nchallenges.\n\nRQ4. What mechanisms are used by developers and\nreviewers to ensure code correctness after refactoring?\nWe pose this research question to study current approaches\nfor testing behavior preservation of refactoring, and to get\nan overview of what different criteria are addressed by these\napproaches.\n\nRQS5. How do developers and reviewers assess and per-\nceive the impact of refactoring on the source code quality?\nFinally, in our last research question, we are interested in\nunderstanding how refactoring connects current research and\npractice. This helps exploring if the implications or outcomes\nof refactoring-aware code review match what outlined in the\nprevious research questions.\n\nB. Research Context and Setting\n\nHost Company and Object of Analysis. To answer the\nabove-mentioned research questions, we conducted our survey\n\nwith developers from the research and development division,\nat Xerox Research Center Webster (XRCW), currently Xerox\u2019s\nlargest research center. The research and development di-\nvision is responsible for implementing and maintaining the\nsoftware that is currently being shipped with Xerox Printers,\n(i.e, ConnectKey interface technology|'p. The software is\ndirectly connected to the hardware and performs various\noperations going from basic scanning and printing to more\ncomplex commands such as exchanging with cloud services.\nThe software is constructed using object-oriented, object-based\nand markup languages. Despite being a legacy, around 20\nyears old, lengthy and complex software, the developers in\ncharge have been successfully evolving it to meet business\nrequirements and provide secure and reliable functionality to\nend users. This reflects the maturity of the engineering process\nwithin the research and development division, which raised\nour interest to understand how they perform code review in\ngeneral, and how they review refactoring in particular.\n\nCode Review Process at Xerox. The research and devel-\nopment division uses a collaborative code review framework\nallowing developers to directly tag submitted code changes\nand request its assignment to a reviewer. Similar to existing\nmodern code review platforms, e.g., Gerrit\u201c| a code change\nauthor opens a code Review Request (ReR) containing a title, a\ndetailed description of the code change being submitted, writ-\nten in natural language, along with the current code changes\nannotated. Once an ReR is submitted, it appears in the requests\nbacklog, open for reviewers to choose. If an ReR remains\nopen for more than 72 hours, a team leader would handle its\nassignment to reviewers. Once reviewers are assigned to the\nReR, they inspect the proposed changes and comment on the\nReR\u2019s thread, to start a discussion with the author, just like\na forum or a live chat. This way, the authors and reviewers\ncan discuss the submitted changes, and reviewers can request\nrevisions to the code being reviewed. Following up discussions\nand revisions, a review decision is made to either accept (i.e.,\nship it!) or decline, and so the proposed code changes are\neither \u201cMerged\u201d to production or \u201cAbandoned\u201d. An activity\ndiagram, modeling a simplified bird\u2019s view of the code review\nprocess, is shown in Figure [I]\n\nC. Pilot Study and Motivation\n\nRationale. As we were analyzing the review process, to\nprepare our survey, we had access to the code review plat-\nform, containing the team\u2019s history of processed ReRs for\n\n",
                    "Table (II) Summary of survey questions (the full list is available in |\n\nCategory Question\nBackground (1) How many years have you worked in the software industry?\n\n(2) How many years have you worked on refactoring?\n\n(3) How many years have you worked on code review?\nMotivation (@) As a code change author, in which situation(s) you typically refactor the code?\n\nDocumentation\n\n(S) As a code change author, what information do you explicitly provide when documenting your refactoring activity?\n\n(6) As a code change author, what phrases (keywords) have you used when documenting refactoring changes for a review?\n\nChallenge (7) As a code reviewer, what challenges have you face when reviewing refactoring changes?\n(8) As a code reviewer, what are the bad refactoring practices you typically catch when reviewing refactoring changes?\n\nVerification (9) As a code change author/code reviewer, what mechanism(s) do you use to ensure the correctness after the application of refactoring?\n\nImplication (10) As a code reviewer, what implication(s) do you typically experience as software evolves through refactoring?\n\n(11) How strongly do you agree with each of the following statements?\n\n\u00ab Ihave guidelines on how to document refactoring acti\n\u00ab Ihave guidelines on how to review refactoring act\n\nies,\nhile performing code review.\n\n\u00ab Reviewing refactoring activities slow down the review process.\n\u00abReviewing refactoring typically takes longer to reach a consensus.\n\nTable (III) Participant professional development experience\nin years.\n\nCode Review Ex-\nperience (%)\n\nIndustrial\nExperience (%)\n\nYears of Experi-\nence\n\nRefactoring Ex-\nperience (%)\n\n1-5 9 (37.5%) 15 (62.5%) 14 (58.33%)\n6-10 5 (20.83%) 4 (16.66%)\n11-15 4 (16.66%) 1 (4.16%) 2 (8.33%)\n\n16+ 6 (25%) 5 (20.83%) 4 (16.66%)\n\nthe ConnectKey software system. After reviewing various\nReRs, we noticed the existence of a number of refactoring-\nspecific ReRs, i.e., requests to specifically review a refactored\ncode. The existence of such refactoring ReRs raised our\ncuriosity to further study in deeper whether these ReRs are\nmore difficult to resolve than other non-refactoring ReRs. We\nhypothesize that refactoring ReRs, take longer time and trigger\nmore discussions between developers and reviewers before\nreaching a decision and closing the ReR. If such hypothesis\nholds, then it further justifies the need for a more detailed\nsurvey targeting these refactoring ReRs.\n\nExtraction of Review Requests Metadata. We aim to\nidentify all recent refactoring ReRs. Similarly to Kim et al.\nwe start with scanning the ReRs repository to distin-\nguish ReRs whose title or description contains the keyword\n\u201crefactor*\u201d. We only considered recent reviews, which were\ncreated between January 2019 and December 2019. We chose\nto analyze recent ReRs to maximize the chance of developers,\nwho authored or reviewed them, as still within the company.\nWe manually analyze the extracted set to verify that each\nselected ReR is indeed about requesting the review of a\nproposed refactoring. This extraction and filtering process\nresulted in identifying 161 refactoring ReR. To perform the\ncomparison, we need to sample 161 non-refactoring ReR from\nthe remaining ones in the review framework. To ensure the\nrepresentativeness of the sample, we use the stratified random\nsampling by choosing ReRs which were (1) created between\nJanuary 2019 and December 2019; (2) created by the same\nset of authors of the refactoring ReRs; and (3) created to\nupdate the same subsystem(s) that were also updated by the\nrefactoring ReRs.\n\nWe then compared both groups based on two factors: (1) re-\nview duration (time from starting the review until a decision of\nclose/merge is made), and (2) number of exchanged responses\n(i.e., review comments) between the author and reviewer(s).\nFigure |2| reports the boxplots depicting the distribution of\neach group values, clustered by two above-mentioned factors.\nTo test the significance of the difference between the groups\nvalues, we use the Mann-Whitney U test, a non-parametric\ntest that checks continuous or ordinal data for a significant\ndifference between two independent groups. Our hypothesis\nis formulated to test whether the values of the refactoring\nReRs group is significantly higher than the values of the\nnon-refactoring ReRs group. The difference is considered\nstatistically significant if the p-value is less than 0.05.\n\nPilot Study Results. According to Figure |2} refactoring\ncode reviews take longer to be completed than the non-\nrefactoring code reviews, as the difference was found to be\nstatistically significant (i.e., p< 0.05). Similarly, refactoring\ncode reviews were found to significantly trigger longer dis-\ncussion between the code author and the reviewers before\nreaching a consensus (i.e., p< 0.05). This motivates us to\nbetter understand the challenges reviewers face when review-\ning refactoring. We designed our survey to ask developers\nof this team about the kind of problems that triggers them\nto refactor, and to close the loop, we asked reviewers about\nwhat they foresee when they are assigned a refactoring code\nreview, along with the issues they typically face for that type\nof assignment. The next subsection details our survey design.\n\nD. Research Method\n\nTo answer our research questions, we follow a mixture\nqualitative and quantitative survey questions, as demonstrated\nin Creswell\u2019s design . The quantitative analysis was per-\nformed by the analysis of ReRs metadata, and the comparison\nbetween refactoring ReRs and non-refactoring ReRs, in terms\nof time to completion and number of exchanged responses.\nDevelopers survey constitutes the qualitative aspect that we\nare going to detail in the next section.\n\nSurvey Design. For our survey design, we followed the\nguidelines proposed by Kitchenham and Pfleeger\n\n",
                    "Rolactoring Review\n\nNon.efactoring Reviow\n\n(a) Review duration\n\nRetactoring Review Non refactoring Review\n\n(b) Number of exchanged responses\n\nFigure (2) Boxplots of (a) review duration and (b) number\nof exchanged responses, for refactoring and non-refactoring\ncode review.\n\nincrease the participation rate, we made our survey anonym-\nous. The survey consisted of 11 questions that are divide:\ninto 2 parts. The first part of the survey includes demo-\ngraphics questions about the participants. In the second part,\nwe asked about the (1) motivations behind refactoring, (2)\ndocumentation of refactoring changes, (3) challenges face:\nwhen reviewing refactoring, (4) verification of refactoring\nchanges, and (5) implications of refactoring on code quality.\nAs suggested by Kitchenham and Pfleeger (27). we constructe\nthe survey to use a 5-point ordered response scale (\u201cLikert\nscale\u201d) question on the general refactoring-related code review,\n2 open-ended questions on the refactoring documentation an\nchallenges, and 5 multiple choice questions on the refactor-\ning motivations, documentation, mechanisms and implications\nwith an optional \u201cOther\u201d category, allowing the respondents\nto share thoughts not mentioned in the list. Table [Il] contains\na summary of the survey questions; the full list is available\nin (25). In order to increase the accuracy of our survey, we\nfollowed the guidelines of Smith et al. (28). and we targeted\ndevelopers who have previously been exposed to refactoring\nin the considered project. So instead of broadcasting the\nsurvey to the entire development body, we only intend to\ncontact developers who have previously authored or reviewed\na refactoring code change. We performed this subject selection\ncriteria to ensure developers\u2019 familiarity with the concept of\nrefactoring so that they can be more prepared to answer the\nquestions. This process resulted in emailing 38 target subjects\nwho are currently active developers and regularly perform\ncode reviews. Participation in the survey was voluntary. In\ntotal, 24 developers participated in the survey (yielding a\nresponse rate of 63%, which is considered high for software\nengineering research (28). The industrial experience of the\n\nrespondents ranged from 1 to 35 years, their refactoring\nexperience ranged from 1 to 30 years, and their experience\nin code review ranged from 1 to 25 years. On average, the\nparticipants had 10.7 years of experience in industry, 7.5 years\nof experience in refactoring, and 6.97 years of experience in\ncode review. Table summarizes developers\u2019 experience in\nindustry, refactoring and code review.\n\nIV. RESULTS & DISCUSSIONS\n\nA. RQI. What motivates developers to apply refactorings in\nthe context of modern code review?\n\nFigure |3| shows developers\u2019 intentions when they refactor\ntheir code. The Code Smell and BugFix categories had the\nhighest number of responses, with a response ratio of 23.7%\nand 22.4%, respectively. The category Functional was the\nthird popular category for refactoring-related commits with\n21.1%, followed by the Internal Quality Attribute and External\nQuality Attribute, which had a ratio of 17.1% and 14.5%,\nrespectively. However, we observe that all motivations do not\nsignificantly vary as all of them are in the interval 14.5% to\n23.7% with no dominant category, as can be seen in Figure [3]\nOnly one participant selected the \u201cother\u201d option stating that,\n\u201cWhen i feel it\u2019s painful to fulfill my current task without\nrefactoring\u201d.\n\nIf we refer to the Fowler\u2019s refactoring book (i). refactoring\nis mainly solicited to enforce best design practices, or to cope\nwith design defects. With bad programming practices, i.e.,\ncode smells, earning 24% of developer responses, these results\ndo not deviate from the Fowler\u2019s refactoring guide. However,\neven though the code smell resolution category is prominent,\nthe observation that we can draw is that motivations driving\nrefactoring vary from structural design improvement to feature\nadditions and bug fixes, i.e., developers interleave refactoring\nwith other development tasks. This observation is aligned with\nthe state-of-the-art studies by Kim et al. , Silva et al.\n{19}, and AlOmar et al. [21]. The sum of the design-related\ncategories, namely code smell, internal, and external quality\nattributes represent the majority with 55.3%. These categories\nencapsulate all developers\u2019 design-improvement changes that\nrange from low level refactoring changes such as renaming\nelements to increase naming quality in the refactored design,\nand decomposing methods to improve the readability of the\ncode, up to higher level refactoring changes such as re-\nmodularizing packages by moving classes, reducing class-level\ncoupling, increasing cohesion by moving methods, etc.\n\nSummary: According to the survey, coping with poor\ndesign and coding style is the main driver for de-\n\nvelopers to apply refactoring in their code changes.\nYet, functional changes and bug fixing activities often\ntrigger developers to refactor their code as well.\n\nB. RQ2. How do developers document their refactorings for\ncode review?\n\nWhen we asked developers, \u201cwhat information do you expli-\ncitly provide when documenting your refactoring activity?\u201d, 21\n",
                    "External QA 14.5%\n\nOther 1.3%\n\nFigure (3) Developers\u2019 refactoring motivations for code re-\nview.\n\nout of the 24 developers (91.3%) indicated that they explicitly\nmention the motivation behind the application of refactoring\nsuch as \u2018improving readability\u2019 and \u2018eliminate code smell\u2019.\nMoreover, only 8 out of the 24 developers (34.8%) indicated\ntheir refactoring strategy by stating explicitly the type of\nrefactoring operation they perform in their submitted code\nchange description, such as \u2018move class\u2019. We observe that\ndevelopers are eager to explain the rationale of their refact-\noring more than the actual refactoring operations performed.\nDue to the nature of inspection, developers need to develop a\n\u201ccase\u201d to justify the need for refactoring, in order to convince\nthe reviewers. Therefore, the majority of participants (91.3%)\nfocus on reporting the motivation rather than the operation.\nMoreover, the identification of the operations can be deducted\nby the reviewers when they inspect the code before and after\nits refactoring. Finally, only a few respondents (6 participants)\nresponded that they thoroughly document their refactoring by\nreporting both the motivation and operation. Moreover, when\nwe asked, \u201cwhat typical keywords you use when documenting\nrefactoring changes for a review?\u201d, the developers answers\ncontain various refactoring phrases. Table [IV|enumerates these\npatterns (keywords in bold indicate that the keyword was\nmentioned by more than one developer).\n\nis quite revealing in several ways. First, we observe\nthat developers state the motivation behind refactoring, and\nthat some of these patterns are not restricted only to fixing\ncode smells, as in the original definition of refactoring in\nFowler\u2019s book (ij. Second, developers tend to use a variety of\ntextual patterns to document their refactoring activities, such as\n\u2018refactor\u2019, \u2018clean up\u2019, and \u2018best practice\u2019. These patterns can\nbe (1) generic to describe the act of refactoring without giving\nany details; or (2) specific to give more insights on how mainly\nprovide a generic description/motivation of the refactoring\nactivity such as \u2019improving readability\u2019. A common trend\namongst developers is that they either report a problem to\nindicate that refactoring action is needed (e.g., \u2018duplicate\u2019,\n\u2018bugs\u2019, \u2018bad code\u2019, etc.), or they state the improvement to the\ncode after the application of refactoring (e.g., \u2018best practice\u2019,\n\u2018ease of use\u2019, \u2018improving code quality\u2019, etc.). By looking at\nthe refactoring discussion (see Figure 22). we realized that\ndevelopers do ask for more details to understand the performed\n\nTable (IV) List of refactoring keywords reported by the\n\nparticipants.\n\nPatterns\n\n(1) allow easier integration with\n(2) bad code\n(3) bad management\n\n(16) fix\n(17) improving code quality\n(18) loose coupling\n\n(31) remove legacy code\n(32) replace hard coded\n(33) reorganiz*\n\n(4) best practice (19) moderniz* (34) restructur*\n(5) break out (20) modif* (35) rewrit*\n\n(6) bugs (21) modulariz* (36) risks\n\n(7) cleanup (22) not documented (37) simply\n\n(8) cohesion (23) open close (38) single responsibility\n\n(9) comment (24) optimiz* (39) single level of abstraction\n(10) complexity (25) performance per function\n\n(11) consistency (26) readability (40) splitting logic\n\n(12) decouple (27) redundaney (Al) strategy pattern\n\n(13) duplicate (28) refactor* (42) stress test results\n\n(14) ease of use\n(15) extract class\n\n(29) regression\n(30) remov*\n\n(43) testing\n(44) uncomment\n\nrefactoring activities.\n\nSummary: Developers rarely report specific refactor-\ning operations as part of their documentation. Instead,\nthey use general keywords to indicate the motivation\nbehind their refactorings. Nevertheless, several pat-\n\nterns are solicited by developers to describe their re-\nfactorings. With the lack of refactoring documentation\nguidelines, reviewers are forced to ask for more details\nin order to recognize the need for refactoring.\n\nC. RQ3. What challenges do reviewers face when reviewing\nrefactoring changes?\n\nAs shown in Figure [4] we report the main challenges faced\nby reviewers when inspecting a refactoring review request.\nThe majority of the developers (17 respondents (70.8%))\ncommunicated that they were concerned about avoiding the\nintroduction of regression in system\u2019s functionality. Interest-\ningly, refactoring by default, ensures the preservation of the\nsystem\u2019s behavior through a set of pre and post conditions,\nyet, reviewers main focus was to validate the behavior of\nthe refactored code. In this context, a recent study have\nshown that developers do not rely on built-in refactoring\nin their Integrated Development Environments (IDEs) and\nthey perform refactoring manually [19], e.g., when moving\na method from one class to another, instead of activating\nthe \u2018move method\u2019 from the refactoring menu, developers\nprefer to cut and paste the method declaration into its new\nlocation, and manually update any corresponding memberships\nand dependencies. Such process is error prone, and therefore,\nreviewers tend to treat refactoring like any other code change\nand inspect the functional aspect of any refactored code.\n\nIn Figure 14 developers (58.3%) revealed the need to\ninvestigate the impact of refactoring on software quality.\nSuch investigation is not trivial, as it has been the focus of\na plethora of previous studies (e.g., 29), finding that not\nall refactoring operations have beneficial impact on software\nquality, and so developers need to be careful as various design\nand coding defects may require different types of refactorings.\nIn this context, we identified, in our previous study which\n\n",
                    "structural metrics (coupling, complexity, etc.) are aligned\nwith the developer\u2019s perception of quality optimization when\ndevelopers explicitly mention in their commit messages that\nthey refactor to improve these quality attributes. Interestingly,\nwe observed that, not all structural metrics capture developers\nintentions of improving quality, which indicated the existence\nof a gap between what developers consider to be a design\nimprovement, and their measurements in the source code.\nWhen asked about their quality verification process, developers\nuse, as part of their internal process, the Quality Gate of\nSonarQube. While SonarQube is a popular, widely adopted\nquality framework, it suffers, like any other static analysis\ntools, from the high false positiveness of its findings, when\nit is not properly tuned.\n\nA moderate subset of 11 developers (45.8%) were con-\ncerned about having inadequate documentation about refact-\noring, whereas 10 developers (41.7%) were concerned about\nunderstanding the motivations for refactoring changes. 9 de-\nvelopers (37.5%) found that reviewing refactoring changes in a\ntimely manner is difficult, whereas 6 of them (25%) found that\nthe challenge is centered around understanding how refactor-\ning changes were implemented. In addition to these challenges,\ntwo participants stated, \u201cThe quality of code readability (being\nable to understand what the code author intended to do with\nthe logic/algorithm even without documentation\u201d, and \u201cStyle\nchanges or personal preference that the author holds and feels\nstrongly about\u201d.\n\nTo get a more qualitative sense, we also study bad refactor-\ning practices that reviewers catch when reviewing refactoring\nchanges. We analyzed the survey responses to this open ques-\ntion to create a comprehensive high-level list of bad refactoring\npractices that are being caught by reviewers. These practices\nare centered around five main topics: (1) interleaving refact-\noring with multiple other development-related tasks, (2) lack\nof refactoring documentation, (3) avoiding refactoring negative\nside effects on software quality, (4) inadequate testing, and (5)\nlack of design knowledge. In the rest of this subsection, we\nprovide more in-depth analysis of these refactoring practices.\n\nChallenge #1: Interleaving refactoring with multiple other\ndevelopment-related tasks. One participant indicated that,\n\u201cRefactoring changes are intermixed with bug fix changes\u201d\nand another mentioned \u201cRefactoring after adding to many\nfeatures\u2019, indicating that these practices are not desirable when\nperforming or reviewing refactoring changes. This suggests\nthat interleaving refactoring with bug fixes and new features\ncould be a challenge from a reviewer\u2019s point of view. Even\nthough we did not ask a specific question concerning interleav-\ning refactorings with other development-related context, three\nparticipants acknowledged that mixing refactoring with any\nother activity is a potential problem. This can be explained by\nthe fact that behavior preservation cannot be guaranteed and\nit may introduce new bugs.\n\nChallenge #2: Lack of refactoring documentation. In con-\ntrast with how developers document bug fixes and functional\nchanges, the documentation of refactoring seems to be vague\n\nand unstructured. If we refer to our findings in our previ-\nous research question, developers lack guidelines on how to\ndescribe their refactoring activities, and they refer to their\npersonal interpretation to justify their decisions. To mitigate\nthis ambiguity, there is a need for proper methodology that\narticulates how developers should document refactoring code\nchanges. Reviewers did explicitly share their concerns during\nthe survey:\n\u201c1. Lack of documentation, 2. Inconsistent variable nam-\ning, 3. Unorganized code, 4. No explanation why changes\nwere made [...]\u201d; \u201c[...],no guideline, different guidelines\nused in the project, bad code practices\u2019; \u201c[...] Not enough\ncomments\u201d\n\nChallenge #3: Avoiding refactoring negative side effects on\nsoftware quality. The majority of the participants commented\nthat wrongly naming code elements and duplicate code are the\ncommon bad refactoring practices that they typically catch. It\nhas been proven by previous studies that a developer may\naccidentally introduce a design anti-pattern while trying to\nfix another (e.g., [30}). One mentioned example was how a\nlong method (large in lines of code, and has more than one\nfunctionality) can be fixed by splitting the method into two,\nusing the extract method refactoring operation. However, if the\nsplit does not create two cohesive methods (i.e., segregation\nof concerns), then the results could be two tightly coupled\nmethods, which one method can envy the other method\u2019s\nattributes (i.e., feature envy anti-pattern). Thus, it is part of the\ncode review to verify the impact of refactoring on the software\ndesign from different perspectives (e.g., code smell removal,\nadherence to object-oriented design practices such as SOLID\nand GRASP, etc.). We report samples of the participants\u2019\ncomments below to illustrate this challenge:\n\u201cPoorly named methods, poorly named variables, lack of\nbasic Object Oriented Design principles and concepts,\nincreased complexity, increased coupling.\u201d \u201cduplication,\nlow-cohesion\u201d, \u201cCode refactoring does not follow the\ncoding standards set by the project. [...]\u201d; \u201cTight coup-\nling, Lack of tests, convoluted logic, inconsistent variable\nnames, outdated comments\u201d\nChallenge #4: Inadequate testing. By default, refactoring is\nsupposed to preserve the behavior of the software. Ideally,\nusing the existing unit tests to verify that the behavior is\nmaintained should be sufficient. However, since refactoring\ncan also be interleaved with other tasks, then there might be a\nchange in the software\u2019s behavior, and so, unit tests, may not\ncapture such changes if they were not revalidated to reflect\nthe newly introduced functionality. This can be a concern\nif developers are unaware of such non behavior preserving\nchanges, and so, deprecated unit tests will not guarantee the\nrefactoring correctness. The following reviewers\u2019 comments\nillustrate this challenge:\n\u201c1) Not testing refactor code changes on all potential\nimpacted areas 2) Not adding newly named functions to\nold test suites [...]\u201d; \u201c[...] partial testing process\u201d; \u201c[...]\n",
                    "99, 6\n\nNo follow-up testing\u201d; \u201c[...] No regression testing\u201d; \u201cTight\n\ncoupling, Lack of tests [...]\u201d\nChallenge #5: Lack of design knowledge. Developers typ-\nically refactor classes and methods that they recently and\nfrequently change. So, the more they change the same code\nelements, the more confident they become about their design\ndecisions. However, not all team members have access to all\nsoftware codebase, and so they do not draw the full picture\nof the software design, which makes their decision adequate\nlocally, but not necessarily at the global level. Moreover,\ndevelopers only reason on the actual screenshot of the current\ndesign, and there is no systematic way for them to recognize\nits evolution by, for instance, accessing previously performed\nrefactorings. This may also narrow their decision making, and\nthey may end up reverting some previous refactorings. These\nconcerns along others were also raised by participants, for\ninstance, one participant stated:\n\n\u201cLack of knowledge about existing design patterns in code\n\n(strategy, builder, etc.) and their context along with lack\n\nof knowledge about SOLID principles (especially open\n\nclose and dependency inversion). I\u2019ve seen people claim\n\nthat the code cannot be tested but in reality the problem\n\nis in the way they\u2019ve structured their code.\u201d\nIt is clear that the code review plays also a major role in\nknowledge transfer between junior and senior developers, and\nin educating software practitioners about writing clean code\nthat meet quality standards.\n\nSummary: Challenges of reviewing refactored code\ninherits challenges of reviewing traditional code\nchanges, as refactoring can also be mixed with func-\ntional changes. Reviewers also report the lack of\nrefactoring documentation, and inspect any negative\n\nside effects of refactorings on design quality The\ninadequate testing of such changes hinder the safety\nof the performed refactoring. Finally, the lack of de-\nveloper\u2019s exposure to whole system design can reduce\nthe visibility of their refactoring decision making.\n\nD. RQ4. What mechanisms are used by developers and re-\nviewers to ensure code correctness after refactoring?\n\nDevelopers reported mechanisms to verify the application\nof refactoring (see Figure 5p. 23 of the participants (95.8%)\nrefer to testing the refactored code; 17 (70.8%) reported\ndoing manual validation; 11 (45.8%) brought up ensuring the\nimprovement of software quality metrics; 9 (37.5%) mentioned\nusing visualization techniques; and 9 (37.5%) selected running\nstatic checkers and linters. Besides performing testing, two\nparticipants mentioned in the \u201cother\u201d option: \u201cAutomated Test\nCoverage\u201d, and \u201cExisting Unit tests\u2019.\n\nWe observe that reviewers treat refactoring like any tra-\nditional code change, and they unit-test it for correctness.\nThis eventually minimizes the introduction of faults. However,\nwhen developers assume refactoring is preserving the behavior,\n\nwhile it is not, then they may not have updated their unit\ntests, and so their execution later by reviewers can become\nunpredictable, i.\u00a2., some test cases may or may not fail because\nof their deprecation. Furthermore, some refactoring operations,\nsuch as \u2019extract method\u2019, do create new code elements that\nare not covered by unit tests. So reviewers need to enforce\ndevelopers to write test cases for any newly introduced code.\n\nReviewers also refer to the quality gate to inspect if they\nrefactoring did not introduce any design debt or anti-patterns\nin the system. Yet, the manual inspection of the code is still the\ntules, some reviewers refer to visualizing the code before and\nafter refactoring to verify the completeness of the refactoring.\n\nSummary: Since reviewers unit test refactoring, just\nlike any other code change, developers need to add or\n\nupdate unit tests to the newly introduced or refactored\ncode. Furthermore, reviewers are manually inspecting\nthe refactored code to guarantee its correctness.\n\nE. RQS5. How do developers and reviewers assess and perceive\nthe impact of refactoring on the source code quality?\n\nAs can be seen from Figure [6] all participants (24, 100%)\nreplied that the code becomes more readable and understand-\nable. Intuitively, the main purpose of refactoring, is to ease\nthe maintenance and evolution of software. So reviewers,\nimplicitly consider refactoring to be an opportunity to clean\nthe code and make it adhere to the team\u2019s coding conventions\nand style. Also, 12 (50%) indicated that it becomes easier to\npass Sonar Qube\u2019s Quality Gate. So, it is expected that the\nrefactored code does not increase the quality deficit index, if\nnot decreasing it. Finally, 11 (45.8%) stated their expectation\nthat refactored, through better renames, and more modular\nobjects, should reduce the code\u2019s proneness to bugs.\n\nSummary: Besides using Quality Gates and static\ncheckers to assess the impact of refactoring on the\n\nsoftware design, reviewers rate the success of refact-\noring to the extent to which the refactored code has\nimproved in terms of readability and understandability.\n\nV. RECOMMENDATIONS\nA. Recommendations for Practitioners\n\nIt is heartening for us to realize that developers refactor\ntheir code and perform reviews for the refactored code. Our\nmain observation, from developers\u2019 responses, is how the\nreview process for refactoring is being hindered by the lack\nof documentation. Therefore, as part of our survey report to\nthe company, we designed a procedure for documenting any\nrefactoring ReR, respecting three dimensions that we refer to\nas the three Zs, namely, Intent, Instruction, and Impact. We\ndetail each one of these dimensions as follows:\n\nIntent. According to our survey results, (cf, Figure [3) it\nis intuitive that reviewers need to understand the purpose of\nthe intended refactoring as part of evaluating its relevance.\n\n",
                    "Avoiding the introduction of 70.8\nregression in system functionalities 5\nUnderstanding the impact of (a 58.3\nrefactoring on quality 5\nUnderstanding the motivation\nbehind refactoring 45.8\nInadequate documentation\nabout factoring | 41.7\nReviewing refactorings in\ntimely nner A 37.5\nUnderstanding how refactoring (mes\nchanges were implemented\na\n\n0 20 40 60 80 100\n\nFigure (4) Challenges faced by developers when reviewing\nrefactoring.\n\nTesting by running the old\nversion and the new versions\nand make sure they still\ngive the same result\n\nManual validation / experience\n\nEnsuring the improverment\nsoftware quality metrics\n\nVisualization of refactored\ncode\n\nRunning static checkers and\nlinters\n\n0 20 40 60 80 100\n\nFigure (5) Mechanisms used to ensure the correctness after\nthe application of refactoring.\n\nCode becomes more\nreadable and understandable\n\n100\n\nIt becomes easier to 50\npass quality gate\n\nCode becomes less prone\nto bugs and errors\n\n45.8\n0 20 40 60 80 100\n\nFigure (6) Implications experienced as software evolves\nthrough refactoring.\n\nTherefore, when preparing the request for review, developers\nneed to start with explicitly stating the motivation of the\nrefactoring. This will provide the context of the proposed\nchanges, for the reviewers, so they can quickly identify how\nthey can comprehend it. According to our initial investigations,\nexamples of refactoring intents, reported in Table |IV| include\nenforcing best practices, removing legacy code, improving\nreadability, optimizing for performance, code clean up, and\nsplitting logic.\n\nInstruction. Our second research question shows how rarely\ndevelopers report refactoring operations as part of their docu-\nmentation. Developers need to clearly report all the refactor-\ning operations they have performed, in order to allow their\nreproducibility by the reviewers. Each instruction needs to\nstate the type of the refactoring (move, extract, rename, etc.)\nalong with the code element being refactored (i.e., package,\nclass, method, etc.), and the results of the refactoring (the\nnew location of a method, the newly extracted class, the new\nname of an identifier, etc.). If developers have applied batch or\ncomposite refactorings, they need to be broken down for the\n\nreviewers. Also, in case of multiple refactorings applied, they\nneed to be reported in their execution chronological order.\n\nImpact. We observe from Figures[4] and[6] that practitioners\ncare about understanding the impact of the applied refactoring.\nThus, the third dimension of the documentation is the need to\ndescribe how developers ensure that they have correctly imple-\nmented their refactoring and how they verified the achievement\nof their intent. For instance, if this refactoring was part of a\nbug fix, developers need to reference the patch. If developers\nhave added or updated the selected unit tests, they need to\nattach them as part of review request. Also, it is critical to self-\nassess the proposed changes using Quality Gate, to report all\nthe variations in the structural measurements and metrics (e.g.,\ncoupling, complexity, cohesion, etc.), and provide necessary\nexplanation in case the proposed changes do not optimize the\nquality deficit index.\n\nUpon its acceptance for trial at Xerox, a set of developers\nhave adopted the Js procedure when submitting any refactoring\nrelated code change. These developers were initially given\nsupport for adopting it by us rewriting samples of their previ-\nous code review requests, using our template. We will closely\nmonitor its adoption, and perform any necessary tweaking. We\nalso plan on following up on whether this practice was able\nto be beneficial for reviewers by (1) empirically validating\nwhether refactoring ReRs, using our template, take less time\nto be reviewed, in comparison with other refactoring ReRs;\nand (2) rescheduling another follow up interview with the\ndevelopers have been using it.\n\nB. Recommendations for Research and Education\n\nProgram Comprehension. Refactoring for readability was\npointed out by the majority of participants. In contrast with\nstructural metrics, being automatically generated by the Qual-\nity Gate, reviewers are currently relying on their own in-\nterpretation to assess the readability improvement, and such\nevaluation can be subjective and time-consuming. There is\na need for a refactoring-aware code readability metrics that\nspecifically evaluate the code elements that were impacted\nby the refactoring. Such metrics help in contextualizing the\nmeasurement to fulfill the developer\u2019s intention.\n\nTeaching Documentation Best Practices. Prospective soft-\nware engineers are mainly taught how to model, develop and\nmaintain software. With the growth of software communities,\nand their organizational and socio-technical issues, it is im-\nportant to also teach the next generation of software engineers\nthe best practices of refactoring documentation. So far, these\nskills can only be acquired by experience or training.\n\nVI. THREATS TO VALIDITY\n\nConstruct & Internal Validity. Concerning the complete-\nness and correctness of our interpretation of open responses\nwithin the survey, we did not extensively discuss all responses\nbecause some of them are open to various interpretations,\nand we need further follow up surveys to clarify them.\nConcerning the selection criteria of the participants, we tar-\ngeted participants whose code review description included the\nkeyword \u201crefactor*\u201d. Since the validity of our study requires\n",
                    "familiarity with the concept of refactoring, we assume that\nparticipants who used this keyword know the meaning and\nthe value of refactoring. Another potential threat relates to\nthe communication channel to identify the motivation driving\ncode review involving refactoring. We examined threaded\ndiscussions and some situations may not have been easily\nobservable. For example, determining whether the reviewer\nconfusion was primarily caused by the refactoring and not\nby another phenomenon is not practically easy to assess\nthrough discussions. Interviewing developers would be a good\ndirection to consider in the future to capture such motivations.\nExternal Validity. Concerning the representativeness of\nthe results, we designed our study with the goal of better\nunderstanding developer perception of code review involving\nrefactoring actions within a specific company. Further research\nin this regard is needed. As with every case study, the results\nmay not generalize to other contexts and other companies. But\nextending this survey with the open-source communities is part\nof our future investigation to challenge our current findings.\n\nVII. CONCLUSION\n\nUnderstanding the practice of refactoring code review is\nof paramount importance to the research community and\nindustry. In this work, we aim to understand the motivations,\ndocumentation, challenges, mechanisms and implications of\nrefactoring-aware code review by carrying out an industrial\ncase study of 24 software engineers at Xerox. In summary,\nwe found that: (1) refactoring is completed for a wide variety\nof reasons, going beyond its traditional definition, such as\nreducing the software\u2019s proneness to bugs, (2) refactoring-\nrelated patterns mainly demonstrate developer perception of\nrefactoring, but practitioners sometimes provide information\nabout refactoring operations performed in the source code, (3)\nparticipants considered avoiding the introduction of regression\nin system functionality as the main challenge during their re-\nview, (4) although participants do use different static checkers,\ntesting is the main driver for developers to ensure correctness\nafter the application of refactoring, and (5) readability and\nunderstandability improvement is the primary implications of\nrefactoring on software evolution.\n\nVIII. ACKNOWLEDGEMENTS\n\nWe would like to thank the Software Development Man-\nager Wendy Abbott for approving the survey and all Xerox\ndevelopers who volunteered their time to participate in this\nresearch.\n\nREFERENCES\n\n[I] M. Fowler, K. Beck, J. Brant, W. Opdyke, and d. Roberts, Refactoring: Improving\nthe Design of Existing Code. Boston, MA, USA: Addison-Wesley Longman\nPublishing Co., Inc., 1999.\n\n[2] W. Cunningham, \u201cThe wycash portfolio management system,\u201d ACM SIGPLAN\nOOPS Messenger, vol. 4, no. 2, pp. 29-30, 1992.\n\n[3] A. Bacchelli and C. Bird, \u201cExpectations, outcomes, and challenges of modern code\nreview,\u201d in International conference on software engineering, pp. 712-721, 2013.\n\n[4] C. Sadowski, E. S\u00e9derberg, L. Church, M. Sipko, and A. Bacchelli, \u201cModern\ncode review: a case study at google,\u201d in International Conference on Software\nEngineering: Software Engineering in Practice, pp. 181-190, 2018.\n\n[5]\n\n[6\n\n(7)\n\n[8\n\n9\n\n[10]\n\noy\n\n[12]\n\n[13]\n\n[14]\n\n15]\n\n[16]\n\n07]\n\n[18]\n\n[19]\n\n[20]\n\nPi\n\n[22]\n\n[23]\n\n[24]\n\n[25\n\n[26]\n\n227]\n\n[28]\n\n[29]\n\n[30]\n\nA. Bosu, J. C. Carver, C. Bird, J. Orbeck, and C. Chockley, \u201cProcess aspects\nand social dynamics of contemporary code review: Insights from open source\ndevelopment and industrial practice at microsoft,\u201d IEEE Transactions on Software\nEngineering, vol. 43, no. 1, pp. 56-75, 2016.\n\nN. Tsantalis, T. Chaikalis, and A. Chatzigeorgiou, \u201cJdeodorant: Identification and\nremoval of type-checking bad smells,\u201d in 2008 12th European Conference on\nSoftware Maintenance and Reengineering, pp. 329-331, IEEE, 2008.\n\nW. Mkaouer, M. Kessentini, A. Shaout, P. Koligheu, S. Bechikh, K. Deb, and\nA. Ouni, \u201cMany-objective software remodularization using nsga-iii;\u201d ACM Transac-\ntions on Software Engineering and Methodology (TOSEM), vol. 24, no. 3, pp. 1-45,\n2015.\n\nA. Ouni, M. Kessentini, H. Sahraoui, K. Inoue, and K. Deb, \u201cMulti-criteria code\nrefactoring using search-based software engineering: An industrial case study,\u201d\nACM Transactions on Software Engineering and Methodology (TOSEM), vol. 25,\nno. 3, p. 23, 2016.\n\nE, Murphy-Hill and A. P. Black, \u201cRefactoring tools: Fitness for purpose,\u201d IEEE\nsoftware, vol. 25, no. 5, pp. 38-44, 2008.\n\nR. Arcoverde, A. Garcia, and E. Figueiredo, \u201cUnderstanding the longevity of code\nsmells: preliminary results of an explanatory survey,\u201d in Proceedings of the 4th\nWorkshop on Refactoring Tools, pp. 33-36, ACM, 2011.\n\nA. Yamashita and L. Moonen, \u201cDo developers care about code smells? an\nexploratory survey,\u201d in Working Conference on Reverse Engineering (WCRE),\npp. 242-251, 2013.\n\nM. Kim, T. Zimmermann, and N. Nagappan, \u201cAn empirical study of refactor-\ningchallenges and benefits at microsoft,\u201d IEEE Transactions on Software Engin-\neering, vol. 40, no. 7, pp. 633-649, 2014.\n\nG. Szdke, C. Nagy, R. Ferenc, and T. Gyim\u00e9thy, \u201cA case study of refactoring large-\nscale industrial systems to efficiently improve source code quality,\u201d in International\nConference on Computational Science and Its Applications, pp. 524-540, Springer,\n2014.\n\nT. Sharma, G. Suryanarayana, and G. Samarthyam, \u201cChallenges to and solutions\nfor refactoring adoption: An industrial perspective,\u201d IEEE Software, vol. 32, no. 6,\npp. 44-51, 2015.\n\nC. D. Newman, M. W. Mkaouer, M. L. Collard, and J. I. Maletic, \u201cA study on\ndeveloper perception of transformation languages for refactoring,\u201d in International\nWorkshop on Refactoring, pp. 34-41, 2018.\n\nX. Ge, S. Sarkar, J. Witschey, and E. Murphy-Hill, \u201cRefactoring-aware code\nreview,\u201d in IEEE Symposium on Visual Languages and Human-Centrie Computing\n(VL/HCC), pp. 71-79, 2017.\n\nE. L. Alves, M. Song, T. Massoni, P. D. Machado, and M. Kim, \u201cRefactoring\ninspection support for manual refactoring edits,\u201d JEEE Transactions on Software\nEngineering, vol. 44, no. 4, pp. 365-383, 2017.\n\nL. MacLeod, M. Greiler, M.-A. Storey, C. Bird, and J. Czerwonka, \u201cCode reviewing\nin the trenches: Challenges and best practices,\u201d IEEE Software, vol. 35, no. 4,\npp. 34-42, 2017.\n\nD. Silva, N. Tsantalis, and M. T. Valente, \u201cWhy we refactor? confessions of\ngithub contributors,\u201d in Proceedings of the 2016 24th ACM SIGSOFT International\nSymposium on Foundations of Software Engineering, FSE 2016, (New York, NY,\nUSA), pp. 858-870, ACM, 2016.\n\nE, Murphy-Hill, C. Parnin, and A. P. Black, \u201cHow we refactor, and how we know\nit?\u2019 IEEE Transactions on Software Engineering, vol. 38, pp. 5-18, Jan 2012.\n\nE, A. AlOmar, A. Peruma, M. W. Mkaouer, C. Newman, A. Ouni, and M. Kes-\nsentini, \u201cHow we refactor and how we document it? on the use of supervised\nmachine learning algorithms to classify refactoring documentation,\u201d Expert Systems\nwith Applications, p. 114176, 2020.\n\nE. A. AlOmar, M. W. Mkaouer, and A. Ouni, \u201cCan refactoring be self-affirmed?\nan exploratory study on how developers document their refactoring activities in\ncommit messages,\u201d in 2019 IEEE/ACM 3rd International Workshop on Refactoring\n(1WoR), pp. 51-58, IEEE, 2019.\n\nE, A. AlOmar, M. W. Mkaouer, A. Ouni, and M. Kessentini, \u201cOn the impact of\nrefactoring on the relationship between quality attributes and design metrics,\u201d in\n2019 ACMAEEE International Symposium on Empirical Software Engineering and\nMeasurement (ESEM), pp. 1-11, IEEE, 2019.\n\nE. A. AlOmar, M. W. Mkaouer, and A. Ouni, \u201cToward the automatic classification\nof self-affirmed refactoring,\u201d Journal of Systems and Software, vol. 171, p. 110821,\n2020.\n\nAlOmar., https://smilevo.github.io/self-affirmed-refactoring/, 2020 (last accessed\nOctober 16, 2020).\n\nJ. W. Creswell, \u201cResearch design: Quantitative, qualitative and mixed methods,\u201d\n2009.\n\nB. A. Kitchenham and S. L. Pfleger, \u201cPersonal opinion surveys,\u201d in Guide to\nadvanced empirical software engineering, pp. 63-92, Springer, 2008.\n\nE, Smith, R. Loftin, E. Murphy-Hill, C. Bird, and T. Zimmermann, \u201cImproving\ndeveloper participation rates in surveys,\u201d in 2013 6th International Workshop on\nCooperative and Human Aspects of Software Engineering (CHASE), pp. 89-92,\nIEEE, 2013.\n\nG. Bavota, A. De Lucia, M. Di Penta, R. Oliveto, and F. Palomba, \u201cAn experimental\ninvestigation on the innate relationship between quality and refactoring,\u201d Journal\nof Systems and Software, vol. 107, pp. 1-14, 2015.\n\nF, Palomba, G. Bavota, M. Di Penta, F. Fasano, R. Oliveto, and A. De Lucia,\n\u201cOn the diffuseness and the impact on maintainability of code smells: a large scale\nempirical investigation,\u201d Empirical Software Engineering, vol. 23, no. 3, pp. 1188-\n1221, 2018.\n\n"
                ]
            }
        },
        {
            "file_name": "S:\\OneDrive\\@Dev\\!GPT\\ScriptGPT\\library\\Refactoring\\Source\\Software Testing and Code Refactoring.pdf",
            "time_taken": 17.901029109954834,
            "data_extracted": {
                "text": [
                    "Software Testing and Code Refactoring: A Survey\nwith Practitioners\n\nDanilo Leandro Lima\nAccenture\nRecife, PE, Brazil\ndaniloleandro@gmail.com\n\nSildemir S. da Silva\nCESAR School\nRecife, PE, Brazil\nsss@cesar.school\n\nAbstract\u2014Nowadays, software testing professionals are com-\nmonly required to develop coding skills to work on test automa-\ntion. One essential skill required from those who code is the\nability to implement code refactoring, a valued quality aspect\nof software development; however, software developers usually\nencounter obstacles in successfully applying this practice. In this\nscenario, the present study aims to explore how software testing\nprofessionals (e.g., software testers, test engineers, test analysts,\nand software QAs) deal with code refactoring to understand\nthe benefits and limitations of this practice in the context of\nsoftware testing. We followed the guidelines to conduct surveys\nin software engineering and applied three sampling techniques,\nnamely convenience sampling, purposive sampling, and snow-\nballing sampling, to collect data from testing professionals. We\nreceived answers from 80 individuals reporting their experience\nrefactoring the code of automated tests. We concluded that in the\ncontext of software testing, refactoring offers several benefits,\nsuch as supporting the maintenance of automated tests and\nimproving the performance of the testing team. However, prac-\ntitioners might encounter barriers in effectively implementing\nthis practice, in particular, the lack of interest from managers\nand leaders. Our study raises discussions on the importance of\nhaving testing professionals implement refactoring in the code of\nautomated tests, allowing them to improve their coding abilities.\n\nIndex Terms\u2014software testing, test automation, code refactor-\ning, test engineers.\n\nI. INTRODUCTION\n\nSoftware quality is defined as the degree to which the\nsoftware meets the expectation of clients and users and the\nextent to which it adheres to both specifications of products\nand processes [1]; this means that software quality includes not\nonly tasks related to the software evaluation (e.g., testing), but\nalso how the whole software development life-cycle adheres\nto product standards, processes, and procedures (e.g., reviews,\ncode quality, automation, among others) [2].\n\nSoftware quality is an umbrella, i.e., an overall evaluation\nprocess composed of multiple attributes that can attest that\nthe development of a system was successful [3]. Testing is the\nmost common quality assurance strategy applied in software\n\nRonnie de Souza Santos\nCape Breton University & CESAR School\nSydney, NS, Canada\nronnie_desouza@cbu.ca\n\nCesar Franga\nCESAR School\nRecife, PE, Brazil\nfranssa.cesar.school\n\nGuilherme Pires Garcia\nAgape2IT\nRecife, PE, Brazil\nguilhermeg@agap2.pt\n\nLuiz Fernando Capretz\nWestern University\nLondon, ON, Canada\nIcapretz@uwo.ca\n\ndevelopment. Software testing involves appraising the system\nfunctionalities to determine that they behave as expected, e.g.,\nsearching for failures [4]. Software testing can be performed\non several levels (e.g., unit, component, integration, system),\nusing different approaches (e.g., white box, black box, or grey\nbox), and essentially performed in two ways, either manually\nor using automation [5].\n\nManual testing requires a great amount of human interac-\ntion, as professionals seek to identify failures by interacting\ndirectly with the system and observing its behavior. On the\nother hand, automated testing relies on building and executing\ncode to simulate human interaction and observe the system\nbehavior to identify failures [6]. Currently, automated testing\nis prioritized in many projects as this technique is presumed to\nbe faster, less repetitive, and provide higher software coverage\n(e.g., more tests can be performed at the same time) [7]-[9].\nHowever, relying on automated tests can be costly as they\nrequire regular maintenance and frequent updating [10].\n\nCode maintenance is a crucial software quality factor;\ntherefore, automated testing demands from software testing\nprofessionals a whole set of technical expertise in coding and\nversioning, in particular, towards refactoring testing code [10],\n[11]. Refactoring is the process of changing the code without\nchanging the external behavior of the software. Software\ndevelopers widely apply this practice to improve the quality\nof the source code [12], [13]. However, professionals can\nencounter difficulties implementing refactoring in their code\n[14].\n\nAs refactoring is a practice related to code maintenance,\nit is only natural that testing professionals who work with\nautomation must use this practice in their work, which raises\nthe following research question:\n\nResearch question: How do software testing profession-\nals deal with code refactoring when they are working with\ntest automation?\n\nFrom this introduction, our study is organized as follows.\n",
                    "In Section II, we discuss existing studies about refactoring. In\nSection III, we describe how we conducted the survey, while\nSection IV presents our results. In Section V, we discuss the\nimplications of our study. Finally, Section VI summarizes our\ncontributions.\n\nIl. BACKGROUND\n\nRefactoring promotes improvements in the software by ap-\nplying changes to its internal components (e.g., the code) while\nmaintaining its observable behavior, i.e., this is a quality strat-\negy that aims to improve code readability, understandability,\nand maintenance and provide professionals with opportunities\nto implement re-engineering [12], [13], [15].\n\nUsually, refactoring is a practice associated with the work\nof developers/programmers, i.e., those who deal more directly\nwith the source code [16]. Developers know that refactoring\nimproves their code; however, several barriers make it difficult\nto do so in many software projects. These barriers include\na lack of resources, tight deadlines, complex changes in\nthe code, high costs, lack of technical knowledge, lack of\nmanagement, and unavailability of appropriate tools [14].\n\nCurrently, with the dynamic software environment result-\ning from agile development, software testing professionals\nare required to acquire more programming skills. Therefore,\nknowing how to code has become one of the primary abilities\nrequested in job advertisements for testing professionals [17].\nSoftware companies require testing professionals to develop a\ncertain level of programming capabilities to successfully build\na career in software testing [18], which means that knowing\nhow to refactor code is a necessary technical skill.\n\nSoftware testing professionals implement code, especially\nwhen working with test automation. Knowing how to automate\ntests is one of the most valued technical skills in the software\nindustry [17], [19], in particular, because of the benefits of test\nautomation, which include the improvement of product quality,\nhigh test coverage, reduced testing time, reliability, reusability,\nreduced human effort, cost reduction, and increased fault\ndetection [20].\n\nHowever, there are challenges associated with testing au-\ntomation that sometimes generates problems for testing pro-\nfessionals, such as high costs, unavailability of appropriate\ntools, inadequate testing structure, insufficient programming\nknowledge, and constant need for code maintenance [21].\nMany of these challenges are similar to or closely related to\nthe barriers that programmers face when refactoring code [14].\n\nIII. METHOD\n\nIn this study, we conducted a cross-sectional survey [22] to\nexplore software testing professionals\u2019 experience with refac-\ntoring the code of automated tests. Our goal is to understand\nhow testing professionals perceive the general benefits and\nlimitations of code refactoring, which are usually associated\nwith the work of software developers (e.g., programmers). To\nachieve this goal, we surveyed practitioners following three\nguidelines for conducting surveys in software engineering\n[23]-[25]. Figure 1 presents an overview of our study, and\nour methodology steps are presented below.\n\nA, Using the Literature to Investigate the Industry Practice\n\nMany studies explore the influence of code refactoring\non the general quality of software [12], [13], [15]. Usually,\nthe difficulties of conducting code refactoring are discussed\nconsidering the work of developers [14]. However, nowadays,\nwe observe in the software industry an increase in the need for\ntesting professionals to improve their programming skills [17],\n[19], which includes dealing with code refactoring. Therefore,\nwe designed this survey using the findings published in the\nliterature to explore how testing professionals are experiencing\ncode refactoring when working with test automation tasks in\nthe industry.\n\nB. Instruments\n\nWe built our questionnaire based on several published evi-\ndence in the literature about how software developers deal with\ncode refactoring [14]-[18], [21]. In particular, we followed\nthe findings discussed by [14] to explore how the general\nchallenges of code refactoring could apply to software testing.\nTherefore, our questionnaire is mainly based on the results\npublished in the literature that investigated how developers\nwork with code refactoring, including:\n\n- Testing professionals\u2019 knowledge about test automation\nand code refactoring: We asked professionals whether\nthey have experience working with test automation\nor not since this is a valued technical skill in the\nsoftware industry nowadays, as discussed in [17], [18].\nIn particular, the perception of code refactoring would\nbe different depending on the experience of participants\nwith programming.\n\n- Benefits and challenges of refactoring: we asked\nparticipants what are the benefits and challenges of code\nrefactoring based on what was reported in previous\nstudies [14]-[16], [21]. These questions were built to\nexplore the types of refactoring performed in the code\nof automated tests, the benefits of this practice, and\nthe challenges associated with refactoring testing code\n\nregularly.\n\n- Practitioners profile: we asked demographic questions\nabout the participants\u2019 backgrounds to understand our\nsample profile. We asked participants to inform their\ngender, the location of the company where they work,\ntheir education level, their experience level (i.e., years\nof experience working with software development), and\nwhether they have testing certifications.\n\n- Volunteer participation: we started the questionnaire by\ninforming practitioners about the goal of our study. In\naddition, we made sure that they knew their participation\nwas voluntary and anonymous. Therefore, in order to\nanswer the questionnaire, the participants needed to\nagree to participate under these conditions.\n",
                    "INDUSTRY PROBLEM\n\nTesting professionals\nare expected to develop\nand improve\nprogramming skills to\nwork with automation.\n\nPREVIOUS RESEARCH\n\nResearchers investigated\ncode refactoring for the\nperspective of software\n\ndevelopers, e.g.,\nprogrammers.\n\nINDUSTRY STUDY\n\nHow do software testing\nprofessionals deal with\ncode refactoring when\n\nthey are working with\ntest automation?\n\nPRESENT PAPER\n\nPerspective of testing\nprofessionals on code\nrefactoring and\nrecommendations to\npractitioners.\n\nMOTIVATION\n\nLITERATURE\n\nSURVEY IMPLICATIONS\n\nFig. 1. Study Overview\n\nDuring the process of designing the survey questionnaire,\nto increase construct validity and delimit the scope of our\ninvestigation, we defined test automation as the automation of\ntest cases performed by software testing professionals at the\nsystem level or for regression testing. Therefore, unit testing\nand integration testing were not explored in this study.\n\nOnce all questions were defined, a pilot questionnaire was\nvalidated by three senior testing professionals who suggested\nchanges in the wording and terms used in questions 7, 10,\n11, and 12 and proposed modifying the sequence of some\nquestions. The participants who validated the pilot question-\nnaire agreed that presenting the questionnaire with mostly\nclosed-ended questions is a good strategy for exploring the\ntopic since refactoring is not consistently common among\ntesting professionals in the industry. Therefore, having options\nto be selected would increase the willingness of participants\nto answer the survey, i.e., extensive open-ended questions or\ninterviews might have increased the difficulty of obtaining\ndata.\n\nThe final questionnaire was composed of 12 questions.\nNone of the questions were optional (e.g., participants needed\nto answer every question). We included an option for the\nparticipants to provide other answers different from those\nlisted in the questionnaire, even though, as stated above, our\nprimary goal is to explore in the industry setting a topic that\nis being mainly explored in the academic context or with\ndifferent practitioners (e.g., developers). Table I presents the\nfinal questionnaire.\n\nC. Data Collection\n\nWe followed the recommendations for treating software pro-\nfessionals as a hidden population [26], which is a strategy that\n\ncan mitigate sampling bias in software engineering surveys, as\nwell as support reaching more individuals of a population.\n\nTreating software professionals as hidden populations\nmakes sense in this study because a hidden population cannot\nbe easily defined or enumerated based on existing knowl-\nedge [26]. In this case, enumerating all software testing\nprofessionals despite their experience with code refactoring is\nimpractical. Recently, the number of studies treating software\nengineers as a hidden population in the literature is increasing\n[27]{29].\n\nConsidering this, we applied three techniques to spread our\nquestionnaire and collect data from software testing profes-\nsionals. All three techniques were non-probability sampling,\nthat is, sampling methods that do not employ randomness. The\ntechniques were applied as follows:\n\n+ Convenience sampling relied on selecting participants\nbased on availability [26]. Following this technique,\nwe used our extensive network of software testing\nprofessionals to share the questionnaire and ask them to\nanswer it, depending on their availability.\n\n- Purposive sampling relied on selecting participants\nfrom a specific site and inviting them to participate in\nthe study [26]. Following this technique, we contacted\nthe QA manager of a large software company, who\nforwarded our questionnaire to over 187 software\ntesting professionals in his organization. In addition, we\nadvertised the questionnaire in online software testing\ncommunities.\n\n- Snowballing sampling relied on identifying some indi-\n",
                    "TABLE I\nSURVEY QUESTIONNAIRE\n\n1. We invite you to participate in the survey: Refactoring code of automated tests.\nThis survey is COMPLETELY ANONYMOUS; no information provided can be\nlinked back to you. Answering the questionnaire will take up to 5 minutes. Do\nyou agree to participate?\n\n( Yes\n\n2. Which gender do you identify with?\n() Female\n\n() Male\n\n() Non-binary\n\n() Prefer not to answer\n\n3. What is your Country?\n\n4, What is your highest educational level?\n() High-School\n\n() Bachelor\u2019s degree\n\n() Post-baccalaureate\n\n() Master\u2019s degree\n\n() PhD.\n\n5. How long have you been working with software testing?\n6. Do you have any testing certification?\n\n7. What is your job level?\n() Trainee\n\n() Junior\n\n() Mid-level\n\n() Senior\n\n() Principal\n\n8. Do you know what code refactoring is?\n() Yes\n() No\n\n9. Have you ever refactored any test automation code?\n() Yes\n() No\n\n10. What refactoring techniques do you know or apply?\n\n11. What are the benefits of code refactoring in test automation?\n() Increase of automation reusability\n\n() Improvement of code readability\n\n() Improvement of team productivity\n\n() Improvement of test automation performance\n\n() Removal of duplicated code\n\n() No observable benefit\n\n() Other benefit:\n\n12. What are the challenges of code refactoring in test automation?\n() Changing test code that is working\n\n() Complexity of changes\n\n() Lack of knowledge on refactoring\n\n() Lack of priority\n\n() No interest from managers/leaders\n\n() No significant gain\n\n() No time available (time-consuming)\n\n() Lack of proper tools\n\n() Other challenge:\n\nviduals from the population and asking them to identify\nother individuals that could participate in the study [26].\nFollowing this technique, we asked participants from\nthe convenience sample to forward our questionnaire to\ncolleagues and co-workers.\n\nD. Data Analysis\n\nThe information collected in this study is mainly quan-\ntitative. Therefore, we applied descriptive statistics [30] to\nanalyze the data collected from participants and summarize\nthe information emerging from our data set.\n\nDescriptive statistics allowed us to present the distribution\nand the frequency of participants\u2019 answers regarding their\nexperience refactoring the code of their automated tests and\nthe difficulties faced in completing this task. Since this is a\nwork in progress, more qualitative data will be collected in\nthe upcoming stages of the research.\n\nQualitative data is not the focus of the survey since we are\nexploring, in the industrial setting, the benefits and limitations\nof code refactoring reported in the literature. In particular,\nwe focus on the discussions presented in [11], [14], [16].\nIn addition, even though we have provided participants with\nan open-ended option to discuss other aspects related to\nthe theme, the vast majority of our sample stuck with the\noptions and the additional data collected (e.g., other benefits\nor challenges in addition to the ones listed in the questionnaire)\nwere not statistically representative to be incorporated to the\nresults. Instead, we are keeping them in mind for a future\ninvestigation of the theme using qualitative approaches.\n\nE. Ethics\n\nNo personal information about the participants was col-\nlected in this study (e.g., name, e-mail, or employer) to main-\ntain participants\u2019 anonymity. As mentioned above, we included\nthe beginning of the questionnaire with general information\nabout the study and the research team and asked participants\nto agree (by checking a yes box) to use their data for scientific\npurposes.\n\nIV. FINDINGS\n\nWe received 82 answered questionnaires. However, we\nconsidered only 80 of them valid, as two participants informed\nhaving no experience with software testing; therefore, not\nbeing part of our targeted population. Below, we present\nthe results obtained from our descriptive analysis. Table II\nsummarizes the profile of professionals that participated in\nthis survey.\n\nA, Demographics\n\nIn general, our sample is composed of experienced software\ntesting professionals, as 48% of the participants have more\nthan five years of experience working in testing activities, and\n46% are working in Senior or Principal positions.\n\nRegarding education, 35% of the participants have a post-\nbaccalaureate certificate in software testing, which shows that\nover 86% of participants have a high level of training in testing\npractices.\n",
                    "Further, 26% of our sample are non-male individuals (i.e.,\nwomen and individuals that preferred not to respond about\ntheir gender). Although this is a relatively low rate, previous\nstudies have discussed the lack of diversity in the software\nindustry, which explains why most of our sample is composed\nof men.\n\nFinally, we have participants working from companies in\neight countries. However, almost 59% of our sample is com-\nposed of professionals that work for companies in Brazil. Two\nfacts can explain this:\n\n- Two data collection techniques started with Brazilian\n\nprofessionals: convenience and purposive samplings.\n\n- The post-pandemic scenario and the possibility of remote\nwork increased the number of professionals that live in\none location and work in another.\n\nEven though most of our sample is from one country,\nwe understand that several participants work on projects that\ninvolve international clients, which increases their experience\nwith practices and processes of software development used\nworldwide.\n\nTABLE II\nDEMOGRAPHICS\n\nParticipants Profile\n\nGender Male 59\nFemale 20\nPrefer not to answer 1\nEducational Level High-School 4\nBachelor\u2019s degree 41\nPost-baccalaureate 28\nMaster\u2019s degree 7\nJob Level Trainee 2\nJunior 13\nMidlevel 28\nSenior 29\nPrincipal 8\nExperience 0-1 Years 5\n2-4 Years 13\n3-5 Years 23\n5+ Years 39\nTesting Certification Yes 60\nNo 20\nLocation Argentina 4\nBrazil 33\nCanada 3\nGermany 1\nIreland 5\nMexico 1\nNetherlands 1\nus 12\n\nB. Experience with Refactoring Automation Code\n\nWe asked participants about their experience with test\nautomation and refactoring, and as a result, 75% (60/80)\nof our sample reported that they are currently working on\ntest automation activities. This is the same percentage of\nprofessionals who performed refactoring in their tests, now,\nor in previous projects. This means that 25% of our sample\n(20/80) have never refactored any code.\n\nFurther analyzing our data, from the amount of software\ntesting professionals that never refactored any code, 3% (6/20)\nreported not knowing how to perform the refactoring. How-\never, two of them are currently working with test automation.\nMost of these professionals are currently at the beginning of\ntheir careers (e.g., Junior professionals), which can explain the\nlack of knowledge about automation and refactoring.\n\nWe also asked participants what type of refactoring they\nused in their automated tests. We compared their answers with\nthe techniques discussed in the literature (e.g., [11], [14], [16])\nto build a list of refactoring types. Our results demonstrate that\nthe most used types of refactoring used in testing are:\n\n- Changing the structure of methods: to make sure that\nthe automated tests are effectively used by the team, the\ntesting methods are modified to improve readability and\nunderstandability.\n\n- Removing useless code: to keep up with the project\nestimations, the automation code is frequently modified\nby simply commenting on the code, thus, requiring\nfuture refactoring to remove dead code.\n\n- Renaming variables: to improve code maintainability,\nreadability, and understandability, automation testing\nrefactoring includes renaming variables to more\ndescriptive and concise names.\n\n- Adding test assertions: to guarantee test coverage,\nrefactoring the testing code with new assertions is\nfrequently necessary, enabling the verification of new\nconditions.\n\n- Other necessary changes: to guarantee that the\nautomation continues running with no problems, other\nrefactorings might be necessary from time to time, e.g.,\nupdating libraries and fixing dependencies related to\ntools and system versions.\n\nTable II] summarizes these results and presents the percent-\nage of answers in our sample.\n\nTABLE III\nTYPES OF REFACTORING IN TESTING CODE\n\n% of Participants\n\n76%\n75%\n\nChanging the structure of methods\nRemoving useless code\n\nRenaming variables 50%\nAdding test assertions 49%\nOther necessary changes 5%\n\nRefactoring Types\n\nC. Benefits of Code Refactoring in Test Automation\n\nBased on the general benefits of code refactoring reported in\nthe literature (e.g., [11], [14], [16]) and usually discussed from\nthe perspective of software developers (e.g., programmers and\nsoftware engineers), we asked software testing professionals\nhow these benefits are perceived in software testing.\n",
                    "As presented in Table IV, our results demonstrate that\nthe general benefits of code refactoring usually observed by\nsoftware developers also apply to refactoring tasks in test\nautomation, as the testing professionals mostly agreed with\nwhat is reported in the literature.\n\nIn addition, 35% of our sample (28/80) reported that refac-\ntoring improves automation performance, i.e., refactoring the\ncode of automated tests could support these professionals in\nimproving and maintaining the tests, therefore saving time.\nMoreover, 26% (21/80) reported that refactoring also increases\nthe performance of the testing team, i.e., once the tests are easy\nto maintain, the professionals would have more time available\nto focus on other quality activities.\n\nTABLE IV\nBENEFITS OF REFACTORING IN TESTING CODE\n\n% of Participants\n\nSupport automation code maintenance 75%\nImprovement of code readability 60%\nIncrease of automation reusability 56%\n\nBenefits Removal of duplicated code 41%\nImprovement of test automation performance 35%\nImprovement of team performance 26%\nNo observable benefit 4%\n\nD. Barriers to Code Refactoring in Test Automation\n\nWe provided participants with a list of difficulties in con-\nducting refactoring that was previously reported in the litera-\nture (e.g., [14]) and asked them to select those that are more\nchallenging when refactoring the code of automated tests.\nAs summarized in Table V, there are at least eight barriers\nthat keep software testing professionals from consistently\nperforming refactoring in their work.\n\nOur result demonstrated that most barriers that challenge\ntesting professionals also challenge software developers. How-\never, software testers struggle with the lack of practical knowl-\nedge of refactoring techniques. Testing professionals might not\nhave enough opportunities to deal with refactoring due to how\ntheir work is organized and how test automation activities are\ndesigned. However, as 75% of our sample have performed\ncode refactoring before (see Section IV-B), this means that this\nlack of knowledge might refer to complex refactoring changes\nin the code, i.e., not easy to implement, as pointed out by 20%\nof the sample.\n\nAlmost 80% of our participants reported that refactoring\nthe code of their automated tests would consume too much\ntime; therefore, they need to focus on other activities instead\nof refactoring. Moreover, 61% of our sample reported a\nlack of interest from software project managers and team\nleaders in prioritizing this type of activity in the project.\nThis result indicates that the main barrier faced by software\ntesting professionals is related to how the project activities\nare planned. Project estimations (e.g., sprint planning) do not\nusually include code refactoring in testing activities, and this\nmight affect how refactoring is perceived in software testing.\n\nTABLE V\nCHALLENGES OF REFACTORING IN TESTING CODE\n\n% of Participants\n\n79%\n59%\n\nTime-consuming\nNo interest from managers/leaders\n\nLack of knowledge on refactoring 51%\n\nChallenges Complexity of changes 20%\nChanging test code that is working 19%\nTools unavailability 16%\nNo significant gain 15%\nLack of priority %\n\nV. DISCUSSION\n\nWe start our discussions by comparing our results with\nthe previous literature. Following this, we discuss threats to\nvalidity. Finally, we present our plans for future research.\n\nA, Enfolding the Literature\n\nIn the industry, software testing professionals encounter\nsimilar difficulties that developers face when dealing with\ncode refactoring, including tight deadlines, complex changes,\nhigh costs, lack of specific knowledge, and unavailability\nof appropriate tools [14]. However, they have an additional\nchallenge to overcome, namely, the lack of interest from\nmanagers and leaders in refactoring the code of automated\ntests.\n\nAlthough the ability to automate tests is frequently required\nin job positions and particularly valued in the software industry\nnowadays [17], testing professionals still need opportunities\nto practice their programming skills. This would require that\nsoftware projects include time, resources, tools, and attention\nto the importance of refactoring for software testing. Software\nmanagers should be aware of this barrier and plan project\nactivities accordingly.\n\nThis study has implications for research, as there is a\nlack of studies in the literature that discuss code refactoring\nof automated tests in the industrial context, e.g., using the\nexperience of professionals to guide the study. We chose to use\nthis perspective, i.e., analyzing how the evidence published in\nthe literature applies to industrial practice as an opportunity to\nwork closely with those effectively experiencing the problem\ndaily. In particular, previous studies have demonstrated the\nexistence of a gap between what academia and practice are\ninterested in software testing [31]. In this study, we tried to\nbring both worlds together.\n\nIn addition, we believe that our findings relate to the discus-\nsions raised in recent studies about how software engineering\nstudents are not considering careers in software testing because\nthey believe there is a lack of programming activities and\nchallenges in the job [32]. Our results demonstrate that, differ-\nent from this general belief, software testing involves several\nchallenging activities since refactoring the code of automated\ntests is a dynamic task performed by testing professionals.\nTherefore, investing time and resources in refactoring and\nother coding-related activities in software testing might help\nincrease the attractiveness of this career.\n",
                    "Our results have implications for industry practice as well.\nTesting is an essential activity in software development, and\ntesting cycles can consume a lot of time and resources\nin a software project [33]. Therefore, test automation and\nrefactoring are essential practices in software development\nnowadays. In this sense, our results highlight various benefits\nof code refactoring to software quality. In particular, because\none of the main costs associated with test automation is\nmaintenance, which includes updating the automated tests (i.\u00a2.,\nrefactoring) [21]. However, to successfully apply this practice,\nprofessionals need to understand the barriers presented in this\nstudy and identify strategies to overcome them.\n\nB. Recommendations\n\nBased on our results, some recommendations can be useful\nfor practitioners and software projects that want to use code\nrefactoring associated with test automation to increase quality:\n\n- Managers and leaders need to plan activities (e.g.,\nsprints) estimating time for refactoring not just for\ndevelopers but for testing professionals as well.\n\n- Testing professionals need to discuss with their teams\nthe importance of refactoring for test automation, in\nparticular, the relevance of this practice in improving\ntesting performance.\n\n- Developers must be supportive in helping testing\nprofessionals to deal with complex refactoring.\n\n- Researchers must focus on the development of tools to\nsupport automation in the context of software testing,\ne.g., automated test code.\n\nIn addition, we understand that our results are a starting\npoint for discussions on improving software testing education\nwith training opportunities on testing automation and refactor-\ning. However, since we did not investigate this topic from the\nperspective of students, we can only hypothesize this.\n\nC. Threats to Validity\n\nSince our study targeted software testing professionals, to\navoid surveying individuals from a different sample [23], we\nfocused only on professionals who specialized in testing and\nquality activities on their teams (e.g., software testers, test\nengineers, test analysts, QAs). We advertised our question-\nnaire directly to these professionals using convenience and\npurposive sampling and asked participants to only forward\nthe questionnaire to professionals working in similar roles\n(snowballing sampling).\n\nTo address construct validity [23], [24], our message in-\nvitation informed participants that the survey was focused\non test automation in the context of system-level testing and\nregression testing, i.e., unit and integration testing was out of\nthe scope of the research.\n\nFor criterion validity and to avoid ambiguity or questions\nthat confuse the participants [23], we validated our pilot ques-\ntionnaire with three senior testing professionals who supported\nus in this study; however, they were not included in the sample.\n\nIn addition, our survey has a limitation related to how the\ndata was collected. Our questionnaire was mostly based on the\nbenefits and limitations of code refactoring faced by software\ndevelopers and reported in the literature. In particular, [11],\n\n[14], [16] were used to build the list of options provided\n\nto participants to select in the survey. Following this, the\n\nliterature limited our results. We acknowledge that other\nstudies might have been published discussing other general\nbenefits and limitations of refactoring; however, we did not\nfollow a systematic review process to identify the papers in\nthe literature. Therefore, we plan to improve our understanding\nof code refactoring in software testing by collecting qualitative\ndata from practitioners in the future.\n\nFinally, our survey has a cultural limitation, as our partic-\nipants are mostly from one country. We plan to solve this\nlimitation by replicating the survey in other regions to in-\ncrease the participation of professionals from other companies.\nHowever, we believe that our current results can be used\nto inform practitioners about code refactoring in software\ntesting, as 75% of our sample has testing certifications that are\nrecognized worldwide (ID), 1.e., they are familiar with practices\nand processes that are applied in software companies from\nmany countries.\n\nD. Future Work\n\nSeveral interesting aspects of the work in software testing\ncan be explored in the industrial context, in particular, consid-\nering agile software development and the impact of software\nquality on software projects, and we expect to investigate them\nin our future studies.\n\nSpecifically, the next steps of this research on supporting\ntest automation will be focused on the following:\n\n- Replicating the survey to increase the sample, targeting\ntesting professionals from other locales to allow\ngeneralization of results.\n\n- Conducting focus groups with testing professionals to\nimprove our discussions on the benefits and limitations\nof refactoring in test automation.\n\n- Exploring tools that can support testing professionals in\ntheir refactoring activities;\n\n- Understanding the impacts of the post-pandemic work\narrangements (e.g., hybrid work) on quality activities,\nincluding test automation and refactoring processes.\n\nWe understand that these studies will contribute significantly\nto the improvement of software quality activities in the indus-\n\ntry.\n",
                    "VI. CONCLUSION\n\nIn this study, we investigated the perspective of software\ntesting professionals on test automation and code refactoring.\nBased on the experience of 80 individuals, we concluded\nthat refactoring offers several benefits, from supporting the\nmaintenance of automated tests to improving the performance\nof the testing team.\n\nHowever, practitioners might encounter many difficulties in\neffectively implementing this practice. Some difficulties are\nsimilar to the ones faced by programmers. In contrast, other\nchallenges are observed only in the testing context, such as the\nlack of attention from managers and leaders toward improving\nthe test automation processes.\n\nOur study has implications for research and practice. We\nexpect that our findings can motivate researchers to develop\ntools to support testing professionals in coding activities, such\nas refactoring. As for industrial practice, we are calling the\nattention of software managers and leaders to the importance\nof supporting testing professionals in learning, exploring, and\napplying refactoring in their work to improve the quality of\nsoftware products.\n\nREFERENCES\n\nJ. Harischandra and S. Hettiarachchi, \u201cOrganizational factors that affect\nthe software quality a case study at the engineering division of a selected\nsoftware development organization in sri lanka,\u201d in 2020 IEEE 7th\nInternational Conference on Industrial Engineering and Applications\n(ICIEA). IEEE, 2020, pp. 984-988.\n\n2] I. Ozkaya, \u201cCan we really achieve software quality?\u201d JEEE Software,\nvol. 38, no. 3, pp. 3-6, 2021.\n\n3] Z. Dong, \u201cAn approach to multiple attribute decision making with\nintuitionistic fuzzy information and its application to software quality\nevaluation,\u201d in JOP Conference Series: Materials Science and Engineer-\ning, vol. 740, no. 1. IOP Publishing, 2020, p. 012202.\n\n4] A. K. Arumugam, \u201cSoftware testing techniques & new trends,\u201d Jnterna-\ntional Journal of Engineering Research & Technology (IJERT), vol. 8,\nno. 12, 2019.\n\n5] M.A. Umar, \u201cComprehensive study of software testing: Categories, lev-\nels, techniques, and types,\u201d Jnternational Journal of Advance Research,\nIdeas and Innovations in Technology, vol. 5, no. 6, pp. 32-40, 2019.\n\n6] K. R. Halani, R. Saxena ef al., \u201cCritical analysis of manual versus\nautomation testing,\u201d in 202/ International Conference on Computational\nPerformance Evaluation (ComPE). YEEE, 2021, pp. 132-135.\n\n7] G. Grano, T. V. Titov, S. Panichella, and H. C. Gall, \u201cHow high\nwill it be? using machine learning models to predict branch coverage\nin automated testing,\u201d in 20/8 IEEE workshop on machine learning\ntechniques for software quality evaluation (MaLTeSQuE). IEEE, 2018,\npp. 19-24.\n\n8] I. Rana, P. Goswami, and H. Maheshwari, \u201cA review of tools and\ntechniques used in software testing,\u201d Int. J. Emerg. Technol. Innovative\nRes, vol. 6, no. 4, pp. 262-266, 2019.\n\n9] P. Kong, L. Li, J. Gao, K. Liu, T. F. Bissyande\u2019, and J. Klein, \u201cAuto-\nmated testing of android apps: A systematic literature review,\u201d JEEE\nTransactions on Reliability, vol. 68, no. 1, pp. 45\u201466, 2018.\n\nM. Sa\u2019nchez-Gordo\u2019n, L. Rijal, and R. Colomo-Palacios, \u201cBeyond tech-\nnical skills in software testing: Automated versus manual testing,\u201d\nin Proceedings of the IEEE/ACM 42nd International Conference on\nSofiware Engineering Workshops, 2020, pp. 161-164.\n\nE. Soares, M. Ribeiro, R. Gheyi, G. Amaral, and A. Santos, \u201cRefactoring\ntest smells with junit 5: Why should developers keep up-to-date?\u201d JEEE\nTransactions on Software Engineering, vol. 49, no. 3, pp. 1152-1170,\n2022.\n\nM. De Stefano, M. S. Gambardella, F. Pecorelli, F. Palomba, and\nA. De Lucia, \u201ccasper: A plug-in for automated code smell detection\nand refactoring,\u201d in Proceedings of the International Conference on\nAdvanced Visual Interfaces, 2020, pp. 1-3.\n\n[10\n\n{ll\n\n[l2\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32]\n\n[33]\n\nM. Wyrich and J. Bogner, \u201cTowards an autonomous bot for automatic\nsource code refactoring,\u201d in 2019 IEEE/ACM Ist international workshop\non bots in software engineering (BotSE). YEEE, 2019, pp. 24-28.\n\nE. Tempero, T. Gorschek, and L. Angelis, \u201cBarriers to refactoring,\u201d\nCommunications of the ACM, vol. 60, no. 10, pp. 54-61, 2017.\n\nG. Lacerda, F. Petrillo, M. Pimenta, and Y. G. Gue\u2019he\u2019neuc, \u201cCode\nsmells and refactoring: A tertiary systematic review of challenges and\nobservations,\u201d Journal of Systems and Software, vol. 167, p. 110610,\n2020.\n\nG. H. Pinto and F. Kamei, \u201cWhat programmers say about refactoring\ntools? an empirical investigation of stack overflow,\u201d in Proceedings of\nthe 2013 ACM workshop on Workshop on refactoring tools, 2013, pp.\n33-36.\n\nM. Cerioli, M. Leotta, and F. Ricca, \u201cWhat 5 million job advertisements\ntell us about testing: a preliminary empirical investigation,\u201d in Proceed-\nings of the 35th Annual ACM Symposium on Applied Computing, 2020,\npp. 1586-1594.\n\nM. Kassab, P. Laplante, J. Defranco, V. V. G. Neto, and G. Destefanis,\n\u201cExploring the profiles of software testing jobs in the united states,\u201d\nIEEE Access, vol. 9, pp. 68 905-68 916, 2021.\n\nR. Florea and V. Stray, \u201cA global view on the hard skills and testing tools\nin software testing,\u201d in 2019 ACM/IEEE 14th International Conference\non Global Software Engineering (ICGSE). YEEE, 2019, pp. 143-151.\nD. M. Rafi, K. R. K. Moses, K. Petersen, and M. V. Ma\u2019ntyla\u2019, \u201cBenefits\nand limitations of automated software testing: Systematic literature\nreview and practitioner survey,\u201d in 2012 7th International Workshop on\nAutomation of Software Test (AST). YEEE, 2012, pp. 36-42.\n\nB. Oliinyk and V. Oleksiuk, \u201cAutomation in software testing, can\nwe automate anything we want,\u201d in Proceedings of the 2nd Student\nWorkshop on Computer Science & Software Engineering (CS&SE@ SW\n2019), Kryvyi Rih, Ukraine, 2019, pp. 224-234.\n\nS. Easterbrook, J. Singer, M.-A. Storey, and D. Damian, \u201cSelecting em-\npirical methods for software engineering research,\u201d Guide to advanced\nempirical software engineering, pp. 285-311, 2008.\n\nJ. Linaker, S. M. Sulaman, M. Ho\u2019st, and R. M. de Mello, \u201cGuidelines\nfor conducting surveys in software engineering v. 1.1,\u201d Lund University,\nvol. 50, 2015.\n\nS. L. Pfleeger and B. A. Kitchenham, \u201cPrinciples of survey research: part\n1: turning lemons into lemonade,\u201d ACM SIGSOFT Software Engineering\nNotes, vol. 26, no. 6, pp. 16-18, 2001.\n\nP. Ralph, N. b. Ali, S. Baltes, D. Bianculli, J. Diaz, Y. Dittrich, N. Ernst,\nM. Felderer, R. Feldt, A. Filieri et a/., \u201cEmpirical standards for software\nengineering research,\u201d arXiv preprint arXiv:2010.03525, 2020.\n\nS. Baltes and P. Ralph, \u201cSampling in software engineering research: A\ncritical review and guidelines,\u201d Empirical Software Engineering, vol. 27,\nno. 4, pp. 1-31, 2022.\n\nI. Manotas, C. Bird, R. Zhang, D. Shepherd, C. Jaspan, C. Sadowski,\nL. Pollock, and J. Clause, \u201cAn empirical study of practitioners\u2019 per-\nspectives on green software engineering,\u201d in Proceedings of the 38th\ninternational conference on software engineering, 2016, pp. 237-248.\nR. de Souza Santos, B. Stuart-Verner, and C. Magalha\u2122es, \u201cWhat do\ntransgender software professionals say about a career in the software\nindustry?\u201d JEEE Software, 2023.\n\nF. Zampetti, G. Fucci, A. Serebrenik, and M. Di Penta, \u201cSelf-admitted\ntechnical debt practices: a comparison between industry and open-\nsource,\u201d Empirical Software Engineering, vol. 26, pp. 1-32, 2021.\n\nD. George and P. Mallery, \u201cDescriptive statistics,\u201d in JBM SPSS Statistics\n25 Step by Step. Routledge, 2018, pp. 126-134.\n\nR. E. Santos, A. Bener, M. T. Baldassarre, C. V. Magalha\u2122es, J. S.\nCorreia-Neto, and F. Q. da Silva, \u201cMind the gap: are practitioners and\nresearchers in software testing speaking the same language?\u201d in 20/9\nIEEE/ACM Joint 7th International Workshop on Conducting Empirical\nStudies in Industry (CESI) and 6th International Workshop on Software\nEngineering Research and Industrial Practice (SER&IP). YEEE, 2019,\npp. 10-17.\n\nR. E. Souza, R. E. de Souza Santos, L. F. Capretz, M. A. de Sousa,\nand C. V. de Magalha\u2019es, \u201cRoadblocks to attracting students to software\ntesting careers: Comparisons of replicated studies,\u201d in Quality of Infor-\nmation and Communications Technology: 15th International Conference,\nQUATIC 2022, Talavera de la Reina, Spain, September 12-14, 2022,\nProceedings. Springer, 2022, pp. 127-139.\n\nR. Ramler, S. Biffl, and P. Gru\u201cnbacher, Value-based management of\nsoftware testing. Springer, 2006.\n"
                ]
            }
        },
        {
            "file_name": "S:\\OneDrive\\@Dev\\!GPT\\ScriptGPT\\library\\Refactoring\\Source\\30 Years of Software Refactoring Research.pdf",
            "time_taken": 57.40592551231384,
            "data_extracted": {
                "text": [
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020 1\n\n30 Years of Software Refactoring Research:\nA Systematic Literature Review\n\nChaima Abid, Vahid Alizadeh,Marouane Kessentini, Thiago do Nascimento Ferreira and Danny Dig\n\nAbstract\u2014Due to the growing complexity of software systems, there has been a dramatic increase and industry demand for tools and\ntechniques on software refactoring in the last ten years, defined traditionally as a set of program transformations intended to improve\nthe system design while preserving the behavior. Refactoring studies are expanded beyond code-level restructuring to be applied at\n\ndifferent levels (architecture, model, requirements, etc.), adopted in many domains beyond the object-oriented paradigm (cloud\ncomputing, mobile, web, etc.), used in industrial settings and considered objectives beyond improving the design to include other\nnon-functional requirements (e.g., improve performance, security, etc.). Thus, challenges to be addressed by refactoring work are,\nnowadays, beyond code transformation to include, but not limited to, scheduling the opportune time to carry refactoring,\nrecommendations of specific refactoring activities, detection of refactoring opportunities, and testing the correctness of applied\nrefactorings. Therefore, the refactoring research efforts are fragmented over several research communities, various domains, and\nobjectives. To structure the field and existing research results, this paper provides a systematic literature review and analyzes the\nresults of 3183 research papers on refactoring covering the last three decades to offer the most scalable and comprehensive literature\nreview of existing refactoring research studies. Based on this survey, we created a taxonomy to classify the existing research, identified\nresearch trends, and highlighted gaps in the literature and avenues for further research.\n\nIndex Terms\u2014Refactoring, systematic literature review, program transformation, software quality.\n\n1. INTRODUCTION\n\nFor decades, code refactoring has been applied in infor-\nmal ways before it was introduced and properly defined in\nacademic work. The first known use of the term Refactoring\nin the published literature was in an article written by\nWilliam Opdyke and Ralph Johnson in September 1990\n[1]. William Griswold\u2019s Ph.D. dissertation [2], published\nin 1991, is also one of the first major academic works\non refactoring functional and procedural programs. The\nauthor defined a set of automatable transformations and\ndescribed their impact on the code structure. One year later,\nWilliam Opdyke also published his Ph.D. dissertation [3]\non the Refactoring of object-oriented programs. In 1999,\nMartin Fowler published the first book about refactoring\nthat has as title Improving the Design of Existing Code [4].\nThis book popularised the practice of code refactoring, set\nits fundamentals, and had a high impact on the world of\nsoftware development. Martin Fowler defined Refactoring\nin his book as a sequence of small changes - called refac-\ntoring operations - made to the internal structure of the\ncode without altering its external behavior. The goal of these\nrefactoring operations is to improve the code readability and\nreusability as well as reduce its complexity and maintenance\ncosts in the long run. Since then, a lot has changed in the\nsoftware development world, but one thing has remained\nthe same: The need for Refactoring.\n\ne = Chaima Abid, Vahid Alizadeh, Marouane Kessentini, and Thiago do\nNascimento Ferreira are with the department of Computer and Informa-\ntion Science, University of Michigan, Dearborn, MI, USA.\n\nDanny Dig is with the Computer Science department, University of\nColorado, Boulder, CO, USA.\nE-mail: marouane@umich.edu\n\nManuscript received on June 2020.\n\nNearly 30 years later, Refactoring has become a crucial\npart of software development practice, especially with the\never-changing landscape of IT and user requirements. It is a\ncore element of agile methodologies, and most professional\nIDEs include refactoring tools. Recent studies show that\nrestructuring software systems may reduce developers\u2019 time\nby over 60% [5]. Others demonstrate how Refactoring can\nhelp detect, fix, and reduce software bugs [6]. Companies\nare becoming more and more aware of the importance of\nRefactoring, and they encourage their developers to contin-\nuously refactor their code to set a clean foundation for future\nupdates.\n\nIt might be difficult for a developer to be justified to\nspend time on improving a piece of code to have the same\nfunctionality. However, it can be seen as an investment\nfor future developments. Specifically, Refactoring is a\ncrucial task on software with longer lifespans with multiple\ndevelopers need to read and understand the codes.\nRefactoring can improve both the quality of software and\nthe productivity of its developers. Increasing the quality of\nthe software is due to decreasing its complexity at design\nand source code level caused by refactoring, which is proved\nby many studies [7], [8]. The long-term effect of Refactoring\nis improving developers\u2019 productivity by increasing two\ncrucial factors, understandability and maintainability of the\ncodes, especially when a new developer joins an existing\nproject. It is shown that Refactoring can help to detect, fix,\nand reduce software bugs and leading to software projects\nwhich are less likely to expose bug in development process\n[6]. Another study claims that there are some specific kinds\nof refactoring methods that are very probable to induce bug\nfixes [9].\n\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n1.1 Problem Description and Motivation\n\nRefactoring is among the fastest-growing software engineer-\ning research areas, if not the fastest. Figure 1 shows the\ndistribution of publications related to refactoring across the\nglobe. Figure 2 reflects the number of publications in the\ntop 10 most active countries in the field of Refactoring. The\nUnited States tops the list of countries with a total of 714\npublications followed by Germany and Canada with a total\nof 317 and 248 publications, respectively. During the past\n4 years, the number of published refactoring studies has\nincreased with an average of 37% in all top 10 countries.\nThis demonstrates a noticeable increase in interest/need in\nRefactoring.\n\nOver 5584 authors from all over the world contributed to\nthe field of Refactoring. We highlight the most active authors\nin Figure 3 and 4, based on both the number of publications\nand citations in the area. Many scholars started research in\nthe refactoring filed prior to 2000. Others are relatively new\nto the field and started their contributions after year 2010.\nAll top 10 authors in the field have a constantly increasing\nnumber of publications over the past 20 years. Marouane\nKessentini heads the list with a total of 43 publications\n(51% of them were published during the past five years)\n\u2018ollowed by Steve Counsell and Danny Dig with a total of\n39 and 36 publications, respectively. Marouane kessentini\nublished an average of more than 4 articles per year while\nall other authors published an average between 1.5 and\n2.75 publications per year. Figure 5 is a histogram showing\nhow many publications were issued each year starting from\n990. The number of published journal articles, conference\napers, and books has increased dramatically during the last\ndecade, reaching a pick of 265 publications in 2016. During\njust the last four years (2016-2019), over 1026 papers were\nublished in the field, with an average of 256 papers each\nyear.\n\nRecently, several researchers and practitioners have\nadopted the use of refactoring operations at higher degrees\nof abstraction than source code level (e.g., databases, Uni-\nied Modeling Language (UML) models, Object Constraint\nLanguage (OCL) rules, etc.). As a result, they often had\nto redefine the principles and guidelines of refactoring\naccording to the requirements and specifications of their\ndomains. For instance, in User Interface Refactoring, de-\nvelopers make changes to the UI to retain its semantics\nand consistency for all users. These refactorings include,\nbut not limited to, Align entry field, Apply common button\nsize, Apply font, Indicate format, and Increase color contrast.\nIn Database Refactoring, developers improve the database\nschema by applying changes such as Rename column, Split\ntable, Move method, Replace LOB with table, and Introduce\ncolumn constraint. Henceforth, the refactoring operations are\ncalled restructuring operations when applied to artifacts\nother than the ones related to object-oriented programming.\nAlthough the different refactoring communities (e.g., soft-\nware maintenance and evolution, model-driven engineer-\ning, formal methods, search-based software engineering,\netc.) are interdependent in many ways, they remain dis-\nconnected, which may create inconsistencies. For example,\nwhen model-level Refactoring does not match the code-\nlevel practice, it can lead to incoherence and technical issues\n\n2\n\nduring development. The detachment is visible not only\nbetween different refactoring domains but also between\npractitioners and researchers. The distance between them\nprimarily originates from the lack of insights into both\nworlds\u2019 recent findings and needs. For instance, developers\ntend to use the refactoring features provided by IDEs due\nto their accessibility and popularity. Most of the time, they\nare uninformed of the benefits that can be derived from\nadopting state-of-the-art advances in academia. All these\nchallenges call for a need to identify, critically appraise, and\nsummarize the existing work published across the different\ndomains. Existing systematic literature reviews examine\nfindings in very specific refactoring areas such as identifying\nthe impact of refactoring on quality metrics [10] or code\nsmells [11]. To the best of our knowledge, no work collects\nand synthesizes existing research, tools, and recent advances\nmade in the refactoring community. This paper is the most\ncomprehensive synthesis of theories and principles of refac-\ntoring intended to help researchers and practitioners make\nquick advances and avoid reinventing or re-implementing\nresearch infrastructure from scratch, wasting time and re-\nsources. We also build a refactoring infrastructure that will\nconnect researchers with practitioners in industry and pro-\nvide a bridge between different refactoring communities in\norder to advance the field of refactoring research.\n\n1.2 Contributions\n\nThe Refactoring area is growing very rapidly, and many\nadvances, challenges, and trends have lately emerged. The\nprimary purpose of this study is to implement a systematic\nliterature review (SLR) for the field of refactoring as a whole.\nThis SLR follows a defined protocol to increase the study\u2019s\nvalidity and rationality so that the output can be high\nin quality and evidence-based. We used various electronic\ndatabases and a large number of articles to comprise all\nthe possible candidate studies and cover more works than\nexisting SLRs.\n\nThis SLR contributes to the existing literature in the\nfollowing ways:\n\ne We identify a set of 3183 studies related to refactor-\ning published until May 2020, fulfilling the quality\nassessment criteria. These studies can be used by\nthe research and industry communities as a reliable\nbasis and help them conduct further research on\nRefactoring.\n\nWe present a comprehensive qualitative and quanti-\ntative synthesis reflecting the state-of-the-art in refac-\ntoring with data extracted from those 3183 high-rigor\nstudies. Our synthesis covers the following themes:\nartifacts, refactoring tools, different approaches, and\nperformance evaluation in refactoring research.\n\nWe provide guidelines and recommendations based\non our findings to support further research in the\narea.\n\nWe implement a platform that includes the following\ncomponents: (1) A searchable repository of refactor-\ning publications based on our proposed taxonomy;\n(2) A searchable repository of authors who con-\ntributed to the refactoring community; (3) Analysis\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n?\nns \u00a9 o\nEi)\n10\n7 GB a\n199 \u00ab8 4\n756% \u2018\nm | 480\nSe ee 99 306\n4 G<V\\ 28\n7 B wo ae\n9 @ %, 237\n4\ni)\n4 1 Biz\n2 30\nt 52 46\n7\n5\n6\n\u2014 I\n\n1000 1500 20.\n\nFig. 1. Distribution of refactoring publications around the world.\n\nUnited states\n\nGermany\n\ncanada\n\nbrazil\n\nchina\n\nUnited Kingdom\n\naly\n\nindia\n\nJapan\n\nFrance\n\n8\n2\n\nEd\n\n2%\n\n24%\n\n41%\n\n22%\n\nmE Before 2016\nwm Between 2016 - 2020,\n\n100\n\n300 00 300 700 \u2018800\n\u2018Total number of documents, with percentage of documents\n\nPublished in the last years 2016 - 2020\n\nFig. 2. Number of publications in the top 10 most active countries in the refactoring field\n\nand visualization of the refactoring trends and tech-\nniques based on the collected papers. The proposed\ninfrastructure will allow researchers and practition-\ners to easily report refactoring publications and up-\nload information about active authors in the field of\nRefactoring. It will also bridge the different commu-\nnities to advance the field of refactoring research and\nprovide opportunities to educate the next refactoring\n\ngeneration.\n\n1.3. Related Surveys\n\nMens et al. [12] provided an overview of existing research\nin the field of software refactoring. They compared and\ndiscussed different approaches based on different criteria\nsuch as refactoring activities, techniques and formalisms,\ntypes of software artifacts that are being refactored, and\nthe effect of refactoring on the software process. Elish et al.\n[13] proposed a classification of refactoring methods based\non their measurable effect on software quality attributes.\nThe investigated software quality attributes are adaptability,\ncompleteness, maintainability, understandability, reusabil-\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nKessentini M.\nDig D.\nCounsell S.\nInoue K.\nBavota G.\n\nLiu H.\n\nRoy C.K.\nMurphy-Hill E.\nOliveto R.\n\nDe Lucia A.\n\n51%\n23%\n19%\n38%\n41%\n\n41%\n\n37%\n4%\n31%\n\nGam Before 2016\n\n9\n32% Gam ~Between 2016 - 2020\n\nT\n\n10\n\no\n\nT\n20\n\nT\n\n30\n\nT\n\n40\n\nTotal number of documents, with percentage of documents\npublished in the last years 2016 - 2020\n\nFig. 3. Top 10 Authors with the highest number of publications and citations in the field of refactoring\n\nity, and testability. Du Bois et al. [14] provided an overview\nof the field of software restructuring and Refactoring. They\nsummarized Refactoring\u2019s current applications and tool\nsupport and discussed the techniques used to implement\nrefactorings, refactoring scalability, dependencies between\nrefactorings, and application of refactorings at higher levels\nof abstraction. Mens et al. [15] identified emerging trends in\nrefactoring research (e.g., refactoring activities, techniques,\ntools, processes, etc.), and enumerates a list of open ques-\ntions, from a practical and theoretical point of views. Misb-\nhauddin et al. [16] provide a systematic overview of existing\nresearch in the field of model Refactoring. Al Dallal et al.\n[17] presented a systematic literature review of existing\nstudies, published through the end of 2013, identifying op-\nportunities for code refactoring activities. In another of their\nwork [10], they presented a systematic literature review that\nsummarizes the impact of refactoring on several internal\nand external quality attributes. Singh et al. [11] published a\nsystematic literature review of refactoring concerning code\nsmells. However, the review of Refactoring is done in a gen-\neral manner, and the identification of code smells and anti-\npatterns is performed in-depth. Abebe et al. [18] conducted\na study to reveal the trends, opportunities, and challenges\nof software refactor researches using a systematic literature\nreview. Baqais et al. [19] performed a systematic literature\nreview of papers that suggest, propose, or implement an\nautomated refactoring process.\n\nThe different studies mentioned above are mainly about\nidentifying the studies related to very specific or specialized\ntopics. In this paper, we are trying to be as comprehensive\nas possible by collecting, categorizing, and summarizing all\nthe papers related to refactoring in general that conform to\nour quality standards.\n\n1.4 Organization\n\nThe rest of the paper is organized as follows: First, Section 2\noutlines the research method and the underlying protocol\nfor the systematic literature review. Section 3 describes\nthe proposed refactoring infrastructure. The results of this\nsystematic review are reported in Sections 4. Finally, Section\n5 presents the conclusions.\n\n2 RESEARCH METHODOLOGY\n\nOur literature review follows the guidelines established by\nKitchenham and Charters [20], which decompose a sys-\ntematic literature review in software engineering into three\nstages: planning, conducting, and reporting the review. We\nhave also taken inspiration from recent systematic litera-\nture reviews in the fields of empirical software engineering\n[10] and search-based software engineering [21]. All the\nsteps of our research are well documented, and all the\nrelated data are available online for further validation and\nexploration []. This section details the performed research\nsteps and the protocol of the literature review. First, section\n2.1 describes the research questions underlying our survey.\nSecond, section 2.2 details the literature search step. Next,\nsection 2.3 highlights the inclusion and exclusion criteria.\nThe data preprocessing step and our proposed taxonomy\nare described in sections 2.4 and 2.5, respectively. The qual-\nity assessment criteria are defined in section 2.6. Finally,\nSection 2.7 discusses threats to the validity of our study.\n\n2.1 Research Questions\n\nThe following research questions have been derived based\non the objectives described in the introduction, which form\nthe basis for the literature review:\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n45\n-@ Kessentini mM. res\n: 5\nao | \u201cae Dig D. go @\n4H Counsell s. S44\n25. ] ~@ Inoue k. 3\nz hk Bavota G. Ss\n5 Liu H. q\n8 304 '\n8 Roy C.K. 34\noS \u2014@ Murphy-till E. 8\ng 257 3 Oliveto R. oC e ~\nS\nE > De Lucia A. o A Vv\n2 204 524 Ce\nvo a\n2 2 O\nS 2\n\u00ae 454\n215 5 oO\n\u2014E i\n3 3\n3 104 814\nvo\ne\n54 3\n=\n<\nte) r r r T T T r O+ T r T r\n1992 1996 2000 2004 2008 2012 2016 2020 40% 60% 80% 100% 120%\n\nPublication year\n\nFig. 4. Evolution of the Top 10 Authors during the past 10 years\n\nFig. 5. Trend of publications in the field of refactoring during the last\nthree decades.\n\ne RQ1: What is the refactoring life-cycle?\n\ne RQ2: What are the types of artifacts that are being\nrefactored at each step of the refactoring life-cycle?\n\ne RQ3: Why do software practitioners and researchers\nperform refactoring?\n\ne\u00ab RQ4: What are the different approaches used by\nsoftware practitioners and researchers to perform\nrefactoring?\n\ne RQ5: What types of datasets are used by software\npractitioners and researchers to validate the refactor-\ning?\n\n2.2 Literature Search Strategy\n\nAll the papers have been queried from a wide range of scien-\ntific literature sources to make our search as comprehensive\nas possible:\n\nPercentage of documents\npublished in the last years 2011 - 2020\n\ne Digital libraries: ACM Library, IEEE Xplore, Science-\nDirect, SpringerLink.\n\ne Citation databases: Web of Science (formerly ISI Web\nof Knowledge), Scopus.\n\ne Citation search engines: DBLP, Google Scholar.\n\nWe first defined a list of terms covering the variety of both\napplication domains and refactoring techniques. For that,\nwe checked the title, keywords, and abstract of the relevant\npapers that were already known to us. Synonyms and\nkeywords were derived from this list. These keywords were\ncombined using logical operators ANDs and ORs to create\nsearch terms. Before starting collecting the primary studies\n(PS), we tested the search terms\u2019 effectiveness on all the\ndata sources. Then, we refined the queries to avoid getting\nirrelevant papers. The string adjustments were agreed on\nby all authors. The final list of search strings are shown\nin Table 1. These search strings were modified to suit the\nspecific requirements of different electronic databases. We\nconducted our search on May 31st, 2020, and identified\nstudies published up until that date. The search was done\nfirst by the corresponding author and then verified by the\nrest of the authors. In our systematic review, we followed a\nmulti-stage model to minimize the probability of missing\nrelevant publications as much as possible. The different\nstages are shown in figure 6 along with the total returned\npublications at each stage. The first stage consists of execut-\ning the search queries on the databases mentioned above;\na total of 6158 references were found. Then, we removed\nthe duplicates, which reduced the list of candidate papers\nto 3882. Then, we performed a manual examination of\ntitles and abstracts to discard irrelevant publications based\non the inclusion and exclusion criteria. We also looked at\nthe body of the paper whenever necessary. This decreased\nthe list of candidate papers to 3161 publications. Next, we\n\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nSelect Scientific Literature Sources\n\nDefine Search Strings\n\nStage 1\n\nExecuting the Search Queries\n(6158 publications)\n\nRemove Duplicates\n(3882 publications)\n\nStage 2 |\n\n-721\n\nManual Examination of Titles and Abstracts\n\nStage 3 (3161 publications)\n\n+3\n\nConsult Web Profiles of Relevant Authors\n\nStage 4 (3164 publications)\n\n+14\n\nCheck Cross-references of Relevant Publications\n\nStage 5 (3178 publications)\n\n+5\n\nContact Authors =\n\nStage 6 (3183 publications)\n\nFig. 6. SLR steps\n\nused the resulting set as input for the snowballing process,\nrecommended by Wohlin [22], to identify additional studies.\nWe consulted web profiles of relevant authors and their\nnetworks. We also checked cross-references until no further\npapers were detected. As a result, 17 new references were\nadded. After that, we contacted the corresponding authors\nof the identified publications to inquire about any missing\nrelevant studies. This led to adding 5 studies.\n\n2.3\n\nTo filter out the irrelevant articles among those selected in\nStage 2 and determine the Primary studies, we considered\nthe following inclusion and exclusion criteria.\n\nInclusion and Exclusion Criteria\n\n2.3.1\n\nAll of the following criteria must be satisfied in the selected\nprimary studies:\n\nInclusion criteria\n\n1) The article must have been published in a peer\nreviewed journal or conference proceeding between\nthe years 1990 and 2020. The main reason for im-\nposing a constraint over the start year is because\nthe first known use of the term \u201crefactoring\u201d in\nthe published literature was in a September, 1990\narticle by William Opdyke and Ralph Johnson [1].\nWe included papers up till May 31st 2020.\n\n2) The article must be related to computer science and\nengineering and propose techniques, methods and\ntools for refactoring.\n\n3) The paper must be written in English.\n\n-2276\n\n6\n\n4) Incase a conference paper has a journal extension,\nwe would include both the conference and journal\npublications.\n\n5) The paper must pass the quality assessment criteria\nthat are elaborated in Section 2.6.\n\n2.3.2 Exclusion criteria\n\nPapers satisfying any of the exclusion criteria were dis-\ncarded, as follows:\n\n1) Studies that are not related to the computer science\nfield.\n\n2) Studies that investigated the impact of general\nmaintenance on code quality. In this case, the main-\ntenance tasks were potentially performed due to\nseveral reasons and not limited to refactoring, and\ntherefore, we cannot judge whether the impact was\ndue to refactoring or to other maintenance tasks\nsuch as corrective or adaptive maintenance.\n\n3) Grey Literature\n\n2.4 Data Preprocessing\n\nA pre-processing technique was applied to improve reliabil-\nity and precision, as detailed in the following sub sections.\n\n2.4.1 Simplifying Author's name\n\nIn general, scientific and bibliographic databases such as\nWeb of Science (WoS) and Scopus have the following incon-\nsistencies in authors names:\n\ne Most journals abbreviate the author\u2019s first name to\nan initial and a dot.\n\ne Most journals use the author name\u2019s special accents.\n\ne WoS uses a comma between the author\u2019s last name\nand first name initial, but Scopus does not.\n\nThese name-related inconsistencies mean that sciento-\nmetrics scripts cannot find all of the similar author\u2019s names.\nFor that reason, ScientoPy script applies the following steps\nto simplify author\u2019s name fields:\n\ne Remove dots and coma from author\u2019s name.\ne Remove special accents from author\u2019s name\n\n2.4.2 Fixing inconsistent country names\n\nSome authors use different naming to refer to the same\ncountry (such as USA and United States). For that reason,\nsome country names were replaced based on Table 3.\n\n2.5 Study Classification\n\nAccording to the research questions listed in Section 2.1, we\nclassified the PSs into five dimensions: (1) refactoring life-\ncycle (related to RQ1), (2) artifacts affected by refactoring\n(related to RQ2), (3) refactoring objectives (related to RQ3),\n(4) refactoring techniques (related to RQ4) and (5) refactor-\ning evaluation (related to RQ5). The determination of the\nattributes of each dimension was performed incrementally.\nThat is, for each dimension, we started with an empty\nset of attributes. The authors of this study screened the\nfull texts of the PSs one by one, analyzed each reported\nstudy based on the considered dimension, and determined\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nTABLE 1\nfinal list of search strings\n\nsearch strings\n\n(software OR system OR code OR service OR diagram OR database OR architecture OR Model OR\n\nGUI OR user interface OR UI OR design OR artifac!\n\nt OR developer OR computer OR programming\n\nOR object-oriented OR implement OR mobile app OR cloud OR document ) AND (refactor OR\n\nrefactoring)\n\nTABLE 2\nPS quality assessment questions [17]\n\nQuestion\n\nAre the applied identification techniques for refactoring opportunities clearly described? |\nDesi Are the refactoring activities considered clearly stated and defined?\n\nesign Was the sample size justified?\nP. J\n\nAre the evaluation measures fully defined?\nConduct Are the data collection methods adequately described?\n\nAre the results of applying the identification techniques evaluated?\n\nAre the data sets adequately described? (size, programming languages, source)\n\nAre the study participants or observational units adequately described?\nAnalysis Are the statistical methods described?\n\nAre the statistical methods justified?\n\nIs the purpose of the analysis clear?\n\nAre the scoring systems (performance evaluation) described?\n\nAre all study questions answered?\n\nAre negative findings presented?\nConclusion | Are the results compared with previous reports?\n\nDo the results add to the literature?\n\nAre validity threats discussed?\n\nTABLE 3\nList of countries and their replacements\n\nCountry Replacement\nRepublic of China China\n\nUSA United States\nEngland, Scotland and Wales England\n\nU Arab Emirates\nRussia\n\nViet Nam\n\nTrinid & Tobago\n\nUnited Arab Emirates\nRussian Federation\nVietnam\n\nTrinidad and Tobago\n\nthe attributes of that dimension as considered by each PS.\nTable 4 outlines the keywords extracted for each category.\nIt should be pointed out that, most of the time, we remove\nall of the affixes (i.e., suffixes, prefixes, etc.) attached to a\nword in order to keep its lexical base, also known as root\nor stem or its dictionary form or lemma. For instance, the\nword document allows us to detect the words documentation\nand documenting. Also, we did not include bi-grams and\ntri-grams that can be detected using one uni-gram. For\nexample, Class Diagram, Object Diagram, Sequence Diagram,\nand Use Case Diagram can all be detected using the word\nDiagram alone.\n\nThe screening of the PSs resulted in determining six\nstages for the refactoring life-cycle (e.g., detection, pri-\noritization, recommendation, testing, documentation, and\nprediction). We also classified the papers according to the\nlevel of automation of the proposed technique (e.g., auto-\nmatic, manual, semi-automatic). The results are described\nin section 4.1. For the second dimension, we identified five\nartifacts on which the impact of refactoring is studied by\nat least one of the PSs. These artifacts are code, architec-\nture, model, GUI, and database. The classification of PSs\nbased on these artifacts is discussed in detail in Section\n4.2. We subdivided the third dimension into five categories\n\n(e.g., External quality, internal quality, performance, migra-\ntion, and security) to reflect the refactoring objective and\nsix categories (e.g., Object-oriented design, Aspect-oriented\ndesign, Model-driven engineering, Documentation, Mobile\ndevelopment, and Cloud computing) to describe the refac-\ntoring paradigms. The classification of PSs based on these\ncategories is discussed in detail in Section 4.3. We divided\nthe fourth dimension into four categories (e.g., data mining,\nsearch-based algorithms, formal methods, and fuzzy logic)\nto reveal the refactoring techniques adopted in the studies\nand into twelve categories (e.g., Java, C, C#, Python, Cobol,\nPHP, Scala, Smalltalk, Ruby, Javascript, MATLAB, and CSS)\nto show the most common programming languages used in\nour PSs. The details of this categorization are reported in\nsection 4.4. Finally, for the fifth dimension, we divide the\nPSs into two categories: open-source and industrial. The\nopen-source category includes studies that validate their\napproaches using open source systems. In contrast, the\nindustrial category consists of the studies that validate their\nwork on systems of their industrial collaborators. These\nfindings are outlined in Section 4.5.\n\n2.6 Study Quality Assessment\n\nTo ensure a level of quality of papers, we only included\nvenues that are known for publishing high-quality software\nengineering research in general with an h-index of at least\n10, as has been done by [23] . Each of the papers that\nwere published before 2019 has to be cited at least once.\nThe quality of each primary study was assessed based on\na quality checklist defined by Kitchenham and Charters\n[20]. This step aims to extract the primary studies with\ninformation suitable for analysis and answering the defined\nresearch questions. The quality checklist, (described in table\n2) were defined by Galster et al. [23]. They are developed\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nTABLE 4\nList of keywords used to detect the different categories\n\nCategory\n\nKeywords\n\nRefactoring life-cycle (RQT)\n\nDetection\nPrioritization\nRecommendation\nTesting\nDocumentation\nPrediction\n\ndetect, opportunity, smell, antipattern, design defect\n\nschedul, sequence, priorit\n\nrecommend, correction, correcting, fixing, suggest\n\ntest, regression testing, test case, unit test\n\ndocument\n\npredict, future release, next release, development history, refactoring history\n\nLevel of automation (RQT)\n\nManual manual\n\nSemi-automatic semi-automat, semi-manual\n\nAutomatic automat\n\nArtifact (RQ2)\n\nCode code, java, object orient, smell, antipattern, anti-pattern, object-orient\nModel design, model, UML, diagram, Unified Modeling Language\nArchitecture architecture, hotspot, hierarchy\n\nGUI gui, user interface, UI\n\nDatabase relational, schema, database, Structured Query Language, SQL\n\nParadigm (RQ3)\n\nObject-oriented design\n\nAspect-oriented design\nModel-driven engineering\nDocumentation\n\nMobile development\nCould computing\n\nobject orient, object-orient, 00, java, c, ++, python, C sharp, c#, css, Python, R, PHP, JavaScript, Ruby,\nPerl, Object Pascal, Objective-C, Dart, Swift, Scala, Kotlin, Common Lisp, MATLAB, Smalltalk\n\naspect\n\nmodel transform, uml, reverse engineering, diagram, Unified Modeling Language\n\ndocument\n\nandroid, mobile, IOS, phone, smartphone, cellphones\n\nweb service, wsdl, restful, cloud, Apache Hadoop, Docker, Middleware, Software-as-a-Service,\nSaaS, XaaS, Anything-as-a-Service, Platform-as-a-Service, PaaS, Infrastructure-as-a-Service, laaS, AWS,\nAmazon EC2, Amazon Simple Storage Service, S3\n\nRefactoring Objectives (RQ\n\nInternal Quality maintainability, cyclomatic, depth of inheritance, coupling, quality, Flexibility, Portability, Re-usability,\nReadability, Testability, Understandability\nPerformance performance, parallel, Response Time, Error Rates, Request Rate, availability\n\nExternal quality\n\nanalysability, changeability, time behaviour, resource, Correctness, Usability, Efficiency, Reliability,\nIntegrity, Adaptability, Accuracy, Robustness\n\nMigration migrat\nSecurity secure, safety, Attack surface, virus, hack, vulnerability, vulnerable, spam\nProgramming languages (RQ4)\n\nJava java\n\nCc c, C++\n\nC# c sharp, c#\nPython python\ncss css\n\nPHP php\nCobol cobol\nScala scala\nJavascript javascript\nRuby ruby\nSmalltalk smalltalk\nMATLAB matlab\n\nAdopted methods (RQ4)\n\nSearch-based algorithms\n\nData mining\n\nFormal methods\nFuzzy logic\n\nsearch, search-base, sbse, genetic, fitness, simulated annealing, tabu search, search space, Hill climbing,\nMulti-objective evolutionary algorithms, multi objective optimization, multi-objective programming,\nvector optimization, multi-criteria optimization, multi-attribute optimization, Pareto optimization,\nEvolutionary Multi-objective Optimization, EMO, Single-Objective Optimization, Many-Objective\nOptimization, multi objective\n\nartificial intelligence, ai , machine learning, naive bayes, decision tree, SVM, support vector machine,\nCluster, Classification, classify, Association, Neural networks, deep learning, random forest, regression,\nreinforcement learning, learning\n\nmodel check, formal method, B-Method, RAISE, Z notation, SPARK Ada\n\nfuzzy\n\nEvaluation method (RQ5)\n\nOpen source\nIndustrial\n\nopen source, Open-source\nproprietary, industrial, industry, collaborator, collaboration\n\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nby considering bias and validity problems that can occur\nat different stages, including the study design, conduct,\nanalysis, and conclusion. Each question is answered by a\n\u201cYes\u201d, \u201cPartially\u201d, or \u201cNo\u201d, which correspond to a score of\n1, 0.5, or 0, respectively. If a question does not apply to a\nstudy, we do not evaluate the study for that question. The\nquality assessment checklist was independently applied to\nall 3882 studies by two of the authors. All disagreements\non the quality assessment results were discussed, and a\nconsensus was reached eventually. Few cases where agree-\nment could not be reached were sent to the third author for\nfurther investigation. 154 studies did not meet the quality\nassessment criteria.\n\n2.7 Threats to Validity\n\nSeveral limitations may affect the generalizability and the\ninterpretations of our results. The first is the possibility of\npaper selection bias. To ensure that the studies were se-\nlected in an unbiased manner, we followed the well-defined\nresearch protocol and guidelines reported by Kitchenham\nand Charters [20] instead of proposing nonstandard quality\nfactors. Also, the final decision on the articles with selection\ndisagreements was performed based on consensus meet-\nings. The Primary studies were assessed by one researcher\nand checked by the other, a technique applied in similar\nstudies [21]. The second threat consists of missing a rele-\nvant study. To overcome this threat, we employed several\nstrategies that we mentioned in Section 2.2. Few related\nstudies were detected after performing the automatic search,\nwhich indicates that the constructed search strings and the\nmentioned utilized libraries were comprehensive enough to\nidentify most of the relevant articles. Another critical issue\nis whether our taxonomy is complete and robust sufficient\nto analyze and classify the primary studies. To overcome\nthis problem, we used an iterative content analysis method\nby going through the papers one by one and continuously\nexpand the taxonomy for every new encountered concept.\nFurthermore, to gather sufficient keywords to detect the\ndifferent categories, we followed the same iterative process,\nand we added synonyms based on the authors\u2019 expertise\nin the field of refactoring. Another threat is related to the\ntagging of the papers according to our taxonomy. To miti-\ngate this problem, we asked 27 graduate students to check\nthe correctness of the classification results by reading the\nabstract, the title, and keywords. They also check the body\nof the paper whenever necessary.\n\n3 REFACTORING INFRASTRUCTURE\n\nWe implemented a large scale platform [24] that collects,\nmanages, and analyzes refactoring related papers to help\nresearchers and practitioners share, report, and discover\nthe latest advancements in software refactoring research. It\nincludes the following components:\n\n1) Asearchable repository of refactoring publications\nbased on our proposed taxonomy. Figure 9 shows a\nscreenshot of the publications\u2019 tab of the refactoring\nrepository website. The papers can be searched by\nauthor, title, or year of publication. Each paper has\n\ntags that describe its content based on our taxonomy\n\n9\n\ndescribed in section 2.5. The papers can also be\nfiltered using those tags and sorted alphabetically\nor chronologically according to the title and year\nof publication, respectively. The user can export the\npublications\u2019 dataset to many formats, including\npdf, excel, and CSV. He can also easily report a new\npublication by entering its link.\n\nA searchable repository of authors who con-\ntributed to the refactoring community. Figure\n8 shows a screenshot of the authors\u2019 tab of the\nrefactoring repository website. The authors can be\nsearched and sorted alphabetically by name, affil-\niation, or country. They can also be sorted based\non the total number of refactoring publications.\nThe user can also consult the Google Scholar and\nScopus profiles of the authors if available. Finally,\nthe user can easily report a new author by entering\ntheir information and their profile. Furthermore, we\ndefined the refactoring h-index, which shows how\nmany papers about refactoring published by the\nauthor have been cited proportionately. A refac-\ntoring h-index of X means that the author has X\npapers about refactoring that have been cited at\nleast X times. Authors can also be sorted according\nto the refactoring h-index and the total number of\ncitations (see figure 11). Besides, we created a co-\nauthor network and corresponding visualizations\n(see figure 12) to get a snapshot view of the breadth\nand depth of an individual\u2019s collaborations in the\nfield of refactoring research. Finally, we generated a\nhistogram (see figure 7) that shows the number of\npublications issued by the top institutions active in\nthe refactoring research by considering the authors\u2019\naffiliations.\n\nAnalysis and visualization of the refactoring\ntrends and techniques based on the collected pa-\npers. Figure 10 shows a screenshot of the refactoring\nrepository dashboard. It contains histograms and\npie charts that show the distribution and percent-\nages of the categories defined in our taxonomy. It\nalso includes maps that reflect the spread of refac-\ntoring activity across the world.\n\n2)\n\n3)\n\nThe proposed infrastructure will enable researchers to\nperform a fair comparison between their new refactoring\napproaches and state-of-the-art tools; enable researchers to\nuse refactoring data of large software systems; facilitate in-\nteractions between researchers from currently disconnected\ndomains/communities of refactoring (model-driven engi-\nneering, service computing, parallelism and performance\noptimization, software quality, testing, etc.); enable practi-\ntioners and researchers to quickly identify relevant existing\nresearch papers and tools for their problems based on the\nproposed taxonomy and classification; create benchmarks\nagainst which various refactoring approaches can be evalu-\nated; enable effective interactions between practitioners and\nrefactoring researchers to identify relevant problems faced\nby the software industry.\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nTop Institutions\n\nFig. 7. Top institutions active in the refactoring field\n\nAuthors of Refactoring Publications\n\nFig. 8. A screenshot of the authors tab of the refactoring repository\nWebsite\n\n4 RESULTS\n\nIn this section, we aim to answer the research questions. To\nprovide an overview of the current state of the art in refac-\ntoring and guide the reader to a specific set of approaches,\ntools, and recent advances that are of interest, we classified\nthe 3183 reviewed papers based on the taxonomy described\nin Section 2.5. Table 5 contains representative references for\nthe categories created for each RQ. We only provided 10\nreferences per category because we cannot possibly report\nin this paper the categorization of all the studies since\nwe are dealing with a total of 3183 papers. The results\nof the classification of all the papers are provided in our\nwebsite [24]. For some taxonomy categories, papers may\nhave multiple values and thus be listed several times. As\na result, percentages in the tables may sum up to more than\n100 percent. Also, not all the papers were classified in all\ndimensions. Consequently, percentages in one dimension\nmay not sum up to 100 percent. The rest of this section\npresents the observations and insights that can be derived\nfrom the visualization of the categories.\n\n4.1 Refactoring life-cycle\n\nGoing through the primary studies, we have been able to\nestablish a refactoring life-cycle that is composed of six\n\nRefactoring Publications\n\n@ siPopers Target \u00bb Hocyeio ~ \u2014 @ Languages ~ @ Objectives \u00bb vohution\u00bb \u2014 @) Folds ~\n\ncord 18 nove\n\nFig. 9. A screenshot of the publications tab of the refactoring repository\nWebsite\n\nPublications per Voor esearch Categorias\n\nwill\n\nPublications per Country\n\n\u2018Authors per Country\n\nnm\n\nFig. 10. A screenshot of the Dashboard of the refactoring repository\nwebsite\n\nstages:\n\ne Refactoring detection: Identifying refactoring op-\nportunities is an important stage that precedes the\nactual refactoring process. It can be done by man-\nually inspecting and analyzing an artifact of a sys-\ntem to identify refactoring opportunities. However,\nthis technique is time-consuming and costly. Re-\nsearchers in this area typically propose fully or semi-\nautomated techniques to identify refactoring oppor-\ntunities. These techniques may be applicable to dif-\nferent artifacts and should be evaluated empirically.\nRefactoring prioritization: The number of refactor-\ning opportunities usually exceeds the amount of\nproblems that the developer can deal with, par-\nticularly when the effort available for performing\nrefactorings is limited. Moreover, not all refactoring\nopportunities are equally relevant to the goals of the\nsystem or its health. In this stage, the refactorings op-\nerations are prioritized using different criteria (e.g.,\nmaximizing the refactoring of classes with a large\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nTABLE 5\nRepresentative references for all categories\n\n1\n\nCategory Percentage | Papers\n\nRefactoring life-cycle (RQ1)\n\nDetection 28.65% S1], [82], [S3], [$4], [S5], [S6], [S7], [S8], [S9], [S10\n\nPrioritization 9.43% $11], [S12], [S13], [S14], [S15], [S16], [S17], [S18], [S19], [S20]\nRecommendation 16.18% $3], [S11], [S12], [S21], [S22], [S23], [S24], [S25], [S26], [S27]\nTesting 18.44% $4], [S6], [S7], [S8], [S13], [S28], [S29], [S30], [S31], [S32]\nDocumention 5.22% $33], [S34], [S35], [S36], [S37], [S38], [S39], [S40], [S41], [S42], [S43]\n\nPrediction 4.818% $44], [$45], [$46], [$47], [$48], [S49], [S50], [$51], [$52], [$53]\n\nLevel of automation (RQ1)\n\nAutomatic 30.95% $54], [S55], [S56], [S57], [S58], [S59], [S60], [S61], [S62], [S63]\nSemi-automatic 1.95% $64], [S65], [S66], [S67], [S68], [S69], [S70], [S71], [S72], [S73], [S74], [S75]\nManual 8.67% $69], [S76], [S77], [$78], [S79], [S80], [S81], [$82], [$83], [S84]\n\nArtifact (RQ2)\n\nCode 72.89% S1], [S2], [$3], [S11], [S65], [S85], [S86], [S87], [S88], [S89]\n\nModel 59.25% $1], [S3], [S28], [S29], [S65], [S87], [S89], [S90], [$91], [S92]\n\nArchitecture 17.25% $28], [S91], [S93], [S94], [S95], [S96], [S97], [S98], [S99], [S100]\n\nGUI 2.58% S6], [S8], [S28], [S87], [$89], [$90], [S101], [$102], [$103], [$104]\nDatabase 4.12% $27], [$36], [S65], [S100], [$105], [$106], [$107], [S108], [S109], [S110]\nParadigm (RQ3)\n\nObject-oriented design 34.09% S1], [S8], [S30], [S85], [S87], [S88], [S101], [S111], [S112], [S113]\nAspect-oriented 10.87% $88], [S96], [$101], [$102], [$103], [S114], [S115], [S116], [$117], [$118]\nModel-driven engineering | 7.35% $3], [S15], [S32], [S58], [S65], [S119], [S120], [S121], [S122], [S123]\nMobile apps development | 3.55% $23], [S87], [S87], [S95], [S99], [S112], [S124], [S125], [S126], [S127]\nCould computing 4.15% $128], [S129], [S130], [S131], [S132], [S133], [S134], [S135], [S136], [S137]\nRefactoring Objective (RQ3)\n\nInternal Quality 41.63% $3], [S12], [S21], [S29], [S30], [S89], [S90], [S94], [S138], [S139]\nPerformance 15.93% S10], [S12], [S28], [$86], [$88], [S91], [S92], [$96], [$115], [$119]\nExternal quality 22.68% $87], [S91], [S92], [$95], [S102], [$140], [$141], [$142], [$143], [$144]\nMigration 3.61% S95], [$100], [S113], [$145], [S146], [$147], [$148], [$149], [$150], [$151]\n\nSecurity 3.11% $113], [S152], [S153], [S154], [S155], [S156], [S157], [S158], [S159], [S160\nProgramming language (RQ4)\n\nJava 17.15% ST], [S8], [S10], [S30], [S85], [S87], [S88], [S112], [S113], [S140]\n\nC 4.65% $59], [S96], [$104], [$105], [$111], [$146], [S161], [$162], [$163], [$164]\non 0.66% $61], [$165], [S166], [S167], [S168], [$169], [S170], [$171], [$172], [$173]\nPython 0.53% $174], [S175], [S176], [S177], [S178], [S179], [S180], [S181], [S182], [S183\ncss 0.5% $147], [S184], [S185], [S186], [S187], [S188], [S189], [S190], [S191], [S192\nPHP 0.35% $169], [S193], [S194], [S195], [S196], [S197], [S198], [S199], [S200], [S201\nCobol 0.31% 12], [S202], [S203], [$205], [$206], [$207], [$208], [$209]\n\nMATLAB 0.28% $210], [S211], [S212], [S213], [S214], [S215], [S216], [S217\n\nSmalltalk 0.79% 25], [$219], [S220], [S221], [S222], [S223], [$224], [$225], [$226], [$227]\nRuby 0.22% $169], [S181], [S228], [S229], [S230], [S231\n\nJavascript 0.72% $112], [S232], [S233], [S234], [S235], [S236], [S237], [S238], [S239], [S240], [S241]\nScala 4.02% $33], [S55], [S86], [S126], [S242], [S243], [S244], [S245], [S246], [S247]\n\nAdopted Method (RQ4)\n\nSearch-based algorithms 25.76% $12], [S248], [S249],\n\n[5250], [S251], [$252], [S253], [S254], [S255], [5256]\nData mining 15.49% S2], [S82], [S107], [S185], [S257], [$258], [S259], [S260], [S261], [S262]\n\nFormal methods 2.92% $42], [S199], [S263], [S264], [S265], [S266], [$267], [$268], [$269]\nFuzzy logic 0.28% $257], [S270], [S271], [S272], [S273], [S273], [S274\n\nEvaluation method (RQ5)\n\nOpen source 16.31% S1], [S7], [S12], [S30], [S32], [S88], [S112], [S139], [S248], [S275]\nIndustrial 10.4% $9], [S12], [$16], [S115], [$120], [$147], [S276], [S277], [S278], [S279]\n\nTop Authors\n\nFig. 11. A screenshot of the refactoring repository dashboard that shows\nthe authors, their h-index and total number of publications and citations\n\nnumber of anti-patterns or with the previous history\n\nof bugs, etc.) according to the needs of developers.\nRefactoring recommendation: Several refactoring\nrecommendation tools have been proposed that dy-\nnamically adapt and suggest refactorings to develop-\ners. The output is sequences of refactorings that de-\nvelopers can apply to improve the quality of systems\nby fixing, for example, code smells or optimizing\nsecurity metrics.\n\nRefactoring testing: After choosing the refactorings\nto be applied, tests need to be done to ensure the cor-\nrectness of artifacts transformations and avoid future\nbugs. This is done by checking the satisfaction of the\npre-and post-conditions of the refactoring operations\nand the preservation of the system behavior.\nRefactoring documentation: After applying and test-\ning the refactorings, we need to document the refac-\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nAuthors Network Graph\n\nFig. 12. A screenshot of the authors network graph from the refactoring\nrepository website\n\ntorings, their locations, why they have been applied,\nand the quality improvements.\n\n\u00a2 Prediction: It is interesting for developers to know\nwhich locations are likely to demand refactoring in\nfuture releases of their software products. This will\nhelp them focus on the relevant artifacts that will\nundergo changes in the future, prepare them for fur-\nther improvements and extensions of functionality,\nand optimize the management of limited resources\nand time. Predicting locations of future refactoring\ncan be done using the development history.\n\nFigure 13 illustrates the percentage of the papers related\nto each stage of the refactoring life-cycle. 33.08% of the\npapers deal with testing. Researchers have invested heavily\nin testing to ensure the reliability of refactoring because\nchanging the structure of code can easily introduce bugs\nin the program and lead to challenging debugging sessions.\nA plenty of effort is made towards the automation of the\ntesting process to facilitate the adoption of refactoring [S54],\n[S55], [S56]. Detecting refactoring opportunities is also a\ntopic of interest to researchers. Several approaches have\nbeen proposed to detect refactoring opportunities including\nbut not limited to techniques that depend on quality metrics\n(e.g., cohesion, coupling, lines of code, etc.), code smells\n(e.g., feature envy, Blob class, etc.), Clustering (similarities\nbetween one method and other methods, distances between\nthe methods and attributes, etc.), Graphs (e.g., represent\nthe dependencies among classes, relations between methods\nand attributes, etc.), and Dynamic analysis (e.g., analyzing\nmethod traces, etc.). Refactoring documentation is an under-\nexplored area of research. Only 5.22% of the collected pa-\npers dived into refactoring documentation. Many studies\nexamined the automation of the different refactoring stages\nto reduce the refactoring effort and, therefore, increase its\nadaption. Figure 14 shows the count of publications dealing\nwith manual, semi-automatic, and automated refactoring.\nIn fact, 30.95% of the papers deal with the automation\nof refactoring. Only 1.95% and 8.67% of the papers used\nmanual and semi-automatic refactoring, respectively.\n\n12\n\n4.2 Artifacts affected by refactoring\n\nAs we mentioned before, refactoring is not limited to soft-\nware code. In fact, it can be applied to any type of soft-\nware artifacts (e.g., software architectures, database schema,\nmodels, user interfaces, and code). Figure 15 shows the per-\ncentage of refactoring publications per artifact. The evidence\nfrom this histogram shows that the most popular refactoring\nartifact is code (72.89%). Model refactoring has also received\nconsiderable attention, with a percentage of 59.25%. Graph-\nical user interfaces (GUIs) and Database refactoring have\nreceived the least attention of all with a fraction of only\n4.12% and 2.58%, respectively. This might be due to the fact\nthat database refactoring is conceptually more difficult than\ncode refactoring; code refactorings only need to maintain\nbehavioral semantics while database refactorings also must\nmaintain informational semantics. Also, GUI refactoring is\nvery demanding, requiring the adoption of user interfaces\narchitectural patterns from the early software design stages.\nFuture research should explore database and user interface\nrefactoring further as they are an indispensable part of\ntoday\u2019s software.\n\n4.3 Refactoring objectives\n\nFive paradigms have been identified from analyzing the\nprimary studies: object-oriented designs, cloud computing,\nmobile apps, model-driven, and aspect-oriented. Object-\noriented programming has gained popularity because it\nmatches the way people actually think in the real world,\nstructuring their code into meaningful objects with relation-\nships that are obvious and intuitive. The increased popular-\nity of the object-oriented paradigm has also increased the\ninterest in object-oriented refactoring. This can be observed\nin figure 16 where more than 34% of the studies related to\nrefactoring focus on object-oriented designs. Less than 5% of\nthe papers investigated refactoring for cloud computing and\nmobile app development. For the refactoring objectives clas-\nsification of the taxonomy, five subcategories are considered:\nexternal quality (e.g. correctness, usability, efficiency, relia-\nbility, etc.) , internal quality (e. g. maintainability, flexibility,\nportability, re-usability, readability etc.) , performance (e.g.\nresponse time, error rate, request rate, memory use, etc.),\nmigration (e.g. Dispersion in the Class Hierarchy, number\nof referenced variables, number of assigned variables etc. ),\nsecurity (e.g. time needed to resolve vulnerabilities, Number\nof viruses and spams blocked, Number of port probes,\nnumber of patches applied, Cost per defect, Attack surface\netc.). Figure 17 is illustrating the reasons why people refactor\ntheir systems. Improving the internal quality takes up the\nlargest portion (41.63%) followed by refactoring to improve\nthe external quality (22.68%). Although security is a major\nconcern for almost all systems, only 3.11% of the papers\ninvestigated refactorings for security reasons.\n\n4.4 Refactoring techniques\n\nObject-oriented programming languages have common\ntraits/properties that facilitate the development of widely\nautomated source code analysis and transformation tools.\nMany studies [25] have given sufficient proof that a refac-\ntoring tool can be built for almost any object-oriented lan-\nguage (Python, PHP, Java, and C++). Support for multiple\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n13\n\n800\n\nQ\n3\n3S\n\n400\n\nCount of studies\n\n200\n\n0%)\n: -\n\n: Detection Prioritization Recommendation\n\nTesting Documentation Prediction\n\nRefactoring life-cycle\n\nFig. 13. Histogram illustrating the percentage of refactoring publications per refactoring life-cycle\n\nCount of studies\n\n9\n\nSemi-automatic\nRefactoring technique\n\nAutomatic\n\nFig. 14. Histogram illustrating the percentage of publications dealing with\nmanual, semi-automatic and automated refactoring\n\nlanguages in a refactoring tool is mentioned by [26]. Java\nis probably the most commercially important recent object-\noriented language with an infrastructure that is designed\nto support analysis. It has generic parsing, tree building,\nprettyprinting, tree manipulation, source-to-source rewrit-\ning, attribute grammar evaluations, control, and data flow\nanalysis. This explains the fact that 17.15% of refactoring\nstudies (see figure 18) provided refactoring techniques and\ntools that support Java. At the same time, most of the other\nprogramming languages have a fraction of less than 1%.\nWe classified the refactoring techniques into four main cat-\negories: data mining (e.g., Clustering, Classification, Deci-\nsion trees, Association, Neural networks, etc.), search-based\nmethods (e.g., Genetic algorithms, Hill climbing, Simulated\nannealing, Multi-objective evolutionary algorithms, etc.),\nformal methods (B-Method, the specification languages\nused in automated theorem proving, RAISE, the Z notation,\n\nCount of studies\n\nDatabase cul \u2018Architecture \u2018Model Code\nRefactoring Artifact\n\nFig. 15. histogram illustrating the count of refactoring publications per\nartifact\n\nSPARK Ada, etc.), and fuzzy logic. More than 25% of the\npapers use Search-based techniques to address refactoring\nproblems (see figure 19). This can be explained by the\nfact that search-based approaches have been proven to be\nefficient at finding solutions for complex and labor-intensive\ntasks. With the growing complexity of software systems,\nthere\u2019s an infinite amount of improvement/changes you can\nmake to any piece of artifact. Exact algorithms are hard to\nuse to solve the refactoring problem within an instance-\ndependent, finite run-time. That\u2019s why finding optimal\nrefactoring solutions are sacrificed for the sake of getting\nperfect solutions in polynomial time using heuristic meth-\nods like search-based algorithms. Data mining techniques\nhave also received significant attention (17.59%) as they are\nknown to be efficient at discovering new information, such\nas unknown patterns or hidden relationships, from huge\ndatabases like, for our case, large code repositories.\n\n4.5 Refactoring evaluation\n\nOpen-source software systems are becoming increasingly\nimportant these days. 61.1% of the studies (see figure 20)\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n1000\n\n800\n\n600\n\nCount of studies\n\n400\n\n200\n\nCould computing Mobile development\n\nModel-driven engineering Aspect-oriented design\nRefactoring Paradigm\n\n14\n\nObject-oriented design\n\nFig. 16. Histogram illustrating the count of refactoring publications per paradigm\n\n1200\n\n1000\n\n800\n\n600\n\nCount of studies\n\n400\n\n200\n\nSecurity Migration\n\nExternal quality\n\nPerformance\n\nInternal Quality\n\nRefactoring Objective\n\nFig. 17. Histogram illustrating the count of publications per refactoring objective\n\nused open-source systems to validate their work compared\nto 38.9% of studies that validated their work on industrial\nprojects. This result is expected because of the availability\nand accessibility of open source systems. However, open-\nsource software is often developed with a different man-\nagement style than the industrial ones. Thus, refactoring\ntechniques and tools must be validated and checked for\nquality and reliability using industrial systems. More indus-\ntrial collaborations are needed to bridge the gap between\nacademic research and the industry\u2019s research needs, and\ntherefore, produce groundbreaking research and innovation\nthat solves complex real-world problems.\n\n5 CONCLUSION\n\nIn this paper, we have conducted a systematic literature\nreview on refactoring accompanied by meta-analysis to an-\nswer the defined research questions. After a comprehensive\n\nsearch that follows a systematic series of steps and assessing\nthe quality of the studies, 3183 publications were identified.\nBased on these selected papers, we derived a taxonomy\nfocused on five key aspects of Refactoring: refactoring life-\ncycle, artifacts affected by refactoring, refactoring objectives,\nrefactoring techniques, and refactoring evaluation. Using\nthis classification scheme, we analyzed the primary studies\nand presented the results in a way that enables researchers\nto relate their work to the current body of knowledge and\nidentify future research directions. We also implemented a\nrepository that helps researchers/practitioners collect and\nreport papers about Refactoring. It also provides visualiza-\ntion charts and graphs that highlight the analysis results of\nour selected studies. This infrastructure will bridge the gap\namong the different refactoring communities and allow for\nmore effortless knowledge transfer. To conclude, we believe\nthat the results of our systematic review will help advance\nthe refactoring research area. Since we expect this research\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n15\n\nCount of studies\n\nscala javascript Ruby \u2018Smalltalk MATLAB Cobol\n\nProgramming Language\n\nFig. 18. histogram illustrating the count of refactoring publications per programming language\n\n800\n\n700\n\n600\n\n500\n\n400\n\nCount of studies\n\n300\n\n200\n\n100\n\n9\n\nFuzzy logic Formalmethods Datamining _ Search-based\nRefactoring Method\n\nFig. 19. histogram illustrating the count of refactoring publications per\nfield\n\narea to continue to grow in the future, we hope that our\nrepository and taxonomy will become useful in organizing,\ndeveloping and judging new approaches.\n\nREFERENCES\n\n1] W. E Opdyke, \u201cRefactoring: An aid in designing application frame-\nworks and evolving object-oriented systems,\u201d in Proc. SOOPPA\u201990:\nSymposium on Object-Oriented Programming Emphasizing Practical\nApplications, 1990.\n\n2] W. G. Griswold, \u201cProgram restructuring as an aid to software\nmaintenance.\u201d 1992.\n\n3] W. FE Opdyke, \u201cRefactoring object-oriented frameworks,\u201d 1992.\n\n4] M. Fowler, K. Beck, J. Brant, W. Opdyke, and D. Roberts, \u201cRefactor-\ning: Improving the Design of Existing Code,\u201d Xtemp01, pp. 1-337,\n1999.\n\n5] C. A. C. Coello, G. B. Lamont, D. A. Van Veldhuizen et al., Evolu-\ntionary algorithms for solving multi-objective problems. Springer, 2007,\nvol. 5.\n\n6] W. Ma, L. Chen, Y. Zhou, and B. Xu, \u201cDo We Have\na Chance to Fix Bugs When Refactoring Code Smells?\u201d\n2016 International Conference on Software Analysis, Testing and\nEvolution (SATE), pp. 24-29, 2016. [Online]. Available: http:\n/ /ieeexplore.ieee.org /document/7780189/\n\n7] K. Stroggylos and D. Spinellis, \u201cRefactoring\u2014Does It Improve Soft-\nware Quality?\u201d Fifth International Workshop on Software Quality\n(WoSQ'07: ICSE Workshops 2007), pp. 3-8, 2007.\n\nIndustrial\n\nOpen source\n\nFig. 20. Pie chart illustrating the percentage of publications in which the\n\nauthors used industrial and/or open source systems in the validation\n\nstep\n\n[8] A. Kaur and M. Kaur, \u201cAnalysis of Code Refactoring Impact on\nSoftware Quality,\u201d MATEC Web of Conferences, vol. 57, p. 02012,\nmay 2016. [Online]. Available: http://www.matec-conferences.\norg / 10.1051 /matecconf /20165702012\nG. Bavota, B. De Carluccio, A. De Lucia, M. Di Penta, R. Oliveto,\nand O. Strollo, \u201cWhen does a refactoring induce bugs? An empirical\nstudy,\u201d Proceedings - 2012 IEEE 12th International Working Conference\non Source Code Analysis and Manipulation, SCAM 2012, pp. 104-113,\n2012.\n[10] J. Al Dallal and A. Abdin, \u201cEmpirical evaluation of the impact of\nobject-oriented code refactoring on quality attributes: A systematic\nliterature review,\u201d IEEE Transactions on Software Engineering, vol. 44,\nno. 1, pp. 44-69, 2017.\n[11] S. Singh and S. Kaur, \u201cA systematic literature review: Refactoring\nfor disclosing code smells in object oriented software,\u201d Ain Shams\nEngineering Journal, vol. 9, no. 4, pp. 2129-2151, 2018.\n[12] T. Mens and T. Tourw\u00e9, \u201cA survey of software refactoring,\u201d IEEE\nTransactions on software engineering, vol. 30, no. 2, pp. 126-139, 2004.\n[13] K. O. Elish and M. Alshayeb, \u201cA classification of refactoring\nmethods based on software quality attributes,\u201d Arabian Journal for\nScience and Engineering, vol. 36, no. 7, pp. 1253-1267, 2011.\n[14] B. Du Bois, P. Van Gorp, A. Amsel, N. Van Eetvelde, H. Stenten,\nS. Demeyer, and T. Mens, \u201cA discussion of refactoring in research\nand practice,\u201d Reporte T\u00e9cnico. Universidad de Antwerpen, B\u00e9lgica,\n2004.\n[15] T. Mens, A. Van Deursen et al., \u201cRefactoring: Emerging trends\nand open problems,\u201d in Proceedings First International Workshop on\n\n[9\n\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n16\n\n17]\n\n18\n\n19\n\n20.\n\n21\n\n22\n\n23,\n\n24\n25,\n\n26\n\nREFactoring: Achievements, Challenges, Effects (REFACE). University\nof Waterloo, 2003.\n\nM. Misbhauddin and M. Alshayeb, \u201cUml model refactoring: a\nsystematic literature review,\u201d Empirical Software Engineering, vol. 20,\nno. 1, pp. 206-251, 2015.\n\nJ. Al Dallal, \u201cIdentifying refactoring opportunities in object-\noriented code: A systematic literature review,\u201d Information and\nsoftware Technology, vol. 58, pp. 231-249, 2015.\n\nM. Abebe and C.-J. Yoo, \u201cTrends, opportunities and challenges of\nsoftware refactoring: A systematic literature review,\u201d International\nJournal of Software Engineering and Its Applications, vol. 8, no. 6, pp.\n299-318, 2014.\n\nA. A. B. Bagais and M. Alshayeb, \u201cAutomatic software refactoring:\na systematic literature review,\u201d Software Quality Journal, pp. 1-44,\n2019.\n\nB. Kitchenham and S. Charters, \u201cGuidelines for performing sys-\ntematic literature reviews in software engineering,\u201d 2007.\n\nA. Ramirez, J. R. Romero, and C. L. Simons, \u201cA systematic review\nof interaction in search-based software engineering,\u201d IEEE Transac-\ntions on Software Engineering, vol. 45, no. 8, pp. 760-781, 2018.\n\nC. Wohlin, \u201cGuidelines for snowballing in systematic literature\nstudies and a replication in software engineering,\u201d in Proceedings\nof the 18th international conference on evaluation and assessment in\nsoftware engineering, 2014, pp. 1-10.\n\nM. Galster, D. Weyns, D. Tofan, B. Michalik, and P. Avgeriou,\n\u201cVariability in software systems\u2014a systematic literature review,\u201d\nIEEE Transactions on Software Engineering, vol. 40, no. 3, pp. 282-\n306, 2013.\n\n(2020) Slr website. URL: https: //slr.iselab.us/.\n\nS. Tichelaar, S. Ducasse, S. Demeyer, and O. Nierstrasz, \u201cA meta-\nmodel for language-independent refactoring,\u201d in Proceedings Inter-\nnational Symposium on Principles of Software Evolution. IEEE, 2000,\npp. 154-164.\n\nM. O. Cinn\u00e9ide and P. Nixon, \u201cA methodology for the automated\nintroduction of design patterns,\u201d in Proceedings IEEE International\nConference on Software Maintenance-1999 (ICSM\u201999).\u2019Software Main-\ntenance for Business Change\u2019(Cat. No. 99CB36360). IEEE, 1999, pp.\n463-472.\n\nPRIMARY SOURCES\n\nS2\n\nS3.\n\nS4\n\nSS.\n\nS6\n\nS7]\n\nS8\n\nSo\n\nS1] H. Sajnani, V. Saini, and C. V. Lopes, \u201cA comparative study of\n\nbug patterns in java cloned and non-cloned code,\u201d in 2014 IEEE\n14th International Working Conference on Source Code Analysis and\nManipulation. IEEE, 2014, pp. 21-30.\n\nJ. Ghofrani, M. Mohseni, and A. Bozorgmehr, \u201cA conceptual\nframework for clone detection using machine learning,\u201d in 2017\nIEEE 4th International Conference on Knowledge-Based Engineering\nand Innovation (KBEI). IEEE, 2017, pp. 0810-0817.\n\nI. Verebi, \u201cA model-based approach to software refactoring,\u201d in\n2015 IEEE International Conference on Software Maintenance and\nEvolution (ICSME). IEEE, 2015, pp. 606-609.\n\nM. Tufano, F. Palomba, G. Bavota, M. Di Penta, R. Oliveto,\nA. De Lucia, and D. Poshyvanyk, \u201cAn empirical investigation\ninto the nature of test smells,\u201d in Proceedings of the 31st IEEE/ACM\nInternational Conference on Automated Software Engineering, 2016,\npp. 4-15.\n\nB. Zhang, G. Huang, Z. Zheng, J. Ren, and C. Hu, \u201cApproach to\nmine the modularity of software network based on the most vital\nnodes,\u201d IEEE Access, vol. 6, pp. 32543-32553, 2018.\n\nG. Balogh, T. Gergely, A. Besz\u00e9des, and T. Gyim\u00e9thy, \u201cAre my\nunit tests in the right package?\u201d in 2016 IEEE 16th International\nWorking Conference on Source Code Analysis and Manipulation\n(SCAM). IEEE, 2016, pp. 137-146.\n\nN. Tsantalis, D. Mazinanian, and G. P. Krishnan, \u201cAssessing the\nrefactorability of software clones,\u201d IEEE Transactions on Software\nEngineering, vol. 41, no. 11, pp. 1055-1090, 2015.\n\nG. Soares, R. Gheyi, and T. Massoni, \u201cAutomated behavioral\ntesting of refactoring engines,\u201d IEEE Transactions on Software\nEngineering, vol. 39, no. 2, pp. 147-162, 2012.\n\nJ. Zhang, S. Han, D. Hao, L. Zhang, and D. Zhang, \u201cAutomated\nrefactoring of nested-if formulae in spreadsheets,\u201d in Proceedings\nof the 2018 26th ACM Joint Meeting on European Software Engi-\nneering Conference and Symposium on the Foundations of Software\nEngineering, 2018, pp. 833-838.\n\nS10\n\nS11\n\n$12\n\nS13.\n\n$14\n\nS15\n\nS16\n\n\u2018S17\n\nS18\n\n$19\n\n$20\n\n$21\n\n$22\n\n$23.\n\nS24\n\nS25\n\nS26\n\nS27\n\n$28\n\n$29\n\n16\n\nY. Kataoka, M. D. Ernst, W. G. Griswold, and D. Notkin, \u201cAu-\ntomated support for program refactoring using invariants,\u201d in\nProceedings IEEE International Conference on Software Maintenance.\nICSM 2001. IEEE, 2001, pp. 736-743.\n\nM. Mondal, C. K. Roy, and K. A. Schneider, \u201cA comparative study\non the bug-proneness of different types of code clones,\u201d in 2015\nIEEE International conference on software maintenance and evolution\n(ICSME). IEEE, 2015, pp. 91-100.\n\nV. Alizadeh, M. Kessentini, W. Mkaouer, M. Ocinneide, A. Ouni,\nand Y. Cai, \u201cAn interactive and dynamic search-based approach\nto software refactoring recommendations,\u201d IEEE Transactions on\nSoftware Engineering, 2018.\n\nW. Snipes, B. Robinson, and E. Murphy-Hill, \u201cCode hot spot: A\ntool for extraction and analysis of code change history,\u201d in 2011\n27th IEEE International Conference on Software Maintenance (ICSM).\nIEEE, 2011, pp. 392-401.\n\nH. Liu, Q. Liu, Z. Niu, and Y. Liu, \u201cDynamic and automatic\nfeedback-based threshold adaptation for code smell detection,\u201d\nIEEE Transactions on Software Engineering, vol. 42, no. 6, pp. 544\u2014\n558, 2015.\n\nV. Cosentino, S. Duenas, A. Zerouali, G. Robles, and J. M.\nGonzalez-Barahona, \u201cGraal: The quest for source code knowl-\nedge.\u201d\n\nS. Charalampidou, A. Ampatzoglou, A. Chatzigeorgiou, A. Gko-\nrtzis, and P. Avgeriou, \u201cIdentifying extract method refactoring\nopportunities based on functional relevance,\u201d IEEE Transactions\non Software Engineering, vol. 43, no. 10, pp. 954-974, 2016.\n\nA. Rani and J. K. Chhabra, \u201cPrioritization of smelly classes: A two\nphase approach (reducing refactoring efforts),\u201d in 2017 3rd Inter-\nnational Conference on Computational Intelligence & Communication\nTechnology (CICT). IEEE, 2017, pp. 1-6.\n\nP. Rachow, \u201cRefactoring decision support for developers and ar-\nchitects based on architectural impact,\u201d in 2019 IEEE International\nConference on Software Architecture Companion (ICSA-C). IEEE,\n2019, pp. 262-266.\n\nH. Liu, Z. Ma, W. Shao, and Z. Niu, \u201cSchedule of bad smell detec-\ntion and resolution: A new way to save effort,\u201d IEEE transactions\non Software Engineering, vol. 38, no. 1, pp. 220-235, 2011.\n\nJ. Kim, D. Batory, and D. Dig, \u201cScripting parametric refactorings\nin java to retrofit design patterns,\u201d in 2015 IEEE International\nConference on Software Maintenance and Evolution (ICSME). IEEE,\n2015, pp. 211-220.\n\nM. A. Parande and G. Koru, \u201cA longitudinal analysis of the\ndependency concentration in smaller modules for open-source\nsoftware products,\u201d in 2010 IEEE International Conference on Soft-\nware Maintenance. IEEE, 2010, pp. 1-5.\n\nH. Liu, Z. Xu, and Y. Zou, \u201cDeep learning based feature envy\ndetection,\u201d in Proceedings of the 33rd ACM/IEEE International\nConference on Automated Software Engineering, 2018, pp. 385-396.\nR. Morales, R. Saborido, F. Khomh, F. Chicano, and G. Anto-\nniol, \u201cEarmo: An energy-aware refactoring approach for mobile\napps,\u201d IEEE Transactions on Software Engineering, vol. 44, no. 12,\npp. 1176-1206, 2017.\n\nH. Liu, L. Yang, Z. Niu, Z. Ma, and W. Shao, \u201cFacilitating software\nrefactoring with appropriate resolution order of bad smells,\u201d\nin Proceedings of the 7th joint meeting of the European software\nengineering conference and the ACM SIGSOFT symposium on The\nfoundations of software engineering, 2009, pp. 265-268.\n\nH. Liu, Q. Liu, Y. Liu, and Z. Wang, \u201cIdentifying renaming op-\nportunities by expanding conducted rename refactorings,\u201d IEEE\nTransactions on Software Engineering, vol. 41, no. 9, pp. 887-900,\n2015.\n\nB. Lin, S. Scalabrino, A. Mocci, R. Oliveto, G. Bavota, and\nM. Lanza, \u201cInvestigating the use of code analysis and nlp to\npromote a consistent usage of identifiers,\u201d in 2017 IEEE 17th\nInternational Working Conference on Source Code Analysis and Ma-\nnipulation (SCAM). IEEE, 2017, pp. 81-90.\n\nG. Bavota, R. Oliveto, M. Gethers, D. Poshyvanyk, and A. De Lu-\ncia, \u201cMethodbook: Recommending move method refactorings via\nrelational topic models,\u201d IEEE Transactions on Software Engineer-\ning, vol. 40, no. 7, pp. 671-694, 2013.\n\nC. Hinds-Charles, J. Adames, Y. Yang, Y. Shen, and Y. Wang,\n\u201cA longitude analysis on bitcoin issue repository,\u201d in 2018 Ist\nIEEE International Conference on Hot Information-Centric Network-\ning (HotICN). YEEE, 2018, pp. 212-217.\n\nT. D. Oyetoyan, D. S. Cruzes, and C. Thurmann-Nielsen, \u201cA\ndecision support system to refactor class cycles,\u201d in 2015 IEEE\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nS30.\n\n$31\n\n$32.\n\n$33,\n\nS34\n\nS35.\n\nS36\n\nS37\n\nS38.\n\nS39\n\nS40\n\nS41\n\n$42\n\nS43,\n\nS44\n\nS45.\n\nS46\n\nS47\n\nS48\n\nS49\n\nS50\n\nInternational Conference on Software Maintenance and Evolution\n(ICSME). IEEE, 2015, pp. 231-240.\n\nN. Rachatasumrit and M. Kim, \u201cAn empirical investigation into\nthe impact of refactoring on regression testing,\u201d in 2012 28th Ieee\nInternational Conference on Software Maintenance (Icsm). IEEE,\n2012, pp. 357-366.\n\nM. Mirzaaghaei, F. Pastore, and M. Pezze, \u201cAutomatically repair-\ning test cases for evolving method declarations,\u201d in 2010 IEEE\nInternational Conference on Software Maintenance. IEEE, 2010, pp.\n15.\n\nB. Van Rompaey, B. Du Bois, and S. Demeyer, \u201cCharacterizing\nthe relative significance of a test smell,\u201d in 2006 22nd IEEE\nInternational Conference on Software Maintenance. IEEE, 2006, pp.\n391-400.\n\nA. Sherwany, N. Zaza, and N. Nystrom, \u201cA refactoring library for\nscala compiler extensions,\u201d in International Conference on Compiler\nConstruction. Springer, 2015, pp. 31-48.\n\nS. Paydar and M. Kahani, \u201cA semantic web based approach\nfor design pattern detection from source code,\u201d in 2012 2nd\nInternational eConference on Computer and Knowledge Engineering\n(ICCKE). IEEE, 2012, pp. 289-294.\n\nT. Haendler, \u201cA card game for learning software-refactoring\nprinciples,\u201d 2019.\n\nC. Kastner, S. Apel, and D. Batory, \u201cA case study implementing\nfeatures using aspectj,\u201d in 11th International Software Product Line\nConference (SPLC 2007). IEEE, 2007, pp. 223-232.\n\nT. Viana, \u201cA catalog of bad smells in design-by-contract method-\nologies with java modeling language,\u201d Journal of Computing Sci-\nence and Engineering, vol. 7, no. 4, pp. 251-262, 2013.\n\nD. Foetsch and E. Pulvermueller, \u201cA concept and implementation.\nof higher-level xml transformation languages,\u201d Knowledge-Based\nSystems, vol. 22, no. 3, pp. 186-194, 2009.\n\nJ. Reutelshoefer, J. Baumeister, and F. Puppe, \u201cA data structure for\nthe refactoring of multimodal knowledge,\u201d in Proceedings of the\n5th Workshop on Knowledge Engineering and Software Engineering,\n2009, pp. 33-45.\n\nS. Mouchawrab, L. C. Briand, and Y. Labiche, \u201cA measurement\nframework for object-oriented software testability,\u201d Information\nand software technology, vol. 47, no. 15, pp. 979-997, 2005.\n\nI. Cassol and G. Ar\u00e9valo, \u201cA methodology to infer and refactor\nan object-oriented model from c applications,\u201d Software: Practice\nand Experience, vol. 48, no. 3, pp. 550-577, 2018.\n\nG. De Ruvo and A. Santone, \u201cA novel methodology based on\nformal methods for analysis and verification of wikis,\u201d in 2014\nIEEE 23rd International WETICE Conference. YEEE, 2014, pp. 411-\n416.\n\nS. Rebai, O. B. Sghaier, V. Alizadeh, M. Kessentini, and M. Chater,\n\u201cInteractive refactoring documentation bot,\u201d in 2019 19th Interna-\ntional Working Conference on Source Code Analysis and Manipulation\n(SCAM). IEEE, 2019, pp. 152-162.\n\nJ. Krinke, \u201cMining execution relations for crosscutting concerns,\u201d\nIET software, vol. 2, no. 2, pp. 65-78, 2008.\n\nD. Bowes, D. Randall, and T. Hall, \u201cThe inconsistent measure-\nment of message chains,\u201d in 2013 4th International Workshop on\nEmerging Trends in Software Metrics (WETSoM). IEEE, 2013, pp.\n62-68.\n\nJ. Liu, \u201cFeature interactions and software derivatives.\u201d Journal of\nObject Technology, vol. 4, no. 3, pp. 13-19, 2004.\n\nA. Swidan, F. Hermans, and R. Koesoemowidjojo, \u201cImproving\nthe performance of a large scale spreadsheet: a case study,\u201d\nin 2016 IEEE 23rd International Conference on Software Analysis,\nEvolution, and Reengineering (SANER), vol. 1. _ IEEE, 2016, pp.\n673-677.\n\nH. Li, S. Thompson, and T. Arts, \u201cExtracting properties from test\ncases by refactoring,\u201d in 2011 IEEE Fourth International Conference\non Software Testing, Verification and Validation Workshops. IEEE,\n2011, pp. 472-473.\n\nS. Ducasse, O. Nierstrasz, N. Scharli, R. Wuyts, and A. P. Black,\n\u201cTraits: A mechanism for fine-grained reuse,\u201d ACM Transactions\non Programming Languages and Systems (TOPLAS), vol. 28, no. 2,\npp. 331-388, 2006.\n\nR. Ramos, J. Castro, J. Aratijo, F. Alencar, and R. Penteado, \u201cDi-\nvide and conquer refactoring: dealing with the large, scattering or\ntangling use case model,\u201d in Proceedings of the 8th Latin American\nConference on Pattern Languages of Programs, 2010, pp. 1-11.\n\nS51\n\nS52\n\nS53.\n\nS54\n\nS55,\n\nS56\n\n\u2018S57\n\nS58\n\nS59\n\nS60\n\nS61\n\nS62\n\nS63.\n\nS64\n\nS65,\n\nS66\n\nS67\n\nS68\n\nS69\n\nS70\n\nS71\n\n17\n\nE. Murphy-Hill, A. P. Black, D. Dig, and C. Parnin, \u201cGathering\nrefactoring data: a comparison of four methods,\u201d in Proceedings\nof the 2nd Workshop on Refactoring Tools, 2008, pp. 1-5.\n\nA. Dereziriska, \u201cA structure-driven process of automated refac-\ntoring to design patterns,\u201d in International Conference on Informa-\ntion Systems Architecture and Technology. Springer, 2017, pp. 39-\n48.\n\nE. Selim, Y. Ghanam, C. Burns, T. Seyed, and F. Maurer, \u201cA test-\ndriven approach for extracting libraries of reusable components\nfrom existing applications,\u201d in International Conference on Agile\nSoftware Development. Springer, 2011, pp. 238-252.\n\nY. Zhang, S. Dong, X. Zhang, H. Liu, and D. Zhang, \u201cAutomated.\nrefactoring for stampedlock,\u201d IEEE Access, vol. 7, pp. 104900-\n104.911, 2019.\n\nH. Xue, S. Sun, G. Venkataramani, and T. Lan, \u201cMachine learning-\nbased analysis of program binaries: A comprehensive study,\u201d\nIEEE Access, vol. 7, pp. 65 889-65 912, 2019.\n\nY. Zhang, S. Shao, H. Liu, J. Qiu, D. Zhang, and G. Zhang,\n\u201cRefactoring java programs for customizable locks based on\nbytecode transformation,\u201d IEEE Access, vol. 7, pp. 66 292-66 303,\n2019.\n\nM. FE Dolz, D. D. R. Astorga, J. Fernandez, J. D. Garcia, and J. Car-\nretero, \u201cTowards automatic parallelization of stream processing\napplications,\u201d IEEE Access, vol. 6, pp. 39 944-39 961, 2018.\n\nB. K. Sidhu, K. Singh, and N. Sharma, \u201cA catalogue of model\nsmells and refactoring operations for object-oriented software,\u201d\nin 2018 Second International Conference on Inventive Communication\nand Computational Technologies (ICICCT). IEEE, 2018, pp. 313-319.\nF. Medeiros, M. Ribeiro, R. Gheyi, and B. F. dos Santos Neto, \u201cA\ncatalogue of refactorings to remove incomplete annotations.\u201d J.\nUCS, vol. 20, no. 5, pp. 746-771, 2014.\n\nP. Ma, Y. Bian, and X. Su, \u201cA clustering method for pruning false\npositive of clonde code detection,\u201d in Proceedings 2013 Interna-\ntional Conference on Mechatronic Sciences, Electric Engineering and\nComputer (MEC). IEEE, 2013, pp. 1917-1920.\n\nG.-S. Cojocar and A.-M. Guran, \u201cA comparative analysis of mon-\nitoring concerns implementation in object oriented systems,\u201d in\n2018 IEEE 12th International Symposium on Applied Computational\nIntelligence and Informatics (SACI). IEEE, 2018, pp. 000355-\n000 360.\n\nS. Negara, N. Chen, M. Vakilian, R. E. Johnson, and D. Dig, \u201cA\ncomparative study of manual and automated refactorings,\u201d in\nEuropean Conference on Object-Oriented Programming. Springer,\n2013, pp. 552-576.\n\nT. Chen and C. He, \u201cA comparison of approaches to legacy\nsystem crosscutting concerns mining,\u201d in 2013 International Con-\nference on Computer Sciences and Applications. IEEE, 2013, pp.\n813-816.\n\nA. Martini, E. Sikander, and N. Madlani, \u201cA semi-automated\nframework for the identification and estimation of architectural\ntechnical debt: A comparative case-study on the modularization\nof a software component,\u201d Information and Software Technology,\nvol. 93, pp. 264-279, 2018.\n\nM. T. Valente, V. Borges, and L. Passos, \u201cA semi-automatic ap-\nproach for extracting software product lines,\u201d IEEE Transactions\non Software Engineering, vol. 38, no. 4, pp. 737-754, 2011.\n\nK. Garc\u00e9s, J. M. Vara, F. Jouault, and E. Marcos, \u201cAdapting trans-\nformations to metamodel changes via external transformation\ncomposition,\u201d Software & Systems Modeling, vol. 13, no. 2, pp.\n789-806, 2014.\n\nS. A. Vidal, C. Marcos, and J. A. Diaz-Pace, \u201cAn approach to\nprioritize code smells for refactoring,\u201d Automated Software Engi-\nneering, vol. 23, no. 3, pp. 501-532, 2016.\n\nC. Brown, H. Li, and S. Thompson, \u201cAn expression processor:\na case study in refactoring haskell programs,\u201d in International\nSymposium on Trends in Functional Programming. Springer, 2010,\npp. 31-49.\n\nM. Marin, A. van Deursen, L. Moonen, and R. van der Rijst,\n\u201cAn integrated crosscutting concern migration strategy and its\nsemi-automated application to jhotdraw,\u201d Automated Software\nEngineering, vol. 16, no. 2, pp. 323-356, 2009.\n\nA. O'Riordan, \u201cAspect-oriented reengineering of an object-\noriented library in a short iteration agile process,\u201d Informatica,\nvol. 35, no. 4, 2011.\n\nK. Fujiwara, K. Fushida, N. Yoshida, and H. lida, \u201cAssessing\nrefactoring instances and the maintainability benefits of them\nfrom version archives,\u201d in International Conference on Product\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nS72\n\nS73.\n\nS74\n\nS75.\n\nS76\n\nS77\n\nS78\n\nS79\n\nS80.\n\nS81\n\nS82.\n\nS83.\n\nS84\n\nS85.\n\nS86\n\nS87\n\nS88.\n\nS89\n\nS90\n\nFocused Software Process Improvement.\n323.\n\nB. Alkhazi, T. Ruas, M. Kessentini, M. Wimmer, and W. I.\nGrosky, \u201cAutomated refactoring of atl model transformations:\na search-based approach,\u201d in Proceedings of the ACM/IEEE 19th\nInternational Conference on Model Driven Engineering Languages and\nSystems, 2016, pp. 295-304.\n\nM. Tanhaei, J. Habibi, and S.-H. Mirian-Hosseinabadi, \u201cAu-\ntomating feature model refactoring: A model transformation\napproach,\u201d Information and Software Technology, vol. 80, pp. 138\u2014\n157, 2016.\n\nV. Alizadeh, H. Fehri, and M. Kessentini, \u201cLess is more: From\nmulti-objective to mono-objective refactoring via developer\u2019s\nknowledge extraction,\u201d in 2019 19th International Working Con-\nference on Source Code Analysis and Manipulation (SCAM). IEEE,\n2019, pp. 181-192.\n\nV. Alizadeh, M. A. Ouali, M. Kessentini, and M. Chater, \u201cRefbot:\nintelligent software refactoring bot,\u201d in 2019 34th IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE).\nTEEE, 2019, pp. 823-834.\n\nZ. Mushtaq, G. Rasool, and B. Shehzad, \u201cMultilingual source\ncode analysis: A systematic literature review,\u201d IEEE Access, vol. 5,\npp. 11 307-11 336, 2017.\n\nF. Schmidt, S. G. MacDonell, and A. M. Connor, \u201cAn auto-\nmatic architecture reconstruction and refactoring framework,\u201d in\nSoftware Engineering Research, Management and Applications 2011.\nSpringer, 2012, pp. 95-111.\n\nG. Cong, H. Wen, I.-h. Chung, D. Klepacki, H. Murata, and\nY. Negishi, \u201cAn efficient framework for multi-dimensional tuning\nof high performance computing applications,\u201d in 2012 IEEE 26th\nInternational Parallel and Distributed Processing Symposium. IEEE,\n2012, pp. 1376-1387.\n\nJ. Park, M. Kim, and D.-H. Bae, \u201cAn empirical study of sup-\nplementary patches in open source projects,\u201d Empirical Software\nEngineering, vol. 22, no. 1, pp. 436-473, 2017.\n\nT. L. Nguyen, A. Fish, and M. Song, \u201cAn empirical study on\nsimilar changes in evolving software,\u201d in 2018 IEEE International\nConference on Electro/Information Technology (EIT). IEEE, 2018, pp.\n0560-0563.\n\nM. Bruntink, A. Van Deursen, T. Tourwe, and R. van Engelen,\n\u201cAn evaluation of clone detection techniques for crosscutting\nconcerns,\u201d in 20th IEEE International Conference on Software Main-\ntenance, 2004. Proceedings. EEE, 2004, pp. 200-209.\n\nY. Kosker, B. Turhan, and A. Bener, \u201cAn expert system for\ndetermining candidate software classes for refactoring,\u201d Expert\nSystems with Applications, vol. 36, no. 6, pp. 10 000-10 003, 2009.\nB. L. Sousa, M. A. Bigonha, and K. A. Ferreira, \u201cAn exploratory\nstudy on cooccurrence of design patterns and bad smells using\nsoftware metrics,\u201d Software: Practice and Experience, vol. 49, no. 7,\npp. 1079-1113, 2019.\n\nO. Mehani, G. Jourjon, T. Rakotoarivelo, and M. Ott, \u201cAn in-\nstrumentation framework for the critical task of measurement\ncollection in the future internet,\u201d Computer Networks, vol. 63, pp.\n68-83, 2014.\n\nM. Schafer, A. Thies, F. Steimann, and F. Tip, \u201cA comprehensive\napproach to naming and accessibility in refactoring java pro-\ngrams,\u201d IEEE Transactions on Software Engineering, vol. 38, no. 6,\npp. 1233-1257, 2012.\n\nD. Dig, \u201cA practical tutorial on refactoring for parallelism,\u201d in\n2010 IEEE International Conference on Software Maintenance. IEEE,\n2010, pp. 1-2.\n\nX. Li and J. P. Gallagher, \u201cA source-level energy optimization\nframework for mobile applications,\u201d in 2016 IEEE 16th Interna-\ntional Working Conference on Source Code Analysis and Manipulation\n(SCAM). IEEE, 2016, pp. 31-40.\n\nR. Khatchadourian, Y. Tang, M. Bagherzadeh, and S. Ahmed,\n\u201cTengineering paper] a tool for optimizing java 8 stream software\nvia automated refactoring,\u201d in 2018 IEEE 18th International Work-\ning Conference on Source Code Analysis and Manipulation (SCAM).\nIEEE, 2018, pp. 34-39.\n\nZ. Xing and E. Stroulia, \u201cApi-evolution support with diff-\ncatchup,\u201d IEEE Transactions on Software Engineering, vol. 33,\nno. 12, pp. 818-836, 2007.\n\nR. Gheyi, T. Massoni, and P. Borba, \u201cA rigorous approach for\nproving model refactorings,\u201d in Proceedings of the 20th IEEE/ACM\ninternational Conference on Automated software engineering, 2005,\npp. 372-375.\n\nSpringer, 2013, pp. 313\u2014\n\n18\n$91] B. Cyganek, \u201cAdding parallelism to the hybrid image processing\nlibrary in multi-threading and multi-core systems,\u201d in 2011 IEEE\n2nd International Conference on Networked Embedded Systems for\nEnterprise Applications. YEEE, 2011, pp. 1-8.\n\nR. Hardt and E. V. Munson, \u201cAn empirical evaluation of ant build\n\nmaintenance using formiga,\u201d in 2015 IEEE International Conference\n\non Software Maintenance and Evolution (ICSME). IEEE, 2015, pp.\n\n201-210.\n\nR. Kolb, D. Muthig, T. Patzke, and K. Yamauchi, \u201cA case study\n\nin refactoring a legacy component for reuse in a product line,\u201d\n\nin 21st IEEE International Conference on Software Maintenance\n\n(ICSM\u201905). IEEE, 2005, pp. 369-378.\n\nD. Strein, R. Lincke, J. Lundberg, and W. L\u00e9we, \u201cAn extensible\n\nmeta-model for program analysis,\u201d IEEE Transactions on Software\n\nEngineering, vol. 33, no. 9, pp. 592-607, 2007.\n\nY.-W. Kwon, \u201cAutomated s/w reengineering for fault-tolerant\n\nand energy-efficient distributed execution,\u201d in 2013 IEEE Inter-\n\nnational Conference on Software Maintenance. IEEE, 2013, pp. 582\u2014\n\n585.\n\nB. Adams, H. Tromp, K. De Schutter, and W. De Meuter, \u201cDe-\n\nsign recovery and maintenance of build systems,\u201d in 2007 IEEE\n\nInternational Conference on Software Maintenance. YEEE, 2007, pp.\n\n114-123.\n\nR. Bahsoon and W. Emmerich, \u201cEvaluating architectural stability\n\nwith real options theory,\u201d in 20th IEEE International Conference on\n\nSoftware Maintenance, 2004. Proceedings. IEEE, 2004, pp. 443-447.\n\nJ. O'neal, K. Weide, and A. Dubey, \u201cExperience report: refactoring\n\nthe mesh interface in flash, a multiphysics software,\u201d in 2018\n\nIEEE 14th International Conference on e-Science (e-Science). IEEE,\n\n2018, pp. 1-6.\n\nM. A. Khan and H. Tembine, \u201cMeta-learning for realizing self-x\n\nmanagement of future networks,\u201d IEEE Access, vol. 5, pp. 19 072\u2014\n\n19083, 2017.\n\n$100] A. Cleve, \u201cProgram analysis and transformation for data-\nintensive system evolution,\u201d in 2010 IEEE International Conference\non Software Maintenance. IEEE, 2010, pp. 1-6.\n\n$101] D. Binkley, M. Ceccato, M. Harman, F. Ricca, and P. Tonella,\n\u201cAutomated refactoring of object oriented code into aspects,\u201d\nin 21st IEEE International Conference on Software Maintenance\n(ICSM\u201905). IEEE, 2005, pp. 27-36.\n\n$102] F. Castor Filho, A. Garcia, and C. M. F. Rubira, \u201cExtracting\nerror handling to aspects: A cookbook,\u201d in 2007 IEEE International\nConference on Software Maintenance. IEEE, 2007, pp. 134-143.\n\n$103] M. Bajammal, D. Mazinanian, and A. Mesbah, \u201cGenerating\nreusable web components from mockups,\u201d in Proceedings of the\n33rd ACM/IEEE International Conference on Automated Software\nEngineering, 2018, pp. 601-611.\n\n$104] N. A. Kraft, E. B. Duffy, and B. A. Malloy, \u201cGrammar recovery\nfrom parse trees and metrics-guided grammar refactoring,\u201d IEEE\nTransactions on Software Engineering, vol. 35, no. 6, pp. 780-794,\n2009.\n\n$105] D. Spinellis, \u201cGlobal analysis and transformations in prepro-\ncessed languages,\u201d IEEE Transactions on Software Engineering,\nvol. 29, no. 11, pp. 1019-1030, 2003.\n\n$106] C. Noguera, A. Kellens, C. De Roover, and V. Jonckers, \u201cRefac-\ntoring in the presence of annotations,\u201d in 2012 28th IEEE Interna-\ntional Conference on Software Maintenance (ICSM). IEEE, 2012, pp.\n337-346.\n\n$107] S. Rongrong, Z. Liping, and Z. Fengrong, \u201cA method for iden-\ntifying and recommending reconstructed clones,\u201d in Proceedings\nof the 2019 3rd International Conference on Management Engineering,\nSoftware Engineering and Service Sciences, 2019, pp. 39-44.\n\n$108] Y. Khan and M. El-Attar, \u201cA model transformation approach\ntowards refactoring use case models based on antipatterns,\u201d\nin 21st International Conference on Software Engineering and Data\nEngineering (SEDE\u201912), Los Angeles, California, USA, 2012, pp. 49\u2014\n54,\n\nS92\n\nS93.\n\nS94\n\nS95,\n\nS96\n\n\u2018S97\n\nS98\n\nS99\n\n$109] K. Grolinger and M. A. Capretz, \u201cA unit test approach for\ndatabase schema evolution,\u201d Information and Software Technology,\nvol. 53, no. 2, pp. 159-170, 2011.\n\n$110] O. Febbraro, K. Reale, and F. Ricca, \u201cAspide: Integrated develop-\nment environment for answer set programming,\u201d in International\nConference on Logic Programming and Nonmonotonic Reasoning.\nSpringer, 2011, pp. 317-330.\n\n$111] A. Garrido and R. Johnson, \u201cAnalyzing multiple configurations\nof ac program,\u201d in 21st IEEE International Conference on Software\nMaintenance (ICSM\u201905). IEEE, 2005, pp. 379-388.\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n$112\n\n$113\n\n$114\n\n$115\n\nS116\n\n$117]\n\n$118\n\n$119\n\n$120\n\n$121\n\n$122\n\n$123\n\n$124\n\n$125\n\nS126\n\n$127]\n\n$128\n\n$129\n\n$130\n\nS131\n\nA. Paltoglou, V. E. Zafeiris, E. A. Giakoumakis, and N. Diaman-\ntidis, \u201cAutomated refactoring of client-side javascript code to es6\nmodules,\u201d in 2018 IEEE 25th International Conference on Software\nAnalysis, Evolution and Reengineering (SANER). IEEE, 2018, pp.\n402-412.\n\nR. Khatchadourian, J. Sawin, and A. Rountev, \u201cAutomated\nrefactoring of legacy java software to enumerated types,\u201d in 2007\nIEEE International Conference on Software Maintenance. IEEE, 2007,\npp. 224-233.\n\nM. Marin, L. Moonen, and A. van Deursen, \u201cA classification\nof crosscutting concerns,\u201d in 21st IEEE International Conference on\nSoftware Maintenance (ICSM\u201905). IEEE, 2005, pp. 673-676.\n\nM. Mortensen, S. Ghosh, and J. Bieman, \u201cAspect-oriented refac-\ntoring of legacy applications: An evaluation,\u201d IEEE Transactions\non Software Engineering, vol. 38, no. 1, pp. 118-140, 2010.\n\nM. Mondal, C. K. Roy, and K. A. Schneider, \u201cAutomatic identi-\nfication of important clones for refactoring and tracking,\u201d in 2014\nIEEE 14th International Working Conference on Source Code Analysis\nand Manipulation. IEEE, 2014, pp. 11-20.\n\nA. Kellens, K. De Schutter, T. D\u2019Hondt, V. Jonckers, and\nH. Doggen, \u201cExperiences in modularizing business rules into\naspects,\u201d in 2008 IEEE International Conference on Software Mainte-\nnance. IEEE, 2008, pp. 448-451.\n\nR. Stoiber, S. Fricker, M. Jehle, and M. Glinz, \u201cFeature unweav-\ning: Refactoring software requirements specifications into soft-\nware product lines,\u201d in 2010 18th IEEE International Requirements\nEngineering Conference. IEEE, 2010, pp. 403-404.\n\nG. Zhao and J. Huang, \u201cDeepsim: deep learning code functional\nsimilarity,\u201d in Proceedings of the 2018 26th ACM Joint Meeting on\nEuropean Software Engineering Conference and Symposium on the\nFoundations of Software Engineering, 2018, pp. 141-151.\n\nS. Demeyer, S. Ducasse, and O. Nierstrasz, \u201cObject-oriented\nreengineering: patterns and techniques,\u201d in 21st IEEE Interna-\ntional Conference on Software Maintenance (ICSM\u201905). IEEE, 2005,\npp. 723-724.\n\nP. Hegedus, \u201cRevealing the effect of coding practices on soft-\nware maintainability,\u201d in 2013 IEEE International Conference on\nSoftware Maintenance. IEEE, 2013, pp. 578-581.\n\nT. Feng, J. Zhang, H. Wang, and X. Wang, \u201cSoftware design\nimprovement through anti-patterns identification,\u201d in 20th IEEE\nInternational Conference on Software Maintenance, 2004. Proceedings.\nTEEE, 2004, p. 524.\n\nS. Meng and L. S. Barbosa, \u201cA coalgebraic semantic framework\nfor reasoning about uml sequence diagrams,\u201d in 2008 The Eighth\nInternational Conference on Quality Software. IEEE, 2008, pp. 17\u2014\n26.\n\nP. Mayer and A. Schroeder, \u201cCross-language code analysis and\nrefactoring,\u201d in 2012 IEEE 12th International Working Conference on\nSource Code Analysis and Manipulation. IEEE, 2012, pp. 94-103.\n\nR. Moser, P. Abrahamsson, W. Pedrycz, A. Sillitti, and G. Succi,\n\u201cA case study on the impact of refactoring on quality and\nproductivity in an agile team,\u201d in IFIP Central and East European\nConference on Software Engineering Techniques. Springer, 2007, pp.\n252-266.\n\nA. L. Candido, F. A. Trinta, L. S. Rocha, P. A. Rego, N. C.\nMendonga, and V. C. Garcia, \u201cA microservice based architecture\nto support offloading in mobile cloud computing,\u201d in Proceedings\nof the XIII Brazilian Symposium on Software Components, Architec-\ntures, and Reuse, 2019, pp. 93-102.\n\nA. Peruma, \u201cA preliminary study of android refactorings,\u201d in\n2019 IEEE/ACM 6th International Conference on Mobile Software\nEngineering and Systems (MOBILESoft). IEEE, 2019, pp. 148-149.\n\nM. Mascheroni and E. Irrazabal, \u201cA design pattern approach\nfor restful tests: A case study,\u201d in IEEE 12th Colombian Computing\nCongress, 2018.\n\nD. Kermek, T. Jakupi\u00e9, and N. Vr\u00e9ek, \u201cA model of heterogeneous\ndistributed system for foreign exchange portfolio analysis,\u201d Jour-\nnal of Information and Organizational Sciences, vol. 30, no. 1, pp.\n83-92, 2006.\n\nG. Rodriguez, A. Teyseyre, A. Soria, and L. Berdun, \u201cA visu-\nalization tool to detect refactoring opportunities in soa applica-\ntions,\u201d in 2017 XLII Latin American Computer Conference (CLEI).\nIEEE, 2017, pp. 1-10.\n\nH. Li, S. Thompson, P. Lamela Seijas, and M. A. Francisco,\n\u201cAutomating property-based testing of evolving web services,\u201d\nin Proceedings of the ACM SIGPLAN 2014 Workshop on Partial\nEvaluation and Program Manipulation, 2014, pp. 169-180.\n\n19\n\n$132] D. Athanasopoulos, A. V. Zarras, G. Miskos, V. Issarny, and\nP. Vassiliadis, \u201cCohesion-driven decomposition of service inter-\nfaces without access to source code,\u201d IEEE Transactions on Services\nComputing, vol. 8, no. 4, pp. 550-562, 2014.\n\n$133] M. Kessentini and H. Wang, \u201cDetecting refactorings among\nmultiple web service releases: A heuristic-based approach,\u201d in\n2017 IEEE International Conference on Web Services (ICWS). IEEE,\n2017, pp. 365-372.\n\n$134] F. Wei, C. Ouyang, and A. Barros, \u201cDiscovering behavioural\ninterfaces for overloaded web services,\u201d in 2015 IEEE World\nCongress on Services. IEEE, 2015, pp. 286-293.\n\n$135] K. Fekete, A. Pelle, and K. Csorba, \u201cEnergy efficient code opti-\nmization in mobile environment,\u201d in 2014 IEEE 36th International\nTelecommunications Energy Conference (INTELEC). IEEE, 2014, pp.\n16.\n\n$136] W. B. Langdon, \u201cGenetic improvement of programs,\u201d in 2014\n16th International Symposium on Symbolic and Numeric Algorithms\nfor Scientific Computing. IEEE, 2014, pp. 14-19.\n\n$137] H. Wang, A. Ouni, M. Kessentini, B. Maxim, and W. I. Grosky,\n\u201cIdentification of web service refactoring opportunities as a\nmulti-objective problem,\u201d in 2016 IEEE International Conference\non Web Services (ICWS). IEEE, 2016, pp. 586-593.\n\n$138] M. Kim, T. Zimmermann, and N. Nagappan, \u201cAn empirical\nstudy of refactoringchallenges and benefits at microsoft,\u201d IEEE\nTransactions on Software Engineering, vol. 40, no. 7, pp. 633-649,\n2014.\n\n$139] S. M. Olbrich, D. S. Cruzes, and D. I. Sjoberg, \u201cAre all code\nsmells harmful? a study of god classes and brain classes in the\nevolution of three open source systems,\u201d in 2010 IEEE Interna-\ntional Conference on Software Maintenance. IEEE, 2010, pp. 1-10.\n\n$140] P. S. Kochhar, F. Thung, and D. Lo, \u201cAutomatic fine-grained\nissue report reclassification,\u201d in 2014 19th International Conference\non Engineering of Complex Computer Systems. IEEE, 2014, pp. 126\u2014\n135.\n\n$141] G. Bastide, A. Seriai, and M. Oussalah, \u201cDynamic adaptation\nof software component structures,\u201d in 2006 IEEE International\nConference on Information Reuse & Integration. IEEE, 2006, pp.\n404409.\n\n$142] G. Zhang, L. Shen, X. Peng, Z. Xing, and W. Zhao, \u201cIncremental\nand iterative reengineering towards software product line: An\nindustrial case study,\u201d in 2011 27th IEEE International Conference\non Software Maintenance (ICSM). IEEE, 2011, pp. 418-427.\n\n$143] G. Bavota, R. Oliveto, A. De Lucia, G. Antoniol, and Y.-G.\nGueheneuc, \u201cPlaying with refactoring: Identifying extract class\nopportunities through game theory,\u201d in 2010 IEEE International\nConference on Software Maintenance. IEEE, 2010, pp. 1-5.\n\n$144] P. S. Kochhar, Y. Tian, and D. Lo, \u201cPotential biases in bug\nlocalization: Do they matter?\u201d in Proceedings of the 29th ACM/IEEE\ninternational conference on Automated software engineering, 2014, pp.\n803-814.\n\n$145] R. Khatchadourian and H. Masuhara, \u201cDefaultification refactor-\ning: A tool for automatically converting java methods to default,\u201d\nin 2017 32nd IEEE/ACM International Conference on Automated\nSoftware Engineering (ASE). IEEE, 2017, pp. 984-989.\n\n$146] H. K. Wright, D. Jasper, M. Klimek, C. Carruth, and Z. Wan,\n\u201cLarge-scale automated refactoring using clangmr,\u201d in 2013 IEEE\nInternational Conference on Software Maintenance. IEEE, 2013, pp.\n548-551.\n\n$147] D. Mazinanian and N. Tsantalis, \u201cMigrating cascading style\nsheets to preprocessors by introducing mixins,\u201d in Proceedings of\nthe 31st IEEE/ACM International Conference on Automated Software\nEngineering, 2016, pp. 672-683.\n\n$148] P. Tonella and M. Ceccato, \u201cMigrating interface implementa-\ntions to aspects,\u201d in 20th IEEE International Conference on Software\nMaintenance, 2004. Proceedings. IEEE, 2004, pp. 220-229.\n\n$149] M. Ceccato, \u201cMigrating object oriented code to aspect oriented\nprogramming,\u201d 2006.\n\n$150] D. Majumdar, \u201cMigration from procedural programming to\naspect oriented paradigm,\u201d in 2009 IEEE/ACM International Con-\nference on Automated Software Engineering. EEE, 2009, pp. 712\u2014\n715.\n\n$151] C. Marcos, S. Vidal, E. Abait, M. Arroqui, and S. Sampaoli,\n\u201cRefactoring of a beef-cattle farm simulator,\u201d IEEE Latin America\nTransactions, vol. 9, no. 7, pp. 1099-1104, 2011.\n\n$152] R. Khatchadourian and B. Muskalla, \u201cEnumeration refactoring:\na tool for automatically converting java constants to enumerated\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\nS153.\n\n$154\n\nS155\n\nS156\n\n$157]\n\nS158\n\nS159\n\nS160\n\nS161\n\nS162\n\nS163.\n\nS164\n\nS165\n\nS166\n\n$167]\n\nS168\n\nS169\n\nS170\n\nS171\n\n$172\n\ntypes,\u201d in Proceedings of the IEEE/ACM international conference on\nAutomated software engineering, 2010, pp. 181-182.\n\nE. L. Alves, M. Song, T. Massoni, P. D. Machado, and M. Kim,\n\u201cRefactoring inspection support for manual refactoring edits,\u201d\nIEEE Transactions on Software Engineering, vol. 44, no. 4, pp. 365\u2014\n383, 2017.\n\nY. Yu, J. Jurjens, and J. Mylopoulos, \u201cTraceability for the main-\ntenance of secure software,\u201d in 2008 IEEE International Conference\non Software Maintenance. IEEE, 2008, pp. 297-306.\n\nC. Kulkarni, \u201cNotice of violation of ieee publication principles a\nqualitative approach for refactoring of code clone opportunities\nusing graph and tree methods,\u201d in 2016 International Conference\non Information Technology (InCITe)-The Next Generation IT Summit\non the Theme-Internet of Things: Connect your Worlds. IEEE, 2016,\npp. 154-159.\n\nA. E. Tappenden, T. Huynh, J. Miller, A. Geras, and M. Smith,\n\u201cAgile development of secure web-based applications,\u201d Inter-\nnational Journal of Information Technology and Web Engineering\n(IJITWE), vol. 1, no. 2, pp. 1-24, 2006.\n\nP. M. Cousot, R. Cousot, F. Logozzo, and M. Barnett, \u201cAn ab-\nstract interpretation framework for refactoring with application\nto extract methods with contracts,\u201d in Proceedings of the ACM\ninternational conference on Object oriented programming systems\nlanguages and applications, 2012, pp. 213-232.\n\nP. Borba, \u201cAn introduction to software product line refactoring,\u201d\nin International Summer School on Generative and Transformational\nTechniques in Software Engineering. Springer, 2009, pp. 1-26.\n\nO. Macek and K. Richta, \u201cApplication and relational database\nco-refactoring,\u201d Computer Science and Information Systems, vol. 11,\nno. 2, pp. 503-524, 2014.\n\nM. S. Feather and L. Z. Markosian, \u201cArchitecting and gener-\nalizing a safety case for critical condition detection software an\nexperience report,\u201d in 2013 1st International Workshop on Assurance\nCases for Software-Intensive Systems (ASSURE). IEEE, 2013, pp.\n29-33.\n\nP. Muntean, M. Monperrus, H. Sun, J. Grossklags, and C. Eck-\nert, \u201cIntrepair: Informed repairing of integer overflows,\u201d IEEE\nTransactions on Software Engineering, 2019.\n\nS. Demeyer, \u201cRefactor conditionals into polymorphism: what's\nthe performance cost of introducing virtual calls?\u201d in 21st IEEE\nInternational Conference on Software Maintenance (ICSM\u201905). IEEE,\n2005, pp. 627-630.\n\nA. Kumar, A. Sutton, and B. Stroustrup, \u201cRejuvenating c++ pro-\ngrams through demacrofication,\u201d in 2012 28th IEEE International\nConference on Software Maintenance (ICSM). IEEE, 2012, pp. 98-\n107.\n\n\u2014\u2014,, \u201cThe demacrofier,\u201d in 2012 28th IEEE International Confer-\nence on Software Maintenance (ICSM). IEEE, 2012, pp. 658-661.\n\nI. Sora, \u201cA meta-model for representing language-independent\nprimary dependency structures.\u201d in ENASE, 2012, pp. 65-74.\n\nM. F. Zibran, R. K. Saha, M. Asaduzzaman, and C. K. Roy, \u201cAn-\nalyzing and forecasting near-miss clones in evolving software:\nAn empirical study,\u201d in 2011 16th IEEE International Conference on\nEngineering of Complex Computer Systems. IEEE, 2011, pp. 295-\n304.\n\nR. Rolim, \u201cAutomating repetitive code changes using exam-\nples,\u201d in Proceedings of the 2016 24th ACM SIGSOFT International\nSymposium on Foundations of Software Engineering, 2016, pp. 1063\u2014\n1065.\n\nW. S. Evans, C. W. Fraser, and F. Ma, \u201cClone detection via\nstructural abstraction,\u201d Software Quality Journal, vol. 17, no. 4, pp.\n309-330, 2009.\n\nA. Khan, H. A. Basit, S. M. Sarwar, and M. M. Yousaf, \u201cCloning\nin popular server side technologies using agile development:\nAn empirical study,\u201d Pakistan Journal of Engineering and Applied\nSciences, no. 1, 2018.\n\nT. D. Oyetoyan, R. Conradi, and D. S. Cruzes, \u201cCriticality of\ndefects in cyclic dependent components,\u201d in 2013 IEEE 13th\nInternational Working Conference on Source Code Analysis and Ma-\nnipulation (SCAM). IEEE, 2013, pp. 21-30.\n\nM. Gatrell, S. Counsell, and T. Hall, \u201cEmpirical support for\ntwo refactoring studies using commercial c# software,\u201d in 13th\nInternational Conference on Evaluation and Assessment in Software\nEngineering (EASE) 13, 2009, pp. 1-10.\n\nR. K. Saha, M. Asaduzzaman, M. F. Zibran, C. K. Roy, and K. A.\nSchneider, \u201cEvaluating code clone genealogies at release level:\n\n20\n\nAn empirical study,\u201d in 2010 10th IEEE Working Conference on\nSource Code Analysis and Manipulation. IEEE, 2010, pp. 87-96.\n\n$173] A. Dereziriska and M. Byczkowski, \u201cEvaluation of design pat-\ntern utilization and software metrics in c# programs,\u201d in Interna-\ntional Conference on Dependability and Complex Systems. Springer,\n2019, pp. 132-142.\n\n$174] Y. A. Liu, M. Gorbovitski, and S. D. Stoller, \u201cA language and\nframework for invariant-driven transformations,\u201d ACM Sigplan\nNotices, vol. 45, no. 2, pp. 55-64, 2009.\n\n$175] I. Lanc, P. Bui, D. Thain, and S. Emrich, \u201cAdapting bioinfor-\nmatics applications for heterogeneous systems: a case study,\u201d\nConcurrency and Computation: Practice and Experience, vol. 26, no. 4,\npp. 866-877, 2014.\n\n$176] L. E. d. S. Amorim, M. J. Steindorfer, S. Erdweg, and E. Visser,\n\u201cDeclarative specification of indentation rules: a tooling perspec-\ntive on parsing and pretty-printing layout-sensitive languages,\u201d\nin Proceedings of the 11th ACM SIGPLAN International Conference\non Software Language Engineering, 2018, pp. 3-15.\n\n$177] Z. Chen, L. Chen, W. Ma, and B. Xu, \u201cDetecting code smells\nin python programs,\u201d in 2016 International Conference on Software\nAnalysis, Testing and Evolution (SATE). IEEE, 2016, pp. 18-23.\n\n$178] C. Chapman and K. T. Stolee, \u201cExploring regular expression us-\nage and context in python,\u201d in Proceedings of the 25th International\nSymposium on Software Testing and Analysis, 2016, pp. 282-293.\n\n$179] J. B. Cabral, B. Sanchez, F. Ramos, S. Gurovich, P. M. Granitto,\nand J. Vanderplas, \u201cFrom fats to feets: Further improvements\nto an astronomical feature extraction tool based on machine\nlearning,\u201d Astronomy and computing, vol. 25, pp. 213-220, 2018.\n\n$180] M. Zhu, F. McKenna, and M. H. Scott, \u201cOpenseespy: Python\nlibrary for the opensees finite element framework,\u201d SoftwareX,\nvol. 7, pp. 6-11, 2018.\n\n$181] M. Furr, J.-h. An, and J. S. Foster, \u201cProfile-guided static typing\nfor dynamic scripting languages,\u201d in Proceedings of the 24th ACM\nSIGPLAN conference on Object oriented programming systems lan-\nguages and applications, 2009, pp. 283-300.\n\n$182] Z. W. Bell, G. G. Davidson, T. M. D\u2019Azevedo, W. Joubert, J. K.\nMunro Jr, D. R. Patlolla, and B. Vacaliuc, \u201cPython for develop-\nment of openmp and cuda kernels for multidimensional data,\u201d\nin Symposium on Application Accelerators in HPC, 2011.\n\n$183] Y. Hu, U. Z. Ahmed, S. Mechtaev, B. Leong, and A. Roy-\nchoudhury, \u201cRe-factoring based program repair applied to pro-\ngramming assignments,\u201d in 2019 34th IEEE/ACM International\nConference on Automated Software Engineering (ASE). IEEE, 2019,\npp. 388-398.\n\n$184] C. Wang, S. Hirasawa, H. Takizawa, and H. Kobayashi, \u201cA\nplatform-specific code smell alert system for high performance\ncomputing applications,\u201d in 2014 IEEE International Parallel &\nDistributed Processing Symposium Workshops. YEEE, 2014, pp. 652\u2014\n661.\n\n$185] C. Biray and F. Buzluca, \u201cA learning-based method for detect-\ning defective classes in object-oriented systems,\u201d in 2015 IEEE\nEighth International Conference on Software Testing, Verification and\nValidation Workshops (ICSTW). IEEE, 2015, pp. 1-8.\n\n$186] W. Hasanain, Y. Labiche, and S. Eldh, \u201cAn analysis of complex\nindustrial test code using clone analysis,\u201d in 2018 IEEE Interna-\ntional Conference on Software Quality, Reliability and Security (QRS).\nIEEE, 2018, pp. 482-489.\n\n$187] D. Mazinanian and N. Tsantalis, \u201cAn empirical study on the use\nof css preprocessors,\u201d in 2016 IEEE 23rd International Conference\non Software Analysis, Evolution, and Reengineering (SANER), vol. 1.\nIEEE, 2016, pp. 168-178.\n\n, \u201cCssdev: refactoring duplication in cascading style\nsheets,\u201d in 2017 IEEE/ACM 39th International Conference on Soft-\nware Engineering Companion (ICSE-C). IEEE, 2017, pp. 63-66.\n\n$189] D. Mazinanian, N. Tsantalis, and A. Mesbah, \u201cDiscovering refac-\ntoring opportunities in cascading style sheets,\u201d in Proceedings of\nthe 22nd ACM SIGSOFT International Symposium on Foundations of\nSoftware Engineering, 2014, pp. 496-506.\n\n$190] D. D. Perez and W. Le, \u201cGenerating predicate callback sum-\nmaries for the android framework,\u201d in 2017 IEEE/ACM 4th In-\nternational Conference on Mobile Software Engineering and Systems\n(MOBILESoft). IEEE, 2017, pp. 68-78.\n\n$191] S. Negara, M. Vakilian, N. Chen, R. E. Johnson, and D. Dig,\n\u201cIs it dangerous to use version control histories to study source\ncode evolution?\u201d in European Conference on Object-Oriented Pro-\ngramming. Springer, 2012, pp. 79-103.\n\n$188]\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020 21\n\n$192] M. Bosch, P. Geneve\u00e9s, and N. Layaida, \u201cReasoning with style,\u201d $213] S. Schlesinger, P. Herber, T. G\u00e9thel, and S. Glesner, \u201cProving\n\nin Twenty-Fourth International Joint Conference on Artificial Intelli- correctness of refactorings for hybrid simulink models with\ngence, 2015. control flow,\u201d in International Workshop on Design, Modeling, and\n$193] H. A. Nguyen, H. V. Nguyen, T. T. Nguyen, and T. N. Nguyen, Evaluation of Cyber Physical Systems. Springer, 2016, pp. 71-86.\n\u201cOutput-oriented refactoring in php-based dynamic web appli- [S214] S. Makka and B. Sagar, \u201cSimulation of a model for refactoring\ncations,\u201d in 2013 IEEE International Conference on Software Mainte- approach for parallelism using parallel computing tool box,\u201d in\nnance. IEEE, 2013, pp. 150-159. Proceedings of First International Conference on Information and Com-\n$194] B. Chen, Z. M. Jiang, P. Matos, and M. Lacaria, \u201cAn industrial ex- munication Technology for Intelligent Systems: Volume 2. Springer,\nperience report on performance-aware refactoring on a database- 2016, pp. 77-84.\ncentric web application,\u201d in 2019 34th IEEE/ACM International $215] V. Pantelic, S. Postma, M. Lawford, M. Jaskolka, B. Mackenzie,\nConference on Automated Software Engineering (ASE). IEEE, 2019, A. Korobkine, M. Bender, J. Ong, G. Marks, and A. Wassyng,\nPpp. 653-664. \u201cSoftware engineering practices and simulink: bridging the gap,\u201d\n$195] L. Eshkevari, F. Dos Santos, J. R. Cordy, and G. Antoniol, \u201cAre International Journal on Software Tools for Technology Transfer,\nphp applications ready for hack?\u201d in 2015 IEEE 22nd Interna- vol. 20, no. 1, pp. 95-117, 2018.\ntional Conference on Software Analysis, Evolution, and Reengineering [S216] H. Zhu, Y. Yu, W. Qi, S. Liu, Y. Weng, T. Yuan, and H. Li, \u201cThe\n(SANER). IEEE, 2015, pp. 63-72. research on fault restoration and refactoring for active distribu-\n$196] J. L. Overbey and R. E. Johnson, \u201cDifferential precondition tion network,\u201d in 2019 Chinese Automation Congress (CAC). IEEE,\nchecking: A lightweight, reusable analysis for refactoring tools,\u201d 2019, pp. 4470-4474. . | |\nin 2011 26th IEEE/ACM International Conference on Automated [S217] V. N. Leonenko, N. V. Pertsev, and M. Artzrouni, \u201cUsing high\nSoftware Engineering (ASE 2011). IEEE, 2011, pp. 303-312. performance algorithms for the hybrid simulation of disease\nS197] J. L. Overbey, R. E. Johnson, and M. Hafiz, \u201cDifferential pre- dynamics on cpu and gpu.\u201d in ICCS, 2015, pp. 150-159. ,\ncondition checking: a language-independent, reusable analysis $218] S. Tichelaar, S. Ducasse, S. Demeyer, and Oz Nierstrasz, \u201cA meta-\nfor refactoring engines,\u201d Automated Software Engineering, vol. 23, model for language-independent refactoring,\u201d in P voce edings In-\nno. 1, pp. 77-104, 2016. ternational Symposium on Principles of Software Evolution. IEEE,\n$198] M. Hills, P. Klint, and J. J. Vinju, \u201cEnabling php software 2000, pp. 154-164. , ;\nengineering research in rascal,\u201d Science of Computer Programming, $219] T. M. T. T. F Munoz, \u201cBeyond the refactoring browser: Ad-\n\nvol. 134, pp. 37-46, 2017. vanced tool support for software refactoring,\u201d 2003.\n\n$199] F. Gauthier, D. Letarte, T. Lavoie, and E. Merlo, \u201cExtraction and S220] A. Garrido and R. Johnson, \u201cChallenges of refactoring C Pro\u201d\ncomprehension of moodle\u2019s access control model: A case study,\u201d grams,\" in Proceedings of the international workshop on Principles of\nin 2011 Ninth Annual International Conference on Privacy, Security software evolution, 2002, PP. 6-14. .\n\nand Trust. IEEE, 2011, pp. 44-51. $221] K. Mens and T. Tourw\u00e9, \u201cDelving source code with formal con-\n$200] M. Hills and P. Klint, \u201cPhp air: Analyzing php systems with cept analysis,\u201d Computer Languages, Systems & Structures, vol. 31,\nrascal,\u201d in 2014 Software Evolution Week-IEEE Conference on Soft- 5222 vv 54 PP. Nhe 200% RE. Johnson, \u201cD: dedi [\nware Maintenance, Reengineering, and Reverse Engineering (CSMR- ]  Y. Lee, N. Chen, and R. E. Johnson, \u201cDrag-and- Pe\nWCRE). IEEE, 2014, pp. 454-457. toring: intuitive and efficient program transformation,\u201d in 2013\n\n$201] Y. Yu, Y. Wang, J. Mylopoulos, S. Liaskos, A. Lapouchnian, and 35th International Conference on Software Engineering (ICSE). IEEE,\n\nJ. C. S. do Prado Leite, \u201cReverse engineering goal models from 2013, pp. 23-32.\n\nlegacy code,\u201d in 13th IEEE International Conference on Requirements 8 Mimente with prowctive declanstve. mcta-progeammning Pe\nEngineering (RE\u201905). | IEEE, 2005, pp. 363-372. Proceedings of the International Workshop on Smalltalk Technologies,\n\n$202] R. Lammel and J. Visser, \u201cA strafunski application letter,\u201d in In- 2009, pp. 68-76.\n\nternational Symposium on Practical Aspects of Declarative Languages. $224] M. Unterholzner, \u201cImproving refactoring tools in smalltalk\n\nSpringer, 2003, pp. 357-375. using static type inference,\u201d Science of Computer Programmin\n$203] G. M. Rama, \u201cA desiderata for refactoring-based software mod- vol. 56, pp. 70-83, 2014. \u2019 P 8 &\n\nularity improvement,\u201d in Proceedings of the 3rd India software\n\nengineering conference, 2010, pp. 93-102.\n\n$204] T. Mens and T. Tourw\u00e9, \u201cA survey of software refactoring,\u201d IEEE\n\nTransactions on software engineering, vol. 30, no. 2, pp. 126-139,\n\n2004. ype preaica \u2018\n\n$205] A. Abadi, R. Ettinger, and Y. A. Feldman, \u201cFine slicing,\u201d in Domai languages, td oe see TOE ACM Symposium on\nInternational Conference on Fundamental Approaches to Software $227] Pp Tesone, G. Polito, L. Fabresse, N. Bouraqadi, and S. Ducasse,\n\nEngineering. Springer, 2012, pp. 471-485. . \u201cPreserving instance state during refactorings in live environ-\n$206] M. Lillack, C. Bucholdt, and D. Schilling, \u201cDetection of code ments,\u201d Future Generation Computer Systems, 2020.\n\n$225] D. Vainsencher, \u201cMudpie: layers in the ball of mud,\u201d Computer\nLanguages, Systems & Structures, vol. 30, no. 1-2, pp. 5-19, 2004.\n\n$226] O. Callati, R. Robbes, E. Tanter, D. R\u00e9othlisberger, and A. Bergel,\n\u201cOn the use of type predicates in object-oriented software: The\n\nclones in software generators,\u201d in Proceedings of the 6th Interna- $228] V. Arnaoudova and C. Constantinides, \u201cAdaptation of refac-\ntional Workshop on Feature-Oriented Software Development, 2014, pp. toring strategies to multiple axes of modularity: characteristics\n37-44. oo . and criteria,\u201d in 2008 Sixth International Conference on Software\n$207] H. M. Sneed and K. Erdoes, \u201cMigrating as400-cobol to java: a Engineering Research, Management and Applications. IEEE, 2008,\nreport from the field,\u201d in 2013 17th European Conference on Software pp. 105-114.\nMaintenance and Reengineering. IEEE, 2013, pp. 231-240. $229] E. Rodrigues Jr, R. S. Durelli, R. W. de Bettio, L. Montecchi, and\n$208] M. K. Smith and T. Laszewski, \u201cModernization case study: R. Terra, \u201cRefactorings for replacing dynamic instructions with\nItalian ministry of instruction, university, and research,\u201d in In- static ones: the case of ruby,\u201d in Proceedings of the XXII Brazilian\nformation Systems Transformation. Elsevier, 2010, pp. 171-191. Symposium on Programming Languages, 2018, pp. 59-66.\n$209] T. Hatano and A. Matsuo, \u201cRemoving code clones from indus- $230] P. Sommerlad, G. Zgraggen, T. Corbat, and L. Felber, \u201cRetaining\ntrial systems using compiler directives,\u201d in 2017 IEEE/ACM 25th comments when refactoring code,\u201d in Companion to the 23rd\nInternational Conference on Program Comprehension (ICPC). IEEE, ACM SIGPLAN conference on Object-oriented programming systems\n2017, pp. 336-345. languages and applications, 2008, pp. 653-662.\n$210] T. Gerlitz, Q. M. Tran, and C. Dziobek, \u201cDetection and handling $231] T. Corbat, L. Felber, M. Stocker, and P. Sommerlad, \u201cRuby\nof model smells for matlab/simulink models.\u201d in MASE@ MoD- refactoring plug-in for eclipse,\u201d in Companion to the 22nd ACM\nELS, 2015, pp. 13-22. SIGPLAN conference on Object-oriented programming systems and\n$211] Z. Zhao, X. Li, L. He, C. Wu, and J. K. Hedrick, \u201cEstimation applications companion, 2007, pp. 779-780.\nof torques transmitted by twin-clutch of dry dual-clutch trans- [$232] R. Chen and H. Miao, \u201cA selenium based approach to automatic\nmission during vehicle\u2019s launching process,\u201d IEEE Transactions test script generation for refactoring javascript code,\u201d in 2013\non Vehicular Technology, vol. 66, no. 6, pp. 4727-4741, 2016. IEEE/ACIS 12th International Conference on Computer and Informa-\n$212] K. Aishwarya, R. Ramesh, P. M. Sobarad, and V. Singh, \u201cLossy tion Science (ICIS). IEEE, 2013, pp. 341-346.\nimage compression using svd coding algorithm,\u201d in 2016 Interna- [S233] H. V. Nguyen, H. A. Nguyen, T. T. Nguyen, and T. N. Nguyen,\ntional Conference on Wireless Contmunications, Signal Processing and \u201cBabelref: detection and renaming tool for cross-language pro-\n\nNetworking (WiSPNET). IEEE, 2016, pp. 1384-1389. gram entities in dynamic web applications,\u201d in 2012 34th Interna-\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n\n$234\n\n$235\n\nS236\n\n$237]\n\n$238\n\n$239\n\n$240\n\nS241\n\n$242\n\nS243\n\n$244\n\nS245\n\nS246\n\nS247]\n\nS248\n\n$249\n\n$250\n\n$251\n\n$252\n\n$253.\n\ntional Conference on Software Engineering (ICSE).\n1391-1394.\n\nK. An and E. Tilevich, \u201cD-goldilocks: Automatic redistribution\nof remote functionalities for performance and efficiency,\u201d in 2020\nIEEE 27th International Conference on Software Analysis, Evolution\nand Reengineering (SANER). IEEE, 2020, pp. 251-260.\n\nC.-Y. Hsieh, C. Le My, K. T. Ho, and Y. C. Cheng, \u201cIdentification\nand refactoring of exception handling code smells in javascript,\u201d\nJournal of Internet Technology, vol. 18, no. 6, pp. 1461-1471, 2017.\n\nT. Mendes, M. T. Valente, and A. Hora, \u201cIdentifying utility\nfunctions in java and javascript,\u201d in 2016 X Brazilian Symposium\non Software Components, Architectures and Reuse (SBCARS). IEEE,\n2016, pp. 121-130.\n\nN. Van Es, Q. Stievenart, J. Nicolay, T. D\u2019Hondt, and\nC. De Roover, \u201cImplementing a performant scheme interpreter\nfor the web in asm. js,\u201d Computer Languages, Systems & Structures,\nvol. 49, pp. 62-81, 2017.\n\nL. Gong, M. Pradel, and K. Sen, \u201cJitprof: pinpointing jit-\nunfriendly javascript code,\u201d in Proceedings of the 2015 10th Joint\nMeeting on Foundations of Software Engineering, 2015, pp. 357-368.\n\nA. M. Fard and A. Mesbah, \u201cJsnose: Detecting javascript code\nsmells,\u201d in 2013 IEEE 13th International Working Conference on\nSource Code Analysis and Manipulation (SCAM). IEEE, 2013, pp.\n116-125.\n\nC. Schuster, T. Disney, and C. Flanagan, \u201cMacrofication: Refac-\ntoring by reverse macro expansion,\u201d in European Symposium on\nProgramming. Springer, 2016, pp. 644-671.\n\nJ. Portner, J. Kerr, and B. Chu, \u201cMoving target defense against\ncross-site scripting attacks (position paper),\u201d in International Sym-\nposium on Foundations and Practice of Security. Springer, 2014, pp.\n85-91.\n\nG. Ortiz, J. A. Caravaca, A. Garcia-de Prado, J. Boubeta-Puig\net al., \u201cReal-time context-aware microservice architecture for pre-\ndictive analytics and smart decision-making,\u201d IEEE Access, vol. 7,\npp. 183 177-183 194, 2019.\n\nM. U. Khan, M. Z. Iqbal, and S. Ali, \u201cA heuristic-based approach\nto refactor crosscutting behaviors in uml state machines,\u201d in\n2014 IEEE International Conference on Software Maintenance and\nEvolution. IEEE, 2014, pp. 557-560.\n\nR. Terra, M. T. Valente, and N. Anquetil, \u201cA lightweight re-\nmodularization process based on structural similarity,\u201d in 2016\nX Brazilian Symposium on Software Components, Architectures and\nReuse (SBCARS). IEEE, 2016, pp. 111-120.\n\nM. Bialy, M. Lawford, V. Pantelic, and A. Wassyng, \u201cA method-\nology for the simplification of tabular designs in model-based\ndevelopment,\u201d in 2015 IEEE/ACM 3rd FME Workshop on Formal\nMethods in Software Engineering. IEEE, 2015, pp. 47-53.\n\nP. Langer, M. Wimmer, P. Brosch, M. Herrmannsd\u00e9rfer, M. Seid],\nK. Wieland, and G. Kappel, \u201cA posteriori operation detection\nin evolving software models,\u201d Journal of Systems and Software,\nvol. 86, no. 2, pp. 551-566, 2013.\n\nA. T. Sampson, J. M. Bjorndalen, and P. S. Andrews, \u201cBirds\non the wall: Distributing a process-oriented simulation,\u201d in 2009\nIEEE Congress on Evolutionary Computation. IEEE, 2009, pp. 225\u2014\n231.\n\nY. Wang, H. Yu, Z. Zhu, W. Zhang, and Y. Zhao, \u201cAutomatic\nsoftware refactoring via weighted clustering in method-level\nnetworks,\u201d IEEE Transactions on Software Engineering, vol. 44,\nno. 3, pp. 202-236, 2017.\n\nA. Ouni, M. Kessentini, M. O Cinn\u00e9ide, H. Sahraoui, K. Deb, and\nK. Inoue, \u201cMore: A multi-objective refactoring recommendation\napproach to introducing design patterns and fixing code smells,\u201d\nJournal of Software: Evolution and Process, vol. 29, no. 5, p. 1843,\n2017.\n\nH. Wang, M. Kessentini, and A. Ouni, \u201cBi-level identification\nof web service defects,\u201d in International Conference on Service-\nOriented Computing. Springer, Cham, 2016, pp. 352-368.\n\nA. Ghannem, G. El Boussaidi, and M. Kessentini, \u201cOn the use\nof design defect examples to detect model refactoring opportuni-\nties,\u201d Software Quality Journal, vol. 24, no. 4, pp. 947-965, 2016.\n\nB. Amal, M. Kessentini, S. Bechikh, J. Dea, and L. B. Said, \u201cOn\nthe use of machine learning and search-based software engi-\nneering for ill-defined fitness function: a case study on software\nrefactoring,\u201d in International Symposium on Search Based Software\nEngineering. Springer, Cham, 2014, pp. 31-45.\n\nM. Kessentini, A. Ouni, P. Langer, M. Wimmer, and S. Bechikh,\n\nIEEE, 2012, pp.\n\n22\n\n\u201cSearch-based metamodel matching with structural and syntactic\nmeasures,\u201d Journal of Systems and Software, vol. 97, pp. 1-14, 2014.\n\n$254] M. Kessentini, R. Mahaouachi, and K. Ghedira, \u201cWhat you like\nin design use to correct bad-smells,\u201d Software Quality Journal,\nvol. 21, no. 4, pp. 551-571, 2013.\n\n$255] A. Ghannem, M. Kessentini, and G. El Boussaidi, \u201cDetecting\nmodel refactoring opportunities using heuristic search,\u201d in Pro-\nceedings of the 2011 Conference of the Center for Advanced Studies on\nCollaborative Research, 2011, pp. 175-187.\n\n$256] M. Boussaa, W. Kessentini, M. Kessentini, S. Bechikh, and S. B.\nChikha, \u201cCompetitive coevolutionary code-smells detection,\u201d\nin International Symposium on Search Based Software Engineering.\nSpringer, Berlin, Heidelberg, 2013, pp. 50-65.\n\n$257] E. Erturk and E. A. Sezer, \u201cA comparison of some soft comput-\ning methods for software fault prediction,\u201d Expert systems with\napplications, vol. 42, no. 4, pp. 1872-1879, 2015.\n\n$258] C. S. Melo, M. M. L. da Cruz, A. D. F. Martins, T. Matos, J. M.\nda Silva Monteiro Filho, and J. de Castro Machado, \u201cA practical\nguide to support change-proneness prediction,\u201d 2019.\n\n$259] L. Kumar, S. M. Satapathy, and A. Krishna, \u201cApplication of\nsmote and Issvm with various kernels for predicting refactoring\nat method level,\u201d in International Conference on Neural Information\nProcessing. Springer, 2018, pp. 150-161.\n\n$260] R. Hill and J. Rideout, \u201cAutomatic method completion,\u201d in\nProceedings. 19th International Conference on Automated Software\nEngineering, 2004. IEEE, 2004, pp. 228-235.\n\n$261] S. Haiduc, G. Bavota, A. Marcus, R. Oliveto, A. De Lucia, and\nT. Menzies, \u201cAutomatic query reformulations for text retrieval\nin software engineering,\u201d in 2013 35th International Conference on\nSoftware Engineering (ICSE). YEEE, 2013, pp. 842-851.\n\n$262] G. M. Ubayawardana and D. D. Karunaratna, \u201cBug prediction\nmodel using code smells,\u201d in 2018 18th International Conference\non Advances in ICT for Emerging Regions (ICTer). IEEE, 2018, pp.\n70-77.\n\n$263] Z. Aliyu, L. A. Rahim, and E. E. Mustapha, \u201cA combine usability\nframework for imcat evaluation,\u201d in 2014 International Conference\non Computer and Information Sciences (ICCOINS). IEEE, 2014, pp.\n15.\n\n$264] A. Herranz and J. J. Moreno-Navarro, \u201cFormal extreme (and\nextremely formal) programming,\u201d in International Conference on\nExtreme Programming and Agile Processes in Software Engineering.\nSpringer, 2003, pp. 88-96.\n\n$265] L. Quan, Q. Zongyan, and Z. Liu, \u201cFormal use of design patterns\nand refactoring,\u201d in International Symposium on Leveraging Appli-\ncations of Formal Methods, Verification and Validation. Springer,\n2008, pp. 323-338.\n\n$266] J. W. Ko and Y. J. Song, \u201cGraph based model transforma-\ntion verification using mapping patterns and graph comparison\nalgorithm,\u201d International Journal of Advancements in Computing\nTechnology, vol. 4, no. 8, 2012.\n\n$267] T. Ruhroth and H. Wehrheim, \u201cModel evolution and refine-\nment,\u201d Science of Computer Programming, vol. 77, no. 3, pp. 270-\u2014\n289, 2012.\n\n$268] S. Stepney, F. Polack, and I. Toyn, \u201cPatterns to guide practical\nrefactoring: examples targetting promotion in z,\u201d in International\nConference of B and Z Users. Springer, 2003, pp. 20-39.\n\n$269] T. v. Enckevort, \u201cRefactoring uml models: using openarchitec-\ntureware to measure uml model quality and perform pattern\nmatching on uml models with ocl queries,\u201d in Proceedings of\nthe 24th ACM SIGPLAN conference companion on Object oriented\nprogramming systems languages and applications, 2009, pp. 635-646.\n\n$270] D. Luciv, D. Koznov, H. A. Basit, and A. N. Terekhov, \u201cOn fuzzy\nrepetitions detection in documentation reuse,\u201d Programming and\nComputer Software, vol. 42, no. 4, pp. 216-224, 2016.\n\n$271] D. Arcelli, V. Cortellessa, and C. Trubiani, \u201cPerformance-based\nsoftware model refactoring in fuzzy contexts,\u201d in International\nConference on Fundamental Approaches to Software Engineering.\nSpringer, 2015, pp. 149-164.\n\n$272] C. Wang and S. Kang, \u201cAdfl: An improved algorithm for ameri-\ncan fuzzy lop in fuzz testing,\u201d in International Conference on Cloud\nComputing and Security. Springer, 2018, pp. 27-36.\n\n$273] P. Lerthathairat and N. Prompoon, \u201cAn approach for source\ncode classification using software metrics and fuzzy logic to\nimprove code quality with refactoring techniques,\u201d in Interna-\ntional Conference on Software Engineering and Computer Systems.\nSpringer, 2011, pp. 478-492.\n",
                    "IEEE TRANSACTIONS OF SOFTWARE ENGINEERING, VOL. 1, NO. 1, JUNE 2020\n$274] Z. Avdagic, D. Boskovic, and A. Delic, \u201cCode evaluation us-\ning fuzzy logic,\u201d in Proceedings of the 9th WSEAS International\nConference on Fuzzy Systems. World Scientific and Engineering\nAcademy and Society (WSEAS), 2008, pp. 20-25.\n\nH. Liu, J. Jin, Z. Xu, Y. Bu, Y. Zou, and L. Zhang, \u201cDeep learning\nbased code smell detection,\u201d IEEE Transactions on Software Engi-\nneering, 2019.\n\nY. Wang, \u201cWhat motivate software engineers to refactor source\ncode? evidences from professional developers,\u201d in 2009 IEEE\nInternational Conference on Software Maintenance. IEEE, 2009, pp.\n413-416.\n\nJ. Grigera, A. Garrido, and G. Rossi, \u201cKobold: web usability\nas a service,\u201d in 2017 32nd IEEE/ACM International Conference on\nAutomated Software Engineering (ASE). IEEE, 2017, pp. 990-995.\n\nM. W. Mkaouer, M. Kessentini, S. Bechikh, K. Deb, and\nM. O Cinn\u00e9ide, \u201cRecommendation system for software refactor-\ning using innovization and interactive dynamic optimization,\u201d\nin Proceedings of the 29th ACM/IEEE international conference on\nAutomated software engineering, 2014, pp. 331-336.\n\nV. Alizadeh and M. Kessentini, \u201cReducing interactive refactor-\ning effort via clustering-based multi-objective search,\u201d in Proceed-\nings of the 33rd ACM/IEEE International Conference on Automated\nSoftware Engineering, 2018, pp. 464-474.\n\n$275\n\nS276\n\n$277]\n\nS278\n\n$279\n\nChaima Abid is currently a PhD student in the\nintelligent Software Engineering group at the\nUniversity of Michigan. Her PhD project is con-\ncerned with the application of intelligent search\nand machine learning in different areas such as\nweb services, refactoring and security. Her cur-\nrent research interests are Search-Based Soft-\nware Engineering, web services, refactoring, se-\ncurity, data analytics and software quality.\n\nVahid Alizadeh is currently a Ph.D. student in\nthe intelligent Software Engineering group at the\nUniversity of Michigan. His Ph.D. project is con-\ncerned with the application of intelligent search\nand machine learning in different software engi-\nneering areas such as refactoring, testing, and\ndocumentation. His current research interests\nare Search-Based Software Engineering, Refac-\ntoring, Artificial Intelligence, data analytics and\nsoftware quality.\n\nMarouane Kessentini is a recipient of the pres-\ntigious 2018 President of Tunisia distinguished\nresearch award, the University distinguished\nteaching award, the University distinguished dig-\nital education award, the College of Engineering\nand Computer Science distinguished research\naward, 4 best paper awards, and his Al-based\nsoftware refactoring invention, licensed and de-\nployed by industrial partners, is selected as one\nof the Top 8 inventions at the University of Michi-\ngan for 2018 (including the three campuses),\namong over 500 inventions, by the UM Technology Transfer Office. He\nis currently a tenured associate professor and leading a research group\non Software Engineering Intelligence. Prior to joining UM in 2013, He\nreceived his Ph.D. from the University of Montreal in Canada in 2012.\nHe received several grants from both industry and federal agencies and\npublished over 110 papers in top journals and conferences. He has\nseveral collaborations with industry on the use of computational search,\nmachine learning and evolutionary algorithms to address software engi-\nneering and services computing problems.\n\n23\n\nThiago do Nascimento Ferreira is a Post-\ndoctoral Researcher at University of Michigan-\nDearborn under the supervision of Dr. Marouane\nKessentini in the ISELab. He received my PhD\nDegree in Computer Science from the Fed-\neral University of Parana in 2019. His research\nmainly focuses on the use of Preference and\nSearch Based Software Engineering to address\nseveral software engineering problems such as\nSoftware Testing and Software Refactoring.\n\n>:\n\nDanny Dig is an associate professor of com-\nputer science at the University of Colorado,\nand an adjunct professor at University of Illinois\nand Oregon State. He successfully pioneered\ninteractive program transformations by opening\nthe field of refactoring in cutting-edge domains\nincluding mobile, concurrency and parallelism,\ncomponent-based, testing, and end-user pro-\ngramming. He earned his Ph.D. from the Univer-\nsity of Illinois at Urbana-Champaign where his\nresearch won the best Ph.D. dissertation award,\nand the First Prize at the ACM Student Research Competition Grand\nFinals. He did a postdoc at MIT. He (co-)authored 50+ journal and\nconference papers that appeared in top places in SE/PL. According\nto Google Scholar his publications have been cited 4000+ times. His\nresearch was recognized with 8 best paper awards at the flagship and\ntop conferences in SE, 4 award runner-ups, and 1 most influential paper\naward (N-10 years) at ICSME\u201915. He received the NSF CAREER award,\nthe Google Faculty Research Award (twice), and the Microsoft Software\nEngineering Innovation Award (twice). He released 9 software systems,\namong them the world\u2019s first open-source refactoring tool. Some of the\ntechniques he developed are shipping with the official release of the\npopular Eclipse, NetBeans, and Visual Studio development environ-\nments (of which Eclipse alone had more than 14M downloads in 2014).\n"
                ]
            }
        },
        {
            "file_name": "S:\\OneDrive\\@Dev\\!GPT\\ScriptGPT\\library\\Refactoring\\Source\\A Survey of Deep Learning Based Software Refactoring'.pdf",
            "time_taken": 82.13549375534058,
            "data_extracted": {
                "text": [
                    "A Survey of Deep Learning Based Software Refactoring\n\nBRIDGET NYIRONGO, Beijing Institute of Technology, China\nYANJIE JIANG\u2019, Peking University, China\nHE JIANG, Dalian University of Technology, China\n\nHUI LIU, Beijing Institute of Technology, China\n\nRefactoring is one of the most important activities in software engineering which is used to improve the quality (especially the\nmaintainability) of a software system. The traditional approaches to software refactoring involve designing a series of heuristics for\nrefactoring detection, solution suggestions, and refactoring execution. However, these approaches usually employ manually designed\nheuristics, which are often tedious, time-consuming, and challenging. With the advancement of deep learning techniques, researchers\nare attempting to apply deep learning techniques to software refactoring. Consequently, dozens of deep learning -based refactoring\napproaches have been proposed. However, there isa lack ofcomprehensive reviews on such works as well asa taxonomy for deep\nlearning-based refactoring. Tothis end, in this paper, we presenta survey ondeeplearning-based software refactoring. Weclassify\nrelated works into five categories according to the major tasks they cover, i.e., the detection of code smells, the recommendation of\nrefactoring solutions, the end-to-end code transformation as refactoring, quality assurance, and the mining ofrefactorings. Among\nthese categories, we further present key aspects (i.e.,code smell types, refactoring types, training strategies, and evaluation) to give\ninsight into the details of the technologies that have supported refactoring through deep learning. The classification indicates that\nthere is an imbalance in the adoption of deep learning techniques for the process of refactoring. Most of the deep learning techniques\nhave been used for the detection of code smells and the recommendation of refactoring solutions as foundin 56.25% and 33.33% of\nthe literature respectively. In contrast, only 6.25% and 4.17% were towards the end-to-end code transformationas refactoringand\nthe mining of refactorings, respectively. Notably, we found no literature representation for the quality assurance for refactoring. We\nalso observe that most of the deep learning techniques have been used to support refactoring processes occurring at the method\nlevel whereas classes and variables attracted minimal attention. Finally, we discuss the challenges and limitations associated with the\n\nemployment of deep learning-based refactorings and present some potential research opportunities for future work.\n\nCCS Concepts: + Software and its engineering \u2014 Software creation and management; - Software post-development issues; -\nSoftware quality assurance;\n\nAdditional Key Words and Phrases: Software refactoring, Deep learning, Code smells\n\n1 INTRODUCTION\n\nSoftware refactoring is used to improve software quality by changing the internal structure of a system software\nwithoutaltering its external behavior. Itisa way of maintaining andimproving the quality ofsoftware code thathelps\nin the following tasks [26]. First, ithelps in the discovery of bugs. When refactoring, more time and work are spent\n\non understanding what the program code does and incorporatingany new understanding into thecode. This process\n\n\u2018Corresponding author\n\nAuthors\u2019 addresses: Bridget Nyirongo, larjean89 @outlook.com, Beijing Institute of Technology, Beijing, China; Yanjie Jiang, yanjiejiang@pku.edu.cn,\nPeking University, Beijing, China; He Jiang, jianghe @dlut. edu.cn, Dalian University of Technology, Dalian, China; Hui Liu, liuhui08@bit.edu.cn, Beijing\nInstitute of Technology, Beijing, China.\n",
                    "2 Nyirongo and Jiang, etal.\n\nhelps to bring to light any assumptions that were previously made, making it less likely that bugs will go unnoticed.\nSecond, refactoring can help to improve the software design. With the development of software, various modifications\nmay be introduced due to misunderstanding of requirements or urgent task assignments. After such modifications,\nthe software will become harder to read and comprehend. Refactoring cleans up the program code as workis done to\nrearrange and remove parts that are not in order and are unnecessary. This helps to retain the program code\u2019s structure\nthereby improving its design. Third, it helps in rapid software development. Refactoring helps in the faster development\nof software because it stops the design of the system from going bad. With frequent refactoring, not much time is\nspent on finding and fixing errors that might arise from poor code design. Fourth, it helps make software easier to\nunderstand. This is because a little more time spent on refactoring could make the code better communicate its purpose.\nThis code would say exactly what the developer writing it meant, as such any other developer using this code might\neasily understand it.\n\nResearchers have dedicated a great amount of time trying to find ways that could make the process of software refactoring\nless tedious and time-consuming. Different techniques, models, and concepts have been used. Semi-automatic and\nautomatic tools [22,86,89,91, 92,94, 100] have been developed to assist in the detection, recommendation, and safe\napplication ofthe refactorings. Most ofthese tools can easily be integrated as plugins in most of the modern IDEslike\nEclipse and Intellij IDE. The integration of these tools in the IDEs is usually alongside the already existing refactoring\nmenus within the IDEs thereby enriching the support rendered towards the process of refactoring. Even though this is\nthe case, it has been noted that most of the approaches used to come up with these tools rely on heuristics which are\nmanual in nature [51, 53]. Thus, researchers have adopted the use of machine learning techniques to minimize the use\nof manually designed heuristics [23, 25].\n\nDeep learning is a sub-field of machine learning that focuses on creating large neural network models that are capable\nof making accurate data-driven decisions. Deep learning is mostly suitable for contexts where data is complex and\nwhere large datasets are available. The unique aspect of deep learning is the approach it takes to feature design which is\ncharacterized by the automatic learning of hierarchical representations from raw data thereby eliminating the need for\nmanual feature engineering. Deep learning models can learn useful features from low-level raw data and complex non-\nlinear mappings from inputs and outputs [29, 43]. This is unlike most of the statistical machine learning models where\nfeature design is ahuman-intensive task that can require deep domain expertise and consumea lot of time and resources.\nAt the core of deep learning are Artificial Neural Networks (ANN or NN) which are composed of interconnected\nnodes organized into layers [1, 12,78]. Deep learning uses several types of Neural Networks each tailored for specific\ntasks. Some of the most common ones are as follows. Feedforward Neural Networks(FNN) also known as Multilayer\nPerceptrons(MLP). These are mostly used for general-purpose tasks including classification and regression. Convolutional\nNeural Networks (CNN). CNN utilizes convolutional layers to automatically learn hierarchical features from input images.\nThese were originally designed for image and grid-like data. Recurrent Neural Networks (RNN).RNN contain loops to\ncapture dependencies in sequential data. Two ofits variants, Long Short Term Memory (LSTM) and Gated Recurrent\nUnit (GRU) address the vanishing gradient problem and improve the modeling of long-range dependencies. Graph\nNeural Networks (GNN). GNN learns to process and extract information from graph-structured inputs. These are used\nin tasks like node classification, link prediction, and recommendation systems. Generative Adversarial Networks(GAN).\nGAN consists ofa generator networkanda discriminator network that are trained simultaneously. These are used for\ngenerating new data samples, such as images, text, and music. Autoencoders. Autoencoders are neural networks that are\n\nused for unsupervised learning and dimensionality reduction. Autoencoders comprise an encoder network to reduce\n",
                    "ASurvey of Deep Learning Based Software Refactoring 3\n\nthe input data\u2019s dimensionality and a decoder network to reconstruct the input data from the reduced representation.\nTransformers. Transformers use a self-attention mechanism to capture relationships between input elements. They are\npopularised by models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative\npretrained Transformer) for NLP tasks, but they have been applied to various othertasks.\n\nRecently, a considerable amount of research [32, 51, 53] has been proposed to investigate and explore the application\nand adoption of deep learning techniques to automate and support the process of software refactoring. Researchers\nhave employed various deep learning models in the different tasks involved in the process of software refactoring.\nTo present the state-of-the-art on the employment of deep learning in refactoring, researchers conducted literature\nreviews [2,56,63, 110] centering around the use of deep learning for refactoring activities, e.g., the detection of code\nsmells. Naik etal. [63] presented and analyzed 17 related works published from 2016 to 2022 to identify which deep\nlearning techniques had been used for code refactoring and how well they worked. Alabza et al. [2] conducted a\nsystematic review focusing on the deep learning approaches for bad smell detection on 67 studies published until\nOctober 2022. Malhotra etal. [56] conducted a systematic literature review examining deep learning\u2019s capability to\nspot code smells on 35 primary studies published from 2013 to July 2023. Zhang etal. [110] conducted a survey on\ncode smell detection based on supervised learning models by analyzing 86 studies published from 2010 to April 2023.\nAlthough such reviews have significantly facilitated the understanding of deep learning-based refactoring, we still\nlack a comprehensive survey that considers the majority of related works, covering all aspects of deep learning-based\nsoftware refactoring (not just confined to code smell detection), and provides a taxonomy for deep learning-based\nrefactoring.\n\nTo this end, in this paper, we conduct a survey by collecting 48 primary studies published from January 2018 to October\n2023 and classify them based on the specific refactoring tasks being supported by the deep learning technique, i.e.,\nthe detection of code smells, the recommendation of refactoring solutions, the end-to-end code transformation as\nrefactoring, quality assurance, and the mining of refactorings. Under each of these categories, we present key aspects (i.e.,\ncode smell types, refactoring types, training strategies) related to theapproaches togive insight into the technologies\nthat have supported software refactoring through deep learning. Based on such a presentation, we can provide a more\ncomprehensive perspective on deep learning-based refactoring. To the best of our knowledge, it is the first survey paper\nthat presents the hierarchical taxonomy for deep learning-based software refactoring. In our survey, we attempt to\ninvestigate the following research questions:\n\n+ RQ1: Which tasks in software refactoring have been supported by deep learning techniques, and how often\nthey have been targeted by the surveyed papers?\n\n\u00a2 RQ2: What are the common deep learning techniques used in software refactoring?\n\u00a2 RQ3: How effective is the use of deep learning models in the process of software refactoring?\n\n* RQ4: Whatarethe limitations and challenges associated with the use of deeplearningtechniquesinsoftware\nrefactoring?\n\nOur classification indicates that there is an imbalance in the refactoring tasks which have been supported by deep\nlearning techniques. Our survey indicates that most of it has been towards the detection of code smells and the\nrecommendation of refactoring solutions, unlike the end-to-end code transformation as refactoring, quality assurance,\n",
                    "4 Nyirongo and Jiang, etal.\n\nand mining of refactorings. Thus, there is aneed for more future workto address the currentimbalance, specifically in\nareas of deep learning supporting the application, quality assurance, and mining ofrefactorings. The rest of the paper\nis structured as follows: Section 2 introduces related work. Section 3 presents the methodology used for the survey.\nSection 4 presents and discusses studies on the detection of code smells. Section 5 presents and discusses studies on the\nrecommendation of refactoring solutions. Section 6 presents and discusses studies on end-to-end code transformation\nas refactoring. Section 7 presents and discusses studies on the mining ofrefactorings. Section 8 discussessome of the\n\nchallenges and opportunities associated with deep learning-based refactoring, and section 9 concludes the survey.\n\n2 RELATED WORK\n\nNaik et al. [63] conducted a systematic review of the current studies on deep learning-based code refactoring. The\nsurvey presented a high-level analysis of 17 primary works published from 2016 to 2022. The key insight of this survey\nwas to present the state-of-the-art in deep learning-based code refactoring. The review addressed research questions\nwhose main focus was on the commonly used deep learning techniques and the performance of the deep learning-based\nrefactoring approaches. This reviewindicated thatCNN, RNN, andGNNarethe commonly used deep learning models\nfor code refactoring with Multilayer Percepton (MLP) performing the best. They also noted that most of the existing\nstudies focus on Java code, method-level refactoring, and single-language refactoring with various evaluation methods.\nCompared to the survey by Naik et al. [63], our survey covers substantially more related works, increasing the number\nofsurveyed papers from 17 to48. Our well-designed search strategy retrieved many closely related papers missed by\ntheir survey. Another difference is that we present a comprehensive and hierarchical taxonomy of deep learning-based\nrefactoring, and classify all related works based on the taxonomy.\n\nAlabza et al. [2] conducted a review on deep learning-based approaches for bad smell detection, and their focus was to\nsummarise and synthesize the studies that used deep learning for bad smell detection. They collected and analyzed 67\nstudies until October 2022. They analyzed deep learning models concerning the purpose of the model, the detected bad\nsmells, the employed training datasets, features, pre-processing techniques, and encoding techniques used for feature\ntransformation. Notably, this survey involved all kinds of code smells. This review indicated that code clonesare the\nmost recurring smell. The review showed that supervised learning is the most adopted learning approach used for\ndeep learning-based code smell detection. Also, they observed that CNN, RNN, DNN, LSTM, Attention models, and\nAutoencodersare the most popularly used deep learning models. Notably, this review focused only onthe use of deep\nlearning models for the detection of code smells which is one of the tasks used for the identification of refactoring\n\nopportunities. In contrast, our survey covers all aspects of refactoring.\n\nMalhotra et al. [56] conducted a literature review examining deep learning\u2019s capability to spot code smells. They\npresented a total of 35 primary study works from the years 2013 to 2023. The consolidated studies highlighted four key\nconcepts, that is, the types of code smells addressed, the deep learning approach utilized in the experiment, evaluation\nstrategies employed in the studies, and the performance analysis of the model proposed. This review showed that the\nmost common code smells detected include feature envy, god class, long method, complex class, and large class. It\nalso indicated that the most common deep learning algorithms used are RNN and CNN, often combined with other\ntechniques for better results. Notably, this systematic analysis did not focus on the whole refactoring process (as what we\ndo). Instead, they only focused on the recommendation of refactoring solutions and the end-to-end code transformation\n\nas refactoring.\n",
                    "ASurvey of Deep Learning Based Software Refactoring 5\n\nZhang et al. [110] conducteda survey on code smell detection based on supervised learning models. They surveyed\n86 papers from January 2010 to April 2023. They formulated a total of 7 research questions which were empirically\nevaluated from different aspects such as dataset construction, data preprocessing, feature selection, and model training.\nBased on their analysis they concluded that most of the existing works suffer from issues such as sample imbalance,\ndifferent attention to types of code smell, and limited feature selection. They also made suggestions for future work,\none of which involves exploring the correlation between features and the perspective of code smells within the context\nof model interpretability. Notably, the core focus of this paper was on the types of deep learning models used for the\ndetection of code smells, and not the use of deep learning models in the process and support of software refactoring.\nOur paper differs from such reviews in that they focus on code smell detection only whereas we covera much larger\nscope, i.e., the whole process of software refactoring.\n\n3 METHODOLOGY\n\nWe surveyed deep learning-based software refactoring by exploring and searching the following databases: https:\n//dl.acm.org, http://ieeexplore.org, https://springer.com, https://sciencedirect.com, https://onlinelibrary.wiley.com/,\nand https://scholar.google.com. Weused these databases since they containa comprehensive coverage ofacademic\npublications, conferences, and journals in the field of software engineering. Utilizing these databases ensured a thorough\nand inclusive exploration of the existing literature, enabling a comprehensive understanding of the landscape of deep\nlearning-based refactoring techniques and advancements. We used a set of keywords based on the research questions\n\noutlined in Section 1 to retrieve studies from these databases. The main keywords were \"deep learning\", \"software\nrefactoring\", and \"code smells\". We also included synonyms of these keywords, such as \"refactoring\", \"code refactoring\",\nand \"bad smells\". These search terms were used with advanced search filters within the databases(e.g., specific year\nrange (2018 to 2023) and language specification (English)). For example, on ACM, we used the following search query:\n\"query\": (deep learning\") AND (\"refactoring\"), \"filter\": publication date: 01/01/2018 TO 10/31/2023, owners.owner=HOSTED.\nWe obtained atotal of 1,755 papers after the filtering, and selected the top 100 (ifthere are more than 100) fromeach\ndatabase based on relevance, resulting in 486 papers. The selection of studies was limited to the top 100 papers from\neach database based on relevance because our initial analysis suggests that items outside the top 100 are often outside\nthe scope of the survey.\n\nThe firstauthor conducted a manual search to decide whether each papershould be included or not. This was done by\nreading through the title and abstract of each candidate paper. To ensure the correctness of the manual search, inclusion\ncriteria were defined, and continuous and open communication was maintained among the authors. The main criterion\nfor the classification was focused on papers that used deep learning for refactoring tasks, such as detecting code smells,\nrecommending refactorings, and the end-to-end code transformation as refactoring. The use of this criterion resulted\nin remaining with a total of 35 papers. To enhance the comprehensiveness of our literature search, we conducted a\nsnowballing process by scrutinizing the reference sections of the initial set of papers. This iterative process involved\nexploring citations within these papers to identify additional relevant studies. Google Scholar was utilized as the primary\ndatabase for this snowballing process, ensuring an expansive and thorough exploration. As a result of this snowballing\nprocedure, we identified and included 13 more papers that were deemed pertinent to our survey focus. Combining these\nnewly discovered papers with the initial set, a total of 48 papers were meticulously collected and considered as primary\nstudies for our survey. This approach allowed us to cast a wider net in the literature search, ensuring the inclusion\nof studies that may not have been initially captured, thereby enriching the depth and scope of our survey. Figure 1\n",
                    "6 Nyirongo and Jiang, etal.\n\n10 al\n3\ni] 8\n2\ng\n4\ng 6\n5\na\nS\n3 4\n2\n\u00a3\nZz\n\n2\n\n2018 2019 2020 2021 2022 2023\nYears\n\nFig. 1. Primary studies through the years\n\npresents the papers on deep learning-based refactoring from January 2018 to October 2023. The figure shows that there\nis a growing interest among researchers to adopt deep learning techniques for software refactoring.\n\nFrom the collected data we have developed a taxonomy classification hierarchy as presented in Figure 2. The classification\nhierarchy for the taxonomy is based on the software refactoring tasks that could be supported by deep learning\ntechniques. These tasks include the detection of code smells, recommendation of refactoring solutions, end-to-end code\ntransformation as refactoring, quality assurance, and the mining of refactorings. Figure 2 presents the total number\nof papers collected for each specific task (as presented onthe lower right corners of the nodes). Our survey indicates\nthat there is an imbalance in the adoption of deep learning techniques for the process of refactoring. Most ofthe deep\nlearning techniques have been used to detect code smells and recommend refactoring solutions as found in 56.25% and\n33.33% of the literature respectively. In contrast, only 6.25% and 4.17% were towards the end-to-end code transformation\nas refactoring and the mining of refactorings, respectively. Notably, we found no literature representation for the\nquality assurance for refactoring. We have also observed that some researchers made contributions to more than one\nrefactoring task in the use of deep learning. For instance, 22.45% of the studies focused on applying deep learning\ntechniques on more than onerefactoring task, i.e., the detection of code smells and recommendations for refactoring\nsolutions. For such papers, we discuss them in different sections regarding their contribution to the different tasks. As\nsuggested by the legend in the left bottom of Figure 2, the leftmost node presents the root of the taxonomy, and the\nnodes in the following layer represent different steps involved in software refactoring. Nodes in the rightmost two\nlayers represent differentaspects (factors) that could be employed to further classify related works within the given\ncategory. For example, the \"code smell types\" node on the third layer suggests that we may classify related works on\n\n\u201cdetection of codesmells\" into sub-categories according to the code smells\u2019 types they support.\n",
                    "ASurvey of Deep Learning Based Software Refactoring 7\n\nime Result Metrics\n\nEvaluation\n\n| Datasets\n\nTraining Strategies\n\n[~ Detection of | _,| __Explainable and\n| L_Code smelis:27 [ Code Smell Types Feedback Centric\n\nt+| Hybrid Approach\n\nDetection Technologies\n\n}+| Graph Based\n\nL,|_ Sequence Modellin\n\nDatasets\nDeep Learning ; - | Result Metrics\nBased Recommendation Evaluation\nRefactoring:48 of Refactoring [Training Strategies\n9 Solutions:16 -\n_| Recommendation\n++ End to End Code Technologies\nTransformation\nas Refactoring:3\nt> Quality As-\nMining of Root Node of Taxonomy\n\nRefactorings:2\n\nRefactoring Tasks\n\nL,| Aspects /Sub-aspects for classification\n\nFig. 2. Taxonomy of deep learning-based refactoring\n\n4 DETECTION OF CODE SMELLS\n\nRefactoring is a crucial task in software engineering. To conduct refactoring, there needs to be awareness of the issues\nin the code that might call for the process of refactoring. One of the most common issues that trigger the need for\nrefactoring is code smells. Code smells are certain structures in the code that indicate the possibility of refactoring [26].\nDetecting code smells manually ona large codebase is a challenging task. Therefore, researchers have explored different\ntechniques, including automatic and semi-automatic ones, to aid in the detection of code smells. Despite various\ntechniques proposed by researchers, most of them rely on manually designed heuristics. The manual heuristics make\nthe process of detecting code smells for refactoring opportunities time-consuming. To solve this issue, researchers\nhave started exploring deep-learning techniques for the detection of code smells. Thus, researchers have proposed\nvarious deep-learning techniques for the detection of code smells. In this section, we discuss and presentthese deep\nlearning techniques based on the detection technologies, code smell types, training strategies, and evaluation techniques\nemployed.\n",
                    "8 Nyirongo and Jiang, etal.\n\n4.1 Detection Technologies\n\nResearchers [51, 53, 80, 109] have used various deep-learning approaches for the detection of code smells. The details of\nthese approaches are presented as follows:\n\n4.1.1 Sequence Modelling Based Approaches. This category pertains to technological approaches that focus on the\nsequence of code, such as source code tokens or characters. The approaches in this group utilize deep learning\ntechniques to capture contextual dependencies within code snippets. Also, these approaches detect code smells based on\nthe sequential nature of code. CNN, RNN, LSTM, Transformers, etc. are some of the deep learning techniques employed\ninthis category. The deep learning techniques in this category can effectively learn patterns and relationships within\ncode sequences, thus aiding in the process of code smell detection.\n\nLiu etal. [53] proposed a deep learning-based approach to detecting feature envy, which is one of the most common\ncode smells. They used a Convolutional Neural Network (CNN) as their deep neural network-based classifier. The\nclassifier\u2019s input was divided into two parts: textual input and numerical input. The textual input was a word sequence\nthat consisted of the method\u2019s name, the name of its enclosing class, and the name of the potential target class. The\ninformation in the textual input had to pass through an embedding layer that converted the text description into\nnumerical vectors. The numerical vectors were then fed into the CNN. They had three CNN layers, each with filters=128,\nkernel size=1, and activation Tanh. The CNN classifier was trained automatically, without any human intervention. The\nlabeled samples for training were generated automatically based on open-source applications. To evaluate the approach,\nthey conducted a two-part verification process. First, they evaluated the approach on 7 well-known open-source\napplications (Junit, PMD, JExcelAPI, Areca, Freeplane, jEdit, and Weka) without automatically injecting feature envy\nsmells. This first evaluation gave an improvement on F-measure against state-of-the-art by 34.32%. The second part of\nthe evaluation was carried out on 3 open-source applications (XMD, JSmooth, and Neuroph) where no smells were\ninjected, and this outperformed the state-of-the-art.\n\nLiu et al. [21] proposed a method called feTruth to enhance the detection of feature envy in software using deep learning\n\nwith real-world examples. The feTruth technique used evolutionary histories of open-source projects stored in version\ncontrol systems such as GitHub to extract real examples of feature envy. The extracted real-world examples were then\nused to train a deep learning-based prediction model. During the testing phase, feTruth would examine the source code\nofa software project and generate a list of feature envy occurrences associated with methods in the project. feTruth\nincluded a heuristics-based filter and a learning-based filter. These two filters were used to exclude false positives\nreported by RefactoringMiner [93]. The heuristic-based filter would exclude false positives if the source class of the\npotential refactoring did not exist in the new version or if the target class of the potential refactoring did not existin\nthe oldversion. The learning-based filter leveraged a decision tree-based classifier to distinguish false positives from\ntrue positives based on a sequence of features of the refactorings. By using these techniques, the researchers were able\nto generate high-quality and large-scale training data for feature envy detection. They useda CNNin the design forthe\ndeep learning-based classifier. The CNN classifier leveraged new features not yet exploited by existing approaches.\nThe feTruth method was compared against Liu\u2019s approach [51], JDeodorant [22], and JMove [89]. The subjects for\ntheir projects were divided into two parts. The first part consisted of 500 Java projects, which were used to discover\nreal-world examples of feature envy. These projects were collected from GitHub by selecting the top 500 most popular\nprojects with the largestnumber of stars. The second part consisted of 5 open-source Java projects, which were used\nto evaluate the proposed approachand the selected baseline. These 5 projects were chosen from Defects4J [42].The\n",
                    "ASurvey of Deep Learning Based Software Refactoring 9\n\nevaluation results on real-world open-source projects suggested that the proposed approach substantially outperforms\nthestate-of-the-artin the detection of feature envy smells. The approachimproves the precision and recallin feature\nenvy detection by 38.5%.\n\nDas et al. [18] proposed a deep learning approach to detect brain class and brain method code smells in software\napplications. They used Convolutional Neural Networks (CNN) to train a neural network-based classifier. The approach\nwas based ona large corpus of software applications, which generated a huge number of training samples. These\nsamples were labeled to indicate whether they were a code smell of kind brain class and brain method. The neural\nnetwork used in this approach had several layers - the first layer had a one-dimensional CNN layer with 256 filters and\nakernel size of 1. The activation function used wasTanh. Thesecond layer wasalsoa one-dimensional CNN layer with\n128 filters and Tanh as the activation function. They added a flattened layer as the third layer to connect the convolution\nlayer with dense layers. The fourth layer had a dense layer with 128 filters and ReLU as the activation function. The\nlastlayer was a dense layer with only one filter and Sigmoid as the activation function. This layer acted as the output\nlayer. They used 30 open-source Java Projects as subject applications, acquired through sharing activities in GitHub\nrepositories. The dataset of the Java projects was split into mutually exclusive training and test sets. The experiment\ndemonstrated high-accuracy results for both code smells.\n\nLin et al. [48] proposed anewapproach for detecting code smells. They used a full convolutional network that could\nidentify and use local correspondences by making use of semantic features. They defined a multidimensional array,\nh*w*d, to represent the convolutional network (where h and d are space dimensions and d is the channel). For their\nexperiment, they used an open-source database that was initially published by the author. They used this databaseto\ndetect various code smells such as long method, lazy class, speculative generality, refused bequest, duplicated code,\ncontrived complexity, shotgun surgery, and uncontrolled side effects.\n\nLiu et al. [53] proposed an approach for code smell detection using deep learning. However, this approach had some\nlimitations. To address these limitations, they presented a new approach in [51]. This approach was generic and\nevaluated on four code smells: feature envy, long method, god class, and misplaced class. They improved the deep\nneural networkused in [53] by using bootstrap aggregating. They used aclassifier that generated several bootstrap\nsamples simultaneously from a given training dataset. The classifier trained multiple binary classifiers that in turn\ndetermined the final classification by voting. The study highlighted that it is difficult to design and train a generic\nclassifier to detect all code smells since different features are needed for different smells. Therefore, they presented\ndifferent classifiers for different code smells. They used a Convolutional Neural Network (CNN) for the detection of\nfeature envy and misplaced class. A dense layer-based classifier was used for the detection of the long method. Dense\nlayers coupled with Long Short Term Memory(LSTM) were used for the detection of the god class. The classifier used\nfor feature envy was similar to the one in their earlier work [53]. The deep neural classifier used for the long method was\ncomposed of five dense layers besides the input and output layers. The resulting features which were extracted by the\nhidden layers were fed into the output layer. This process mapped the features into a single output to suggest if the\nfeature is associated with long method smell. The classifier for the god class was composed of two parts: a textual part\nand a numerical (code metric) input. The textual input was a word sequence formulated by concatenating the names of\nattributes and methods declared within the class under test. After converting the textual input into numerical vectors\nthrough embedding, the resulting vectors were handled by an LSTM layer. This was unlike with the code metrics which\nwere fed straight into a dense layer. This output was then merged with the output of the LSTM before being fed into\n",
                    "10 Nyirongo and Jiang, etal.\n\nanother dense layer. The output of the dense layer was the one that indicated whether aclass should be decomposed\nor not. The classifier employed for the detection of the misplaced class was similar to that of feature envy because both\nthese smells are caused by misplaced software entities like methods, classes, etc. They evaluated the approach on 10\nopen-source applications. The approach was compared against JDeodorant [22], DECOR [61], and TACO [68]. The\nresults indicated that the approach outperformed the state-of-the-art. They also evaluated the proposed approach on\nreal-world applications without any injection of smells. The two-step evaluation of the approach using generated\ndata and real-world data gave considerable differences in the performance of their proposed approach. This led to the\nconclusion that perhaps the evaluation of code smell detection approaches should rely more on manually validated\ntesting data that are often more reliable than generated data.\n\n4.1.2. Graph Based Approaches. This category refers to the use of deep learning techniques to analyze the structural\nproperties of code representation. Specifically, models like GNN and GCN are utilized to detect code smells by analyzing\nthe structural relationship between different elements in the code. By using these models, complex dependencies and\ninteractions can be captured, thereby improving the detection of code smells.\n\nYu et al. [105] proposed a Graph Neural Network (GNN) based approach to address the issue of inherent calling\nrelationships between methods that often result in low detection efficiency for feature envy detection. To achieve this\napproach, the authors collected code metrics and calling relationships. The collected features were then converted\ninto agraph where nodes represented the code metrics of a method and edges represented the calling relationships\nbetween methods. To address the imbalance of positive and negative samples, they introduced a graph augmenter\nto obtain an enhanced graph. They then fed the enhanced graph into a GNN model for training and prediction. The\nGNN classifier had four layers: the input layer, the GraphSAGE layer, the dropout layer, a fully connected layer, and\nan output layer. The input layer received the augmented graph after oversampling. The GraphSAGE updated the\nembedding of nodes based on the embedding space. The dropout layer prevented the classifier from overfitting during\nnode classification training. The fully connected layers converted the output of the dropout layer into a one-dimensional\nvector for final classification. The output layer received the vector and outputted the prediction of the classifier through\nthe activation function Sigmoid. For their experiment, the authors used five open-source projects (BinNavi, ActiveMQ,\nKafka, Alluxio, and Realm-java) collected from a dataset labeled by Sharma and Kessentini [82]. The dataset contained\n86,652 open-source projects mainly written in Java and C# on GitHub. To select the five projects, they considered\nprojects whose updates were: 1) within two years, 2) had more than 2,000 stars on GitHub, and 3) had more than 5,000\nmethodsand 500 classes. The approach for detecting the feature envy smell achieved an average F1-score of 78.90%,\nwhich is 37.98% higher than other comparison approaches.\n\nHanyu et al. [35] proposed a graph-based deep learning approach to detect long methods. Their approach extended the\nProgram Dependency Graph (PDG) into a Directed-Heterogeneous Graph. The Directed-Heterogeneous Graph was then\nused as the input graph. They employed the Graph Convolutional Network (GCN) to construct a graph neural network\nfor long method detection. The input in this approach consisted of two kinds of nodes (method node and statement\nnode) and four types of edges (include edge, control flow edge, control dependency edge, and data dependency edge).\nThe Graph Convolutional Network had two layers and one linear layer. To obtain enough data samples for the deep\nlearning classifier, they introduced a semi-automatic approach to generate a large number of data samples. To validate\ntheir approach, they compared it with existing methods using five groups of manually reviewed datasets.\n",
                    "ASurvey of Deep Learning Based Software Refactoring 11\n\n4.1.3 Hybrid Approaches. This category refers to the use of deep learning techniques in combination with other\nmethods to enhance the accuracy and effectiveness of code smell detection. For instance, deep learning techniques such\nas CNN, GNN, and attention mechanisms can be combined to leverage the strengths of each approach. Hybrid-based\napproaches aim to gain a more comprehensive understanding of the code, potentially leading to better performance in\ndetecting code smells.\n\nCombination of Structural and Semantic Features. Zhang et al. [109] proposed a new approach called DeleSmell to detect\ncode smells using a deep learning model and Latent Semantic Analysis (LSA). They argued that most of the existing\napproaches suffer from two things: 1) incomplete feature extraction and 2) an unbalanced distribution between positive\nand negative samples. Toaddress these issues, they developeda refactoring tool to transform good source code into\nsmelly code and generate positive samples based on real-world project cases. They builta dataset with over 200,000\nsamples from 24 real-world projects to improve dataset imbalance. DeleSmell collected both structural features through\niPlasma and semantic features via Latent Semantic Analysis and Word2Vec. DeleSmell\u2019s model comprised a CNN\nbranch,a Gate Recurrent Unit (GRU)-attention branch, dense layers, and an SVM branch. The input was processed by\nthe GRU-attention branch and CNN branchin parallel. The CNN branch hada feature extraction component followed\nby a classification component. The feature extraction component included a set of hidden layers, including convolution,\nbatch normalization, and dropout layers. The output layer of the last dropout layer was connected to the input of a\ndensely connected network that comprised a stack of two dense layers. An attention mechanism was introduced in the\nGRU branch to learn the important features in the dataset while suppressing the interference of irrelevant information\non the classification results. For the SVM, the kernel method was used to map the nonlinear samples to high dimensional\nspace and identify the optimal hyperplane by maximizing the classification interval between the two samples. The\ninput of the last dense layer consisted of features concatenated by the GRU attention branch and the CNN branch\nand was connected to the SVM for the final classification. Grid search was used to tune the hyperparameters of the\nclassifiers. ReLU was used as the activation function of this approach. DeleSmell was used to detect brain class and\n\nbrain method code smells.\n\nIn their research, Ma et al. [55] explored the use of a pre-trained model called CodeT5 to detect feature envy, one\nofthe mostcommon code smells. They also investigated the performance of different pre-trained models on feature\nenvy detection by comparing CodeT5 with two other models, CodeBERT and CodeGPT. CodeTS is an encoder-decoder\nmodel that considers the token type information in code. CodeGPT is a transformer-based language model that is\npre-trained on programming languages for code completion and text-to-code generation tasks. CodeBERT is a multilayer\ntransformer model that uses the same JavaTokenizer as CodeTS to extract token sequences from source code. The\nresearchers used these models to extract semantic relationships between code snippets and compared their performance.\nThey evaluated their approach on ten open-source projects (Junit, PMD, JExtractAPI, Areca, Freeplane, JEdit, Weka,\nAdbextract, Aoi, and Grinder). Liu etal.\u2019s[51] approach was used as their baseline forcomparison. The results showed\nthat their approach improved the F-measure by 29.32% on feature envy detection compared to thestate- of-the-art.\n\nIn their study, Hadj-Kacem and Bouassida proposed a hybrid approach for detecting code smells using deep autoencoder\nand Artificial Neural Network (ANN) [32]. Both unsupervised and supervised algorithms were used to identify the code\nsmells. The approach had two phases. In the first phase, a deep autoencoder was used for dimensionality reduction,\nwhich extracted the most relevant features. Once the feature space was reduced with a small reconstruction error,\nthe ANN classifier would then learn the newly generated data and output the final results. The second phase used a\n",
                    "12 Nyirongo and Jiang, etal.\n\nsupervised learning classification by using the ANN. The approach was applied to four code smells: god class, data\nclass, feature envy, and long method. The study adopted a set of four datasets that were extracted from 74 open-source\nsystems. The results showed high accuracy with precision and recall values. The best F-measure value was 98.93%,\nwhich was achieved with the god class code smell. Even at the method level, the F-measure surpassed 96%. These results\n\nvalidated the effectiveness of the approach.\n\nSharma et al. [80, 81] conducted a study on the feasibility of using deep learning models for detecting code smells\nwithout the need for extensive feature engineering. They investigated the possibility of applying transfer learningin\nthis context. The researchers trained smell detection models based on Convolutional Neural Networks (CNN), Recurrent\nNeural Networks (RNN),andautoencoder models. The CNN layer consisted ofa feature extraction partfollowed bya\nclassification part. The feature extraction part was composed of an ensemble of layers, including convolution, batch\nnormalization, and max pooling layers. These layers formed the hidden layers of their architecture. The convolution\nlayer performed convolution operations based on the specified filter and kernel parameters. The convolution layer also\ncomputed the network\u2019s weights to the next layer. The max pooling layer reduced the dimensionality of the feature\nspace. The batch normalization layer mitigated the effects of varied input distribution for each training mini-batch\nwhich optimized the training. The output of the max pooling layer was connected to the dropout layer, which performed\nanother regularization byignoring some randomly selected nodes during training to prevent overfitting. The output\nof the last dropout layer was fed into a densely connected classifier network that had a stack of two dense layers.\nThese classifiers processed one-dimensional vectors, whereas the incoming output from the last hidden layer was a\nthree-dimensional tensor. For this reason, the flattened layer was used first to transform the data into the appropriate\nformat before feeding them into the first dense layer with 32 units and ReLU activation. This was followed by the\nsecond dense layer with one unit and Sigmoid activation. This second layer comprised the output layer and contained a\nsingle neuron to make predictions on whether a given instance belongs to the positive or negative class in terms of\nsmell investigation. The layer used the Sigmoid function to produce a probability within a range of 0 to 1. The RNN\nwas comprised of an embedding layer followed by a feature learning part (a hidden LSTM layer). It was succeeded by a\nregularization (a dropout layer) and classification (a dense layer) part. The embedding layer mapped discrete tokens into\ncompact vector representations. To avoid the noise produced by the padded zeros in the input arrays, they set the mask\nzero parameters by the Keras embedding layerimplementation. Thus, the padding wasignored, and onlymeaningful\nparts of the input data were taken into account. The dropout and the recurrent dropout parameters of LSTM were set to\nlayer0.1.The output from the embedding layer was fed into the LSTM layer, which, in turn, gave output to the dropout\nlayer. The trainingand evaluation samples for this approach were generated by downloading repositories containing\nC#and Java code from GitHub after filtering out low-quality repositories by RepoReapers. They downloaded 922 C#\n\nand 922 Java repositories in total. They applied this technique for detecting complex method, complex conditional,\nfeature envy, and multifaceted abstraction. Through this study, they discovered that although deep learning methods\ncould be used for code smell detection, the performance is smell-specific. That is, it is very difficult to have a simple and\ndirect solution for this. They also noted that they could not find a clear superior method between one-dimensional\nand two-dimensional CNNs. One-dimensional CNNs performed slightly better for the smells\u2019empty catch block and\nmultifaceted abstraction\u2019, while two-dimensional CNNs performed better than their one-dimensional counterpart\n\nfor \u2018complex method and magic number\u2019 [80].\n\nIntheir paper, Hadj-Kacem and Bouassida proposed a method for detecting code smells in software using a deep learning\nalgorithm [33]. They used an abstract syntax tree and a variational autoencoder to extract semantic information from\n",
                    "ASurvey of Deep Learning Based Software Refactoring 13\n\nthe source code. Firstly, they parsed the source code into the AST and transformed each tree into a vector representation\nthat was then fed into the variational autoencoder. The autoencoder generated a latent representation which was used\ntoreconstructthe original input data. A logisticregression classifier was then applied to determine whether the code\nwas a code smell or not. The approach was evaluated on the Landfill dataset [67]. The results showed that the proposed\nmethod was effective in detecting code smells such as blob, feature envy, and long method.\n\nXuand Zhang [102] proposed adeep-learning approach to detect code smells based on Abstract Syntax Trees (ASTs).\nThe approach captures the structural and semantic features of code fragments from the ASTs, by utilizing sequences\nof statement trees. The sequences ofstatement trees were encoded using bi-directional GRU and maximum pooling.\nThen, semantic and structural features were extracted from the encoded sequence to obtain final vector representations\nof the code fragments. The approach was applied to four types of code smells: insufficient modularization, deficient\nencapsulation, feature envy, and empty catch block. The final detection results were obtained through fully connected\nlayers. The approach was applied to 500 high-quality Java projects from GitHub, outperforming state-of-the-artdeep\nlearning models for both small-grained and larger-grained code smells.\n\nAttention Mechanism and Enhanced Neural Networks. Zhang and Dong proposed a new approach for detecting code\nsmells called MARS, whichis based ona Metric-Attention-based Residual network [108]. Thisapproach was used to\nidentify brain class and brain method code smells. MARS addresses the issue of gradient degradation by utilizingan\nimproved Residual Network (ResNet). The reason they chose ResNetis that it can reduce model parameters while\nspeeding up the training process. ResNet extracts deep feature information to enhance the accuracy of code smell\ndetection. The approach increases the weight value of important code metrics to label smelly samples by introducing a\nmetric attention mechanism. The attention mechanism used in this approach was inspired by SENet [37]. The approach\ncomprised a fully connected layer, used Tanh as the activation function to accelerate the convergence speed of the\nmodel, and used Sigmoidas the gated function. The improved ResNet had a convolution layer, a batch normalization\nlayer that accelerated the convergence speed of the network, used ReLU as the activation function, and had an addition\nas the sum operation. To train MARS, they extracted more than 270,000 samples from 20 real-world applications to\ngenerate a dataset called BrainCode, which is publicly available. They evaluated the effectiveness of the proposed\napproach by answering five research questions. The results showed that MARS achieved an average of 2.01% higher\naccuracy than the existing approaches.\n\nZhao etal. [111] developed a model to detect feature envy, which is based on dual attention and correlation feature\nmining. Firstly, they proposed a strategy for entity representation using multiple views. This strategy increased the\nmodel's robustness and improved the correlation feature and the model\u2019s suitability. Secondly, they added an attention\nmechanism to CNN\u2019s channel and spatial dimensions. The addition of the attention mechanism enabled the accurate\ncapturing of the correlation features between entities and controlled the information flow. They compared theirapproach\nagainst Liu\u2019s [53] method, JMove [89] and JDeodorant [22]. Five open-source projects were used as subject applications.\nThe experimental evaluation was divided into two parts: 1) large-scale data with feature envy automatically injected for\ntraining and classifier verification, and 2) small-scale data without feature envy injected for evaluating the approach\u2019s\neffectiveness on real projects. The evaluation results for both feature envy-injected and non-injected projects showed\nthat the proposed approach outperformed the state-of-the-art.\n\nIn their paper, Wang et al. proposed a new model for detecting feature envy using a Bi-LSTM with self-attention\nmechanism [98]. They approached the problem as a deep learning task and used two input parts: a simpler distance\n",
                    "14 Nyirongo and Jiang, etal.\n\nmetric and text features extracted from the source code. Their approach consisted of three modules: text feature extract\nfor processing the text input, distance value enhancement for handling the distance metric input, anda feedforward\nneural network for classification. Notably, they introduced a basic attention function (AttentionBasic) thatis widely\nused in language modeling, in addition to the three score functions related to attention mechanism (AttentionAdd,\nAttentionDot, and AttentionMinus) [98]. They evaluated their approach using a dataset generated from seven open-\nsource Java projects, which was released by Liu et al. [51]. The results showed that their approach outperformed\n\nJDeodorant [22], JMove [89], and the first deep learning method proposed by Liu et al. [51].\n\nGuo et al. [31] proposed a method to detect feature envy code smell using a deep semantics-based approach that\ncombined method representation and a CNN model. The method representation technique was based on an attention\nmechanism and an LSTM network. The LSTM network represented the textual information of methods in source code.\nThis technique extracted semantic features from textual information and reflected the contextual relationships among\ncode parts. The attention mechanism helped in extracting specific features that were significant to code smell detection.\nThe semantic features supplemented the limited structural information in the code metrics and improved the accuracy\nof code smell detection. To achieve this approach, they first converted the textual information into vectors representing\nthe description of the method using the method representation. In the second part, the metric cluster was fed into a\nCNN-based model which had three convolutional layers and did not set the pooling layer. The CNN could fully extract\nthe features from the structural information in the code metrics to reflect the relations between adjacent metrics. After\nthis, they applied a flattened layer to turn the shape of the input into a one-dimensional layer. In the third part, they\nuseda Multilayer Perceptron-based neural network. The outputs of the CNN modeland method representation were\nconnected at the connection layer, which concatenated all inputs including the features from the textual input and\nthe metrics input. Behind the connection layer, they set two dense layers and one output layer to facilitate the final\nclassification which mapped the textual input and the metrics input into a single output. The output layer had only one\nneuron which represented the result of the identifier, i.e., smelly or non-smelly. Sigmoid was used as the activation\nfunction. The approach was evaluated using a dataset from 74 open-source projects. The results suggested that the\n\napproach achieved significantly better performance than the state-of-the-artapproaches.\n\nIna study by Liu et al. [54], an automated method was proposed to spot and refactor inconsistent method names. They\nused a graph vector and Convolutional Neural Network (CNN) to extract deep representations of the method names\nand bodies, respectively. The approach worked by computing two sets of similarnames when givena method name.\nThe first set included those that could be identified by the trained model of method names. The second set included\nnames of methods whose bodies were positively identified as similar to the body of the input method. If the two sets\nintersected to some extent, the method name was identified to be consistent. If not, it was identified as inconsistent.\nThey then leveraged the second set of consistent names to suggest new names when the input method was flaggedas\ninconsistent. The CNN used in this approach had two pairs of convolutional and subsampling layers. These layers were\nusedto capture the local features of methodsand decrease the dimensions ofinput data. The network layers fromthe\nsecond subsampling layertothe subsequent layers were fully connected. This meantthatthey could combinealllocal\nfeatures captured by convolutional and subsampling layers. For this approach, the output of dense layers was chosen\nto be the vector representation of method bodies, which synthesized all local features captured by the other layers.\nThe researchers collected both thetraining and test data from open-source projects from four differentcommunities\n(Apache, Spring, Hibernate, and Google). They only considered 430 Java projects with at least 100 commits to ensure\n",
                    "ASurvey of Deep Learning Based Software Refactoring 15\n\nthat the projects had been well maintained. The experimental results showed that the approach achieved an F-measure\nof 67.9% on identifying inconsistent method names, improving about 15 percentage points over the state-of-the-art.\n\nLiand Zhang [47] proposed a hybrid model with a multi-level code representation to optimize code smell detection.\nThey first parsed the code into an Abstract Syntax Tree (AST) with control and data flow edges. Then, they applied a\nGraph Convolutional Network to get the prediction at the syntactic and semantic level. Next, they analyzed the code\ntokenatthe token level using the bidirectional LongShort Term Memory networkwithanattention mechanism. They\napplied this approach to a total of 9 code smells (magic number, long identifier, long statement, missing default, complex\nmethod, long parameter list, complex conditional, long method, empty catch clause, and multi smells) and combined\nthem to come up witha multi-labeled dataset.\n\nZhang and Jia [107] proposed a new technique to detect feature envy using self-attention and Long Short Time Memory\n(LSTM). They were inspired by the transformer model in the formulation of this approach. The researchers added posi-\ntional encoding to preserve the meaning of sequences and compensate for the lack of positional information in the pure\nattention mechanism. They built on the existing deep learning model proposed by Liuetal.[51].Also, the researchers\nconsidered attention mechanism, LSTM structure, and snapshot ensemble to improve the detection performance. In\ntheir approach, they utilized self-attention as the attention mechanism. This mechanism was implemented with the\npositional encoding layer right after the embedding layer. The self-attention score was calculated using the Softmax\nfunction and then multiplied with the original inputto obtain the word embedding vector with attention score. They\nalso included an LSTM block after the attention layer to extract deeper semantic information. For the CNN model, they\ninitially tried adding a pooling layer but found it to be less effective than a dense layer. They changed the convolution\nblock\u2019s structure from 128 dimensions to 64 dimensions and then to 32 dimensions. The researchers believed that this\nstructure could filter out the critical features while reducing the consumption of time and computing time. After the\nCNN, they inserted a dense layer with 1024 nodes right after concatenation. They also included the concept of integrated\nlearning by incorporating a snapshot ensemble in their approach, inspired by Huang et al. [39]. However, they proposed\nanew periodic function instead of using the cosine function to adjust the learning rate. The results showed that their\nmodelachieved better performance on four evaluation metrics, with precision increasing by 0.048, recall increasing\nby 0.035, F-measure increasing by 0.043, and AUC increasing by 0.056. The introduction of the attention mechanism\nand LSTM illustrated the correlation between code smell detection and natural language processing. Compared to the\nmodel of feature envy detection in Liuetal. [51], this study optimized it from three aspects: modifying and expanding\nthe model structure, introducing the self-attention mechanism, and applying a snapshot ensemble.\n\nTraditional Machine Learning and Deep Learning. Menshawy et al. [60] proposed a mechanism to detect feature envy\ncode smell using machine and deep learning techniques. The study applied 6 deep learning techniques (CNN, Long\nShort Time Memory (LSTM), Bidirectional LSTM (BILSTM), Gated Recurrent Unit (GRU), Bidirectional Gated Recurrent\nUnit (BIGRU), and Autoencoder) and 11 machine learning techniques based on code structural features. The deep\nlearning models were implemented using TensorFlow and Keras frameworks [41]. The CNN model was inspired by an\nimage classification model. The CNN comprised of an input layer that passed the input features to the embedding layer.\nThe role ofthis layerwas to map vocabularies in high-dimension space to vectors of fixed size. The embedding output\nwas fed to the convolution one-dimensional layer of a specific kernel and filter parameters. The new weights were\ncomputed to the next max pooling one-dimensional layer which reduced the dimensionality of the feature space. To\navoid overfitting, the weights were passed to a dropout layer to randomly disregard a specific percentage of nodes\n",
                    "16 Nyirongo and Jiang, etal.\n\nduring training. The weights were connected to a flattened layer and then a stack of three dense layers to predict\nifa given instance was smelly or belonged to the non-smelly data. The LSTM and GRU architectures were inspired\nby a typical NLP model. The input layer fed the next embedding layer with input textual features. The embedding\nlayer mapped the input tokens to vectors. The new vectors were passed to the LSTM layer in the LSTM model or\nthe GRU layer in the GRU model. Both networks (LSTM and GRU) had recurrent dropout and dropout values of 0.1.\nThe new weights were fed to the dropout layer to avoid overfitting and then fed to a flattened layer and a stack of\nthree dense layers to avoid underfitting. Similar to the CNN architecture, the input to the dense layers was one unit.\nThe Sigmoid function was applied to decide whether the output belongs to the positive class or the negative class.\nA bidirectional network was applied to the LSTM and GRU layers with the same layers and hyperparameters of the\nLSTM and GRU models respectively. For the machine learning approach, eleven individual algorithms of different\nclassifier families were applied. These included Decision Table, Instance-based learning with parameter K, J48, JRip,\nMultilayer Perception, Naive Bayes, Random Forest, Simple Logistics, Sequential minimal optimization, AdaBoost, and\nBagging. Both approaches (deep learning and machine learning) were applied to open-source Java projects from the\nQualitas Corpus dataset. In the machine learning approach, the Designate Java tool was used to detect the code smell\nand to extract the corresponding metrics in CSV files. The machine learning data processor module splits the extracted\ndata into positive and negative samples. The machine learning algorithms were then applied to the processed output\nCSV samples to train the models and to evaluate the classifier\u2019s performance. In the deep learning approach, the data\nwas tokenized by the JavaTokenizer tool which exported tokenized text files. The deep learning data processingstage\nsplits the tokenized files into positive and negative samples according to the extracted detection information from\nDesignateJava. The tokenized input was then fed to the deep learning models to detect the code smells. The results\nshowed that deep learning techniques are promising and that they tend to achieve good results compared with the\nmachine learning approach. Based on their evaluation of the 6 deep learning techniques against machine learning\nmodels, the autoencoder models achieved superiority among all the deep learning techniques. In contrast, CNN achieved\nthe lowest F-score. Overall, the deep learning techniques showed high potential in predicting feature envy. However,\nthe deep learning techniques based on semantic features are not capable of detecting all code smell types.\n\nHamdy and Tazy [34] proposed an approach for detecting the occurrence of the god class smell in source code. Their\napproach utilized both the source code textual features and metrics to train three deep learning models (Long Short\nTerm Memory, Gated Recurrent Unit, and Convolutional Neural Network). They built a dataset for the god class\nsmell in source code acquired from the Qualitas Corpus repository. They extracted the textual features of the source\ncode using natural language processing techniques and integrated them with metric features. They then trained the\nthreedeep learning models using different types of source code features, suchas metrics, textual features,andhybrid\nmetrics-textual features. The Convolutional Neural Network (CNN) used in the deep learning approach comprised a\nstack of convolution stages, for feature selection, followed by a stack of dense layers for classification. The CNN applied\naset of filters on the input sequence and produced the feature map, which represented an input to the max pooling layer.\nThe output of the last max pooling layer was connected to a dropout layer, which performed regularization by ignoring\nsome random nodes during training to prevent overfitting. In the experiment, they setthe dropout rate tobe equal to\n\n0.5. The output of the last dropout layer was fed into a dense layer, which had a fully connected multi-perceptron neural\nnetwork that worked like a classifier. They used a stack of two dense layers: the first dense layer had 32 units and ReLU\nactivation, followed bya second dense layer with the number of outputs set to equal to one. The second dense layer\nmade predictions on whether the god class smell occurs or not in a given source code. This layer used the Sigmoid\n",
                    "ASurvey of Deep Learning Based Software Refactoring 17\n\nactivation function toproducea probability within the range of 0 and 1.The LSTM and GRU models comprised a stack\nof LSTM/GRU layers,adropoutlayer,and adense layer. The LSTM/GRU layer learned the representation ofeachclass.\nThe regular dropout in the dropout layer was set to 0.5, while the recurrent dropout parameters of the LSTM/GRU were\nsetto 0.1.For the machinelearning approach, three traditional machine learning techniques were used: Naive Bayes,\nRandom Forest, and Decision tree (C4.5). These traditional approaches were mainly used to compare the effectiveness\nof the proposed deep learning technique. The evaluation results showed that the deep learningtechnique performed\nbetter than the latter.\n\nDewangan et al. [19] proposed a machine learning-based approach to predict code smells in software and identify the\nmetrics that play a significant role in detecting them. They used four code smell datasets, i.e., god class, data class,\nfeature envy, and long method, generated from 74 open-source systems (Qualitas Corpus) obtained from Fontana\netal. [24]. Six different algorithms, including Naive Bayes, KNN, Decision tree, logistic regression, Random Forest,\nand Multilayer Perceptron, were used in the statistical-heuristic machine learning models. The performance of each\ntechnique was evaluated individually for the four code smells. The Multilayer perceptron (MLP) algorithm was found\n\nto perform the best in terms of accuracy for detecting the data class code smell.\n\nBarbezet al. [10] proposed a machine learning-based ensemble method called Smart Aggregation of Anti-patterns\nDetectors (SMAD) to create an improved classifier compared to standalone approaches. To train and evaluate the model,\nthey created an oracle that contained the occurrences of god class and feature envy in eight open-source systems.\nHowever, neural networks often perform poorly onimbalanced datasets like their oracle, so they designeda training\nprocedure that maximizes the expected Matthews Correlation Coefficient (MCC). They then evaluated SMAD on the\noracle and compared its performance with other aggregated tools and competing ensemble methods. Although SMAD\nis intended for detecting antipatterns, it can serve as a benchmark for researchers looking to develop standalone tools\n\nfor identifying common code smells during the refactoring process.\n\n4.1.4 Explainable and Feedback Centric. This category refers to studies that have used explanation mechanisms or\nintegrated user feedback in their approach to detecting code smells using deep learning. Explanation mechanisms\ntypically aim to incorporate methods and techniques that facilitate the understanding and interpretability of the deep\nlearning model\u2019s decision-making process during code smell detection. This is particularly important for developers, as\nitenablesthem to have faith in the process, especially when the detected code smells lead to critical code changes. On\nthe other hand, user feedback integration involves actively seeking and incorporating feedback from developers into\nthe deep learning-based code smell detection process. Continuous interaction with the developers ensures adaptability\n\nto evolving coding practices, preferences, and domain-specific knowledge.\n\nFor the use ofexplainable mechanisms, Yin et al. [104] have proposed an explainable approach to detecting feature envy\nbased on localand global features. Tomake the most of the code information, they designed differentrepresentation\nmodels for global and local code. They extracted different feature envy features and automatically combined those\nthat were beneficial for detection accuracy. They further designed a Code Semantic Dependency (CSD) to make the\ndetection result easy to explain. The global feature contained a global semantic feature and a global metrics feature.\nThey splice the extracted method name, enclosing class name, and target class name together to create the global\nsemantic features. The global semantic features were metric values calculated with plugins. LSTM was used for the\nglobal semantic features to extract context from the input statements. Then, it extracted the semantic relations from\ncontext features. LSTM focused on the cell state in the neural network and used three gates to determine how much\n",
                    "18 Nyirongo and Jiang, etal.\n\ncell state information was retained. The gate structure was used to select the appropriate call state information so\nthat LSTM could easily capture the context information between the legal name, the class name, and the target class\nname. For the global metrics, CNN was used to obtain complex mapping relations from simple metric information. They\nopted for CNN due to its ability to automatically extract features from the original features, which could reveal the\nrelationship between metrics information and code smell. CNN is also highly suitable for parallel training on GPU\nwhich could greatly reduce the training time. To achieve the CSD, a siamese network was formed based on a local\nfeature representation model. It had the same branch with different inputs. The code input would first pass through an\nembedding layer and then through an attention layer, meanwhile, the parameters were also being shared. Atthe final\nstage, they would calculate the code dependency ofthe feature obtained from each branch. Toevaluate the approach,\nthey used manually constructed code smell projects (Junit, PMD, JExcelAPI, Areca, Freeplane, jEdit,and Weka) and 3\nreal-world projects (Xmd, JSmooth, and Neuroph). They compared their approach against Liu et al.\u2019s [53] approach,\nJDeodorant [22], and JMove [89]. The F-measure for the two experimental setups were 2.85% and 6.46%, respectively,\nwhich was higher compared to the state-of-the-art approaches.\n\nFor theintegration of feedback, Nanadani etal. [64] conducted a study to investigate the effects ofhuman feedbackon the\nperformance oftrained models in detecting code smells, which are subjective perceptions of developers. Tocreatearobust\nandadaptable system, the study combined deep learning techniques, user feedback, and acontainerized deployment\narchitecture fora locally-run web server. The deep learning techniques used in the study were an autoencoder witha\ndense multilayer perceptron, an autoencoder witha Long Short Term Memory, and a variational autoencoder witha\nthreshold strategy for classification. The first step in this approach was to train the autoencoder, which was used to\ncompress the input data into a lower dimensional representation called the latent representation. The autoencoder\nhad an encoder and a decoder, with the encoder starting with an input layer followed by a series of dense layers. To\nimprove the training stability and efficiency, batch normalization was added to standardize inputs for each mini-batch.\nThe decoder was then constructed in thereverse order ofthe layers. The variational autoencoder was used toserve as\na deep generative model that employed Bayesian inference to estimate the latent representation. The approach was\nused to detect complex method, long parameter lists, and multifaceted abstraction code smells. A plugin for IntelliJ\nIDEA was created, and a container-based web server was developed to offer services of the baseline deep learning\nmodel. The setup allowed developers to see code smells within the IDEA and provide feedback. Using this setup, the\nresearchers conducted a controlled experiment with 14 participants divided into experimental and control groups. In\nthe first round of the experiment, the code smells predicted by using the baseline deep learning model were shown,\nand feedback was collected from the participants. In the second round, the researchers fine-tuned and reevaluated the\nmodel's performance before and after adjustment. The results showed that calibration improves the performance of the\nsmell detection model by an average of 15.49% in F1 score across the participants of the experimental group.\n\n4.2 Code Smell Types of Refactoring Opportunities\n\nCode smells are defined as certain structures in the code that call for the process of refactoring. Based on this definition\nFowler [26] proposed 22 types of code smells, for example, long method, duplicate class, feature envy, duplicate class, etc.\nCode smells have been analyzed and categorized based on the implementation, design, and architectural levels (based on\ntheir scope, granularity, and impact) [26-28, 81]. Essentially, code smells may occur at different levels of the codebase,\nthat is class, method, or variable levels. This occurrence at different levels affects how these smells may be detected\n\nwhen employing deep learning models. From our literature collection, we note and observe, as presented in Table 1, that\n",
                    "ASurvey of Deep Learning Based Software Refactoring 19\n\natotal of 26 different types of code smells have been detected using deep learning techniques which could potentially\nlead to the identification of refactoring opportunities. The data collected shows that the smells detected were occurring\nat different levels of the code base from class up to variable level. Also, we observe that some researchers focused on the\ndetection ofonly one type of codesmell [21,53, 105] while others [18, 32,48] focused onthe detection ofatleast more\nthan one type of code smell. Overall, our data suggest that feature envy being one of the mostcommon code smells is\nthe one whose detection researchers are employing the use of deep learning techniques more often. Feature envy code\nsmell appeared in at least 27.87% of the primary studies, seconded by long method 9.84%, god class 6.56%, complex\nmethod 6.56%, andthe rest ofthe smells. These smells, i.e., feature envy andlong method could potentially lead to the\nrecommendation and suggestion of refactorings which involve 1) moving features between objects, e.g., move method\nrefactoring. 2) composing methods to ensure that they are much easier to understand, e.g., extract method refactoring.\nThrough our literature search we have noted that other researchers like Xuand Zhang [102] combined several code\n\nsmells to create multi smells as a way of enhancing the efficiency and validation of their approach.\n\n4.3 Training Strategies\n\nDetecting code smells using deep learning models requires the adoption of effective training strategies. These strategies\ninvolve an organized approach to teaching the model to recognize patterns, make predictions, and perform tasks\nbased on the data. Training strategies are crucial in developingarobust and accurate model for detecting codesmells.\nResearchers use various techniques to create effective training strategies that produce efficient models. Our analysis of\nthe literature focuses on how researchers preprocess and engineer features from various metrics to effectively represent\ncode smells. Wealso explore the embeddings and representations used to capture semantic relationships within the\ncode, which aids the model\u2019s ability to learn code smells. Researchers use various data preprocessing, feature extraction,\nand data balancing techniques to enable the deep learning model to identify different code smells effectively.\n\n4.3.1 Data Preprocessing. Data preprocessing is an essential step to improve the quality of data that will be fed into deep\nlearning models. During the process of data preprocessing, several techniques are utilized, including data cleaning, data\nintegration, data transformation, and data regularization. Based on the nature of the datasets used, various techniques\nmay be applied to preprocess the candidate metrics such as code, text, graph data, etc. For instance, Sharma et al. [80, 81]\nperformed their preprocessing by analyzing the data using Designate and DesignateJava on their C# and Java code,\nrespectively. Then they split their code using Codesplit before applying tokenization. They performed duplicate removal\nafter tokenization toensure thatno duplicate data was fed into the deep learning model. Incontrast, Himeshetal. [64]\nconducted their duplicate removal before tokenization using a hash function to compute a unique hash value for each\ncode instance and compared the hash values to identify any duplicates. Regularization is another widely adopted\ntechnique used during data preprocessing to prevent deep models from learning specific and irrelevant features of their\ndata. Barbez etal. [10] usedregularization to adda special term to the loss function, which encourages the weights to\nbe small this was as defined by Witten [99]. Overall, data preprocessing is crucial in enhancing the quality and integrity\n\nof data, which in turn leads to better deep learning model performance [103].\n\n43.2 Feature Extraction. Various features from data candidates are extracted for deep learning models to detect\nvarious code smells. The selection of features to be extracted playsa key role inthe way a deep learning model makes\npredictions. The features that could be extracted for code smell detection include but are not limited to the following\n",
                    "20 Nyirongo and Jiang, etal.\n\nTable 1. List of code smells detected in the primary studies.\n\nCode Smells Frequencies References\nFeature envy 17 [10, 19, 21, 31, 33, 51,53, 55, 60, 80, 81, 98, 102, 104, 105, 107,111]\nLong method 6 [19, 33, 35, 47, 48, 51]\nGod class 4 [10, 19, 32, 34]\nMisplaced class 1 [51]\nBrain class and Brain method 3 [18, 108, 109]\nComplex method 4 [47, 64, 80, 81]\nComplex conditional 2 [47, 81]\nMultifaceted abstraction 3 [64, 80, 81]\nLazy class [48]\nSpeculative generality [48]\nRefused bequest [48]\nDuplicate code [48]\nShotgun surgery [48]\nContrived complexity [48]\nUncontrolled side effects [48]\nInsufficient modularization [102]\nDeficient encapsulation [102]\nEmpty catch block/clause 3 [47, 80, 102]\nMagic number 2 [47, 80]\nLong identifier [47]\nLong statement [47]\nMissing default [47]\nLong parameter list 2 [47, 64]\nBlob [33]\nInconsistent method names [54]\nData class 2 [19, 32]\n**Multi Smells [47]\n\nstructural features, semantic features, naming conventions and documentation, patterns, etc. Several ofourprimary\nstudies [18, 108, 109] were found to detect similar code smells such as feature envy, long method, god class, brain\nclass/method, etc. respectively. However, we note that different features were employed for the same code smell across\ntheliterature. Liu etal. [51,53] leveraged the use ofsemanticand structural features to detectfeature envy code smell\nbut ignored the semantic information contained in input sequences. Thus, Zhang et al. [107] proceeded to include\nthe semantic information contained in the inputsequences as a way ofimproving the efficiency of the deep learning\nmodel. Apart from just using structural and semantic features, we note that in the detection of feature envy code smells,\nresearchers used global metric features [104], calling relationships [105], and others even extracted ASTs from the code\nfragments and formed sequences of statementtrees like the case of Xuand Zhang [102]. For the detection of the long\nmethod, we notethatinasmuchas most ofthe researchers employed the use of source code metrics, there wasa slight\ndifference in how these were processed for them to be extracted as features. For example, Lin et al. [48] converted\n",
                    "ASurvey of Deep Learning Based Software Refactoring 21\n\nTable 2. Representative feature sets.\n\nCode Smells Features References\n\ncode metrics,textualfeatures [10, 19, 21, 31,33, 51, 53,55, 60, 80, 81, 98, 102, 104, 105, 107,111]\nFeature envy\n\nglobal metrics,asts\n\nLong method code metrics, source code (xml) [19, 33, 35, 47, 48, 51]\nGod class code metrics, textual features [10, 19, 32, 34]\nBrain class/method code metrics, textual features [18, 108, 109]\nComplex method source code, code metrics [47, 64, 80, 81]\nComplex conditional source code, code metrics [47, 81]\nMultifaceted abstraction source code code metrics [64, 80, 81]\nEmpty catch block source code, code metrics [47, 80, 102]\nMagic number source code, code metrics [47, 80]\n\nLong parameter list code metrics,source code [47, 64]\n\nData class code metrics [19, 32]\n\nthe metrics into XML while Hadj-Kacem and Bouassida [33] parsed the metrics into an AST. To detect god class code\nsmell, Hamdy etal. [34] used textual features from sourcecode while Hadj-Kacem and Bouassida [32] and Dewangan\netal. [19] extracted their features from code metrics. Similarly, for brain class/method Zhang et al. [109] used both\nsemantic and structural features while Zhang and Dong [108]and Das etal. [18] used code metrics. Table 2 gives a\nrepresentative tabulation of the scenarios being highlighted. From Table 3 we note that it is only in the detection of\ndata class where researchers used the same feature set.\n\n4.3.3 Feature Embedding. Deep learning models require the features extracted to be converted intoa suitable format\nfor the model to extract the relevant features. Tokenization and vectorization are common processes used to achieve this.\nTokenization breaks down text into smaller units (tokens), while vectorization converts these tokens into numerical\nvectors. Researchers also use embedding techniques to convert the candidate data into the required format. Embedding\nis a specific type of vectorization that represents words as dense vectors in a continuous space, capturing semantic\nrelationships between words. Table 3 lists some of the tools that researchers used to execute these processes. Our data\nindicates that most researchers used Word2Vecas an aid to conducting embedding for their data candidates, with 23.08%\nof the primary studies using it, followed by iplasma at 19.23%. Wealso discovered that at least 15% of the studies did not\nexplicitly highlight the tools used for the feature extraction processes.\n\n4.3.4 Data Balancing. Data balancing is the process of ensuring that the different categories or labels in a dataset\nare represented equally in terms of their frequency. This is particularly important in classification problems where\nthe model tries to predict different classes. A balanced dataset prevents the model from being biased towards the\n",
                    "22 Nyirongo and Jiang, etal.\n\nTable 3. Feature extraction tools.\n\nTools References\nWord2Vec [51, 53, 54, 102, 104, 109]\niplasma [18, 31, 32, 108, 109]\nJavalang 34\n\nNLTK 34\nTokenizer [80, 81]\n\nFluid tool [81, 32]\nJavatokenizer 60\nGraphSAGE [105]\nWrapper 19\n\nCodeT5S 55\n\nmajority classand ensures that it performs wellinall classes. Imbalanced datasets, onthe other hand, can lead to poor\nperformance of the model, especially for the minority class. The detection of code smells also requires a balanced dataset\nwhere the number of positive samples (with code smell) and negative samples (without code smell) are maintained.\nDifferent techniques can be used to balance the dataset such as under-sampling or oversampling. Under-sampling\nremoves samples from the majority classes while oversampling generates new samples for the minority class. Synthetic\ndata generation and ensemble methods can also be used to reduce the disadvantages of having an imbalanced dataset.\nResearchers have used the Synthetic Minority Over Sampling Algorithm (SMOTE) to balance their data in the detection\nof code smells. SMOTE is an effective oversampling technique that has been widely used in practice. However, its\neffectiveness may vary depending on the specific characteristics of the data and the problem at hand. It is worth noting\n\nthat while 78.5% of the primary studies were clear about the techniques they used to balance their data, 21.5% did not\nmention any issues with data imbalance.\n\n4.3.5 Model Training. Various approaches exist for training deep learning models. According to our literature collection,\nmodels can undergo training using labeled data, unlabeled data, or a blend of both. The process of labeling data can\noccur automatically, semi-automatically, or manually. These training methodologies are commonly categorized as\nsupervised, unsupervised, or a combination of both, where a simultaneous application of both techniques is employed.\nThese techniques are presented in Figure 3. Our survey found that the majority of primary studies (65.38%) used\nsupervised learning techniques, while 23.08% used unsupervised techniques and 11.54% used a combination of both.\n\n4.4 Evaluation\n\n4.4.1 Datasets. Based on our survey findings, a majority of the primary studies utilized applications exclusively\ndeveloped ina single programming language, such as Java, for conducting experiments and evaluations. In contrast,\nsome studies opted for a diverse approach by employing a combination of applications developed in different languages,\nsuch as Java and C#, for their experiments and performance assessments. As depicted in Figure 4, the data reveals\nthat 87.5% of the primary studies relied on projects developed in a singular programming language, while only 12.5%\n",
                    "ASurvey of Deep Learning Based Software Refactoring 23\n\nSupervised\n\nCombination of both\n\nUnsupervised\n\nFig. 3. Model training strategies\n\nemployed a combination of programming languages in their application projects. Additionally, as depicted in Figure 5,\na significant majority, specifically 91.67%, of the studies employed projects that are publicly accessible, i.e., general\nopen-source projects available in various repositories, and real-world open-source projects. In contrast, 8.33% utilized\n\nprivate projects with restricted access.\n\nSingle language\nPublic\n\nPrivate\nMultiple languages\n\nFig. 4.Programming languages Fig. 5. Dataset types\n\n4.4.2 ResultMetrics. Researchers use metrics to evaluate the performance ofa particular approach.Commonly used\nmetrics for this calculationinclude precision, recall, F-measure,and accuracy. Precision measuresthe accuracy ofthe\npositive predictions. Its formula is defined as (Precision = TP / (TP + FP)), where TP represents true positive and FP\nrepresents false positive. Itis important in situations where false positives should be minimized, and there is a cost\nassociated with making incorrect positive predictions. Recall measures the model\u2019s ability to capture all relevant cases.\nIts formula is defined as (Recall = TP / (TP + FN)), where FN represents a false negative. It is important when the\ngoal is to capture as many positive cases as possible, and there is acost associated with missing positive predictions.\nThe F1 score or F-measure provides a balance between precision and recall. Its formula is defined as (F1 Score = 2 *\n(Precision * Recall) / (Precision + Recall)). It is useful when both false positives and false negatives need to be minimized.\nAccuracy measures the ratio of correctly predicted observations to the total observations. Its formula is defined as\n(Accuracy = (TP + TN) / (TP +TN + FP +FN)), where TN represents true negative. While accuracy isa common metric,\nit may not be suitable for imbalanced datasets as it can be misleading. Accuracy is widely used when the classes are\n",
                    "24 Nyirongo and Jiang, etal.\n\n25%\n\n204\n\nNumber of papers whichused the metric\n\n0 >\nPrecision Recall Fl-score Accuracy AUC MCC Kappa\n\nPerformance assessment\n\nFig. 6. Metrics used for performance assessment\n\nbalanced. However, in imbalanced datasets, accuracy alone may not provide a complete picture of model performance.\nFrom our primary studies, we have noted that researchers use other metrics apart from the outlined (precision, recall,\nf-measure, and accuracy). For example, researchers [35, 51,80, 107] have used Area Under Cover(AUC) to represent\nthe trade-off between true positive rate and false positive rate at various thresholds. For instance [10, 51], used the\nMatthews Correlation Coefficient (MCC). MCC isa correlation coefficientbetween the observedand predicted binary\nclassifications. It takes into account true and false positives and negatives and is particularly useful when dealing with\nimbalanced datasets. Its formula is defined as (MCC = (TP *TN - FP* FN) /sqrt((TP + FP) *(TP+ FN) *(TN+FP)*(TN\n+FN))).MCCisa balanced metricthat works well forimbalanced datasets. Lin etal. used [48] Kappa (Cohen\u2019s Kappa).\nappa measures the agreement between observed and predicted classifications, correcting for the agreement that could\noccur by chance. Its formula is defined by ( Kappa = (Po - Pe) / (1 - Pe)), where Po is the observed agreement, and Pe\nis the expected agreement. Kappa is useful for classification problems where class distribution is imbalanced, and it\naccounts for the agreement that could occur by chance. From Figure 6 itbecomes apparent that precision, recall, and\nF-measure are commonly used for performance assessment. Notably, 96.15% of our primary studies utilized precision,\nwhile 92.31% and 88.46% employed recall and F-measure, respectively.\n\n5 RECOMMENDATION OF REFACTORING SOLUTIONS\n\nhe recommendation of refactoring solutions involves identifying areas of code that could benefit from refactoring and\nsuggesting potential approaches or strategies for improving them. Recommendations are often made based oncode\nreviews, automated analysis tools, or architectural discussions. The recommendation of refactoring solutions does not\ninvolvedirectly altering the code but rather advising on possible improvements. The recommendation of refactoring\nsolutions is more about identifying problems and proposing solutions than actually implementing them. In this section,\nwe will explore how deep learning techniques have been used by researchers to suggest refactoring solutions. We will\ndiscuss the recommendation technologies, the suggested refactorings, the training strategies, and the evaluation metrics\n\nused to assess the performance of the approach.\n",
                    "ASurvey of Deep Learning Based Software Refactoring 25\n\n5.1 Recommendation technologies\n\nThrough the use of various deep-learning techniques, researchers have been able to suggest solutions for refactoring.\nFrom our literature search, we note that some researchers [3, 9, 44] have utilized deep learning models to predict\nspecific refactoring solutions. Conversely, others [51,53,55] haveutilized deep learning models to initially detect the\npresence of code smells in a given codebase and subsequently recommend refactoring solutions based on the identified\ncode smells. Consequently, we have categorized these approaches into two distinct groups: refactoring-based and code\nsmell-based. We will discuss the technologies used in these studies in the following sections.\n\n5.1.1 Refactoring Based Technologies. Alenezi et al. [3] conducted a study on the effectiveness of deep learning\nalgorithms in building refactoring prediction models at the class level. They used a Gated Recurrent Unit (GRU) as\na key structural parameter. The GRU comprised a hidden layer, an epoch, and a normalization batch. The GRU had\nthree hidden layers and utilized the number of epochs between 10 to 2500, with a batch size of 2 to 25. The deep\nlearning algorithm proposed by Qasem et al. [76] was used after the three hidden layers. The study used 7 open-source\nJava-based projects to assess the effectiveness of the proposed algorithm, which had two main stages. In the first stage,\na set of necessary preprocessing procedures were performed on the datasets. During this initial stage, SMOTE was used\nto prevent any imbalance. In the second stage, the deep learning algorithm was applied to the dataset to predict the\nneed for refactoring at the class level by using the Gated Recurrent Unit algorithm. For the experiment, two sets of\ndatasets, balanced and unbalanced, were used to check if the balance of the dataset would affect the predictability of the\ndeep learning model. The results indicated that a balanced dataset enhances the prediction of class-level refactorings.\n\nAniche etal. [9] conducted a large-scale study to evaluate the effectiveness of various supervised machine learning\nalgorithms in predicting software refactoring. They aimed to demonstrate that machine learning methods can accurately\nmodel the refactoring recommendation problem. The study used a dataset containing over two million real-world\nrefactorings extracted from more than 11,000 real-world projects from Apache, F-Droid, and GitHub repositories. The\nstudy evaluated six machine learning algorithms, of which five were traditional machine learning approaches (logistic\nregression, naive Bayes, support vector machine, decision trees, and random forest), and one was a neural network used\nas the deep learning technique. The neural networkused was asequential network ofthree dense layers with 128,64,\nand 1 units, respectively. To avoid overfitting, dropout layers were added between the sequential dense layers, keeping\nlearning in 80% of the units in dense layers. The number of epochs was set to 1000. All the algorithms employed for\nthis technique werethen individually used to predictrefactoringsat different levels: class, method, and variable. The\nresulting models were able to predict 20 refactorings at the different levels with an accuracy higher than 90%.\n\nKumar et al. [44] developed a recommendation system that suggests which methods require refactoring. They used 25\ndifferent sourcecode metrics at the method levelas input features ina machinelearning framework. The system then\npredicts the need for refactoring. The authors conducted a series of experiments ona publicly available annotated dataset\nof five software systems to investigate the performance of the approach. The approach used ten different machine\nlearning classifiers, of which seven (AdaBoost, LegitBoost, Log, NB, BayesNet, RF, and RBEN) were traditional machine\nlearning techniques. Only three out of the ten were based on deep learning, namely MLP, ANN-GD, and ANN-LM.\nThe first phase of the approach focused on analyzing relevant features. The considered dataset was preprocessed to\nextract relevant features. At this stage, the Wilcoxon rank sum test was applied to handle uncertainty in the dataset,\nand also for extracting features. Subsequently, ULR analysis was applied to identify the final set of source code metrics\nconsidered for further implementation. The second phase involved the model-building process and the assessment of the\n",
                    "26 Nyirongo and Jiang, etal.\n\nperformance of the proposed model. During the model-building process, the dataset was normalized through min-max\nnormalization. The class imbalance issue of the dataset was addressed using SMOTE, RUSBoost, and Up-Sample. Then,\na 10-fold cross-validation was employed with the 10 machine learning techniques to implement the proposed approach.\nThe results obtained from each technique were compared with different performance measures to evaluate them. The\nauthors concluded that overall, from the experiments conducted, method-level refactoring prediction using source code\n\nmetrics and machine learning classifiers is possible.\n\nPanigrahietal. [69] designed arecommendation system for predicting refactoring instances at the class level using\nmachine learning techniques, various data sampling approaches to address the data imbalance problem, and feature\nselection techniques. The approach centered around five main research questions: 1) how does data balancing improve\nthe prediction model\u2019s capability? 2) how does feature selection improve the results of the refactoring prediction model?\n3) how do machine learning classifiers predict the refactoring model's results across different performance parameters?\n4) how can refactoring instances be predicted in the case of cross-project? 5) how can refactoring instances be predicted\nin the case of intra-project? The authors highlighted the importance of using machine learning to refactor prediction\nmodels for large object-oriented software systems. They recommended using ensemble approaches and enhanced\nmachine learning classification algorithms to predict superior performance across different performance parameters.\nThe classifiers recommended for use as standalone or ensemble classifiers include SVM, LSSVM, Naive Bayes, Random\nForest, ELM-based, K-nearest-neighbors, and deep learning models such as LSTM and CNN. Forthe approach in this\nstudy, the authors used LSTM and CNN classifiers. The first phase involved calculating software metrics using the\nsource meter tool, while the second phase involved data normalization. Relevant features were selected from all the\nfeatures extracted from the dataset using Principal Component Analysis (PCA), correlation tests, and Wilcoxon rank\ntests. The classifiers were then utilized to predict refactoring instances. To improve the refactoring model\u2019s performance,\nthey implemented ensemble techniques, LSTM, and CNN. Performance was evaluated using different performance\nmetrics suchas recall, accuracy, F-measure, and precision. For unbalanced data, the Area Under the Curve (AUC) was\n\ncomputed.\n\nSagar et al. [79] conducted research to determine whether code metrics can be used to predict refactoring activities in\nthe source code. They approached this by formulating refactoring operation type prediction asa multi-classification\nproblem and implementing both supervised learning and LSTM models. They used the metrics extracted from committed\ncode changes to extract the features that best represent each class and predict the method level refactoring being applied\n(move method, rename method, extract method, inline method, pull up method, and push down method) for any\nproject. For this approach, they developed two types of models - a metric-based model that included traditional machine\nlearning techniques (random forest, SVM, and logistic regression classifiers) and a text-based model with LSTM. The\ntext-based model had an input layer of word embedding metrics and an LSTM layer. The LSTM layer provided the\nfinal dense layer output. For the LSTM layer, they used 128 neurons for the dense layer, and five neurons since there\nwere five differentrefactoring classes. They used Softmax as an activation functionin the dense layer and categorical\ncross-entropy as the loss function. The metric-based model was built with supervised machine learning models to\npredict the refactoring class. The random forest, SVM, and logistic regression were trained with 70% of the data. Initially,\nthe proposed metric model was implemented with only commit messages as input, but the authors realized that this\napproach was insufficient. Therefore, they combined commit messages with code metrics in the second experiment.\nThe model built with LSTM produced 54.3% accuracy. The model built with sixty-four different code metrics dealing\n",
                    "ASurvey of Deep Learning Based Software Refactoring 27\n\nwith cohesion and coupling characteristics of the code produced 75% accuracy when tested with 30% of data. The study\nshowed that commit messages with little vocabulary are not sufficient for training machine learning models.\n\nCui et al. [17] have proposed an approach for recommending move method refactoring called Rmove. The proposed\napproach involves automatically learning both structural and semantic representations from code snippets. To achieve\nthis, they first extracted method structural and semantic information from a dataset. Next, they created the structural\nand semantic representation and concatenated them. Finally, they trained a machine learning classifier to guide the\nmovement ofthe method to a suitable class. The authors used a total of nine classifiers, of which six (Decision Tree, Naive\nBayes, SVM, Logistic Regression, Random Forest, and Extreme Gradient Boosting) were machine learning -based. Only\nthree were based on deep learning, namely CNN, LSTM, and GRU. The approach demonstrated significant improvement,\nwith an increase of 14%-36% in precision, 19%-45% in recall, and 27%-44% in F-measure compared to the state-of-the-art\ntechniques, such as PathMove [45], JDeodorant [22], and JMove [89].\n\nNyamawe etal. [66] proposed alearning-based approach for recommending refactoring types based on the history\nof feature requests, code smells information, and the applied refactorings on the respective commits. The proposed\napproach learned from the training dataset associated with a set of applications. The approach could be used to\nsuggest refactoring types for feature requests associated with other applications or that are associated with the training\napplications. The proposed approach had six main steps. Firstly, the feature requests were extracted from the issue\ntracker JICA and their respective commits were retrieved from a repository on GitHub. Secondly, the previously applied\nrefactorings on the retrieved commits were recovered. Thirdly, RefDiff [84] and RMiner [95] were used to identify the\ncode smells associated with the source code in each of the retrieved commits. Fourthly, text processing was applied to\nthe contents of the file to prepare textual data into a numerical representation for training the classifiers. The fifth step\ninvolved the training of the classifiers which gave the prediction model for predicting and recommending refactorings\nfor new feature requests. Six classifiers were employed, out of which five (SVM, MNB, LR, RF, and DT) were machine\nlearning-based, while only one was deep learning-based (CNN). The study noted that CNN performed slightly lower\nthan the rest of the classifiers, partly because deep learning classifiers generally require significantly larger datasets to\nachieve competitive performance. Nonetheless, the overall evaluation of the approach based on two tasks (the need for\nrefactoring and recommending refactoring types) indicated that the approach attained an accuracy of up to 76.01% and\n83.19%, respectively.\n\nIna similar vein to the work done in Nyamawe et al. [66], Nyamawe [65] proposed a machine learning approach\nthat made use of commit messages toimprove software refactoring recommendations. This approach identified past\nrefactorings that were applied to commits used for implementing feature requests by analyzing the commit messages.\nThe approach employed six algorithms, including five based on machine learning (SVM, MNB, Random Forest, Logistic\nRegression, and Decision Tree) and one based on deep learning (CNN). Toevaluate the approach, a dataset of commit\nmessages from 65 open-source projects was used. The results showed that leveraging commit messages improved\nrefactoring recommendation accuracy significantly compared to the state-of-the-art.\n\nAstudy conducted by Mastropaolo et al. [59] explored the potential of data-driven approaches to automate variable\nrenaming. They experimented with three techniques - a statistical language model and two deep learning-based\nmodels. Three datasets were used to train and evaluate the models. The first dataset was used to train the models,\ntunetheir parameters, andassess their performance. The second and third datasets were used to further evaluate the\nperformance of the techniques. The researchers found that under certain conditions, these techniques can provide\n",
                    "28 Nyirongo and Jiang, etal.\n\nvaluable recommendations and can be integrated into rename refactoring tools. The three representative techniques\nused were a statistical model, an n-gram cached language model proposed by Hellendoorn [36], TS proposed by Raffel\net al. [77], anda transformer-based model presented by Liu et al. [50]. The study demonstrated that deep learning\nmodels, particularly those that generate predictions with high confidence, can be valuable support for variable rename\nrefactoring.\n\nAlomar etal. [4] developed a tool called AntiCopyPaster, which is a plugin for IntelliJ IDEA. The tool aims to provide\nrecommendations for extract method refactoring opportunities as soon as duplicate code is introduced in the opened\nfile in the IDE. The tool takes into account various semantic and syntactic code metrics as input and makes a binary\ndecision on whether the code fragment should be extracted or not. The goal of this approach is to increase the adoption\nof extract method refactoring while maintaining the workflow of the developer. To achieve this goal, Alomar et al. in [5]\ninvestigated the effectiveness of machine learning and deep learning algorithms. They defined the detection of extract\nmethod refactoring as a binary classification problem. Their proposed approach relied on mining prior applied extract\nmethod refactorings and extracting their features to train a deep learning classifier that detected them in the user\u2019s code.\nThe approach wasstructured into four phases: data collection, refactoring detection, code metrics selection,andtool\ndesign and evaluation. The deep learning model used in the approach was CNN. The CNN comprised multiple layers\nof fully connected nodes, structured into convolutional, deconvolutional, and dense layers. A dropout stage was also\nincluded to prevent overfitting. The input to the CNN was avector of 78 metric values which were batch-normalized\nto stabilize their distribution. The batch normalized inputs were then fed intoa convolution that reduced the feature\nspace from 78 to 32. ReLU was used as the activation function for the convolutional layers. The convoluted data was\nthen fed into the deconvolutional layer, which was followed by a max pooling layer with a filter size of 2 that took the\nlargestnumber inthe filter. The final layer was the dense layer, in which each nodereceivedinputfromall nodes of the\nprevious layer. The approach wasimplementedasa plugin in IntelliJ IDEA, whichis a popular IDE for Java. The plugin\nconsists of four main components: Duplicate detector, Code analyzer, Method extractor, and Refactoring launcher.\nThe results showed that CNN recommended the appropriate extract method refactorings with an F-measure of 0.82.\nThese results solidify that machine learning models can recommend extract method refactorings while maintaining the\nworkflow of the developer.\n\nCui et al. [16] have proposed an automated approach called Representation -based Extract Method Refactoring Recom-\nmender System (REMS) to suggest appropriate extract method refactoring opportunities. The approach involves mining\nmulti-view representations from a code property graph. First, code property graphs were extracted from training and\ntesting samples. Then, multi-view representations such as tree-view and flow-view representations were generated from\nthe code property graph. Compact bilinear pooling was used to fuse the tree-viewand the flow-viewrepresentations.\nFinally, machine learning classifiers were trained to guide the extraction ofsuitable lines of code asa new method. Six\nrelevant embedding techniques such as CodeBERT, GraphCodeBERT, CodeGPT, CodeT5, PLBART, and CoTexT were\nused to generate various representations of the abstract syntax tree, which were referred to as tree-view representations.\nThe researchers explored the impact of these representations on recommendation performance. The REMS operates in\nthree phases, including 1) feature extraction from code property graphs of training and testing samples, 2) model training\nbased on machine learning techniques, and 3) applicable analysis of behavior preservation and functional usefulness.\nSeven traditional machine learning models such as Decision Tree, K-nearest neighbor, Logistic Regression, Naive Bayes,\nRandom Forest, Support Vector Machine, and Extreme Gradient Boosting, were used. CNN and LSTM were the only\n",
                    "ASurvey of Deep Learning Based Software Refactoring 29\n\ndeep-learningtechniques employed.The results showed thatthe REMS approach outperformed five state-of-the-art\nrefactoring tools, including GEMS [101], JExtract [83], SEMI [14], JDeodorant [22], and Segmentation [90].\n\nPantiuchina [71] has developed techniques to create a new generation of refactoring recommenders. These recom-\nmenders can predict code components thatare likely to be affected by code smells inthe near future andrecommend\nmeaningful refactorings that emulate the ones that developers would perform. They refer to this approach as just-in-time\nrational refactoring, which has two main goals. First, predicting code quality decay aims to develop techniques that\nalert the developer when acode component is deviating from good design principles before design flaws are introduced.\nSecond, learning refactoring transformations investigates the possibility of applying deep learning models to learn code\nchanges performed by software developers. The researchers plan to investigate if neural machine translation models\ncan be used for the replication of refactoring operations performed by software developers.\n\nPinheiro et al. [74] investigated how trivial class-level refactorings could affect the prediction of non-trivial refactorings\nusing machine learning techniques. They selected 884 open-source projects and extracted the type of refactoring\nfrom classes involved in some operation and code metrics. The researchers grouped the refactorings into trivial and\nnon-trivial ones based on their level of change. Trivial refactorings included adding class annotations, changing access\nmodifiers, removing class annotations, etc. Non-trivial refactorings included extract class, move class, merge class, etc.\nAdditionally, they proposed contexts based on combinations of the refactoring types thatmade itpossible to increase\nthe accuracy of supervised learning models. They used four traditional machine learning models, Decision Tree, Logistic\nRegression, Naive Bayes, and Random Forest. They employed a Neural Network as the only deep-learning technique for\nthis approach. They followed asequence offive steps: selection of software projects, refactoring, and feature mining,\nselection of contexts, training and testing of the machine learning-based models, and evaluation of the results. The\nselection of contexts in step number three had to do with creating several datasets with different combinations of\nrefactoring types. The datasets constructed by the combinations of C1, C2, and C3 were used to predict the refactorings.\nThe four machine learning models were used through the Scikit-learn library, while the Neural Network was used\nthrough Tensorflow Keras. After training, each generated model was validated by predicting the refactorings of the\nfeatures in the test set. The main findings of this approach were: 1) machine learning with tree-based models, such\nas Random Forest and Decision Tree, performed very well when trained with code metrics to detect refactorings, 2)\nseparating trivial and non-trivial refactorings into different classes resulted in a more efficient model, indicative to\nimprove the accuracy of machine learning-based automated solutions, and 3) using balancing techniques that increase\nor decrease samples randomly is not the best strategy to improve datasets composed of code metrics.\n\nPanigrahi et al. [70] developed a refactoring prediction model using an ensemble-based approach. They identified the\noptimal set of code metrics and their association with refactoring proneness by analyzing the structural artifacts of\nthe software program. This approach involved refactoring data preparation, feature extraction, multiphased feature\nextraction, sampling, and heterogeneous ensemble structure refactoring prediction. The proposed model extracted 125\nsoftware metrics from object-oriented software systems using a robust multi-phased feature selection method, which\nincluded Wilcoxon significant text, Pearson correlation test, and Principal Component Analysis (PCA). The optimal\nfeatures characterizing inheritance, size, coupling, cohesion, and complexity were retained. A novel heterogeneous\nensemble classifier was developed using techniques such as ANN-Gradient Descent, ANN-Levenberg Marquardt, ANN-\nGDX, ANN-Radial Basis Function support vector machine, LSSVM-Linear, LSSVM-Polynomial, LSSVM-RBF, Decision\nTree algorithm, Logistic Regression algorithm, and Extreme Learning Machine (ELM) model as the base classifiers. The\n",
                    "30 Nyirongo and Jiang, etal.\n\nresults indicated that the Maximum Voting Ensemble (MVE) achieved better accuracy, recall, precision, and F-measure\nvalues (99.76, 99.93, 98.96, 98.44) compared to the Base Trained Ensemble (BTE). Additionally, it experienced fewer\nerrors (MAE = 0.0057, MORE = 0.0701, RMSE = 0.0068, and SEM = 0.0107) during the implementation to develop\nthe refactoring model. The experimental results recommended that MVE with up-sampling could be implemented to\nimprove the performance of the refactoring prediction model at the class level.\n\n5.1.2 Code Smell Based Technologies. Apart from proposing a deep learning-based approach to identify feature envy\nsmells, one ofthe most common code smells, Liu etal. [53], also used their approach described in Section 4 to recommend\nmove method refactorings. For methods that were predicted to be smelly (with feature envy), they suggested that such\nmethods should be moved via move method refactorings. If only one (noted as inputj) of the testing items generated\nfor a method m was predicted as positive, they suggested moving 1 to the target class tcj that was associated with\nthe positive testing item inputj. Ifmore than one testing items were predictedas positive, they selected one (noted as\ninputi) with the greatest output and suggested moving method m to class tci that associated with inputi. Although their\nneural network described in Section 4 was a binary classifier the output of the neural network wasa decimal varying\nfrom zero to one. The neural network interpreted the predictionas positive if and only ifthe output was greater than\n0.5. [15]. Their results indicated that the approach was accurate in recommending destinations for the smelly methods.\nThe approach achieved, onaverage, an accuracy of 74.94%. They also observed that the approach was more accurate\nthan the state-of-the-art tool in the recommendation of move method refactorings, i.e., JMove [89], JDeodorant [22].\nThis study was extended in Liu et al. [51] where in addition to using deep learning to detect code smells they also\nexplored the recommendation of refactoring solutions for feature envy and misplaced class. The recommendation\nof refactoring solutions for feature envy was the same as in Liu et al. [53]. The CNN used for misplaced class was\nthe same as described in Section 4. To decide whether a given class should be moved from its enclosing package to\nanother package, they leveraged two categories of features code metrics and textual features. The used code metrics\nincluded coupling between objects and message-passing coupling. To evaluate the approach they compared their\nproposed approach to TACO [68] in recommendation of target packages for misplaced classes. The accuracy of the\nrecommendation was critical because if misplaced classes are moved to the wrong positions they remain misplaced.\nThe approach resulted ina greater number ofaccepted recommendations. In total, 488 ofits recommendations were\naccepted whereas the number was reduced to 342 for TACO [68]. Secondly, TACO [68] was more accurate than the\nproposed approach in recommending target packages. It improved the average accuracy from 49.80 to 62.98 percent.\nHowever, on the same code smells where both the proposed approach and TACO [68] made recommendations, their\naccuracy in recommending target packages was close to each other 62.6% for TACO [68] and 60.05% for the proposed\napproach. Based on this analysis they concluded that the proposed approach outperformed the baseline in identifying\nmisplaced classes and it could be comparable to the baseline in recommending target packages.\n\nMaetal. [55] in their pursuit of using pre-trained model CodeTS to extract the semantic relationship between code\nsnippets to detect feature envy code smell, also explored the effectiveness of their approach inrecommending refactoring\nsolutions for the code smell. They wanted to find out if their approach could exceed the state-of-the-art approachesin\nrecommending destinations for the methods to be moved. In their approach, for methods that were predicted as smelly\n(with feature envy, inputecwas predicted positive), they suggested where sucha method should be moved viamove\nmethod refactoring. Then, they fed all testing items inputptci=<code(m),code(ptci)> into the neural network. If only one\n(noted as inputptci) of the testing items generated for m is predicted as positive, then they suggested moving m to the\n\npotential target class (ptcj) that is associated with the positive testing item inputptcj. Ifmore than one testing item is\n",
                    "ASurvey of Deep Learning Based Software Refactoring 31\n\npredicted as positive, they selected the one (noted as inputptci) with the greatest output and suggested moving method\nm to class ptci that is associated inputptci. The neural network interpreted the prediction as positive ifand only if the\noutput was greater than 0.5. This approach was compared to Liuetal.\u2019sapproach [51]. The results indicated thattheir\napproach improved the accuracy in recommending target classes as itattained an accuracy rate of greater than 90%\nwhile Liu et al. was below 90%.\n\nThe Bi-LSTM witha self-attention mechanism that was proposed by Wangetal. [98] to detect feature envy code smell\nwas also used to recommend refactoring destinations for the methods to be moved. For a method m that had been\nmarked as positive, they predicted its refactoring destination as follows. If there is only one positive example, they\nregard the target class related to this example as the destination. If there was more than one, they chose the target\nclass related to the highest probability in the outputset. The rationale behind this was that higher probability meant\nhigher confidence in the deep neural network obtained. This solution was presented as destination=Cmax where Cmax\nrelated to Pmax=Maxp1,p1,. ,pk. This functional mapping of input to output was presented and computed by the deep\n\nlearning model. This approach was compared to JMove [89], JDeodorant [22], and Liu et al.\u2019s [53] approach. The results\nindicated that their approach was more accurate on destination recommendation than the state-of-the-art.\n\nYuetal. [105] did not onlysolve the problem ofinherentcalling relationships between methods which usually cause\nunimpressive detection efficiency by proposing a Graph Neural Network (GNN) based approach towards feature envy\ndetection butalso utilized the strength ofthe calling relationship of onemethod to anotherto recommend refactoring\nsolutions for the feature envy code smell. For the methods that were predicted to have a smell, they provided refactoring\nsuggestions to move these methods to the classes that best fit their functional implementation. If a smelly method\naccessed only one external class, they recommended movingit to that external class. If the smelly method was interested\nin two or more external classes, they employed an algorithm to suggest the most suitable external class. For this\nalgorithm, they regarded the calling strength as the weight of the edge and obtained the calling strength graph. The\ncalling strength graph was constructed as G2=V,E, W where Vrepresented the set ofnodes, E represented the set of edges\nand Wrepresented the weight matrix of edges. To validate their approach, they compared itagainst, JDeodorant [22],\nJMove [89], and Liuetal.\u2019s[51] approach. The results indicated thattheirapproach had higheraccuracy than the three\nbenchmarksin refactoring recommendations. Specifically, compared with Liuetal.\u2019s[51] work, JDeodorant [22],and\nJMove [89], it improved the accuracy by 10.10%, 5.13% and 11.00% respectively.\n\nLiu et al. [54] proposed an automated approach to detecting and improving inconsistent method names. In addition to\nidentifying inconsistent names, their approach provided a list of ranked suggestions for new names for a given method.\nThe ranked list of similar names was generated using four ranking strategies. The first strategy (R1) relied solely on\nthe similarities between method bodies, ranking the names of similar method bodies according to their similarity to\nthe given method body. The second strategy (R2) groupedidentical names, ranked distinctnames based on the size of\nthe associated groups, and broke ties based on the similarities between method bodies as per R1. The third strategy\n(R3) was similar to R2, but it ranked groups based on the average similarity, regardless of the group size. To avoid\nhighly-ranked but small groups, the fourth strategy (R4) re-ranked all groups produced in R3, downgradingall 1-size\ngroups to the lowest position. To evaluate the performance of their approach in suggesting new names for inconsistent\nnames, the suggested names were ranked using the aforementioned strategies. To ensure a comprehensive assessment,\nthree different scenarios were considered: inconsistency avoidance, first token accuracy, and full name accuracy. The\napproach achieved an accuracy of 34-50% in suggesting subtokens and 16-25% accuracy in suggesting full names.\n",
                    "32 Nyirongo and Jiang, etal.\n\nLiuetal. [21] conducted a study to enhance the deeplearning-based feature envy detection approaches by providing\nreal-world examples. They used their approach to identify feature envy methods that should be moved from their\nenclosing classes to other classes that they envy. They achieved this by using a heuristic-based filtering method, as\noutlined in Section 4. Their approach, feTruth, utilized a trained classifier and a sequence of heuristic rules to predict\nwhether a given method in the testing project was associated with feature envy smell. If the method was associated\nwith feature envy smell, feTruth would suggest the class to which the method should be moved. The accuracy of feTruth\nwas compared against other approaches, namely JDeodorant [22], JMove [89], and Liu et al.\u2019s [51] approach. The results\nshowed that feTruth was accurate in suggesting destination classes for feature envy methods with an accuracy of 93.1%.\nThis was higher than JDeodorant\u2019s accuracy of 80% and comparable to Liu et al.\u2019s[51] and JMove\u2019s accuracy of 87.5%\n\nand 100%, respectively.\n\n5.2 Refactoring Types\n\nFromthe primary studies presentedin this section, we note that various refactoring solutions wererecommendedas\na way of addressing issues related to a particular codebase. When recommending arefactoring solution, a particular\nrefactoring that could be applied is suggested to resolve the identified issue. Refactorings are techniques that are applied\nto the codebase to improve its quality without altering its external behavior. Refactorings are usually applied at different\nlevels of the codebase namely class, method, and variable. Refactorings applied at the class level are usually aimed\nat enhancing the overall structure, maintainability, and readability of the codebase by making changes at the class\nlevel. Examples of these include extract class, collapse hierarchy, rename class, etc. Method-level refactorings involve\nmodifying the internal structure of methods to enhance readability, maintainability, and performance without altering\nthe external behavior of the code. The goal of these is to create cleaner, more efficient, and easier-to-understand methods.\nExamples include extract method, rename method, move method, inline method, etc. Variable-level refactorings involve\nmaking changes to the variables within the codebase to improve the quality, readability, and maintainability without\naltering the external behavior of the program. Examples of these include rename variable, extract variable, inline\n\nvariable, encapsulate field, etc.\n\nTable 4 presents the level of the refactoring solutions that were recommended by the primary studies in our survey\nplusthe representative refactorings suggested at thatlevel. Analyzing the presented data, itbecomesapparentthata\nmajority of the recommended refactoring solutions focus on the method level, with 58.06% of researchers proposing\nsolutions at this granularity. Class-level refactoring follows at 25.81%, and variable-level solutions are suggested in\n16.13% ofthe primary studies. Through this data, we also observed that underthe method level refactoring, the most\ncommon refactoring types being suggested were the extract method and the move method. We noted that extract\nmethod refactoring was found in 62.50% ofthe studies that employed refactoring based recommendation approach\nwhile move method was found in 85.71% of the studies that employed code smell-based approach to recommendation\n\nrefactoring.\n\n5.3 Training Strategies\n\nAs previously mentioned, training strategies play a crucial role in developing a robust and accurate deep-learning\nmodel. In this section, we will discuss the technicalities, tools, and procedures that researchers have used to build deep\n\nlearning models that are effective in recommending refactoring solutions. Researchers have adopted various procedures\n",
                    "ASurvey of Deep Learning Based Software Refactoring 33\n\nTable 4. Representative refactoring types.\n\nLevel Refactorings References\n\nClass Extract class, Move class, Rename class [3, 9, 65, 66, 69-71, 74]\nMethod Extract method, Move method, Rename method [4,6,9,16,17,21,44,51,53-55,65,66, 71,71,79,98, 105]\n\nVariable Extract variable,Rename variable, Move attribute [9, 59, 65, 66, 71]\n\nrelated to data preprocessing, feature extraction, and data balancing to ensure that they build a deep learning model\nthat is trained with high-quality data for accurate and efficient recommendations of refactoring solutions.\n\n5.3.1 Data Preprocessing. According to our survey, researchers have employed various processes such as tokenization,\nlemmatization, stop word removal, noise removal, and normalization to ensure that their data is well-preprocessed and\ncleaned. Tokenization breaks texts into words, phrases, symbols, or other meaningful elements called tokens. This is\nused to split text into constituent sets of words. Lemmatization replaces the suffix of a word or removes it to obtain\nthe basic word form. Itis used for part of speech identification, sentence separation, and key phrase extraction. The\ngoal of lemmatization is to group different inflected forms of a word so that they can be analyzed as a single item.\nStop word removal involves filtering out common words thatare considered to be of little value in understanding the\nmeaning ofatext. Noise removal refers to the process ofreducing or eliminating irrelevant or unwanted information,\noften referred toas \"noise,\" froma dataset. The goal ofnoise removal isto improve the quality of the data or signal for\nmore accurate analysis or interpretation. For instance, Sagar etal. [79] had to remove and clean HTML tags sincetheir\ndata came from the web. Sagar et al. [79] also checked for special characters, numbers, and punctuation to remove any\nnoise. Normalization refers to the process of transforming data into a standard scale. The goalis to bring the values of\ndifferent variables or features into acomparablerange, preventing one variable from dominating the analysis simply\nbecause of its larger scale. In this process, textual data may be converted to the standard required case, either lowercase,\nuppercase, camel case, etc. We also note through our literature search that other researchers, for example, Alenezi et\nal. [3], employed the process of basic data cleaning by first deleting all unnecessary features from their dataset, then\nfinalizing the process by deleting refactoring type features and replacing the summation of refactoring feature.\n\n5.3.2 Feature Extraction. Various types of features can be extracted to enable a deep learning model to recommend\nappropriate refactoring solutions. These features can include semantic, structural, code metrics, commit messages, or\ndocumentation. Most studies extract features from source code, which includes different metrics depending on the\nrefactoring solution they want to suggest. However, some researchers, such as Aniche et al. [9], used process and\nownership metrics instead of merely employing source code features. For the process metrics, Aniche et al. [9] collected\nfive different types of metrics: the number of commits, the sum of lines added and removed, the number of bug fixes, and\nthe number of previous refactoring operations. They calculated the number of bug fixes by using a heuristic whenever\nany ofthe keywords \"bug, error, mistake, fault, wrong, fail, and fix\" appeared in the commitmessage,and counted one\nor more bug fixes to that class. The number of previous refactoring operations was calculated based on the refactorings\nthey gathered from the refactoring mining tool. For the code ownership metrics, Aniche et al. [9] adopted the suite\nownership metrics as proposed by Bird et al. [11]. The number of authors was the total number of developers who had\ncontributed to the given software artifact. The minor authors were the number of contributors who had authored less\nthan 5% (in terms of commits) of an artifact. The major authors were the number of developers who contributed at\n",
                    "34 Nyirongo and Jiang, etal.\n\nleast 5% to anartifact. With this, ownership was calculated as the proportion of commits achieved by the mostactive\n\ndeveloper.\n\n5.3.3 Feature Embedding. The primary studies have employed various tools and techniques for feature embedding.\nEmbedding is a type of vectorization that represents words as dense vectors in a continuous space which captures\nsemantic relationships between them. For instance, Cui et al. [16,17] used code and graphembedding techniques to\ngenerate corresponding structural and semantic representations. They also used them to create hybrid representations.\nFor code embedding, they used Code2vec [8] and Code2Seq [7]. Code2Vec is a neural network that automatically\ngenerates vectors from source code, while Code2Seq is a neural network that produces sequences from code snippets.\nFor graph embedding techniques, they explored the use of Deepwalk [72], Node2Vec [30], Walklets [73], GraRep [13],\nLine [88], ProNE[106],and SDNE [97]. DeepWalkand Node2Vec employ random walk to construct sample neighbor-\nhoods for nodes in a graph based on a Skip-gram Natural Language Processing (NLP) model. The goal of Skip-gram is to\nmaximize the likelihood of words appearing in a sliding window co-occurring. Walklets is another random walk-based\ngraph embedding technique that explicitly encodes multi-scale relationships between nodes to produce multi-scale\nrepresentations for them. GraRep is a matrix factorization-based graph embedding technique that constructs matrices\nfrom connections between nodes and factorizes them to produce the embedding result. Line calculates graph embedding\nresults by specifying two functions, one for the first-order node proximity and the other for the second-order node prox-\nimity. ProNEisa fastand scalable graph embeddingtechnique that was recently introduced. Itincludestwosteps, the\nfirstis to effectively initialize graph embedding results by phrasing the problemas sparse matrix factorization, motivated\nby the long-tailed distribution of most graphs and their sparsity. The second stage is to propagate the initial embedding\nresult using the higher-order Cheeger\u2019s inequality [46], aiming at capturing the graph\u2019s localized clustering information.\nSDNE employs deep autoencoders to generate embedding results. Other techniques also emerged from the researchers\ninthe primary studies, for example, the employment of word embedding technologies, e.g., Word2Vec[51,53,79,98],\nvector space models [65, 66], and CodeT5 [105] to achieve embedding.\n\n5.3.4 DataBalancing. Researchers often use various techniques to address the issue of data imbalance forrecommending\nrefactoring solutions. These techniques include the Synthetic Oversampling Technique (SMOTE) and its variants\n(BLSMOTE, SVSMOTE, GraphSMOTE, etc.), UpSample, RUSBoost, Down sampling, and Random sampling. SMOTE\ntechnique is based on the oversampling approach in which synthetic examples are used for oversampling the minority\nclass rather than oversampling with replacement. UpSample is used to improve the number of samples of the minority\nclass by inserting zeros between the samples. RUSBoost is a hybrid approach of data sampling and boosting algorithm\nused to improvethe performance of models trained on skewed data. Toreduce the bias thatmay arise due to the use of\nimbalanced datasets, data balancing techniques are usually employed to create abalanced dataset. Mostresearchers\nemploy the use of sampling techniques to achieve data balance in their datasets. In our survey, 82.00% of primary\nstudies were found to employ sampling techniques in their variant forms. However, Pinheiro et al. [74] concluded that\nusing balancing techniques that increase or decrease samples randomly is not the best strategy for improving datasets\n\ncomposed of code metrics.\n\n5.4 Evaluation\n\n5.4.1 Datasets. Different types of datasets are utilized to suggest accurate refactoring solutions through deep learning\nmodels. These datasets are carefully chosen to ensure that the deep learning model receives the appropriate data for\nmaking the right recommendations. Based onour survey, we found thatresearchers typically clone or download the\n",
                    "ASurvey of Deep Learning Based Software Refactoring 35\n\nsubject projects from repositories and extract data relevant to their study refactorings. RefactoringMiner [93] was found\nto be the most popular tool for refactoring data mining tasks. RefactoringMiner was used by at least 91.30% of the\nprimary studies. As shown in Figure 7, 86.96% of the researchers used publicly available projects, while only 13.04%\nused private projects with restricted access. Interestingly, 100% of the studies in this category the applications that\nwere used for the experiments and evaluation were developed in the Java language. We also came across the work\nof Mastropaolo et al. [59], who created three datasets to train and evaluate their deep learning model. They built a\nlarge-scale dataset for training the model, tuning parameters, and performing an initialassessment of performance.\nAdditionally, they created reviewed and developers datasets to further evaluate the performance of theirtechnique.\n\nPublic\n\nPrivate\n\nFig. 7. Datasets\n\n5.4.2 Result Metrics. As previously discussed, result metrics are used to measure the effectiveness of a particular\napproach. The standard metrics for this calculation are precision, recall, F-measure, and accuracy. Our literature\nsearch findings on the metrics used by researchers in this category are shown in Figure 8. According to Figure 8, F-\nmeasure, recall, and precision are the most commonly used metrics for evaluating various approaches in recommending\nrefactoring solutions. F-measure was used in at least 95.65% of the primary works, while precision and recall were\nemployed in 95.65% and 86.95% of the studies, respectively.\n",
                    "36 Nyirongo and Jiang, etal.\n\n2\n5 204\no\ng\no\nSs\nSs\nEs)\nQ 154\n3\na\na\n=\na 104\ng\no\na.\noO\na.\n\u2018S\n5 54\no\n2\nEE\nE}\nZz\n0 >\nPrecision Recall F1-score Accuracy AUC MCC\nPerformance assessment\n\nFig. 8. Metrics used for performance assessment.\n\n6 END-TO-END CODE TRANSFORMATION AS REFACTORING\n\nThe end-to-end code transformation as refactoring refers to the actual process of modifying an existing codebase\nto improve its structure, readability, maintainability, or performance without changing its external behavior. The\nend-to-end code transformation of refactorings differs from the other refactoring tasks (i.e., the detection of code\nsmells, the recommendation of refactoring solutions) in thatit involves the application of the suggested refactoring\nto the codebase, reviewing the refactoring code to ensure itadheres to coding standards, validating that the external\nbehavior of the codebase remains unchanged, and updating any documentation to reflect the changes made during the\nrefactoring, such as comments, inline documentation, and external documentation if necessary. This process focuses on\nactively making changes to the codebase which can include the actual renaming of variables, the moving of methods,\nextracting methods, simplifying complex expressions, restructuring code, and so on. In this section, we will explore\nhow researchers have utilized deep learning models to conduct end-to-end code transformations as refactorings. Our\nspecific focus will be on the technologies utilized for conducting the refactoring code transformations.\n\nSzalontai et al. [87] have developed a method using deep learning to refactor source code, which was initially developed\nfor the general-purpose programming language and runtime environment, Erlang. This approach has two main\ncomponents: a localizer and a refactoring component. Together, they enable the localization and refactoring of non-\nidiomatic code patterns into their idiomatic counterparts. The method processes the source code as a sequence of\ntokens, making it capable of transforming even incomplete or non-compilable code. To do this, the source code is\ntransformed into a sequence of tokens, using the Erlang module tok to tokenize the source code. The module obtains\ntoken types such as atom, integer, variable, etc. These tokens are then provided as input into the neural network to\nlocalize non-idiomatic functions. The neural network consists of convolutional, recurrent, and feedforward components.\nThe tokens provided as input are embedded into a 64-dimensional vector space, and thena one-dimensional convolution\nis applied to each code chunk using 126=8 filters and a kernel size of 5. Two pooling operators are applied to the\nconvolutional outputs, average and minimax. These two operations yield two intermediate representations foreach\n",
                    "ASurvey of Deep Learning Based Software Refactoring 37\n\nchunk of the source code. The idiomatic alternative is generated using a recurrent sequence -to-sequence architecture\nwithan attention mechanism. The non-idiomatic tokenized code is first fed to the encoder, which produces a hidden\nrepresentation of the input sequence. This is achieved through the use of a Recurrent Neural Network consisting of four\nBiLSTM layers with 64 units each. The decoder uses a single LSTM layer with 256 units to generate an output sequence\nelement by element, producing the idiomatic alternative. Both the localizer and the refactoring models were evaluated\nona test set that was separated from the training data before the training process. The accuracy of the localizer was\nmeasured as the ratio of classified code chunks to the total number of chunks in the test set and was found to be\n99.09%. For the refactoring component, the ratio of error-free transformations against the total number of attempted\ntransformations was measured resulting in an accuracy of 99.46%. These results indicate that the presented models were\ntrained successfully and are capable of performing refactorings that are similar to the onesin the training datasets.\n\nTufano et al. [96] quantitatively investigated the ability of a Neural Machine Translation (NMT) model to learn\nhow to automatically apply code changes implemented by developers during pull requests. They harnessed NMT to\nautomatically translate a code component from its state before the implementation ofthe pull request (pre-PR) and\nafter the pull request has been merged (post-PR), thereby emulating the code changes thatwould beimplemented by\ndevelopers in the pull request. This is the first work that used deep learning techniques to learn and create a taxonomy\nfrom a variety of code transformations taken from the developer\u2019s pull requests. In this investigation, they first mined a\ndataset of complete and meaningful code changes performed by developers in merged pullrequests, extracted from\nthree Gerrit repositories (Android, Google, and Ovirt). Then they trained the NMT models to translate pre-PR code into\npost-PR code, effectively learning code transformations as performed by developers. RNN Encoder-Decoder and Beam\nSearch Decoding were used as NMT models for this approach. The RNN Encoder-Decoder architecture was coupled with\nan attention mechanism which is commonly adopted in NMT tasks. The RNN encoder was used for encoding a sequence\nof tokens x into vector representation while the RNN Decoder was used for decoding the representation into another\nsequence of tokens y. The primary purpose of the employed beam search decoder was to improve the quality of the\ngenerated token sequences by exploring multiple possible paths instead of simply selecting the most likely next token\nat each step. The NMT model was able to predict and learn from some transformations. This was used to develop a\ntaxonomy of the transformations with three subcategories grouping the code transformation into bug fixing, refactoring,\nand other. The refactoring subtree included all code transformations that modified the internal structure of the system\nby improving one or more of its non-functional attributes without changing the system\u2019s external behavior. Under this\nsubtree five subcategories were formulated, namely, inheritance (forbid method overriding by adding the final keyword\nto the method declaration, invoke overriding method instead of overridden by removing the super keyword to the\nmethod invocation and making a method abstract through the abstract keyword and deleting the method body), methods\ninteraction (add parameter refactoring (i.e., a value previously computed in the method body is now passed as parameter\nto it), and broadening the return type of a method by using the Java wildcard (?) symbol), readability (braces added to if\nstatements with the only goal of clearly delimiting their scope, the merging of two statements defining and initializing\n\navariable intoa single statement, the addition/removal of the this qualifier, to match the project\u2019s coding standards,\nreducing the verbosity ofa generic declaration by using the Java diamond operator, refactoring anonymous classes\nimplementing one method tolambda expressions, to make the code morereadable, simplifying Boolean expressions,\nand merging two catch blocks capturing different exceptions into one catch block capturing both exceptions using\nthe or operator), naming (renaming of methods, parameters, and variables), and encapsulation (modifying the access\nmodifiers, e.g., changing a public method to a private one). The results showed that NMT models are capable of learning\n",
                    "38 Nyirongo and Jiang, etal.\n\ncode changes and perfectly predict code transformations in up to 21% of the cases when only a single translation is\ngenerated and upto32% when 10 possible guesses are generated. These results highlight the ability ofthe models to\nlearn froma heterogeneous set of pull requests belonging to different datasets, indicating the possibility of transfer\nlearning access projects and domains.\n\nTo facilitate the rename refactoring process and reduce the cognitive load of developers, Liu et al. [52] proposed a\ntwo-stage pre-trained framework called RefBERT. This framework is based on the BERT architecture and was designed\nto automatically suggest a meaningful variable name, which is considered a challenging task. The researchers focused on\nrefactoring variable names, which is more complex than refactoring other types of identifiers, such as method names and\ntype names. RefBERT uses 12 RoBERTa layers, which area replicated version of the original BERT model with improved\nperformance. The approach is based on three observations. First, rename refactoringis similar to Masked Language\nModelling (MLM),a pretext task commonly used in pre-training BERT. MLM fills the masked part ofa text according\nto its context. Similarly, rename refactoring aims to suggest a meaningful variable name according to the context.\nTherefore, MLM can be adopted for training an automatic rename refactoring model. Second, unlike the variable name\nprediction task, where only the context of the target variable is known, in rename refactoring, both the context of the\ntarget variable and the variable name before refactoring are known. Contrastive learning, which contrasts positive and\nnegative samples for improving representation learning, is an ideal learning paradigm for automatic rename refactoring.\nThe researchers expected the generated name to be close tothe variable nameafter refactoring but far away from the\nvariable name before refactoring. Third, unlike natural language text where words should follow a strict order to ensure\ngrammatical correctness, subtokens in avariable name do not have such a restriction. Different orders of subtokens\nfor a variable name do not significantly affect our understanding of the variable. Thus, the standard cross-entropy\nloss that emphasizes the strict alignment between the prediction and the targetis suboptimal for automatic rename\nrefactoring. RefBERT was trained to generate refactorings in two steps: Length Prediction (LP), where itpredicts the\nnumber of tokens in the refactored variable name, and Token Generation (TG), where given the predicted number\nof tokens, RefBERT generates tokens in the refactored variable name. To train RefBERT, the researchers used the\nCodeSearchnet [4.0] and Java-Small [7] datasets in the pretraining stage. During the fine-tuning stage, they also used\nJavaRef and TL-Codesum [38] datasets. JavaRef was constructed by the researchers by applying data collection and\npreprocessing procedures on open-source datasets collected from GitHub. The experimental results demonstrated the\neffectiveness of RefBERT in automatic rename refactoring.\n\nFrom the literature presented, we note from Table 5, that Recurrent Neural Networks (RNN) through its variants (LSTM\nand GRU) have mostly been used for the end-to-end transformation as refactorings. The usage of RNN was found inat\nleast 75% of the studies. This was followed by Transformer technologies (e.g., BERT), which were utilized in 25% of the\nstudies. Notably, to enhance the performance of the proposed techniques the researchers adopted the inclusion of other\ntechniques in their approaches. Szantotai et al. [87] and Tufano et al. [96] used an attention mechanism to improve the\nmodel's ability to focus on relevant parts of the input sequence when generating the output sequence. In contrast, Liu\netal. [52] used contrastive learning to contrast positive and negative samples forimproving representation learning,\nand was used for automatic rename refactoring. The inclusion of contrastive learning helped the model to understand\nthe context of the target variable and the variable name before refactoring.\n\nFrom the presented studies, we note that researchers have employed various deep-learning techniques to perform\nthe code transformation for different types of refactorings. Szalontai et al. focused on using a deep learning model\n",
                    "ASurvey of Deep Learning Based Software Refactoring 39\n\nTable 5. Technologies used for the end-to-end refactorings transformation\n\nTechnologies Refactorings Deployment tools/plaforms References\nLSTM+GRU+attention mechanism Non idiomatic components Erlang [87]\nNMT(RNN+Beam search)+attention mechanism Non functional attributes Java [96]\nBERT(12RoBERTA)+contrastive learning Rename refactoring RefBERT-Java [52]\n\nto refactor nonidiomatic code patterns into idiomatic ones across various levels of code organization such as class,\nmethod, and variable. Typically, the choice of the appropriate level depends on the specific issues identified in the\ncodebase, with refactorings like extract class, extract method, and rename variable being associated with these patterns.\nSzalontai etal.\u2019s approach primarily targeted the general-purpose programming language Erlang. Conversely, Liu et\nal.\u2019s [52] and Tufano et al. [96] approaches specifically targeted the renaming refactoring for variables and refactoring\nofnon-function attributes, respectively, using the Java language. Thus, from Table 5, we observe that researchers are\nemploying a specific approach to the conduction ofend-to-end transformation of refactorings as found in 66.67% of\nthe studies i.e., variable renaming and non-function attribute refactoring. In contrast, 33.33% of the studies opted for a\ngeneral approach in the conduction of the refactoring transformation where the changes could be used at different\n\nlevels (i-e., class, method, variable).\n\n7 MINING OF REFACTORINGS\n\nMining of refactorings refers to the automatic process of identifying and extracting instances of refactorings from\nexisting codebases. To conduct the mining of refactorings for deep learning-based refactoring, traditional refactoring\nminers are used. The refactoring miners utilize various techniques such as static analysis, pattern recognition, and\nheuristic-based methods to identify and discover refactoring activities that were carried out within codebases. The\noutputs generated by these miners might consist of labeled examples that indicate where and howrefactorings have\nbeen applied. These labeled examples serve as ground truth data, forming the foundation for training and evaluating\ndeep learning models. By leveraging the outputs of traditional refactoring miners, large datasets of labeled refactorings,\nenabling the development ofaccurate and robust deep-learning models capable of automating software refactoring\nprocesses can be created. This integration of mining approaches with advanced deep-learning methodologies accelerates\nthe advancement of intelligent tools aimed at enhancing code quality and maintainability.\n\nSeveral traditional refactoring miners have been proposed by researchers to aid in the mining of refactorings. Tsantalis\nproposed RefatoringMiner [93] which represents the implementation of software entities as abstract syntax trees (ASTs),\nand computes the similarity between two entities according to the name-based similarity and the statement-based\nsimilarity. With such similarities, RefactoringMiner maps entities between two successive versions and leverages a\nsequence of heuristics to discover software refactorings based on the mapping. RefactoringCrawler developed by Dig et\nal. [20], is an analysis tool that detects refactorings that happened between two versions of a component. The strength\nof the tool lies in the combination ofa fast syntactic analysis to detect refactoring candidates, and a more expensive\nsemanticanalysis to refine these candidates. Silva et al. [85] proposed RefDiffwhich utilizes static analysis and code\nsimilarity to detect various refactorings. It begins by tokenizing the source code of the project. Each code element (such\n",
                    "40 Nyirongo and Jiang, etal.\n\nas classes, methods, and fields) is transformed into a bag of tokens. Ref-Finder proposed by Prete et al. [75] encodes code\nelements (e.g,, classes, methods, and fields) and their relationships using logic predicates to detect the refactorings. Liu\net al. [49] proposed ReMapper an automated iterative approach used to match software entities between two successive\nversions for the discovery of refactorings. ReMapper takes full advantage of the qualified names, the implementations,\nand the references of software entities. It leverages an iterative matching algorithm to handle the interdependence\nbetween entity matching and the computation of reference-based similarity. Researchers have utilized some of these\n\ntraditional refactoring miners to mine refactorings to train deep learning models in the process of refactoring as follows.\n\nAlthough deep learning technologies have notyet been employed to distinguish refactorings from other source code\nmodifications as RefactoringMiner or Ref-Finder do, deep learning technologies have been successfully employed to\nidentify refactoring-containing commits by analyzing their associated commit messages. For example, Marmolejos et\nal. [58] developed a framework that used text-mining, natural language preprocessing, and supervised machine learning\ntechniques to automatically identify and classify refactoring activities in commit messages. The framework focused on\ndetecting Self-Affirmed Refactorings (SAR), which are refactoring activities reported in commit messages. The approach\nused a binary classification method to overcome the limitations of the manual process proposed in previous studies. The\nframework had four main parts. The first part involved preparing the data and processing the content of the commit\nmessages to remove unnecessary and irrelevant information, as well as normalize the data. In the second part, the\ndata was converted into hash values, with each hash value representing one or more features in the commit messages.\nThe third part involved filtering the features to select only the most important ones from the dataset. Finally, in the\nfourth part, machine learning algorithms were trained and tested based on the selected features. The resulting two-class\nclassifier was able to operate over unlabelled texts. For this approach, the authors used four classifiers, including Bayes\nPoint Machine, Logistic Regression, Boosted Decision Tree, and Average Perceptron, as well as one deep learning-based\nclassifier, Neural Network. The datasetused in this approach contained 1,208,970 refactoring operations, extracted\nusing Refactoring Miner [93] from 3,795 open-source Java projects. From this dataset, the authors extracted commit\nmessages containing the required patterns to create their refactoring dataset. Since the employed machine learning\ntechniques could not directly identify text, the authors converted the collected data into hashes. They used the feature\nhashing technique, also known as a hashing trick, to derive features. In this technique, various words with varying\nlengths were mapped to different features based on the hash value. To determine the relevance of each attribute in\nthe dataset, Chi-Square (CHI) was used to give a score, while Fisher Score (FS) was used to select a subset of features\nand score the distance between them. The machine learning classifiers were trained usinga stratified train-testsplit\nmethodology, where 70% of the rows of the transformed dataset from the selected features were used for training and\nthe remaining 30% were used to measure the error rate. The approach proved to be efficient, as the authors obtained\n\nsubstantial accuracy.\n\nAlomar et al. [6] aimed to investigate whether different words and phrases used in refactoring commit messages\nare unique to different types of refactorings. To achieve this, they employed machine learning techniques to predict\nrefactoring operation types based on the commit messages. The prediction of the refactoring operation was formulated\nas a multiclass classification problem, which relied on textual mining of commit messages to extract relevant features\nfor each class. The researchers collected a dataset of refactorings from 800 projects, where each instance presented\nacommit message and arefactoring type. They identified six preferred method-level refactorings, including extract\nmethod, inline method, move method, pull-up method, push-down method, and rename method. To identify relevant\nfeatures, they used the n-gram technique proposed by Manningand Hinrich [57]. Nine supervised machine learning\n",
                    "ASurvey of Deep Learning Based Software Refactoring 41\n\nalgorithms were applied, and the results were compared against a keyword-based baseline approach used in Murphy\net al. [62]. The results revealed that the predictive accuracy for rename method, extract method, and move method\nranged from 63% to 93% in terms of F-measure. Nevertheless, the model encountered challenges in accurately discerning\nbetween Inline Method, Pull-up Method, and Push-down Method, with F-measure scores falling within the range of\n42% to 45%. Additionally, it\u2019s noteworthy that the keyword-based approach exhibited significantly lower performance\ncompared to the machine learning models.\n\n8 CHALLENGES AND OPPORTUNITIES\n\nAs the use of deep learning models in the domain of software refactoring continues to grow, itbecomes imperative to\nclosely look at the challenges and opportunities linked to their adoption. Despite showcasing promising capabilities in\naiding different tasks of the refactoring process, there are still some challenges associated with their application. This\nsection explores the challenges confronted by deep learning models in supporting the process of software refactoring,\nconcurrently shedding light on prospective opportunities for future work.\n\n8.1 Challenges\n\nWhile deep learning models have proven to be effective in supporting the process of software refactoring, their adoption\ninto this field is not without challenges. From our survey, we note the following challenges.\n\n+ Limited generalization of deep learning techniques across diverse paradigms is a significant concern. Many\nstudies have developed approaches concentrating on specific code smells (e.g., feature envy, brain class, brain\nmethod) within a particular language, such as Java. This specialization restricts the applicability of these models\ntodifferent code smells or programming languages. Given that codesmells can manifestdifferently in diverse\ncontexts, amodel trained on one set of smells may not exhibit robust performance on others. Moreover, code\nsmells often coexist and exhibit interactions. For instance, a lengthy method may signal a broader design issue,\nlike a godclass. Approaches focused onindividual smells in the detection of code smells might overlook these\nintricate interactions, resulting in incomplete or inaccurate outcomes. Notably, based on the compiled primary\nworks, asubstantial majority (at least87.50%) employed datasets developed ina singular programming language,\ni.e., Java, posing a challenge for the generalization of these approaches to other programming languages.\n\n* Challenges in creating generic classification and feature engineering. Developing a universal classifier for\ndiverse refactoring processes has proven challenging. This challenge is particularly evident in the detection of\ncode smells, where different types of code smells necessitate distinct features and characteristics for accurate\nidentification. Employing a one-size-fits-all approach may lead to diminished precision and recall, especially\nin studies utilizing sequential modeling-based deep-learning approaches to support software refactoring.\nAdditionally, extracting pertinent features from abstract syntax trees or sequences of statements presents\ndifficulties. The model\u2019s effectiveness relies on the accurate capture of both semanticand structural features.\nEstablishing suitable mechanisms for feature extraction is pivotal for the success of deep learning-based\napproaches.\n\n+ Concerns about data quality and representativeness are pivotal factors influencing the performance of deep\nlearning models. Certain approaches utilized automatically generated labeled data, potentially lacking accurate\n",
                    "42 Nyirongo and Jiang, etal.\n\nrepresentation of real-world scenarios. Incorporating real-world examples could enhance the effectiveness of\ndeep learning models.\n\n+ Limited adoption. Developers may be resistant to adopting automated refactoring tools supported by deep\nlearning due to concerns about the reliability and trustworthiness of the generated code changes. Notably, most\nof the current studies on the end-to-end code transformation for refactorings by deep learning models have\nused prototype tools and non-industrial datasets which do notreflect the actual tools and data pools used by\n\nprogrammers.\n\n+ Need for continuous learning. Software systems evolve as software systems undergo maintenance and updates.\nModels trained ona static dataset may become outdated and may not effectively support the process of software\nrefactoring in such without continuous learning mechanisms.\n\n8.2 Opportunities\n\nAmidst these challenges presented in Section 8.1, there exist some opportunities for advancing the integration of deep\nlearning models in software refactoring. Some of the opportunities, according to our survey, are as follows.\n\n+ According to the taxonomy presented, deep learning models have been used for various tasks, including\ndetecting code smells, recommending refactoring solutions, conducting refactorings, and mining refactorings.\nOur survey of the literature shows that deep learning models are primarily employed for detecting code smells,\naccounting forat least 56.25% inthis category. The recommendation ofrefactoring solutions accounts for 33.33%,\nend-to-end codetransformationas refactoring for 6.25%, and mining ofrefactorings for 4.17%. However, we\ndid not find any significant study on the use of deep learning for software refactoring quality assurance inour\nliteraturesearch. This revelation shows thatthere is an imbalance in howdeep learning has beenemployed in\nsupporting refactoring tasks and thus, presents an opportunity for future work. Itis highly valuable to fill this\n\ngap and ensure that all the tasks of software refactoring are fully supported by deep learning models.\n\n+ According to the primary studies presented, various types of deep learning models have been used for software\nrefactoring. Most of the studies focused on utilizing hybrid models that combine deep learning models with\nother techniques to improve model performance. The data shows that at least 58.69% of the primary studies used\nhybridapproaches, while the remaining 41.31% used generic deep learning models. Thelatter encompassed\nsequential modeling, explainable and feedback-centric approaches that fit into developers\u2019 workflow. Among\nthe deep learning models, CNN was the most commonly used model, found in at least 43.48% of the primary\nworks, followed by RNN at 34.78%, and its variants (such as GRU, LSTM, GRU, etc.). The other 21.74% ofthe\nstudies employed other deep learning models, such as GCN, GNN, ResNet, MLP, etc. However, only a few\nstudies, such as Sharma et al. [80, 81], explored the use of transfer learning techniques to enable deep learning\nmodels trained on one project to be effectively used for refactoring in different projects. There is a need to carry\nout more exploration of transfer learning approaches as they can result in better generalization and minimize\nthe need for extensive project-specific training data. Notably, Himesh et al. [64] and Yin et al. [104] werea\nfew of the researchers who explored the inclusion of feedback and developments of explainable deep learning\nmodels for refactoring. To improve the adoption of deep learning models in the software refactoring process by\nsoftware developers, itis necessary to do more explorations in the inclusion of feedback and explainability in\nthe deep learning models.\n",
                    "ASurvey of Deep Learning Based Software Refactoring 43\n\n* Our literature search has revealed that deep learning models have been predominantly used for method-level\nrefactorings. Among the primary studies we surveyed, 55.41% applied deep learning models for refactorings\natthe method level, followed by 30.45% forclass level, 10.12% for variable level,and 4.02% for other types of\nrefactorings. The most frequent use of deep learning techniques was for detecting and applying move method\nand extract method refactorings, both of which occur at the method level of the codebase. Itis worth noting\nthat most of the work on identifying refactoring opportunities by detecting code smells has been focused on\nmethod-level code smells, particularly the feature envy smell which often leads to the recommendation and\nsuggestion of move method refactoring. This opens up room for more future work in employing deep learning\nmodels for refactorings occurring at other levels than just the method level.\n\n+ Based on the literature, it has been found that deep learning models are effective in supporting the process of\nsoftware refactoring. These models have outperformed the existing approaches or tools used in refactoring by\nachieving an average F-measure of 76%, whichis a significant improvement of 30% onaverage compared to\nthe state-of-the-artapproaches. However, thereis stilla need to develop dynamicand adaptive deep-learning\nmodels that can continuously learn and adjust to changes in coding standards, project goals, and evolving best\npractices. This could involve using reinforcementlearning approaches or other adaptive learningstrategies.\nAdditionally, according to our survey, most of the primary studies did not use industrial datasets, which makes it\ndifficult to generalize the findings of most of the techniques. Toimprove the effectiveness of deep learning-based\n\ntechniques for refactoring, it is necessary to incorporate more real-world industrial data.\n\n9 CONCLUSIONS\n\nIn this paper, we have presented a survey on deep learning-based software refactoring. Our focus was on the process of\nsoftware refactoring and how it can be supported by deep learning models. We have categorized the studies based on the\nspecific refactoring task that was supported by deep learning models. Our taxonomy has identified five main categories\nwhich include the detection of code smells, recommendation of refactoring solutions, end-to-end code transformation\nas refactoring, quality assurance for refactoring, and mining of refactorings. We have presented key aspects under\nthese categories which have provided insight into the research direction in the deployment of deep learning models for\n\nsoftware refactoring.\n\nREFERENCES\n\n1] Charu C. Aggarwal. 2018. Neural Networks and Deep Learning: a textbook. Cham (Switzerland) Springer 2018. https: /linkspringer.com/book/10.\n1007/978-3-319-94463-0\n\n2] AmalAlazba, Hamoud Aljamaan, and Mohammad R.Alshayeb. 2023. Deep learning approaches forbad smell detection:a systematic literature\nreview. Empirical Software Engineering 28 (2023). https://api.semanticscholar.org/CorpusID:258591793\n\n3] Mamdouh Alenezi, Mohammed Akour, and Osama Al Qasem. 2020. Harnessing deep learning algorithms to predict software refactoring.\nTELKOMNIKA Telecommunication Computing Electronics and Control 18 (2020), 2977-2982. https://api.semanticscholar.org/CorpusID:225015544\n4] Eman Abdullah Alomar, Anton Ivanov, Zarina Kurbatova, Yaroslav Golubev, Mohamed Wiem Mkaouer, Ali Ouni, Timofey Bryksin, Le Nguyen,\nAmit Dilip Kini, and Aditya Thakur. 2021. AntiCopyPaster: Extracting Code Duplicates AsSoon As They Are Introduced in the IDE. Proceedings of\nthe 37th IEEE/ACM International Conference on Automated Software Engineering (2021). https: //api.semanticscholar.org/CorpusID:245634394\n\n5] Eman Abdullah Alomar, Anton Ivanov, Zarina Kurbatova, Yaroslav Golubev, Mohamed Wiem Mkaouer, Ali Ouni, Timofey Bryksin, Le Nguyen,\nAmit Dilip Kini, and Aditya Thakur. 2023. Just-in-Time Code Duplicates Extraction. Inf. Softw. Technol. 158 (2023), 107169. https://api.\nsemanticscholar.org/CorpusID:256621589\n\n6] Eman Abdullah Alomar, Jiaqian Liu, Kenneth Addo, Mohamed Wiem Mkaouer, Christian D. Newman, Ali Ouni, and Zhe Yu. 2021. On the\ndocumentation of refactoring types. Automated Software Engineering 29 (2021). https://api.semanticscholar.org/Corpus|D:244896267\n\n",
                    "44\n\n17]\n[8\n\n(9\n\n[10]\n\n11\n\n12\n\n13\n\n14\n\n15\n16\n\n17\n\n18\n\n19\n\n20)\n\n21\n\n22\n\n23\n\n24)\n\n25)\n\n26)\n\n27)\n\n28)\n29)\n\n30)\n\n31\n\n32\n\n33\n\nNyirongo and Jiang, etal.\n\nUri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2018. code2seq: Generating Sequences from Structured Representations of Code. ArXiv\nabs /1808.01400 (2018). https://apisemanticscholar.org/CorpusID:51926976\n\nUri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. code2vec: learning distributed representations of code. Proceedings of the ACM on\nProgramming Languages 3 (2018), 1 - 29. https://apisemanticscholar.org/Corpus!D:4710028\n\nMauricio Finavaro Aniche, Erick Galani Maziero, Rafael Serapilha Durelli, and Vinicius H. S. Durelli. 2020. The Effectiveness of Supervised\nMachine Learning Algorithms in Predicting Software Refactoring. IEEE Transactions on Software Engineering 48 (2020), 1432-1450. https:\n//api.semanticscholar.org/Corpus!D:210157308\n\nAntoine Barbez, Foutse Khomh, and Yann-Ga\u00e9l Gu\u00e9h\u00e9neuc. 2019. A Machine-learning Based Ensemble Method For Anti-patterns Detection. ArXiv\nabs /1903.01899 (2019). https://apisemanticscholar.org/Corpus|D:67877051\n\nChristian Bird, Nachiappan Nagappan, Brendan Murphy, Harald Gall, and Premkumar Devanbu. 2011. Don\u2019t Touch My Code! Examining the\nEffects of Ownership on Software Quality. SIGSOFT/FSE 2011 - Proceedings of the 19th ACM SIGSOFT Symposium on Foundations of Software\nEngineering, 4-14. https://doi.org/10.1145/2025113.2025119\n\nChristopher M. Bishop. 1995, Neural networks for pattern recognition. https://api.semanticscholar.org/CorpusID:60563397\n\nShaosheng Cao, WeiLu,and Qiongkai Xu. 2015. GraRep: Learning Graph Representations with Global Structural Information. Proceedings of the\n24th ACM International on Conference on Information and Knowledge Management (2015). https://apisemanticscholar.org /CorpusID:17341970\nSofia Charalampidou, Apostolos Ampatzoglou, Alexander Chatzigeorgiou, Antonios Gkortzis, and Paris Avgeriou. 2017. IdentifyingExtract\nMethod Refactoring Opportunities Based on Functional Relevance. IEEE Transactions on Software Engineering 43 (2017), 954-974. https:\n//api.semanticscholar.org/Corpus!D:4642697\n\nFrangois Chollet. 2018. Keras: The Python Deep Learning library. https://api.semanticscholar.org/CorpusID:215844202\n\nDi Cui, Qiangqiang Wang, Siqi Wang, Jianlei Chi, Jianan Li, Lu Wang, and Qingshan Li. 2023, REMS: Recommending Extract Method Refactoring\nOpportunities via Multi-view Representation of Code Property Graph. 2023 IEEE/ACM 31st International Conference on Program Comprehension\n(ICPC) (2023), 191-202. https: //apisemanticscholar.org/Corpus!D:259860601\n\nDiCui, Siqi Wang, YongLuo, Xingyu Li, Jie Dai, LuWang, and Qingshan Li. 2022. RMove: Recommending Move Method Refactoring Opportunities\nusing Structural and Semantic Representations of Code. 2022 IEEE International Conference on Software Maintenance and Evolution (ICSME) (2022),\n281-292. https://api.semanticscholar.org/CorpusID:254902806\n\nAnanta Kumar Das, Shikhar Yadav, and Subhasish Dhal. 2019, Detecting Code Smells using Deep Learning. In TENCON 2019 - 2019 IEEE Region 10\nConference (TENCON). 2081-2086. https://doi.org/10.1109/TENCON.2019.8929628\n\nSeema Dewangan, Rajwant Singh Rao, Alok Mishra, and Manjari Gupta. 2021. A Novel Approach for Code Smell Detection: An Empirical Study.\nIEEE Access PP (2021), 1-1. https://api.semanticscholar.org/CorpusID:245065600\n\nDanny Dig, Can Comertoglu, Darko Marinov, and Ralph E. Johnson. 2006. Automated Detection of Refactorings in Evolving Components. In\nEuropean Conference on Object-Oriented Programming. https: //apisemanticscholar.org/CorpuslD:12303996\n\nBo Liu et al. 22023, Deep Learning Based Feature Envy Detection Boosted by Real-World Examples. (22023), https://lyoubo.github.io/papers/\nDeep_Learning_Based_Feature_Envy_Detection_Boosted_by_Real-World_Examples.pdf\n\nMarios Fokaefs, Nikolaos Tsantalis, and Alexander Chatzigeorgiou. 2007. JDeodorant: Identification and Removal of Feature Envy Bad Smells. In\nInternational Conference on Smart Multimedia, https://apisemanticscholar.org/CorpusID:19001314\n\nFrancesca Arcelli Fontana, Mika Mantyla, Marco Zanoni, and Alessandro Marino. 2016. Comparingand experimenting machine learning techniques\nforcodesmell detection. Empirical Software Engineering 21 (2016), 1143-1191. https://apisemanticscholar.org/CorpusID:16222152\nFrancesca Arcelli Fontana, Mika Mantyla, Marco Zanoni, and Alessandro Marino. 2016. Comparingand experimenting machine learning techniques\nforcodesmell detection. Empirical Software Engineering 21 (2016), 1143-1191. https://apisemanticscholar.org/CorpusID:16222152\nFrancesca Arcelli Fontana and Marco Zanoni. 2017. Code smell severity classification using machine learning techniques. Knowl. Based Syst. 128\n(2017), 43-58. https://api.semanticscholar.org/CorpusID:39781104\n\nMartin Fowler, 2002. Refactoring: Improving the Design of Existing Code. In Extreme Programming and Agile Methods \u2014 XP/Agile Universe 2002,\nDon Wells and Laurie Williams (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 256-256.\n\nJoshua Garcia, Daniel Popescu, George T. Edwards, and Nenad Medvidovi\u00e9, 2009. Identifying Architectural Bad Smells. 2009 13th European\nConference on Software Maintenance and Reengineering (2009), 255-258. https: //api.semanticscholar.org/CorpusID:1847981\nSuryanarayana Girish, Samarthyam Ganesh, and Sharma Tushar. 2015. Refactoring for Software Design Smells: Managing Technical Debt.\n\nIan Goodfellow, Yoshua Bengio,and Aaron Courville.2016. Deep Learning. Cambridge (Massachusetts): MIT Press. https://doiorg/10.1007/s10710-\n017-9314-z\n\nAditya GroverandJure Leskovec.2016. node2vec: Scalable Feature Learning forNetworks. Proceedings ofthe 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (2016). https://api.semanticscholar.org/Corpus!D:207238980\n\nXueliang Guo, Chongyang Shi, and He Jiang. 2019. Deep semantic-Based Feature Envy Identification. Proceedings of the 11th Asia-Pacific Symposium\non Internetware (2019). https://api.semanticscholar.org/CorpusID:207811924\n\nMouna Hadj-Kacemand Nadia Bouassida. 2018. A Hybrid Approach To Detect Code Smells using Deep Learning. In International Conference on\nEvaluation of Novel Approaches to Software Engineering. https://api.semanticscholar.org/CorpusID:14006917\n\nMouna Hadj-Kacem and Nadia Bouassida. 2019. Deep Representation Learning for Code Smells Detection using Variational Auto-Encoder. 2019\nInternational Joint Conference on Neural Networks (IJCNN) (2019), 1-8. https://api.semanticscholar.org/Corpus!D:203605428\n",
                    "ASurvey of Deep Learning Based Software Refactoring 45\n\n34)\n35)\n\n36)\n\n37)\n\n38)\n\n39)\n\n40)\n\n41\n\n42\n\n43\n\n44)\n\n45)\n\n46)\n\n47)\n\n48)\n\n49)\n\n50)\n\n51\n\n52\n\n53\n\n54)\n\n55)\n\n56)\n\n57)\n58)\n\n59)\n\nAbeer Hamdy and Mostafa Tazy. 2020. Deep Hybrid Features for Code Smells Detection. https://api.semanticscholar.org/CorpusID:221505409\nZhang Hanyu and Tomoji Kishi. 2023. Long Method Detection Using Graph Convolutional Networks. Journal of Information Processing 31 (08\n2023), 469-477. https://doiorg/10.2197/ipsjjip.31.469\n\nVincent J. Hellendoorn and Premkumar T. Devanbu. 2017. Are deep neural networks the best choice for modeling source code? Proceedings of the\n2017 11th Joint Meeting on Foundations of Software Engineering (2017). https://api.semanticscholar.org/Corpus|D:21164835\n\nJie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. 2017. Squeeze-and-Excitation Networks. 2018 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (2017), 7132-7141. https://api.semanticscholar.org/CorpusID:140309863\n\nXing Hu, Ge Li, Xin Xia, D. Lo, and Zhi Jin. 2019. Deep code comment generation with hybrid lexical and syntactical information. Empirical\nSoftware Engineering 25 (2019), 2179 - 2217. https: //api.semanticscholar.org/CorpusID:189927337\n\nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. 2017. Snapshot Ensembles: Train 1, get M forfree.\nArXiv abs/1704.00109 (2017). https://api.semanticscholar.org/CorpusID:6820006\n\nHamel Husain, Hongqiu Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of\nSemantic Code Search. ArXiv abs/1909.09436 (2019). https: //api.semanticscholar.org/CorpusID:202712680\n\nFerdin Joe John Joseph, Sarayut Nonsiri, and Annop Monsakul. 2021. Keras and TensorFlow: A Hands-On Experience. Advanced Deep Learning for\nEngineers and Scientists (2021). https://api.semanticscholar.org/CorpusID:237998052\n\nRen\u00e9 Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: a database of existing faults to enable controlled testing studies for Java programs.\nIn International Symposium on Software Testing and Analysis. https://api.semanticscholar.org/CorpusID:12796895\n\nJohn D Keller. 2019. Deep Learning. Cambridge (Massachusetts): MIT Press. https://direct.mitedu/books/book/4556/Deep-Learning\nLov Kumar, Shashank Mouli Satapathy, and Lalita Bhanu Murthy Neti. 2019. Method Level Refactoring Prediction on Five Open Source Java\nProjects using Machine Learning Techniques. Proceedings of the 12th Innovations on Software Engineering Conference (formerly known as India\nSoftware Engineering Conference) (2019). https://api.semanticscholar.org/CorpusID:60441115\n\nZarina Kurbatova, Ivan Veselov, Yaroslav Golubev, and Timofey Bryksin. 2020. Recommendation of Move Method Refactoring Using Path-\nBased Representation of Code. Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops (2020). https:\n//api.semanticscholar.org/CorpusID:211133313\n\nJames R. Lee, Shayan Oveis Gharan, and Luca Trevisan. 2011. Multi-way spectral partitioning and higher-order cheeger inequalities. In Symposium\non the Theory of Computing. https://apisemanticscholar.org/CorpusID:8212381\n\nYichen Liand Xiaofang Zhang. 2022. Multi-Label Code Smell Detection with Hybrid Model based on Deep Learning, In International Conference on\nSoftware Engineering and Knowledge Engineering. https://api.semanticscholar.org/CorpusID:252098564\n\nTao Lin, Xue Fu, Fu Chen, and Luqun Li. 2021. A Novel Approach for Code Smells Detection Based on Deep Leaning. Lecture Notes ofthe Institute\n\u2018for Computer Sciences, Social Informatics and Telecommunications Engineering (2021). https://apisemanticscholar.org/CorpusID:238020089\n\nBo Liu, Hui Liu, Nan Niu, Yuxia Zhang, Guangjie Li, and Yanjie Jiang. 2023. Automated Software Entity Matching Between Successive Versions. 2023\n38th IEEE/ACM International Conference on Automated Software Engineering (ASE) (2023), 1615-1627. https://api.semanticscholar.org/Corpus|D:\n265056437\n\nF. Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task Learning based Pre-trained Language Model for Code Completion. 2020 35th IEEE/ACM\nInternational Conference on Automated Software Engineering (ASE) (2020), 473-485. https: //apisemanticscholar.org/CorpusID:229703606\n\nHui Liu, Jiahao Jin, Zhifeng Xu, Yanzhen Zou, Yifan Bu, and Lu Zhang. 2021. Deep Learning Based Code Smell Detection. [EEE Transactions on\nSoftware Engineering 47,9 (2021), 1811-1837. https://doi.org/10.1109/TSE.2019.2936376\n\nHao Liu, Yanlin Wang, Zhao Wei, Yongxue Xu, Juhong Wang, Hui Li, and Rongrong Ji. 2023. RefBERT: A Two-Stage Pre-trained Framework\nfor Automatic Rename Refactoring. Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis (2023).\nhttps://api.semanticscholar.org/Corpus!D:258960338\n\nHui Liu, Zhifeng Xu, and Yanzhen Zou. 2018. Deep Learning Based Feature Envy Detection. In 2018 33rd IEEE/ACM International Conferenceon\nAutomated Software Engineering (ASE). 385-396. https://doi.org/10.1145/3238147.3238166\n\nKui Liu, Dongsun Kim, Tegawend\u00e9 F. Bissyand\u00e9, Tae young Kim, Kisub Kim, Anil Koyuncu, Suntae Kim, and Yves Le Traon. 2019. Learning\nto Spot and Refactor Inconsistent Method Names. 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (2019), 1-12.\nhttps://api.semanticscholar.org/CorpusID:155144146\n\nWenhao Ma, Yaoxiang Yu, Xiaoming Ruan, and Bo Cai. 2023. Pre-trained Model Based Feature Envy Detection. 2023 IEEE/ACM 20thInternational\nConference on Mining Software Repositories (MSR) (2023), 430-440. https://api.semanticscholar.org/CorpusID:259835399\n\nRuchika Malhotra, Bhawna Jain, and Marouane Kessentini. 2023. Examining deep learnings capability to spot code smells: a systematic literature\nreview. Cluster Computing 26 (2023), 3473 - 3501. https: //api.semanticscholar.org/CorpusID:263654376\n\nChristopher D. Manning and Hinrich Schiitze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA, USA.\nLicelot Marmolejos, Eman Abdullah Alomar, Mohamed Wiem Mkaouer, Christian D. Newman, and Ali Ouni. 2021. Onthe use of textual feature\nextraction techniques to support the automated detection of refactoring documentation. Innovations in Systems and Software Engineering 18 (2021),\n233 - 249. https://api.semanticscholar.org/CorpusID:233807785\n\nAntonio Mastropaolo, Emad Aghajani, Luca Pascarella, and Gabriele Bavota. 2022. Automated variable renaming: are we there yet? Empirical\nSoftware Engineering 28 (2022). https://api.semanticscholar.org/CorpusID:254564632\n",
                    "46\n\n60)\n\n61\n\n62\n\n63\n\n64)\n\n65)\n\n66)\n\n67)\n\n68)\n\n69)\n\n70)\n\n[71]\n\n72\n\n73\n\n74,\n\n75)\n\n76)\n\n77)\n\n78)\n\n79)\n\n80)\n\n81\n\n82\n\n83\n\n84)\n\nNyirongo and Jiang, etal.\n\nRana S, Menshawy, Ahmed H. Yousef, and Ashraf Salem. [n. d.]. Comparing the Effectiveness of Machine Learning and Deep Learning Techniques\nfor Feature Envy Detection in Software Systems. In 2023 Intelligent Methods, Systems, and Applications (IMSA). 470-475. https://doi.org/10.1109/\nIMSA58542.2023.10217458\n\nNaouel Moha, Yann-Gael Gueheneuc, Laurence Duchien, and Anne-Francoise Le Meur. 2010. DECOR: A Method for the Specification and Detection\nof Code and Design Smells. IEEE Transactions on Software Engineering 36, 1 (2010), 20-36. https://doi.org/10.1109/TSE.2009.50\n\nEmerson R. Murphy-Hill, Chris Parnin, and Andrew P. Black, 2009. How we refactor, and how we know it. 2009 IEEE 31st International Conference\non Software Engineering (2009), 287-297. https://api.semanticscholar.org/Corpus|D:5856772\n\nPurnima Naik, Salomi Nelaballi, Venkata Sai Pusuluri, and Dae-Kyoo Kim. 2023. Deep Learning-Based Code Refactoring: A Review of Current\nKnowledge. SSRN Electronic Journal (2023). https://api.semanticscholar.org/Corpus!D:254267544\n\nHimesh Nanadani, Mootez Saad, and Tushar Sharma. 2023. Calibrating Deep Learning-based Code Smell Detection using Human Feedback. (2023).\nhttps://tusharma in/preprints /SCAM23_HumanFeedbackOnSmells.pdf\n\nAllyS. Nyamawe. 2022. Mining commit messages to enhance software refactorings recommendation: A machine learning approach. Machine\nLearning with Applications (2022). https://api.semanticscholar.org/CorpusID:248807768\n\nAlly S. Nyamawe, Hui Liu, Nan Niu, Qasim Umer, and Zhendong Niu. 2020, Feature requests-based recommendation of software refactorings.\nEmpirical Software Engineering 25 (2020), 4315-4347. https: //api.semanticscholar.org/CorpusID:221521552\n\nFabio Palomba, Dario Di Nucci, Michele Tufano, Gabriele Bavota, Rocco Oliveto, Denys Poshyvanyk, and Andrea De Lucia. 2015. Landfill: An\nOpen Dataset of Code Smells with Public Evaluation, 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories (2015), 482-485.\nhttps://apisemanticscholar.org/CorpusID:16120092\n\nFabio Palomba, Annibale Panichella, Andrea De Lucia, Rocco Oliveto, and Andy Zaidman, 2016. A textual-based technique for Smell Detection.\n2016 IEEE 24th International Conference on Program Comprehension (ICPC) (2016), 1-10. https: //apisemanticscholar.org/Corpus!D:36114894\nRasmita Panigrahi, Sanjay Kumar Kuanar, and Lov Kumar.2022. Machine Learning Implementation for Refactoring Prediction. In 2022 IEEE 4th PhD\nColloquium on Emerging Domain Innovation and Technology for Society (PhD EDITS). 1-2. https://doi.org/10.1109 /PhDEDITS56681.2022.9955297\nRasmita Panigrahi, Sanjay Kumar Kuanar, Sanjay Misra, and Lov Kumar. 2022. Class-Level Refactoring Prediction by Ensemble Learning with\nVarious Feature Selection Techniques. Applied Sciences (2022). https://api.semanticscholar.org/CorpusID:254363225\n\nJevgenija Pantiuchina, 2019. Towards Just-In-Time Rational Refactoring. 2019 IEEE/ACM 41st International Conference on Software Engineering:\nCompanion Proceedings (ICSE-Companion) (2019), 180-181. https://api.semanticscholar.org/CorpusID:174799906\n\nBryan Perozzi, Rami Al-Rfou, and Steven S. Skiena. 2014. DeepWalk: online learning of social representations. Proceedings of the 20th ACM SIGKDD\ninternational conference on Knowledge discovery and data mining (2014). https://apisemanticscholar.org/CorpusID:3051291\n\nBryan Perozzi, Vivek Kulkarni, Haochen Chen, and Steven S, Skiena. 2016. Don\u2019t Walk, Skip!: Online Learning of Multi-scale Network\nEmbeddings. Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017 (2016).\nhttps://api.semanticscholar.org/CorpusID:207699173\n\nDarwin Pinheiro, Carla Ilane Moreira Bezerra, and Anderson G.Uch\u00e9a. 2022, How do Trivial Refactorings Affect Classification Prediction Models?\nProceedings of the 16th Brazilian Symposium on Software Components, Architectures, and Reuse (2022). https://apisemanticscholar.org/CorpusID:\n252497875\n\nKyle Prete, Napol Rachatasumrit, Nikita Sudan, and Miryung Kim. 2010. Template-based reconstruction of complex refactorings. 2010 IEEE\nInternational Conference on Software Maintenance (2010), 1-10. https://api.semanticscholar.org/CorpusID:2659467\n\nOsama Al Qasem, Mohammed Akour, and M. Alenezi. 2020. The Influence of Deep Learning Algorithms Factors in Software Fault Prediction, IEEE\nAccess 8 (2020), 63945-63960. https://api.semanticscholar.org/CorpusID:215816960\n\nColin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019.\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res. 21 (2019), 140:1-140:67. https:\n//api.semanticscholar.org/Corpus!D:204838007\n\nSanjiban Roy, Valentina Balas, Pijush Samui, and Sharma D. 2019. Handbook of Deep Learning Applications.\n\nPriyadarshni Suresh Sagar, Eman Abdullah Alomar, Mohamed Wiem Mkaouer, Ali Ouni, and Christian D. Newman. 2021. Comparing Commit\nMessages and Source Code Metrics for the Prediction Refactoring Activities. Algorithms 14 (2021), 289. https://api.semanticscholar.org/CorpuslD:\n244175361\n\nTushar Sharma, Vasiliki Efstathiou, Panagiotis Louridas, and Diomidis D. Spinellis. 2019. On the Feasibility of Transfer-learning Code Smells using\nDeep Learning. ArXiv abs/1904.03031 (2019). https://api.semanticscholar.org/CorpusID:102351639\n\nTushar Sharma, Vasiliki Efstathiou, Panos Louridas,and DiomidisD.Spinellis.2021. Code smell detection by deep direct-learningandtransfer-\nlearning. J. Syst. Softw. 176 (2021), 110936. https://api.semanticscholar.org/Corpus!D:233329781\n\nTushar Sharma and Marouane Kessentini. 2021. QScored: A Large Dataset of Code Smells and Quality Metrics, 2021 IEEE/ACM 18th International\nConference on Mining Software Repositories (MSR) (2021), 590-594. https://api.semanticscholar.org/CorpusID:232165224\n\nDanilo Silva, Ricardo Terra, and Marco Tulio Valente. 2015. JExtract: An Eclipse Plug-in for Recommending Automated Extract Method Refactorings.\nArXiv abs/1506.06086 (2015). https://api.semanticscholar.org/CorpusID:707971\n\nDanilo Silva and Marco Tilio Valente. 2017. RefDiff: Detecting Refactorings in Version Histories. 2017 IEEE/ACM 14th International Conference on\nMining Software Repositories (MSR) (2017), 269-279. https://api.semanticscholar.org/CorpusID:11506870\n",
                    "ASurvey of Deep Learning Based Software Refactoring 47\n\n85] Danilo Silva and Marco Tulio Valente, 2017. RefDiff: Detecting Refactorings in Version Histories. In 2017 IEEE/ACM 14th International Conference\non Mining Software Repositories (MSR). 269-279. https://doi.org/10.1109/MSR.2017.14\n\n86] Gustavo Soares. 2010. Making program refactoring safer. In 2010 ACM/IEEE 32nd International Conference on Software Engineering, Vol. 2. 521-522.\nhttps://doi.org/10.1145/1810295.1810461\n\n87] Balazs Szalontai, P\u00e9ter Bereczky, and Daniel Horpacsi. 2023. Deep Learning-Based Refactoring with Formally Verified Training Data. Infocommu-\nnications journal (2023). https://api.semanticscholar.org/Corpus!D:261570010\n\n88] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei, 2015. LINE: Large-scale Information Network Embedding, Proceedings\nof the 24th International Conference on World Wide Web (2015). https://api.semanticscholar.org/CorpusID:8399404\n\n89] Ricardo Terra, Marco Tilio Valente, Sergio Miranda, and Vitor Sales. 2018. JMove: Anovel heuristic and tool to detect move method refactoring\nopportunities. J. Syst. Softw. 138 (2018), 19-36. https://apisemanticscholar.org/CorpusID:4412526\n\n[90] Omkarendra Tiwari and Rushikesh K. Joshi. 2022. Identifying Extract Method Refactorings. 15th Innovations in Software Engineering Conference\n(2022). https://api.semanticscholar.org/CorpusID:246828642\n\n91] Nikolaos Tsantalis and Alexander Chatzigeorgiou. 2009. Identification of Extract Method Refactoring Opportunities. In 2009 13th European\nConference on Software Maintenance and Reengineering. 119-128, https://doiorg/10.1109/CSMR.2009.23\n\n92] Nikolaos Tsantalis and Alexander Chatzigeorgiou. 2009. Identification of Move Method Refactoring Opportunities, [EEE Transactions on Software\nEngineering 35, 3 (2009), 347-367. https://doi.org/10.1109/TSE.2009.1\n\n93] Nikolaos Tsantalis, Ameya Ketkar, and Danny Dig, 2022. RefactoringMiner 2.0. IEEE Transactions on Software Engineering 48, 3 (2022), 930-950.\nhttps://doi.org/10.1109/TSE.2020.3007722\n\n94] Nikolaos Tsantalis, Matin Mansouri, Laleh Eshkevari, Davood Mazinanian, and Danny Dig, 2018. Accurate and Efficient Refactoring Detection in\nCommit History. In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). 483-494, https://doi.org/10.1145/3180155.3180206\n95] Nikolaos Tsantalis, Matin Mansouri, Laleh Mousavi Eshkevari, Davood Mazinanian, and Danny Dig, 2018. Accurate and Efficient Refactoring\nDetection in Commit History. 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE) (2018), 483-494. https://api.\nsemanticscholar.org/CorpuslD:49665673\n\n96] Michele Tufano, Jevgenija Pantiuchina, Cody Watson, Gabriele Bavota, and Denys Poshyvanyk. 2019. On Learning Meaningful Code Changes Via\nNeural Machine Translation. 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (2019), 25-36. https://apisemanticscholar.\norg/CorpusID:59316445\n\n97] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural Deep Network Embedding, Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (2016). https://api.semanticscholar.org/CorpusID:207238964\n\n98] Hongze Wang, Jing Liu, Jiexiang Kang, Wei Yin, Haiying Sun, and Hui Wang. 2020. Feature Envy Detection based on Bi-LSTM with Self-Attention\nMechanism. 2020 IEEE Intl Confon Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing &\nCommunications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom) (2020), 448-457. https://api.semanticscholar.org/Corpus|D:\n235340022\n\n99] Ian H. Witten, Eibe Frank, and Mark A. Hall. 2011. Data Mining: Practical Machine Learning Tools and Techniques (3rd ed.). Morgan Kaufmann\nPublishers Inc, San Francisco, CA, USA.\n\n100] Sihan Xu, Aishwarya Sivaraman, Siau-Cheng Khoo, and Jing Xu. 2017. GEMS: An Extract Method Refactoring Recommender. In 2017 IEEE 28th\nInternational Symposium on Software Reliability Engineering (ISSRE). 24-34. https: //doi.org/10.1109/ISSRE.2017.35\n\n101] Sihan Xu, Aishwarya Sivaraman, Siau-Cheng Khoo, and Jing Xu. 2017. GEMS: An Extract Method Refactoring Recommender. 2017 IEEE 28th\nInternational Symposium on Software Reliability Engineering (ISSRE) (2017), 24-34. https://api.semanticscholar.org/Corpus|D:38648648\n102] Weiwei Xu and Xiaofang Zhang, 2021, Multi-Granularity Code Smell Detection using Deep Learning Method based on Abstract Syntax Tree. In\n\nInternational Conference on Software Engineering and Knowledge Engineering, https://api.semanticscholar.org/Corpus|D:239678857\n\n103] Yanming Yang, Xin Xia, David Lo, and John Grundy. 2020. A Survey on Deep Learning for Software Engineering.\n\n104] Xin Yin, Chongyang Shi, and Shuxin Zhao. 2021. Local and Global Feature Based Explainable Feature Envy Detection. 2021 IEEE 45th Annual\n\nComputers, Software, and Applications Conference (COMPSAC) (2021), 942-951, https://api.semanticscholar.org/CorpusID:237474266\n\n105] Dongjin Yu, Yihang Xu, Lehui Weng, Jie Chen, Xin Chen, and Quanxin Yang. 2022, Detecting and Refactoring Feature Envy Based on Graph\n\nNeural Network. 2022 IEEE 33rd International Symposium on Software Reliability Engineering (ISSRE) (2022), 458-469. https://apisemanticscholar.\n\norg/CorpusID:254930430\n\n106] Jie Zhang, Yuxiao Dong, Yan Wang, Jie Tang, and Ming Ding. 2019. ProNE: Fastand Scalable Network Representation Learning, In International\n\nJoint Conference on Artificial Intelligence. https://api.semanticscholar.org/CorpusID:189808933\n\n107] Minnan Zhang and Jingdong Jia. 2022. Feature Envy Detection with Deep Learning and Snapshot Ensemble. 2022 9th International Conferenceon\n\nDependable Systems and Their Applications (DSA) (2022), 215-223. https://apisemanticscholar.org/CorpusID:253124697\n\n108] Yang Zhang and Chunhao Dong, 2021. MARS: Detecting brain class /method code smell based on metric-attention mechanism and residual\n\nnetwork. Journal of Software: Evolution and Process (2021). https://api.semanticscholar.org/CorpuslD:243792860\n\n109] Yang Zhang, Chuyan Ge, Shuai Hong, Ruili Tian, Chun-Ru Dong, and J. Liu. 2022. DeleSmell: Code smell detection based on deep learning and\nlatent semantic analysis. Knowl. Based Syst. 255 (2022), 109737. https://api.semanticscholar.org/CorpusID:251751777\n\n[110] Yang Zhang, Chuyan Ge, Haiyang Liu, and Kun Zheng, 2024. Code smell detection based on supervised learning models: A survey. Neurocomputing\n\n565 (2024), 127014. https://doi.org/10.1016 /jneucom.2023.127014\n\n",
                    "48 Nyirongo and Jiang, etal.\n\n[111] Shuxin Zhao, Chongyang Shi, Shaojun Ren, and Hufsa Mohsin. 2022. Correlation Feature Mining Model Based on Dual Attention for Feature Envy\nDetection. In International Conference on Software Engineering and Knowledge Engineering. https://api.semanticscholar.org/CorpusID:252100954\n"
                ]
            }
        },
        {
            "file_name": "S:\\OneDrive\\@Dev\\!GPT\\ScriptGPT\\library\\Refactoring\\Source\\TOSEM-refactoring-ouni-2016.pdf",
            "time_taken": 92.93192434310913,
            "data_extracted": {
                "text": [
                    "Multi-criteria Code Refactoring Using Search-Based Software\nEngineering: An Industrial Case Study\n\nALI OUNI, Osaka University\n\nMAROUANE KESSENTINI, University of Michigan-Dearborn\nHOUARI SAHRAOUI, University of Montreal\n\nKATSURO INOUE, Osaka University\n\nKALYANMOY DEB, Michigan State University\n\nOne of the most widely used techniques to improve the quality of existing software systems is refactoring \u2014\nthe process of improving the design of existing code by changing its internal structure without altering its\nexternal behavior. While it is important to suggest refactorings that improve the quality and structure of\nthe system, many other criteria are also important to consider such as reducing the number of code changes,\npreserving the semantics of the software design and not only its behavior, and maintaining consistency with\nthe previously applied refactorings. In this paper, we propose a multi-objective search-based approach for\nautomating the recommendation of refactorings. The process aims at finding the optimal sequence of refac-\ntorings that (i) improves the quality by minimizing the number of design defects, (ii) minimizes code changes\nrequired to fix those defects, (ii) preserves design semantics, and (iv) maximizes the consistency with the\npreviously code changes. We evaluated the efficiency of our approach using a benchmark of six open-source\nsystems, 11 different types of refactorings (move method, move field, pull up method, pull up field, push\ndown method, push down field, inline class, move class, extract class, extract method and extract interface)\nand 6 commonly occurring design defect types (blob, spaghetti code, functional decomposition, data class,\nshotgun surgery and feature envy) through an empirical study conducted with experts. In addition, we per-\nformed an industrial validation of our technique, with 10 software engineers, on a large project provided by\nour industrial partner. We found that the proposed refactorings succeed in preserving the design coherence\nof the code, with an acceptable level of code change score while reusing knowledge from recorded refactorings\napplied in the past to similar contexts.\n\nGeneral Terms: Algorithms, Reliability\n\nAdditional Key Words and Phrases: Search-based Software Engineering, Refactoring, Software Mainte-\nnance, Multi-Objective Optimization, Software Evolution\n\nACM Reference Format:\n\nAli Ouni, Marouane Kessentini, Houari Sahraoui, Katsuro Inoue, Kalynmoy Deb, 2015. Multi-criteria Code\nRefactoring using Search-Based Software Engineering: An Industrial Case Study. ACM Trans. Softw. Eng.\nMethodol. 9, 4, Article 39 (March 2015), 54 pages.\n\nDOI: 0000001.0000001\n\n1. INTRODUCTION\n\nLarge scale software systems exhibit high complexity and become difficult to main-\ntain. In fact, it has been reported that the software cost attributable to maintenance\nand evolution activities is more than 80% of total software costs |Erlikh 2000]. To facil-\nitate maintenance tasks, one of the most widely used techniques is refactoring which\n\nimproves design structure while preserving external behavior |Mens and Tourw\u00e9 2004\nOpdyke 1992]\n\nAuthor\u2019s addresses: Ali Ouni, Osaka University, ali@ist.osaka-u.acjp, Marouane Kessentini, Uni-\nversity of Michigan-Dearborn, marouane@umich.edu, Houari Sahraoui, University of Montreal,\nsahraouh@iro.umontreal.ca, Katsuro Inoue, Osaka University, inoue@ist.osaka-u.ac.jp, and Kalyanmoy Deb,\nMichigan State University, kdeb@egr.msu.edu.\n\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted\nwithout fee provided that copies are not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights for third-party components of this\nwork must be honored. For all other uses, contact the owner/author(s).\n\n\u00a9 2015 Copyright held by the owner/author(s). 1049-331X/2015/03-ART39 $15.00\n\nDOI: 0000001.0000001\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:2 A. Ouni et al.\n\nEven though most of the existing refactoring recommendation approaches are pow-\nerful enough to suggest refactoring solutions to be applied, several issues are still need\nto be addressed. One of the most important issues is the semantic coherence of the\nrefactored program, which is not considered by most of the existing approaches [Ouni|\n\" Comse-\nquently, the refactored program could be syntactically correct, implement the correct\nbehavior, but be semantically incoherent. For example, a refactoring solution might\nmove a method calculateSalary() from class Employee to class Car. This refactoring\ncould improve the program structure by reducing the complexity and coupling of class\nEmployee and satisfy the pre- and post-conditions to preserve program behavior. How-\never, having a method calculateSalary() in class Car does not make any sense from\nthe domain semantics standpoint, and is likely to lead to comprehension problems in\nthe future. Another issue is related to the number of code changes required to ap-\nply refactorings, something that is not considered in existing refactoring approaches\nwhose only aim is to improve code quality independently of the cost of code changes.\nConsequently, applying a particular refactoring may require a radical change in the\nsystem or even its re-implementation from scratch. Thus, it is important to minimize\ncode changes to help developers in understanding the design after applying the pro-\nposed refactorings. In addition, the use of development history can be an efficient aid\nwhen proposing refactorings. Code fragments that have previously been modified in\nthe same time period are likely to be semantically related (e.g., refer to the same fea-\nture). Furthermore, code fragments that have been extensively refactored in the past\nhave a high probability of being refactored again in the future. Moreover, the code to\nrefactor can be similar to some refactoring patterns that are to be found in the devel-\nopment history, thus, developers can easily adapt and reuse them.\n\nOne of the limitations of the existing works in software refactoring |Du Bois et al.\n2004} |Qayum and Heckel 2009} |Fokaefs et al. 2011} Harman and Tratt 2007} /Moha|\net al. 200 eng et al. 2006] is that the definition of semantic coherence is closely\n\nrelated to behavior preservation. Preserving the behavior does not means that the de-\nsign semantics of the refactored program is also preserved. Another issue is that the\nexisting techniques are limited to a small number of refactorings and thus it could\nnot be generalized and adapted for an exhaustive list of refactorings. Indeed, semantic\ncoherence is still hard to ensure since existing approaches do not provide a pragmatic\ntechnique or an empirical study to prove whether the semantic coherence of the refac-\ntored program is preserved.\n\nIn this paper, we propose a multi-objective search-based approach to address the\nabove-mentioned limitations. The process aims at finding the sequence of refactorings\nthat: (1) improves design quality; (2) preserves the design coherence and consistency\nof the refactored program; (3) minimizes code changes; and (4) maximizes the consis-\ntency with development change history. We evaluated our approach on six open-source\n\nsystems using an existing benchmark [Ouni et al. 2012al\nphos) We report the results of the efficiency and effectiveness of our approach, com-\npared to existing approaches [|[Harman and Tratt 2007;|Kessentini et al. 2011]. In ad-\ndition, we provide an industrial validation of our approach on a large-scale project\nin which the results were manually evaluated by 10 active software engineers. The\nstudy also evaluated the relevance and usefulness of our refactoring technique in an\nindustrial setting.\n\nThe remainder of this paper is structured as follows. Section |2| provides the nec-\nessary background and challenges related to refactoring and code smells. Section\ndefines refactoring recommendation as a multi-objective optimization problem,\n\nwhile Section |4]introduces our search-based approach to this problem using the non-\ndominated sorting genetic algorithm (NSGA-II) [Deb et al. 2002]. Section [5] describes\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:3\n\nthe method used in our empirical studies and presents the obtained results, while Sec-\ntion [6] provides further discussions. Section |7| presents an industrial case study long\nwith a discussion of the obtained results. Section 8] discusses the threats to validity\nand the limitations of the proposed approach, while Section |9| describes the related\nwork. Finally, Section[I0|concludes and presents directions for future work.\n\n2. CHALLENGES IN AUTOMATED REFACTORING RECOMMENDATION\nIn this section, we define the issues and challenges related to software refactoring.\n\n2.1. Background and definitions\nRefactoring is defined as the process of improving a code after it has been written by\n\nchanging its internal structure without changing its external behavior [Opdyke 1992].\nThe idea is to reorganize variables, classes and methods, mainly to facilitate future\nadaptations and extensions. This reorganization is used to improve various aspects of\n\nsoftware quality such as maintainability, extensibility, reusability, etc. [Fowler 1999)\nBaar and Markovi\u00e9 2007]. The refactoring process consists of 6 distinct steps [Baar\nand Markovic 2007]:\n\n(1) Identify where the software should be refactored.\n\n(2) Determine which refactoring(s) should be applied to the identified places.\n\n(3) Guarantee that the applied refactoring preserves behavior.\n\n(4) Apply the refactoring.\n\n(5) Assess the effect of the refactoring on quality characteristics of the software (e.g.,\ncomplexity, understandability, maintainability) or the process (e.g., productivity,\ncost, effort).\n\n(6) Maintain the consistency between the refactored program code and other software\nartifacts (such as documentation, design documents, requirement specifications,\ntests, etc.).\n\nWe focus in this paper on steps 1, 2 and 5. In order to find out which parts of the\nsource code need to be refactored, most existing work [Dhambri et al. 2008}|Moha et al.\nrelies on the notion of design\ndefects or bad smells. In this paper, we do not focus on the first step related to the\ndetection of refactoring opportunities. We assume that a number of different design\ndefects have already been detected, and need to be corrected. Typically, design defects,\n\nalso called anomalies [Brown et al. 1998], design flaws |Marinescu 2004], bad smells\n[Fenton and Pfleeger 1998], or anti-patterns [Fowler 1999], refer to design situations\n\nthat adversely affect the development of software. As stated by Fenton and Pfleeger\n\n[Fenton and Pfleeger 1998], design defects are unlikely to cause failures directly, but\nmay do so indirectly [Yamashita and Moonen 2013]. In general, they make a system\ndifficult to change, which may often introduce bugs. In this paper, we focus on the\n\nfollowing six design defect types [Brown et al. 1998} |Murphy-Hill and Black 2010)\nMantyla et al. 2003 :\n\n] to evaluate our approac\n\n\u2014 Blob: It is found in designs where much of the functionality of a system (or part of\nit) is centralized in one large class, while the other related classes primarily expose\ndata and provide little functionality.\n\n\u2014 Spaghetti Code: This involves a code fragment with a complex and tangled control\nstructure. This code smell is characteristic of procedural thinking in object-oriented\nprogramming. Spaghetti Code is revealed by classes declaring long methods with no\nparameters, and utilising global variables. Names of classes and methods may sug-\ngest procedural programming. Spaghetti Code does not exploit, and indeed prevents\nthe use of, object-oriented mechanisms such as inheritance and polymorphism.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:4 A. Ouni et al.\n\n\u2014Functional Decomposition: This design defect consists of a main class in which in-\nheritance and polymorphism are hardly used, that is associated with small classes,\nwhich declare many private fields and implement only a few methods. This is fre-\nquently found in code produced by inexperienced object-oriented developers.\n\n\u2014Data Class: It is a class that contains only data and performs no processing on\nthese data. It is typically composed of highly cohesive fields and accessors. However,\ndepending on the programming context some Data Classes might suit perfectly and,\ntherefore, not design defects.\n\n\u2014 Shotgun Surgery: This occurs when a method has a large number of external meth-\nods calling it, and these methods are spread over a significant number of classes. As\na result, the impact of a change in this method will be large and widespread.\n\n\u2014Feature Envy: It is found when a method heavily uses attributes and data from\none or more external classes, directly or via accessor operations. Furthermore, in\naccessing external data, the method uses data intensively from at least one external\nsource.\n\nWe choose these design defect types in our experiments because they are the most\nimportant and common ones in object-oriented industrial projects based on recent em-\npirical studies |Ouni et al. 2012a} Moha et al. 2008} Ouni et al. 2013]. Moreover, it\nis widely believed that design defects have a negative impact on software quality that\noften leads to bugs and failures [Li and Shatnawi 2007}|D\u2019Ambros et al. 2010}|Deligian-\n. Consequently, design defects should be identifie\nand corrected by the development team as early as possible for maintainability and\nevolution considerations. For example, after detecting a blob defect, many refactoring\noperations can be used to reduce the number of functionalities in a specific class, such\nas move method and extract class.\n\nIn the next subsection, we discuss the different challenges related to fixing design\ndefects using refactoring.\n\n2.2. Problem statement\n\nEven though most existing refactoring approaches are powerful enough to provide\nrefactoring solutions, some open issues need to be targeted to provide an efficient and\nfully automated refactoring recommendation.\n\nQuality improvement: Most of the existing approaches |Qayum and Heckel 2009\nO'Keeffe and Cinn\u00e9ide 2008}|Moha et al. 2008}|Seng et al. 2006] consider refactoring as\nthe process to improve code quality by improving structural metrics. However, these\n\nmetrics can be conflicting and it is difficult to find a compromise between them. For\nexample, moving methods to reduce the size or complexity of a class may increase the\nglobal coupling. Furthermore, improving some quality metrics does not guarantee that\nthe detected design defects are fixed. Moreover, there is no consensus about the metrics\nthat need to be improved in order to fix defects. Indeed, the same type of defect can be\nfixed by improving completely different metrics.\n\nSemantic coherence: In object-oriented programs, objects reify domain concepts\nand/or physical objects, implementing their characteristics and behavior. Methods and\nfields of classes characterize the structure and behavior of the implemented domain el-\nements. Consequently, a program could be syntactically correct, implement the appro-\npriate behavior, but violate the domain semantics if the reification of domain elements\nis incorrect. During the initial design/implementation, programs usually capture well\nthe domain semantics when object-oriented principles are applied. However, when\nthese programs are (semi-)automatically modified/refactored during maintenance, the\nadequacy with regards to domain semantics could be compromised. Indeed, semantic\ncoherence is an important issue to consider when applying refactorings.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:5\n\nMost of the existing approaches suggest refactorings mainly with the perspective of\nonly improving some design/quality metrics. As explained, this may not be sufficient.\nWe need to preserve the rationale behind why and how code elements are grouped and\nconnected when applying refactoring operations to improve code quality.\n\nCode changes: When applying refactorings, various code changes are performed.\nThe amount of code changes corresponds to the number of code elements (e.g., classes,\nmethods, fields, relationships, field references, etc.) modified through adding, deleting,\nor moving operations. Minimizing code changes when suggesting refactorings is im-\nportant to reduce the effort and help developers understand the modified/improved\ndesign. In fact, most developers want to keep as much as possible with the original\ndesign structure when fixing design defects Prowler 1989). Hence, improving software\nquality and reducing code changes are conflicting. In some cases, correcting some de-\nsign defects corresponds to changing radically a large portion of the system or is some-\ntimes equivalent to re-implementing a large part of the system. Indeed, a refactoring\nsolution that fixes all defects is not necessarily the optimal one due to the high code\nadaptation/modification that may be required.\n\nConsistency with development/maintenance history: The majority of the ex-\nisting work does not consider the history of changes applied in the past when propos-\ning new refactoring solutions. However, the history of code changes can be helpful in\nincreasing the confidence of new refactoring recommendations. To better guide the\nsearch process, recorded code changes applied in the past can be considered when\nproposing new refactorings in similar contexts. This knowledge can be combined with\nstructural and textual information to improve the automation of refactoring sugges-\ntions.\n\n2.3. Motivating example\n\nTo illustrate some of these issues, Figure [I] shows a concrete example extracted from\nJFreeChart}| v1.0.9, a well-known Java open-source charting library. We consider\na design fragment containing four classes XYLineAndShapeRenderer, XYDotRenderer,\nSegmentedTimeline, and XYSplineRenderer. Using design defect detection rules pro-\nposed in our previous work (Ressentini et al. 2017) the class XYLineAndShapeRenderer\nis detected as a design defect: blob (i.e., a large class that monopolizes the behavior of\na large part of the system).\n\nWe consider the scenario of a refactoring solution that consists of moving the\nmethod drawItem() from class XYLineAndShapeRenderer to class SegmentedTimeline.\nThis refactoring can improve the design quality by reducing the number of func-\ntionalities in this blob class. However, from the design semantics standpoint, this\nrefactoring is incoherent since SegmentedTimeline functionalities are related to pre-\nsenting a series of values to be used for a curve axis (mainly for Date related\naxis) and not for the task of drawing objects/items. Based on textual and structural\ninformation, using respectively a semantic lexicon (Amaro_et al. 2006}, and cohe-\nsion/coupling [Ouni et al. 2012b], many other target classes are possible including\nXYDotRenderer and XYSplineRenderer. These two classes have approximately the same\nstructure that can be formalized using quality metrics (e.g., number of methods, num-\nber of attributes, etc.) and their textual similarity is close to XYLineAndShapeRenderer\nusing a vocabulary-based measure. Thus, moving elements between these three\nclasses is likely to be semantically coherent and meaningful. On the other hand,\nfrom previous versions of JFreeChart, we recorded that there are some meth-\nods such as drawPrimaryLineAsPath(), initialise(), and equals() that have been\nmoved from class XYLineAndShapeRenderer to class XYSplineRenderer. As a conse-\n\n\u2018http //www.jfree.org/jfreechart/\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n\n",
                    "39:6 A. Ouni et al.\n\nSegmentedTimeline\n\nXYLineAndShapeRenderer | _|workingCalendar: Calendar\nsegmentSize : long\nstartTime : long\n\nserialVersionUID : long\nlinesVisible : Boolean\nlegendLine : Shape\n\nshapesVisible : Boolean getStarttime()\n\nuseFillPaint : boolean getBaseTimeline()\n\nuseOutlinePaint : boolean | | totimelineValue()\n\nbaseShapesfilled : boolean | | :oxiliisecond() XYDotRenderer\ndrawOutlines : boolean getSegmentsSize() serialVersionUID : long\nshapesFilled : Boolean clone() JdotWidth : int\nbaseShapesVisible: boolean! | equals() dotHeight : int\n\nlegendShape : Shape\n\ngetDrawSeriesLineAsPath() - =\n\nsetDrawSeriesLineAsPath() XYDotRenderer()\n\ngetPassCount() getDotWidth()\n\ngetLegendLine() t\u2014\u2014\u2014 setLegendShape()\n\ngetBaseShapesVisible() drawltem()\n\ngetSeriesShapesFilled() equals(Object)\n\ngetUseFillPaint() clone()\n\ninitialise() readObject()\n\ngetLinesVisible() writeObject()\n\nsetLinesVisible() we\n\ndrawitem()\n\ngetLegenditem() 3\nclone() juggested refactorings: '\n\ndrawPrimaryLine()\nsetDrawOutlines()\ngetUseFillPaint()\nsetUseOutlinePaint()\n\nmove method(XYLineAndShapeRenderer:: drawltem(), XYSplineRenderer) !\n\ndrawSecondaryPass() XYSplineRenderer\ngetLegenditem(int, int) points : Vector\nreadObject()\n\nprecision : int\n\nwriteObject()\ndrawPrimaryLine()\ndrawFirstPassShape()\n\nXYSplineRenderer()\ngetPrecision()\nsetPrecision()\n\ninitialise()\ndrawPrimaryLineAsPath()\nequals()\n\nsolveTridiag()\n\nDesign defect: Blob\n\nmove method(XYLineAndShapeRenderer:: initialise(), XYSplineRenderer)\nmove method(XYLineAndShapeRenderer:: equals(), XYSplineRenderer)\n\nFig. 1: Design fragment extracted from JFreeChart v1.0.9.\n\nquence, moving methods and/or attributes from class XYLineAndShapeRenderer to class\nXYSplineRenderer has higher correctness probability than moving methods or at-\ntributes to class XYDotRenderer or SegmentedTimeline.\n\nBased on these observations, we believe that it is important to consider additional\nobjectives rather than using only structural metrics to ensure quality improvement.\nHowever, in most of the existing work, design semantics, amount of code changes, and\ndevelopment history are not considered. Improving code structure, minimizing design\nincoherencies, reducing code changes, and maintaining consistency with development\nchange history are conflicting goals. In some cases, improving the program structure\ncould provide a design that does not make sense semantically or could change radically\nthe initial design. For this reasons, an effective refactoring strategy needs to find a\ncompromise between all of these objectives. These observations are the motivation for\nthe work described in this paper.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:7\n\n3. REFACTORING: A MULTI-OBJECTIVE PERSPECTIVE\n3.1. Overview\n\nOur approach aims at exploring a large search space to find refactoring solutions, i.e.,\na sequence of refactoring operations, to correct bad smells. The search space is deter-\nmined not only by the number of possible refactoring combinations, but also by the\norder in which they are applied. A heuristic-based optimization method is used to gen-\nerate refactoring solutions. We have four objectives to optimize: 1) maximize quality\nimprovement (bad smells correction); 2) minimize the number of design coherence er-\nrors by preserving the way code elements are semantically grouped and connected\ntogether; 3) minimize code changes needed to apply the refactorings; and 4) maximize\nthe consistency with development change history. We thus consider the refactoring\ntask as a multi-objective optimization problem using the non-dominated sorting ge-\nnetic algorithm (NSGA-II) (Deb et al. 2002)\n\nThe general structure of our approach is sketched in Figure [2] It takes as input the\nsource code of the program to be refactored, a list of possible refactorings that can\nbe applied (label A), a set of bad smell detection rules (label B) (Quni et-al- 20728,\nour technique for approximating code changes needed to apply refactorings (label C),\na set of textual and design coherence measures described in Section 3 (label D), and\na history of applied refactorings to previous versions of the system (label E). Our ap-\nproach generates as output a near-optimal sequence of refactorings that improves the\nsoftware quality by minimizing as much as possible the number of design defects, min-\nimizing code changes required to apply the refactorings, preserving design semantics,\nand maximizing the consistency with development change history. Our approach cur-\nrently supports eleven refactoring operations including move method, move field, pull\nup field, pull up method, push down field, push down method, inline class, extract\nmethod, extract class, move class, and extract interface (cf. Table[IIp [Fowler 1999], but\nnot all refactorings in the literatur\u00a2?| We selected these refactorings because they are\nthe most frequently used refactorings and they are implemented in most modern IDEs\nsuch as Eclipse and Netbeans. In the following, we describe the formal formulation of\nthe four objectives to optimize.\n\n3.2. Modeling the refactoring process as a multi-objective problem\n\n3.2.1. Quality. The Quality criterion is evaluated using the fitness function given in\nEquation |1| The quality value increases when the number of defects in the code is\nreduced after refactoring. This function returns the complement of the ratio of the\nnumber of design defects after refactoring (detected using bad smells detection rules)\nover the total number of defects that are detected before refactoring. The detection of\ndefects is based on some metrics-based rules according to which a code fragment can\nbe classified as a design defect or not (without a probability/risk score), i.e., 0 or 1, as\ndefined in our previous work [Kessentini et al. 2011} Ouni et al. 2012a]. The accuracy of\nthe genetic programming approach for code smells detection proposed in our previous\n\nstudies was an average of 91% of precision and 87% of recall on 8 large-scale systems.\nThe defect correction ratio function is defined as follows:\n\n# defects after applying refactorings\n\nDCR =1-\u2014\n# defects before applying refactorings\n\n()\n\n3.2.2. Code changes. Refactoring Operations (ROs) are classified into two types: Low-\n\nLevel ROs (LLR) and High-Level ROs (HLR) [Ouni et al. 2012a]. A HLR is a sequence\n*http://refactoring.com/catalog/\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n\n",
                    "39:8 A. Ouni et al.\n\nList of possible\nrefactorings\n\nDesign defects\ndetection rules\n\nOutpht: recorded refactorings\n\nCode changes\namount\napproximation\n\nInput:\nRefactoring efforts\n\nMulti-Objective Suggested\nSearch-based ones\n\n* refactorings\nInput: ; Refactoring\nsource code Semantic\nmeasures\n+ call graphs Outpht: semantic measures\n\nInput:\nList of previous\nprogram versions\n\nSimilarity with\ngood recorded\nrefactorings\n\nOutput: recorded refactorings\nFig. 2: Multi-objective search-based refactoring framework.\n\nof two or more ROs. An LLR is an elementary refactoring consisting of just one basic\nRO (e.g., Create Class, Delete Method, Add Field). The weight w; for each RO is an\ninteger number in the range [1, 2, 3] depending on code fragment complexity, and on\nchange impact. For a refactoring solution consisting of p ROs, the code changes score\nis computed as:\nPp\nCode_changes = > Wi (2)\ni=1\nTable[[|shows how the code change score is calculated for each refactoring operation.\nAs described in the table, to estimate the number of required code changes for a high\nlevel refactoring, our method considers the number of low level refactoring operations\n(atomic changes) needed to actually implement such a refactoring based on the Soot\ntool. For instance, to move a method m from a class c; to a class c2, the required number\nof chance is calculated as follows: 1 add method with a weight w; = 1, 1 delete method\nwith a w; = 1, n redirect method call with a w; = 2, and n redirect field access with\naw; = 2 as described in Table|I| Using appropriate static code analysis, Soot allows\nto easily calculate the value n, by capturing the number of field references/accesses\nfrom a method, the number of calls that should be redirected based on call graph), the\nnumber of return types and parameters of a method, as well as the control flow graph\nof a method, and so on.\n\n3.2.3. Similarity with recorded code changes. We defined the following function to calcu-\nlate the similarity score between a proposed refactoring operation and a recorded code\nchange:\n\nn\nSim_refactoring_history(RO) = > ej (3)\nj=l\nwhere n is the number of recorded refactoring operations applied to the system in the\npast, and ej is a refactoring weight that reflects the similarity between the suggested\nrefactoring operation (RO) and the recorded refactoring operation j. The weight e; is\ncomputed as follows: if the suggested and the recorded refactorings being compared are\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:9\n\nTable I: High and low level refactoring operations and their associated change scores.\n\nLow level refactoring\n\n3 3 7\n$ 2 5\n38 ,. 2 8B\nz Re a me]\n223 64 37 2 222 3 3 &\n3 3s @ ica \u2014o\u00ab FI & & & 3\noS Ss 8 F ZF &\u00a3 BB E ~\u00a2 %@ 2g 2g\n\u00a23i323 FPR GEESE\n2s 2s 83s 3 3 = E& & EE\n. . SoS Aa 4 A \u00a34 A Re He 4 HF Fe w\nHight level refactoring\nWeight wi 2 3 3 1 3 2 2 1 2 1 1 1\nMove method 1 1 n n\nMove field 1 1 n\nPull up field 1 1 n n\nPull up method 1 1 n n n\nPush down field 1 1 n n\nPush down method 1 1 n n n\nInline class 1 n\nExtract method 1 n n n n n\nExtract class 1 n n n n n n\nMove class 1 1 n n n\nExtract interface 1 n n n n n n n n\n\nidentical, e.g., Move Method between the same source and target classes, then weight\ne; = 2. If the suggested and the recorded refactorings are similar, then e; = 1. We\nconsider two refactoring operations as similar if one of them is composed of the other or\nif their implementations are similar, using equivalent controlling parameters, i.e., the\nsame code fragments, as described in Table Some complex refactoring operations,\nsuch as Extract Class can be composed of other refactoring operations such as Move\nMethod, Move Field, Create New Class, etc., the weight w; = 1. Otherwise, w; = 0.\nMore details about the similarity scores between refactoring operations can be found\n\nin [Ouni et al. 2013].\n\n3.2.4. Semantics. To the best of our knowledge, there is no consensual way to investi-\ngate whether refactoring can preserve the design semantics of the original program.\nWe formulate semantic coherence using a meta-model in which we describe the con-\ncepts from a perspective that helps in automating the refactoring recommendation\ntask. The aim is to provide a terminology that will be used throughout this paper.\nFigure[3|shows the semantic-based refactoring meta-model. The class Refactoring rep-\nresents the main entity in the meta-model. As mentioned earlier, we classify refactor-\ning operations into two types: low-level ROs (LLR) and high-level ROs (HLR). A LLR\nis an elementary/basic program transformation for adding, removing, and renaming\nprogram elements (e.g., Add Method, Remove Field, Add Relationship). LLRs can be\ncombined to perform more complex refactoring operations (HLRs) (e.g., Move Method,\nExtract Class). A HLR consists of a sequence of two or more LLRs or HLRs; for exam-\nple, to perform Extract Class we need to Create New Empty Class and apply a set of\nMove Method and Move Field operations.\n\nTo apply a refactoring operation we need to specify which actors, i.e., code fragments,\nare involved in this refactoring and which roles they play when performing the refac-\ntoring operation. As illustrated in Figure |3| an actor can be a package, class, field,\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:10 A. Ouni et al.\n\nTable II: Refactoring operations and their involved actors and roles.\n\nRefactoring operation Actors Roles\n\nclass source class, target class\nMove method method moved method\n\nclass source class, target class\nMove field field moved field\n\nclass source class, target class\nPull up field field moved field\n\nclass source class, target class\nPull up method method moved method\n\nclass source class, target class\nPush down field field moved field\n\nclass source class, target class\nPush down method method moved method\n\n| Inline class class source class, target class\n\nclass source class, target class\nExtract method method source method, new method\n\nstatement | moved statements\n\nclass source class, new class\nExtract class field moved fields\n\nmethod moved methods\nMove class package source package, target package\n\nclass moved class\n\nclass source classes, new interface\nExtract interface field moved fields\n\nmethod moved methods\n\nmethod, parameter, statement, or variable. In Table|II} we specify for each refactoring\noperation the involved actors and their roles.\n\nPackage f\u20ac\nO*\nLo\ncss (5\nHigh-level Me Low-level\no.* Refactoring [\u00a9 Refactoring 0.\"\nField\nWo. 0.\"\naccess\nActor:\nRefactoring Method\nperforms \u2018\u2018 \u00a5\nLay oll\ninvolves\n7 Role Statement\nLo\nhas\nConstraints\nT* _satisly\nParameter }\u20ac>\noo] Structural Semantic\nConstraints Constraints\n[ I T T 1\nPre-Condition | | Post-Condition Vocabulary ||Shared|| Shared || Implementatio||Feature inheritance ||Cohesion-based|\n1 1.\" |based similarity || fields || methods usefulness dependancy\n\nFig. 3: Semantics-based refactoring meta-model.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:11\n\n3.3. Design coherence measures\n\n3.3.1. Vocabulary-based similarity (VS). This kind of similarity is interesting to consider\nwhen moving methods, fields, or classes. For example, when a method has to be moved\nfrom one class to another, the refactoring would make sense if both actors (source class\nand target class) use similar vocabularies (Quni ef al. 2012). The vocabulary could be\nused as an indicator of the semantic/textual similarity between different actors that\nare involved when performing a refactoring operation. We start from the assumption\nthat the vocabulary of an actor is borrowed from the domain terminology and therefore\ncan be used to determine which part of the domain semantics an actor encodes. Thus,\ntwo actors are likely to be semantically similar if they use similar vocabularies.\n\nThe vocabulary can be extracted from the names of methods, fields, variables, pa-\nrameters, types, etc. Tokenisation is performed using the Camel Case Splitter\nlet al. 2012}, which is one of the most used techniques in Software Maintenance tools\n\nor the preprocessing of identifiers. A more pertinent vocabulary can also be extracted\nfrom comments, commit information, and documentation. We calculate the semantic\nsimilarity between actors using an information retrieval-based technique, namely co-\nsine similarity, as shown in Equation |4] Each actor is represented as an n-dimensional\nvector, where each dimension corresponds to a vocabulary term. The cosine of the angle\nbetween two vectors is considered as an indicator of similarity. Using cosine similarity,\nthe conceptual similarity between two actors c; and c2 is determined as follows:\n\nC1 + Wi X Wi\nSim(c1,\u00a22) = Cos(4,) = ~4-? _ = Dien Wi X We (4)\nHeil Heal \\/yre yw, x OL uP,\nwhere \u00a2j = (wi,1,-..,Wn,1) is the term vector corresponding to actor c; and 6 =\n\n(w1,2,-+-;Wn,2) is the term vector corresponding to c2. The weights wi,j can be com-\nputed using information retrieval based techniques such as the Term Frequency \u2014 In-\nverse Term Frequency (TF-IDF) method. We used a method similar to that described\nin to determine the vocabulary and represent the actors as term vectors.\n\n3.3.2. Dependency-based similarity (DS). We approximate domain semantics closeness\nbetween actors starting from their mutual dependencies. The intuition is that actors\nthat are strongly connected (i.e., having dependency links) are semantically related. As\na consequence, refactoring operations requiring semantic closeness between involved\nactors are likely to be successful when these actors are strongly connected. We con-\nsider two types of dependency links based on use the Jaccard similarity coefficient as\n\nthe way you compute the similarity |Jaccard 1901]:\n\n\u2014 Shared Field Access (SFA) that can be calculated by capturing all field references\nthat occur using static analysis to identify dependencies based on field accesses\n(read or modify). We assume that two software elements are semantically related\nif they read or modify the same fields. The rate of shared fields (read or modified)\nbetween two actors c; and cz is calculated according to Equation} In this equation,\nfieldRW (c;) computes the number of fields that may be read or modified by each\nmethod of the actor c;. Note that only direct field access is considered (indirect field\naccesses through other methods are not taken into account). By applying a suitable\nstatic program analysis to the whole method body, all field references that occur can\nbe easily computed.\n\n| fieldRW (cx) N fieldRW (c2)\n| fieldRW (cx) U fieldRW (c2)\n\nsharedFieldsRW (ci, c2) (5)\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:12 A. Ouni et al.\n\n\u2014 Shared Method Calls (SMC) that can be captured from call graphs derived from\nthe whole program using CHA (Class Hierarchy Analysis) Walleeraiet l. 2000}\nA call graph is a directed graph which represents the different calls (call in and call\nout) among all methods of the entire program. Nodes represent methods, and edges\nrepresent calls between these methods. CHA is a basic call graph that considers\nclass hierarchy information, e.g, for a call c.m/(...) assume that any m(...) is reachable\nthat is declared in a subtype or sometimes supertype of the declared type of c. For\na pair of actors, shared calls are captured through this graph by identifying shared\nneighbours of nodes related to each actor. We consider both, shared call-out and\nshared call-in. Equations Bjand[7Jare used to measure respectively the shared call-\nout and the shared call-in between two actors c; and c2 (two classes, for example).\n\n| callOut(c1) N callOut(c2)\n\nsharedCallOut(c1,\u00a2 >\nsharedCallOut(cr, c2) | callOut(c1) U callOut(c2)\n\n(6)\n\n| callIn(e1) M callIn(c2) |\n| eallIn(cy) U callIn(cg) |\n\nA shared method call is defined as the average of shared call-in and call-out.\n\n(7)\n\nsharedCallIn(c,, c2) =\n\n3.3.3. Implementation-based similarity (IS). For some refactorings like Pull Up Method,\nmethods having similar implementations in all subclasses of a super class should be\nmoved to the super class (Fowler 1999). The implementation similarity of the meth-\nods in the subclasses is investigated at two levels: signature level and body level. To\ncompare the signatures of methods, a semantic comparison algorithm is applied. It\nconsiders the methods names, the parameter lists, and return types. Let Sig(m;) be\nthe signature of method m;. The signature similarity for two methods m, and m2 is\ncomputed as follows:\n\n| Sig(ma) 1 Sig(ma) |\n| Sig(mi) U Sig(ma) |\n\nTo compare method bodies, we use Soot ||Vall\u00e9e-Rai et al. 2000], a Java optimization\nframework, which compares the statements in the body, the used local variables, the\nexceptions handled, the call-outs, and the field references. Let Body(m) (set of state-\n\nments, local variables, exceptions, call-outs, and field references) be the body of method\nm. The body similarity for two methods m, and mz is computed as follows:\n\n(8)\n\nSig_sim(m,,m2) =\n\n| Body(m1) 1 Body(ma) |\n| Body(m1) U Body(mz) |\n\nThe implementation similarity between two methods is the average of their Sig_Sim\nand Body_Sim values.\n\n(9)\n\nBody_sim(m1, m2)\n\n3.3.4. Feature inheritance usefulness (FIU) . This factor is useful when applying the Push\nDown Method and Push Down Field operations. In general, when method or field is\nused by only few subclasses of a super class, it is better to move it, i.e., push it down,\nfrom the super class to the subclasses using it [Fowler 1999]. To do this for a method,\nwe need to assess the usefulness of the method in the subclasses in which it appears.\n\nWe use a call graph and consider polymorphic calls derived using XTA (Separate Type\nAnalysis) Tip and Palsberg 2000]. XTA is more precise than CHA by giving a more\nlocal view of what types are available. We are using Soot [Vall\u00e9e-Rai et al. 2000] as a\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:13\n\nstandalone tool to implement and test all the program analysis techniques required in\nour approach. The inheritance usefulness of a method is given by Equation[10}\n\nLiki call(m, i) (10)\n\nn\n\nFIU(m,c) =1-\n\nwhere n is the number of subclasses of the superclass c, m is the method to be pushed\ndown, and call is a function that returns 1 if m is used (called) in the subclass i, and 0\notherwise.\n\nFor the refactoring operation Push Down Field, a suitable field reference analysis is\nused. The inheritance usefulness of a field is given by Equation{I]}\n\nSL, use(f, ci) ay\n\nn\n\nFIU(f,c) =1-\n\nwhere n is the number of subclasses of the superclass c, fis the field to be pushed down,\nand use is a function that return 1 if f is used (read or modified) in the subclass c;, and\n0 otherwise.\n\n3.3.5. Cohesion-based dependency (CD). We use a cohesion-based dependency measure\nfor the Extract Class refactoring operation. The cohesion metric is typically one of the\n\nimportant metrics used to identify and fix design defects |[Moha et al. 2010}|Moha et al.\n- How-\never, the cohesion-based similarity that we propose for code refactoring, in particular\nwhen applying extract class refactoring, is defined to find a cohesive set of methods\nand attributes to be moved to the newly extracted class. A new class can be extracted\nfrom a source class by moving a set of strongly related (cohesive) fields and methods\nfrom the original class to the new class. Extracting this set will improve the cohesion of\nthe original class and minimize the coupling with the new class. Applying the Extract\nClass refactoring operation on a specific class will result in this class being split into\ntwo classes. We need to calculate the semantic similarity between the elements in the\noriginal class to decide how to split the original class into two classes.\n\nWe use vocabulary-based similarity and dependency-based similarity to find the co-\nhesive set of actors (methods and fields). Consider a source class that contains n meth-\nods {mj,...mn,} and m fields {f1,...fm}. We calculate the similarity between each pair\nof elements (method-field and method-method) in a cohesion matrix as shown in Table\naul\n\nThe cohesion matrix is obtained as follows: for the method-method similarity, we\nconsider both vocabulary and dependency-based similarity. For the method-field simi-\nlarity, if the method m; may access (read or write) the field f;, then the similarity value\nis 1. Otherwise, the similarity value is 0. The column \u201cAverage\u201d\u201d contains the average\nof similarity values for each line. The suitable set of methods and fields to be moved to\na new class is obtained as follows: we consider the line with the highest average value\nand construct a set that consists of the elements in this line that have a similarity\nvalue that is higher than a threshold equals to 0.5. We used a trial and error strategy\nto find this suitable threshold value after executing our similarity measure more than\n30 times.\n\nOur decision to use such a technique is driven by the computation complexity since\nheavy and complex techniques might affect the whole search process. While cohesion is\none of the strongest metrics which is already used in related work [Fokaefs et al. 2011|\n[Bavotal\n\n0] for identifying extract class refactoring opportunities, we are planning to\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:14 A. Ouni et al.\n\nTable III: Example of a cohesion matrix.\n\nfi | fo +++ fm | mi m2 \u00abss mn Average\nmy, 1 0 1 1 0.15 0.1 0.42\nm2 0 1 I 1 I 0 0.6\nip I 0 0 0.6 0.2 T 0.32\n\ncombine it with coupling metric, in order to reduce coupling between the extracted\nclass and the original one.\n\n4. NSGA-II FOR SOFTWARE REFACTORING\n\nThis section is dedicated to describing how we encoded the problem of finding a good\nrefactoring sequence as an optimization problem using the non-dominated sorting ge-\n\nnetic algorithm NSGA-II [Deb et al. 2002].\n\n4.1. NSGA-II overview\n\nOne of the most powerful multi-objective search techniques is NSGA-II\nthat has shown good performance in solving several software engineering prob-\nems [Harman et al. 2012].\n\nA high-level view of NSGA-II is depicted in Algorithm[]] NSGA-II starts by randomly\ncreating an initial population P) of individuals encoded using a specific representation\n(line 1). Then, a child population Qo is generated from the population of parents Po\n(line 2) using genetic operators (crossover and mutation). Both populations are merged\ninto an initial population Ro of size N (line 5). Fast-non-dominated-sort\nis the technique used by NSGA-II to classify individual solutions into different\ndominance levels (line 6). Indeed, the concept of non-dominance consists of comparing\neach solution x with every other solution in the population until it is dominated (or\nnot) by one of them. According to Pareto optimality: \u201cA solution x; is said to dominate\nanother solution ro, if x; is no worse than x2 in all objectives and x, is strictly better\nthan \u00a32 in at least one objective\u201d. Formally, if we consider a set of objectives f; , i \u20ac 1..n,\nto maximize, a solution x; dominates 72 :\n\niff Vi, fi(w2) < fi(w1) and 33 | fi(w2) < fi (x1)\n\nThe whole population that contains N individuals (solutions) is sorted using the\ndominance principle into several fronts (line 6). Solutions on the first Pareto-front Fo\nget assigned dominance level of 0 Then, after taking these solutions out, fast-non-\ndominated-sort calculates the Pareto-front F, of the remaining population; solutions\non this second front get assigned dominance level of 1, and so on. The dominance level\nbecomes the basis of selection of individual solutions for the next generation. Fronts\nare added successively until the parent population P,; is filled with N solutions (line\n8). When NSGA-II has to cut off a front F; and select a subset of individual solutions\nwith the same dominance level, it relies on the crowding distance\nto make the selection (line 9). This parameter is used to promote diversity within the\npopulation. The crowding distance of a non-dominated solution serves for getting an\nestimate of the density of solutions surrounding it in the population. It is calculated by\nthe size of the largest cuboid enclosing each particle without including any other point.\nHence, the crowding distance mechanism ensures the selection of diversified solutions\nhaving the same dominance level. The front F; to be split, is sorted in descending\norder (line 13), and the first (N- |P,,:|) elements of F; are chosen (line 14). Then a\nnew population Q;+1 is created using selection, crossover and mutation (line 15). This\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:15\n\nAlgorithm 1 High level pseudo code for NSGA-II\n\n1: Create an initial population Po\n\n2: Create an offspring population Qo\n\n3: t=0\n\n4: while stopping criteria not reached do\nH 1=PpUQ\n\n5:\n\n6: F = fast-non-dominated-sort(R;)\n7 Pi =Oandi=1\n\n8: while | Pi41|+ | Fi |< N do\n\n9: Apply crowding-distance-assignment(F;)\n10: Pua = Py UF:\n\n11: t=it+l1\n\n12: end while\n\n13: Sort(Fi, < n)\n\n14.00 Pry = Pry UF [N\u2014- | Pras |]\n15:  Qi41 = create-new-pop(P:+1)\n16: t=t+1\n\n17: end while\n\nprocess will be repeated until reaching the last iteration according to stop criteria (line\n4).\n\n4.2. NSGA-II adaptation\nThis section describes how NSGA-II [Deb et al. 2002] can be used to find refactoring\n\nsolutions with multiple conflicting objectives. To apply NSGA-II to a specific problem,\nthe following elements have to be defined: representation of the individuals, creation of\na population of individuals, evaluation of individuals using a fitness function for each\nobjective to be optimized to determine a quantitative measure of their ability to solve\nthe problem under consideration, selection of the individuals to transmit from one\ngeneration to another, creation of new individuals using genetic operators (crossover\nand mutation) to explore the search space, generation of a new population.\n\nThe next sections explain the adaptation of the design of these elements for the\ngeneration of refactoring solutions using NSGA-II.\n\n4.2.1. Solution representation. To represent a candidate solution (individual), we used\na vector representation. Each vector\u2019s dimension represents a refactoring operation.\nThus, a solution is defined as a sequence of refactorings applied to different parts of\nthe system to fix design defects. When created, the order of applying these refactorings\ncorresponds to their positions in the vector. In addition, for each refactoring, a set of\ncontrolling parameters (stored in the vector), e.g., actors and roles, as illustrated in\nTable are randomly picked from the program to be refactored and stored in the\nsame vector. An example of a solution is presented in Figure/dal\n\nMoreover, when creating a sequence of refactorings (an individual), it is important\nto guarantee that they are feasible and that they can be legally applied. The first work\nin the literature was proposed by who introduced a way of formalizing\nthe preconditions that must be met before a refactoring can be applied and ensure that\nthe behavior of the system is preserved. Opdyke created functions which could be used\nto formalize constraints. These constraints are similar to the Analysis Functions used\nlater by [Cinn\u00e9ide 2001] and [Roberts and Johnson 1999].\n\nFor each refactoring operation we specify a set of pre- and post-conditions to ensure\nthe feasibility of applying them using a static analysis. For example, to apply the refac-\ntoring operation move method(Person, Employee, getSalary()), a number of necessary\npreconditions should be satisfied, e.g., Person and Employee should exists and should be\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:16 A. Ouni et al.\n\nclasses; getSalary() should exist and should be a method; classes Person and Employee\nshould not be in the same inheritance hierarchy; the method getSalary() should be\nimplemented in Person; the method signature of getSalary() should not be present\nin class Employee. As postconditions, Person, Employee, and getSalary() should ex-\nist; getSalary() declaration should be in class Employee; and getSalary() declaration\nshould not exist in class Person. Figure|4b| describes for each refactoring operation its\npre and post conditions that should be satisfied. To express these conditions we defined\na set of functions. These functions include:\n\n\u2014isClass(c): checks whether c is a class (similarly for areClasses()).\n\n\u2014isInterface(c): checks whether c is an interface (similarly for areInterfaces()).\n\n\u2014 isMethod(m): checks whether m is a method.\n\n\u2014Sig(m): returns the signature of the method m.\n\n\u2014 isField(f): checks whether f is a field.\n\n\u2014defines(c,e): checks whether the code element e (method or field) is implemented\nin the class/interface c.\n\n\u2014exists(e): checks whether the code element e exists in the current version of the\ncode model (Similarly for exist ()).\n\n\u2014 inheritanceHierarchy(ci,c2): checks whether both classes c1 and c2 belong to the\nsame inheritance hierarchy.\n\n\u2014 isSuperClassOf (c1,c2): checks whether ci is a superclass of c2.\n\n\u2014 isSubClassOf (c1,c2): checks whether ci is a subclass of c2.\n\n\u2014fields(c): returns the list of fields defined in the class or interface c.\n\n\u2014methods(c): returns the list of methods implemented in class or interface c.\n\nFor composite refactorings, such as extract class and inline class, the overall pre\nand post conditions should be checked. For a sequence of refactorings which may be\nof any length, we simplify the computation of its full precondition by analyzing the\nprecondition of each refactoring in the sequence and the corresponding effects on the\ncode model (postconditions).\n\n4.2.2. Fitness functions. After creating a solution, it should be evaluated using fitness\nfunction to ensure its ability to solve the problem under consideration. Since we have\nfour objectives to optimize, we are using four different fitness functions to include in\nour NSGA-II adaptation. We used the four fitness functions described in the previous\nsection:\n\n(1) Quality fitness function. It aims at calculating the number of fixed design de-\nfects after applying the suggested refactorings.\n\n(2) Design coherence fitness function. It aims at approximating the design preser-\nvation after applying the suggested refactorings. In Table |IV| we specify, for each\nrefactoring operation, which measures are taken into account to ensure that the\nrefactoring operation preserves design coherence.\n\n(3) Code changes fitness function. It calculates the amount of code changes re-\nquired to apply the suggested refactorings.\n\n(4) History of changes fitness function. It calculates the consistency of the sug-\ngested refactorings with prior code changes.\n\n4.2.3. Selection. To guide the selection process, NSGA-II uses a binary tournament\nselection based on dominance and crowding distance Deb et al. 2002}. NSGA-II sorts\nthe population using the dominance principle which classifies individual solutions into\ndifferent dominance levels. Then, to construct a new offspring population Q;1, NSGA-\nII uses a comparison operator based on a calculation of the crowding distance\nto select potential individuals having the same dominance level.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:17\n\nmove field (Person, Employee, salary)\n\nextract method (Person, printInfo(), printContactInfo())\n\nmove method (Person, Employee, getSalary())\npush down field (Person, Student, studentId)\ninline class (Car, Vehicle)\n\nmove method (Person, Employee, setSalary())\n\nmove field (Person, Employee, tax)\n\nextract,class(Person, Adress, streetNo, city, zipCode, getAdress(), updateAdress())\n\n(a) Solution representation.\n\nRefactorings Pre and post-conditions\nPre. | \u20acxist(cl,e2, m) AND areClasses(cl,\u00a22) AND isMethod(m) AND NOT(nheritanceHierarchy(cl,\u00a22))\nre\nMove Method(cl,c2,m) AND defines(cl,m) AND NOT(defines(c2,sig(m))\n\nPost: | exist(cl,c2,m) AND defines(c2,m) AND NOT(defines(c1,m))\n\nexist(cl, c2,f) AND areClasses(cl,c2) AND isField(f) AND NOT(inheritanceHierarchy(cl,c2))\nMove Field(cl,c2,f) AND defines(c1,f) AND NOT(defines(c2,f))\n\nPost: | exist(cl,c2, f) AND defines(c2,f) AND NOT(defines(cl,f))\n\nexist(cl, c2,f) AND areClasses(cl, c2) AND isField(f) AND isSuperClassOf(c2,c1) AND\nPull Up Field(c1,c2,f) * | defines(c1,f) AND NOT(defines(c2, f))\n\nPost: | exist(cl,c2,f) AND defines(c2,f) AND NOT(defines(c1,f))\n\nexist(cl,c2,m) AND areClasses(c1,c2) AND isMethod(m) AND isSuperClassOf(c2,cl) AND\nPull Up Method(cl,c2,m) | defines(cl,m) AND NOT(defines(c2,sig(m))\n\nPost: | exist(cl,c2,m) AND defines(c2,m) AND NOT(defines(c1,m))\n\nexist(cl,c2, f) AND areClasses (cl,c2) AND isField(f) AND isSubClassOf{c2,c1) AND\nPush Down Field(c1,c2,f) | defines(c1,f) AND NOT(defines(c2,f)\n\nPost: | exist(cl) AND exists(c2) AND exits(m) AND defines(c2,m) AND NOT(defines(c1,m))\nexist(cl,c2,m) AND areClasses (cl,c2) AND isMethod(m) AND isSubClassOf(c2,c1) AND\nPush Down Method(cl,c2,m) |__| defines(c1,m) AND NOT(defines(c2,sig(m))\n\nPost: | exist(cl,c2,m) AND defines(c2,m) AND NOT(defines(c1,m))\n\nPre: | exist(cl,c2) AND areClasses(cl,c2)\n\nPost: | exists(cl) AND NOT\u00a2(exists(c2))\n\nPre: | exists(cl) AND NOT(exists(c2)) AND isClass(cl) AND |methods(cl)|>2\n\nPost: | exist(cl,c2) AND isClass(c2)\n\nPre: | exists(cl) AND NOT(exists(c2)) AND isInterface(cl) AND |methods(c1)| >2\n\nPost: | exist(cl,c2) AND isInterface(c2)\n\nPre: | exists(cl) AND NOT(exists(c2)) AND isClass(cl) AND |methods(cl)|>2\n\nPost: | exist(cl,c2) AND isClass (c2) AND isSuperClass(c1,c2)\n\nPre: | exists(cl) AND NOT(exists(c2)) AND isClass(cl) AND |methods(cl)|>2\n\nPost: | exist(cl,c2) AND isClass(c2) AND isSubClass(cl,c2)\n\nInline Class(c1,c2)\n\nExtract Class(cl,c2)\n\nExtract Interface(c1,c2)\n\nExtract Super Class(cl,c2)\n\nExtract Sub Class(cl,c2)\n\n(b) Pre- and post- conditions of refactorings.\n\nFig. 4: Representation of an NSGA-II individual and used constraints.\n\n4.2.4. Genetic operators. To better explore the search space, crossover and mutation\noperators are defined.\n\nFor crossover, we use a single, random, cut-point crossover. It starts by selecting and\nsplitting at random two parent solutions. Then crossover creates two child solutions by\nputting, for the first child, the first part of the first parent with the second part of the\nsecond parent, and, for the second child, the first part of the second parent with the\nsecond part of the first parent. This operator must ensure that the length limits are re-\nected by eliminating randomly some refactoring operations. As illustrated in Figure\ncrossover splits the parent solutions in the position i = 3 within their representa-\ntive vectors in order to generate new child solutions. Each child combines some of the\nrefactoring operations of the first parent with some ones of the second parent. In any\ngiven generation, each solution will be the parent in at most one crossover operation.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:18 A. Ouni et al.\n\nTable IV: Refactoring operations and their semantic measures.\n\nRefactorings vs DS Is FIU cD\nmove method x x\nmove field x x\npull up field x x x\npull up method x x x\npush down field x x x\npush down method x x x\ninline class x x\nextract class x x x\nmove class x x\nextract interface x x x\nmove field move field\nextract class extract class\nParent 1 | move method Child 1 | move method\npull up field move field\nextract class extract class\ninline class Crossover\nmove method\nmove method (i= 3) inline class\nParent 2 inline class Child 2 push down field\npush down field pull up field\nmove field extract class\nextract class inline class\nBefore crossover After crossover\n\nFig. 5: Crossover operator.\n\nThe mutation operator picks randomly one or more operations from a sequence and\nreplaces them with other ones from the initial list of possible refactorings. An example\nis shown in Figure [6|where a mutation operator is applied with two random positions\nto modify two dimensions of the vector in the third and the fifth dimensions (j = 3 and\nk=5).\n\nmove field move field\nParent Child\n\nextract class Mutati extract class\nmove method uration move field\npull up field pull up field\nextract class (=3, k=5) move method\ninline class inline class\nBefore mutation After mutation\n\nFig. 6: Mutation operator.\n\nAfter applying genetic operators (mutation and crossover), we verify the feasibility\nof the generated sequence of refactoring by checking the pre and post conditions. Each\nrefactoring operation that is not feasible due to unsatisfied preconditions will be re-\nmoved from the generated refactoring sequence. The new sequence is considered valid\nin our NSGA-II adaptation if the number of rejected refactorings is less than 5% of the\ntotal sequence size. We used trial and error to find this threshold value after several\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:19\n\nexecutions of our algorithm. The rejected refactorings will not be considered anymore\nin the solution.\n\n5. VALIDATION AND EXPERIMENTATION DESIGN\n\nIn order to evaluate the feasibility and the efficiency of our approach for generating\ngood refactoring suggestions, we conducted an experiment based on different versions\nof open-source systems. We start by presenting our research questions. Then, we de-\nee discuss the obtained results. All experimentation materials are available\nonlin\n\n5.1. Research questions\n\nIn our study, we assess the performance of our refactoring approach by determining\nwhether it can generate meaningful sequences of refactorings that fix design defects\nwhile minimizing the number of code changes, preserving the semantics of the design,\nand reusing, as much as possible a base of recorded refactoring operations applied\nin the past in similar contexts. Our study aims at addressing the research questions\noutlined below.\n\nThe first four research questions evaluate the ability of our proposal to find a com-\npromise between the four considered objectives that can lead to good refactoring rec-\nommendation solutions.\n\n\u2014RQl1.1: To what extent can the proposed approach fix different types of design de-\nfects?\n\n\u2014 RQI1.2: To what extent does the proposed approach preserve design semantics when\nfixing defects?\n\n\u2014RQ1.3: To what extent can the proposed approach minimize code changes when\nfixing defects?\n\n\u2014RQI1.4: To what extent can the use of previously-applied refactorings improve the\neffectiveness of the proposed refactorings?\n\n\u2014 RQ2: How does the proposed multi-objective approach based on NSGA-II perform\ncompared to other existing search-based refactoring approaches and other search\nalgorithms?\n\n\u2014RQ3: How does the proposed approach perform compared to existing approaches\nnot based on heuristic search?\n\n\u2014RQz4: Is our multi-objective refactoring approach useful for software engineers in\nreal-world setting?\n\nTo answer RQ1.1, we validate the proposed refactoring operations to fix design de-\nfects by calculating the defect correction ratio (DCR) on a benchmark composed of six\nopen-source systems. DCR is given by Equation |1| which corresponds to the comple-\nment of the ratio of the number of design defects after refactoring (detected using bad\nsmells detection rules) over the total number of defects that are detected before refac-\ntoring.\n\nTo Snswer RQ1.2, we use two different validation methods: manual validation and\nautomatic validation to evaluate the efficiency of the proposed refactorings. For the\nmanual validation, we asked groups of potential users of our refactoring tool to eval-\nuate, manually, whether the suggested refactorings are feasible and make sense se-\nmantically. We define the metric \u201crefactoring precision\u201d (RP), which corresponds to the\nnumber of meaningful refactoring operations (low-level and high-level), in terms of\nsemantics, over the total number of suggested refactoring operations. RP is given by\n\nhttp ://www-personal.umd.umich.edu/~marouane/tosemref.html\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n\n",
                    "39:20 A. Ouni et al.\n\nEquation{12}\n\n___# coherent refactorings\n# suggested refactorings\n\n\u20ac [0,1] (12)\n\nFor the automatic validation we compare the proposed refactorings with the ex-\npected ones using an existing benchmark |Ouni et al. 2012a}|Moha et al. 2010} Moha|\nin terms of recall (Equation and precision ( quation (14). The expected\nrefactorings are those applied by the software development team to the next software\nrelease. To collect these expected refactorings, we use Ref-Finder (Prete et al. 2010),\nan Eclipse plug-in designed to detect refactorings between two program versions. Ref-\n\nFinder allows us to detect the list of refactorings applied to the current version of a\nsystem (see Table [VIp.\n\n| suggested refactorings M expected refactorings |\n| expected refactorings |\n\nRE yecall \u20ac [0,1] (13)\n\n| suggested refactorings N expected refactorings |\n| suggested refactorings |\n\nREprecision = \u20ac [0,1] (14)\nThe intuition behind this metric is to assess whether the suggested refactorings are\nsimilar to the ones that a programmer would expect and perform.\n\nTo answer RQ1.3, we evaluate, using our benchmark, if the proposed refactorings\nare useful to fix detected defects with low code changes by calculating the code change\nscore. The code change score is calculated using our model described in Section\nWe then compare the obtained code change scores with and without integrating the\ncode change minimization objective in our tool.\n\nTo answer RQ1.4, we use the metric RP to evaluate the usefulness of the recorded\nrefactorings and their impact on the quality of the suggested refactorings in terms of\ndesign coherence (RP). Consequently, we compare the obtained code RP scores with\nand without integrating the reuse of recorded refactorings in our tool. In addition, in\norder to evaluate the importance of reusing recorded refactorings in similar contexts,\nwe define the metric \u201creused refactoring\" (RR) that calculates the percentage of oper-\nations from the base of recorded refactorings used _to generate the optimal refactoring\nsolution by our proposal. RR is given by Equation[15]\n\n#used refactorings from the base of recorded refactorings\n\nRR #refactorings in the base of recorded refactorings\n\n\u20ac [0,1] (15)\n\nTo answer RQ2, we compared our approach to two other existing search-based refac-\n\ntoring approaches: (7) Kessentini et al. (Kessentini et al. 2011), and (ii) Harman et al.\n{Harman and Tratt 2007] that consider the refactoring suggestion task only from the\nquality improvement perspective. Kessentini et al. formulated refactoring suggestion\nas a single objective problem to reduce as much as possible the number design de-\n\nfects, while Harman et al. formulated refactoring recommendation as multi-objective\nto find a trade-off between two quality metrics, CBO (coupling between objects) and\n\nSDMPC (standard deviation of methods per class). Both approaches [Kessentini et al.|\n2011] [Harman and Tratt 2007] did not consider the design coherence, the history o\nchang\n\nes and the required effort when suggesting refactorings. Moreover, we assessed\n\nthe performance of our multi-objective algorithm NSGA-II compared to another multi-\n\nobjective algorithm (i) MOGA, (ii) random search, and (ii) mono-objective genetic algo-\n\nrithm (GA) where one fitness function is used (an average of the four objective func-\ntions).\n\nTo answer RQ3, we compared our refactoring results with a popular design defects\n\ndetection and correction tool JDeodorant (Fokaefs ot al. 2011}|Fokaefs ot al. 2012) that\n\ndoes not use heuristic search techniques in terms of DCR, change score and RP. The\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:21\n\ncurrent version of JDeodorant |Fokaefs et al. 2012] is implemented as an Eclipse plug-\n\nin that identifies some types of design defects using quality metrics and then proposes\na list of refactoring strategies to fix them.\n\nTo answer RQ4, we asked 6 software engineers (2 groups of 3 developers each) to\nrefactor manually some of the design defects, and then compare the results with those\nproposed by our tool. We, thus, define the following precision metric:\n\n| RiARm |\nRn\n\nwhere R; is the set of refactorings suggested by our tool, and R,, is the set of refactor-\nings suggested manually by software engineers. We calculated an exact matching score\nwhen comparing between the parameters (i.e., actors as described in Table [i of the\nrefactoring suggested by our approach and the ones identified by developers. However,\nwe do not consider the order of the parameters in the comparison formula.\n\nPrecision =\n\n\u20ac [0,1] (16)\n\n5.2. Experimental setting and instrumentation\n\nThe goal of the study is to evaluate the usefulness and the effectiveness of our refac-\ntoring tool in practice. We conducted an evaluation with potential users of our tool.\nThus, refactoring operations should not only remove design defects, but should also be\nmeaningful from a developer\u2019s point of view.\n\n5.2.1. Subjects. Our study involved a total number of 24 subjects divided into 8 groups\n(3 subjects each). All the subjects are volunteers and familiar with Java development.\nThe experience of these subjects on Java programming ranged from 2 to 15 years.\nThe participants who evaluated the open source systems have a good knowledge about\nthese systems and they did similar experiments in the past on the same systems. We\nselected also the groups based on their familiarity with the studied systems.\n\nThe first six groups are drawn from several diverse affiliations: the University of\nMichigan (USA), University of Montreal (Canada), Missouri University of Science and\nTechnology (USA), University of Sousse (Tunisia) and a software development and web\ndesign company. The groups include 4 undergraduate students, 7 master students, 8\nPhD students, one faculty member, and 4 junior software developers. The three master\nstudents are working also at General Motors as senior software engineers. Subjects\nwere familiar with the practice of refactoring.\n\n5.2.2. Systems studied and data collection. We applied our approach to a set of six\nwell-known_and well-commented industrial open source Java projects: Xerces-\nJFreeChart? GanttProject\u00a5| Apache Ant} J HotDraw_] and Rind Xerces-J is a fam-\nily of software packages for parsing XML. JFreeChart is a powerful and flexible Java\nlibrary for generating charts. GanttProject is a cross-platform tool for project schedul-\ning. Apache Ant is a build tool and library specifically conceived for Java applications.\nJHotDraw is a GUI framework for drawing editors. Finally, Rhino is a JavaScript in-\nterpreter and compiler written in Java and developed for the Mozilla/Firefox browser.\nWe selected these systems for our validation because they range from medium to large-\nsized open-source projects, which have been actively developed over the past 10 years,\n\nThttp://xerces.apache.org/xerces-j/|\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:22 A. Ouni et al.\n\nTable V: Programs statistics\n\nSystems Release #classes # design defects KLOC\nXerces-J v2.7.0 991 91 240\nJFreeChart v1.0.9 521 72 170\nGanttProject v1.10.2 245 49 41\nApache Ant v1.8.2 1191 112 255\nJHotDraw v6.1 585 25 21\n\nRhino v1.7R1 305 69 42\n\nTable VI: Analysed versions and refactorings collection\n\nExpected refactorings Collected refactorings\nSystems\nNext release #Refactorings Previous releases # Refactorings\nXerces-J v2.8.1 39 v1.4.2 - v2.7.0 70\nJFreeChart -v1.0.11 31 v1.0.6 - v1.0.9 76\nGanttProject v1.11.2 46 v1.7 - v1.10.2 91\nApache Ant v1.8.4 78 v1.2 - v1.8.2 247\nJHotDraw v6.2 27 v5.1 - v6.1 64\nRhino 1.7R4 46 v1.4R3 - 1.7R1 124\n\nand their design has not been responsible for a slowdown of their developments. Table\n[V] provides some descriptive statistics about these six programs.\n\nTo collect refactorings applied in previous program versions, and the expected refac-\ntorings applied to next version of studied systems, we use Ref-Finder [Prete et al.|\n[2010]. Ref-Finder, implemented as an Eclipse plug-in, can identify refactoring opera-\ntions applied between two releases of a software system. Table|VI|reports the analyzed\nversions and the number of refactoring operations, identified by Ref-Finder, between\neach subsequent couple of analyzed versions, after the manual validation. In our study,\nwe consider only refactoring types described in Table[I]]|\n\n5.2.3. Scenarios. We designed the study to answer our research questions. Our exper-\nimental study consists of two main scenarios: (1) the first scenario is to evaluate the\nquality of the suggested refactoring solutions with potential users (RQ1-3), and (2) the\nsecond scenario is to fix manually a set of design defects and compare the manual\nresults with those proposed by our tool (RQ4). All the recommended refactorings are\nexecuted using the Eclipse platform.\n\nAll the software engineers who accepted an invitation to participate in the study, re-\nceived a questionnaire, a manuscript guide that helps to fill the questionnaire, and the\nsource code of the studied systems, in order to evaluate the relevance of the suggested\nrefactorings to fix. The questionnaire is organized in an excel file with hyperlinks to\nvisualize the source code of the affected code elements easily. The participants were\nable to edit and navigate the code through Eclipse.\n\nScenario 1: The groups of subjects were invited to fill a questionnaire that aims\nto evaluate our suggested refactorings. The questionnaires rely on a four-point Lik-\nert scale in which we offered a choice of pre-coded responses for every\nquestion with no \u2018neutral\u2019 option. Thereafter, we assigned to each group a set of refac-\ntoring solutions suggested by our tool to evaluate manually. The participants were\nable to edit and navigate the code through the Eclipse IDE. Table [VII] describes the\nset of refactoring solutions to be evaluated for each studied system in order to an-\nswer our research questions. We have three multi-objective algorithms to be tested\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:23\n\nTable VII: Refactoring solutions for each studied system considering each objective:\nquality (Q), Semantic coherence (S), Code changes (CC) , Recorded refactorings (RR),\nCBO (Coupling Between Objects) and SDMPC (Standard Deviation of Methods Per\nClass).\n\nRef. Solution Algorithm/ Approach # objective Functions Objectives considered\n\nSolution 1 NSGA-II 4 Q,8, CC, RR\nSolution 2 MOGA 4 Q,8, CC, RR\nSolution 3 Random Search (RS) 4 Q, 5S, CC, RR\nSolution 4 Genetic Algorithm 1 Q+S+CC+RR\nSolution 5 Kessentini et al. 1 Q\nSolution 6 Harman et al. 2 CBO, SDMPC\nfor the refactoring suggestion task: NSGA-II (Non-dominated Sorting Genetic Algo-\nrithm) (De al. 2003 MOGA (Multi-Objective Genetic Algorithm)\n, and RS (Random Search) [Zitzler and Thiele 1998]. Moreover, we compared our\n\nresults with a mono-objective genetic algorithm to assess the need for a multi-\nobjective formulation. In addition, two refactoring solutions of both state-of-the art\nworks (Kessentini et al and Harman et al.\n) are empirically evaluated in order to compare them to our approach in terms of\ndesign coherence.\n\nAs shown in Table[WI] for each system, 6 refactoring solutions have to be evaluated.\nDue to the large number of refactoring operations to be evaluated (36 solutions in\ntotal, each solution consists of a large set of refactoring operations), we pick at random\na sample of 10 sequential refactorings per solution to be evaluated in our study. In\nTable we summarize how we divided subjects into groups in order to cover the\nevaluation of all refactoring solutions. In addition, as illustrated in Table we are\nusing a cross-validation for the first scenario to reduce the impact of subjects (groups\nA-F) on the evaluation. Each subject evaluates different refactoring solutions for three\ndifferent systems.\n\nSubjects (groups A-F) were aware that they are going to evaluate the design coher-\nence of refactoring operations, but do not know the particular experiment research\nquestions (algorithms used, different objectives used and their combinations). Con-\nsequently, each group of subjects who accepted to participate to the study, received\na questionnaire, a manuscript guide to help them to fill the questionnaire, and the\nsource code of the studied systems, in order to evaluate 6 solutions (10 refactorings\nper solution). The questionnaire is organized within a spreadsheet with hyperlinks to\nvisualize easily the source code of the affected code elements. Subjects are invited to\nselect for each refactoring operation one of the possibilities: \u201cYes\u201d (coherent change),\n\u201cNo\u201d (non-coherent change), or \u201cMay be\u201d (if not sure). All the study material is avail-\nable in [Deb et al. 2002). Since the application of refactorings to fix design defects is a\nsubjective process, it is normal that not all the programmers have the same opinion.\nIn our case, we considered the majority of votes to determine if a suggested refactoring\nis correct or not.\n\nScenario 2: The aim of this scenario is to compare our refactoring results for fixing\ndesign defects suggested by our tool with manual refactorings identified by developers.\nThereafter, we asked two groups of subjects (groups G and H) to fix a set of 72 design\ndefect instances that are randomly selected from each subject system (12 defects per\nsystem) covering all the six different defect types considered. Then we compared their\nsequences of refactorings that are suggested manually with those proposed by our\napproach. The more our refactorings are similar to the manual ones, the more our tool\nis assessed to be useful and efficient in practice.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:24 A. Ouni et al.\n\nTable VIII: Survey organization.\n\nScenarios Subject Systems Algorithm / Approach | Solutions\ngroups\n. NSGA-II Solution 1\nGanttProject Genetic Algorithm Solution 4\nGroup A | Xerces MOGA Solution 2\nuP Harman et al. Solution 6\nRS Solution 3\nJFreeChart Kessentini et al. Solution 5\n. MOGA Solution 2\nGanttProject Harman et al. Solution 6\nRS Solution 3\nGroup B | Xerces Kessentini et al. Solution 5\nNSGA-II Solution 1\nJFreeChart Genetic Algorithm Solution 4\n. RS Solution 3\nGanttProject Kessentini et al. Solution 5\nGroup C | Xerces NSGA-II Solution 1\nuP Genetic Algorithm Solution 4\nMOGA Solution 2\nScenario 1 JFreeChart Harman et al. Solution 6\n\u2018ApacheAnt NSGA-II Solution 1\nP Genetic Algorithm Solution 4\nMOGA Solution 2\nGroup D | JHotDraw Harman et al. Solution 6\nRhino RS Solution 3\nKessentini et al. Solution 5\nMOGA Solution 2\nApacheAnt Harman et al. Solution 6\nRS, Solution 3\nGroup E | JHotDraw Kessentini et al. Solution 5\nRhino NSGA-II Solution 1\n. Genetic Algorithm Solution 4\nRS Solution 3\nApacheAnt Kessentini et al. Solution 5\nNSGA-II Solution 1\nGroup F | JHotDraw Genetic Algorithm Solution 5\nRhino MOGA, Solution 2\nt Harman et al. Solution 6\nManual correction of\n. Group G | All systems design defects NA.\nScenario 2 Manual correction of\nGroup H | All systems design defects NA.\n\n5.2.4. Algorithms configuration. In our experiments, we use and compare different mono\nand multi-objective algorithms. For each algorithm, to generate an initial population,\nwe start by defining the maximum vector length (maximum number of operations per\nsolution). The vector length is proportional to the number of refactorings that are con-\nsidered, the size of the program to be refactored, and the number of detected design\ndefects. A higher number of operations in a solution does not necessarily mean that the\nresults will be better. Ideally, a small number of operations should be sufficient to pro-\nvide a good trade-off between the fitness functions. This parameter can be specified by\nthe user or derived randomly from the sizes of the program and the employed refactor-\ning list. During the creation, the solutions have random sizes inside the allowed range.\nFor all algorithms NSGA-II, MOGA, Random search (RS), and genetic algorithm (GA),\nwe fixed the maximum vector length to 700 refactorings, and the population size to 200\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:25\n\nindividuals (refactoring solutions), and the maximum number of iterations to 6,000 it-\nerations. We also designed our NSGA-II adaptation to be flexible in a way that we can\nconfigure the number of objectives and which objectives to consider in the execution.\n\nWe consider a list of 11 possible refactorings to restructure the design of the original\nprogram by moving code elements (methods, attributes) from classes in the same or\ndifferent packages or inheritance hierarchies or splitting/merging classes/interfaces.\nAlthough we believe that our list of refactorings is sufficient at least to fix these specific\ntypes of code smells, our refactoring tool is developed in a flexible way so that new\nrefactorings and code smell types can be considered in the future. Moreover, our list\nof possible refactoring is significantly larger than those of existing design defect fixing\ntechniques.\n\nAnother element that should be considered when comparing the results of the four\nalgorithms is that NSGA-II does not produce a single solution like GA, but a set of\noptimal solutions (non-dominated solutions). The maintainer can choose a solution\nfrom them depending on their preferences in terms of compromise. However, at least\nfor our evaluation, we need to select only one solution. Thereafter, and in order to fully\nautomate our approach, we proposed to extract and suggest only one best solution from\nthe returned set of solutions. In our case, the ideal solution has the best value of quality\n(equal to 1), of design coherence (equal to 1), and of refactoring reuse (equal to 1), and\ncode changes (normalized value equal to 1). Hence, we select the nearest solution to\n\nthe ideal one in terms of Euclidian distance, as described in |Ouni et al. 2012b].\n\n5.2.5. Inferential Statistical Test Methods Used. Our approach is stochastic by nature, i.e.,\ntwo different executions of the same algorithm with the same parameters on the same\nsystems generally leads to different sets of suggested refactorings. For this reason,\nour experimental study is performed based on 31 independent simulation runs for\neach problem instance, and the obtained results are statistically analyzed by using\nthe Wilcoxon rank sum test with a 95% confidence level (a = 0.05). The Wilcoxon\nsigned-rank test is a non-parametric statistical hypothesis test used when comparing\ntwo related samples to verify whether their population mean-ranks differ or not. In\nthis way, we could decide whether the difference in performance between our approach\nand the other detection algorithms is statistically significant or just a random result.\n\nThe Wilcoxon rank sum test allows verifying whether the results are statistically\ndifferent or not. However, it does not give any idea about the difference magnitude.\nWe, thus, investigate the effect size using the Cliff\u2019s Delta statistic [Cliff 1993]. The\neffect size is considered: (1) negligible if | d |< 0.147, (2) small if 0.147 <] d |< 0.33, (3)\nmedium if 0.33 <| d |< 0.474, or (4) large if | d |> 0.474.\n\n5.3. Empirical study results\nThis section reports the results of our empirical study, which are further discussed\nin the next sections. We first start by answering our research questions. We use two\ndifferent validations: manual and automatic validations to evaluate the efficiency of\nthe proposed refactorings.\n\nResults for RQ1.1: As described in Table after applying the proposed refac-\ntoring operations by our approach (NSGA-I]), we found that, on average, 84% of the\ndetected defects were fixed (DCR) for all the six studied systems. This high score is con-\nsidered significant in terms of improving the quality of the refactored systems by fixing\nthe majority of defects of various types (blob, spaghetti code, functional decomposition,\ndata class, shotgun surgery, and future envy\net al. 2011)). For the different systems, the total number of refactorings generated by\nour approach was between 91 and 119 as described in the refactoring precision (RP)\ncolumn of Table Furthermore, we assessed the required time to implement the\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:26 A. Ouni et al.\n\nsuggested refactorings on three systems. The average time required by 6 of the partic-\nipants in our experiments to implement all the suggested refactorings was 11.5 hours\nper developer for each system, including the time required to understand and inspect\nthe code before and after applying the refactorings. We believe that this required time\nis quite acceptable comparing to the time that the developer may spend to identify\nthese refactoring opportunities manually from hundreds or thousands of classes and\nmillions of lines of code. In addition, while the effect of refactoring is clearly trans-\nlated by fixing the vast majority of design defects (84%) and significantly improving\nquality factors (see Section|6.1}, other effects on the systems quality (maintainability,\nextendibility, etc.) cannot be assessed immediately.\n\nResults for RQ1.2: To answer RQ1.2, we evaluated the correctness/meaningful-\nness of the suggested refactorings from the developers\u2019 point of view. We reported the\nresults of our empirical evaluation in Table (RP column) related to Scenario 1. On\naverage, for all of our six studied systems, 80% of proposed refactoring operations are\nconsidered by potential users to be semantically meaningful and do not generate de-\nsign incoherence. We also automatically evaluated our approach. Thus, we compared\nthe proposed refactorings with the expected ones. The expected refactorings are those\napplied by the software development team for the next software release as described\nin Table We used Ref-Finder to identify refactoring operations\nthat are applied between the program version under analysis and the next version. Ta-\nble [EX] (RP-automatic column) summarizes our results. We found that a considerable\nnumber of proposed refactorings (an average of 36% for all studied systems in terms\nof recall) were already applied to the next version by a software development team.\nOf course, this precision score is low because that not all refactorings applied to next\nversion are related to quality improvement, but also to add new functionalities, in-\ncrease security, fix bugs, etc. Moreover, the obtained results provide evidence that our\napproach is relatively stable through different executions as the standard deviation is\nstill less than 3.23 in terms of DCR, 3.09 in terms of RP-automatic and 123.3 in terms\nof code change:\n\nTo conclude, we found that our approach produces good refactoring suggestions in\nterms of defect-correction ratio, design coherence from the point of view of (1) potential\nusers of our refactoring tool and (2) expected refactorings applied to the next program\nversion.\n\nResults for RQ1.3 and RQ1.4: To answer these two research questions, we need\nto compare different objective combinations (two, three, or four objectives) to ensure\nthe efficiency and the impact of using each of the objectives we defined. We executed\nthe NSGA-II algorithm with different combinations of objectives: maximize quality\n(Q), minimize design incoherence (S), minimize code changes (CC), and maximize the\nreuse of recorded refactorings (RR) as presented in Table|X/and Figure[7|\n\nTo answer RQ1.3, we present in Figure[7a]and Table [X] the code change scores ob-\ntained when the CC objective is considered (Q+S+RC+CC). We found that our approach\nsucceeded in suggesting refactoring solutions that do not require high code changes\n(an average of only 2,937) with a relatively stable standard deviation of while having\nmore than 3,888 as a code change score when the CC objective is not considered in the\nother combinations. At the same time, we found that the DCR score (Figure (Zc) is not\nsignificantly affected with and without considering the CC objective.\n\nTo answer RQ1.4, we present the obtained results in Figure[7b] The best RP scores\nare obtained when the recorded code changes (RC) are considered (Q+S+RC), while\n\n1\u00b0Note that only for the RP metric, we did not report the standard deviation as we directly conducted the\nqualitative evaluation with subjects on the suggested refactoring solution having the median DCR score\nfrom 31 independent runs.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:27\n\nTable IX: Empirical study results on 31 runs (Median & STDev). The results was sta-\ntistically significant on 31 independent runs using the Wilcoxon rank sum test with a\n95% confidence level (p \u2014 value < 0.05) in terms of defect correction ratio (DCR), code\nchanges score, refactoring precision (RP), and RP-automatic.\n\nSystems Approach DCR Code changes RP RP-automatic\ny PP Median | STDev | Median | STDev Median | STDev\n83% : 81% 26%\nNSGA-II (76191) 1.58 3,843 | 123.3 (74191) | (10139) 2.09\nHarman et al. 07 NA NA 2,669 78.4 41% el 30) 2.09\nXerces K tini et al. 711 89% 2.24 4,998 | 102.8 37 % 13% 2.97\nessentini et al. (81/91) 5 , . \u20180 (5139) .\n86% 82 % 35%\nNSGA-IL (62172) | 244 | 2016 | 898 | (gz) 106)| iis | 2-98\nHarman et al. 07 NA NA 3,269 86.2 36 % 9% 1.51\n(0131)\nJFreeChart 90% 13%\nKessentini et al. 711 (65172) 2.86 3,389 85.82 37 % (4131) 2.65\n85% : 80 % 46%\nNSGA-II (42149) 3.23 2,826 | 73.82 (63178) | (21146) 2.27\nHarman et al. 07 NA NA 4,790 | 83.72 23 % 0% 1.01\n. (0146)\nGanttProject 95% 5%\nKessentini et al. 711 (47149) 2.96 4,697 86.7 27 % (7146) 2.45\n78% 78 % 31% ,\nNSGA-IL (s71112)| 118 | 4690 | 1129 | gsi i199] (24i78) | 2-22\nHarman et al. 07 NA NA 6,987 | 77.63 40 % 04% 0.96\n(3178)\nApacheAnt \u2014 ; 30% 0%\nKessentini et al. 711 (901112) 1.89 6,797 83.1 30 % (0178) 17\n84% \u2018 \u2018 80 % 44%\nNSGA-II (21125) 3.21 2,231 | 97.65 (79198) | (1814) 3.09\nHarman et al. 07 NA NA 3,654 | 77.63 37 % 10% 2.69\n(4141)\nJHotDraw 34% 7%\nKessentini et al. 711 (21125) 5.32 3,875 90.83 43 % (3141) 2.73\n85% 80 % 33%\nNSGA-IL 9i69) | 269 | L914 | 89.77 | gj 112)| 5146) | 2-92\nHarman et al. 07 NA NA 2,698 | 77.63 37 % ose) 1.003\nRhino 87% 0%\nKessentini et al. 711 (60169) 3,365 717.61 32 % (4146) 2.97\n\u2018Average NSGA-II 84% 2,937 80 % 36%\n(all s ed) Harman et al. 07 NA 4,011 36 % 4%\ny' Kessentini et al. 11 89% 4,520 34 % 9%\n\nhaving good correction ration DCR (Figure[7c). In addition, we need more quantitative\nevaluation to investigate the effect of the use of recorded refactorings, on the design\ncoherence (RP). To this end, we compare the RP score with and without using recorded\nrefactorings. In most of the systems when recorded refactoring is combined with se-\nmantics, the RP value is improved. For example, for Apache Ant RP is 83% when only\nquality and semantics are considered, however, when recorded refactoring reuse is in-\ncluded the RP is improved to 87% (Figure|7b).\n\nWe notice also that when code changes reduction is included with quality, semantics\nand recorded changes, the RP and DCR scores are not significantly affected. Moreover,\nwe notice in Figure|7c|that there is no significant variation in terms of DCR with all\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:28 A. Ouni et al.\n\nTable X: Median refactoring results and standard deviation (STDev) of different objec-\ntive combinations with NSGA-II (average of all the systems) on 31 runs in terms of\ndefect correction ratio (DCR), refactoring precision (RP), code changes reduction and\nrecorded refactorings (RR). The results was statistically significant on 31 independent\nruns using the Wilcoxon rank sum test with a 95% confidence level (p \u2014 value < 0.05).\n\nObjectives DCR Code changes RR RP (empirical\ncombinations Median STDev Median STDev Median STDev _ evaluation)\nQ+CC 75% 1.84 2591 87.12 NA. NA. 45%\nQ+S 81% 1.93 4355 94.6 NA. NA. 82%\nQ+RC 85% 2.16 3989 89.76 41% 2.87 54%\nQ+S+RC 81% 1.56 3888 106.24 35% 3.21 84%\nQ+S+RC+CC 84% 2.39 2917 97.91 36% 3.82 80%\n\ndifferent objectives combinations. When four objectives are combined the DCR value\ninduces a slight degradation with an average of 82% in all systems which is even\nconsidered as promising results. Thus, the slight loss in the defect-correction ratio is\nlargely compensated by the significant improvement of the design coherence and code\nchanges reduction. Moreover, we found that the optimal refactoring solutions found by\nour approach are obtained with a considerable percentage of reused refactoring history\n(RR) (more than 35% as shown in Table LX). Thus, the obtained results support the\nclaim that recorded refactorings applied in the past are useful to generate coherent and\nmeaningful refactoring solutions and can effectively drive the refactoring suggestion\ntask.\n\nIn conclusion, we found that the best compromise is obtained between the four objec-\ntives using NSGA-II comparing to the use of only two or three objectives. By default,\nthe tool considers the four objectives to find refactoring solutions. Thus, a software en-\ngineer can consider the multi-objective algorithm as a black-box and he do not need\nto configure anything related to the objectives to consider. The four objectives should\nbe considered and there is no need to select the objectives by the user based on our\nexperimentation results.\n\nResults for RQ2: To answer RQ2, we evaluate the efficiency of our approach com-\nparing to two other contributions of Harman et al. [Harman and Tratt 2007] and\nKessentini et al. (Kessentini et al. 2011]. In\nal. proposed a multi-objective approach that uses two quality metrics to improve CBO\n(coupling between objects) and SDMPC (standard deviation of methods per class) after\napplying the refactorings sequence. In (Kessentini ot al. 2013}, a single-objective ge-\nnetic algorithm is used to correct defects by finding the best refactoring sequence that\nreduces the number of defects. The comparison is performed in terms of: (1) defect\ncorrection ratio (DCR) that is calculated using defect detection rules, (2) refactoring\nprecision (RP) that represents the results of the subject judgments (Scenario 1), and\n(3) code changes needed to apply the suggested refactorings. We adapted our technique\nfor calculating code changes scores for both approaches Harman et al. and Kessentini\net al. Table 8 summarizes our findings and reports the median values and standard\ndeviation (STDev) of each of our evaluation metrics obtained for 31 simulation runs of\nall projects.\n\nAs described in Table after applying the proposed refactoring operations, we\nfound that more than 84% of detected defects were fixed (DCR) as an average for all the\nsix studied systems. This score is comparable to the correction score of Kessentini et\nal. (89%), an approach that does not consider design coherence preservation, nor code\nchange reduction nor recorded refactorings reuse (DCR is not considered in Harman\net al. since their aim is to improve only some quality metrics).\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:29\n\n\u2014= Xerces-J\n8000 + \u00b0. = JFreeChart\n\noe  GanttProject\n= AntApache\n7000 + \u00b0 = THotbraw\nRhino\n\n6000 +\n\n4900 +\n\n3000 +\n\n2000 +\n\nT\narco as arRe arstRe arstRo+e\n\n(a) Code Change score (CC).\n\n= KercesJ\n<= dFreeChart\n= GanttProject\n\u201c= AntApache\n= JHotbraw\n= Rhino\n\nT\narseRcee\n\n\u201c= Xeroes-J\neo J = JFreeChart\n= GantiProject\n<= AntApache\n= WHotbraw\nso | = Rhino\nT T T T T\n\navce as arc arseRe arseRcrc\n\n(c) Defect Correction Ratio (DCR).\n\nFig. 7: Refactoring results of different objectives combination with NSGA-II in terms\nof (a) code changes reduction (CC), (b) design preservation (RP), (c) defects correction\nratio (DCR).\n\nRegarding the semantic coherence, for all of our six studied systems, an average of\n80% of proposed refactoring operations are considered as semantically feasible and do\nnot generate design incoherence. This score is significantly higher than the scores of\nthe two other approaches having respectively only 36% and 34% as RP scores. Thus,\nour approach performs clearly better for RP and code changes score with the cost of a\nslight degradation in DCR compared to Kessentini et al. This slight loss in the DCR is\nlargely compensated by the significant improvement in terms of design coherence and\ncode change reduction.\n\nWe compared the three approaches in terms of automatic RE;ecai, aS depicted in\nFigure|8| We found that a considerable number of proposed refactorings, an average of\n36% for all studied systems in terms of recall, are already applied to the next version\nby the software development team. By comparison, the results for Harman et al. and\nKessentini et al. are only 4% and 9% respectively, as reported in figure [8b] Moreover,\nthis score shows that our approach is useful in practice unlike both other approaches.\nIn fact, the RE}ccai of Harman et al. is not significant, since only the move method\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:30 A. Ouni et al.\n\nrefactoring is considered when searching for refactoring solutions to improve coupling\nand standard deviation of methods per class. Moreover, expected refactorings are not\nrelated only to quality improvement, but also for adding new functionalities, and other\nmaintenance tasks. This is not considered in our approach when we search for the\noptimal refactoring solution that satisfies our four objectives. However, we manually\ninspected expected refactorings and we found that they are mainly related to adding\nnew functionality (related to adding new packages, classes or methods).\n\nIn conclusion, our approach produces good refactoring suggestions in terms of defect-\ncorrection ratio, design coherence, and code change reduction from the point of view of\n(1) potential users of our refactoring tool, and (2) expected refactorings applied to the\nnext program version.\n\n100 \u2014 -+- NSGA-II 100\n= Rarman et al.\n= Kessentini etal.\n80 4 80\nod\n\u00a7 eo 4 60 4\noO\na) \u00b0\n\u00b0\n\u00b0\n20 | \u00b0\u2014 n 20 4 \u2014_\na a\u2014\u2014\u2014 4\u2014__ =\n: a t\net on : => =\nT T T T\n\n\u00b0\nT T T T T\nXerces-J \u2014 AntApache JFreeChart GanttProject Rhino JHotDraw Harman et al. Kessentini et al. NSGA-II\n\n(a) RE_recall results for each system. (b) Boxplots for RE_recall.\n\nFig. 8: Automatic refactoring score (RE_recall) comparison between our approach\n(NSGA-II), Harman et al. and Kessentini et al.\n\nFurthermore, to justify the use of NSGA-II, we compared the performance of our\nproposal to two other multi-objective algorithms: MOGA, and a random search and a\nmono-objective algorithm (genetic algorithm). In a random search, the change opera-\ntors (crossover and mutations) are not used, and populations are generated randomly\nand evaluated using the four objective functions. In our mono-objective adaptation, we\nconsidered a single fitness function, which is the normalized average score of the four\nobjectives using a genetic algorithm. Moreover, since in our NSGA-II adaptation, we\nselect a single solution without giving more importance to some objectives, we give\nequal weights for each fitness function value. As shown in Figure |9} NSGA-II out-\nperforms significantly MOGA, random-search, and the mono-objective algorithm in\nterms of defects-correction ratio (DCR), semantic coherence preservation (RP), and\ncode change reduction. For instance, in JFreeChart, NSGA-II performs much better\nthan MOGA, random search, and genetic algorithm in terms of DCR and RP scores\n(respectively Figurealand Figure(9b). In addition, NSGA-II reduces significantly code\nchanges for all studied systems. For example, for Rhino, the number of code changes\nwas reduced to almost the half comparing to random search as shown in Figure\n\nFurthermore, an interesting finding is that the random search (RS) works as we\nthe single-objective GA. Indeed, we used RS with a multi-objective version by switch-\ning off individual selection based on fitness value, in our original framework. The per-\nformance of RS is clearly less than the other multi-objective algorithms being com-\npared (NSGA-II and MOGA). Some of the results of RS can be considered acceptable,\nthis can be explained by the limited number of refactoring types considered in our ex-\nperiments (limited search space). For GA, after 2,000 generations, we noticed that the\nsearch produced entire populations with high DCR and CC values but lower S and RR\nvalues that has resulted in a relative increase in the combined fitness function which\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:31\n\nled to comparable results to the multi-objective RS. The comparable results between\nRS and GA suggest that our formulation to the refactoring recommendation problem\nas a multi-objective formulation is adequate.\n\nAnother interesting observation from the results in figure [B]is that MOGA has less\ncode changes and higher RP value than NSGA-II in Apache Ant. By looking at the\nproduced results, we noticed that none of the blob design defects was fixed in Apache\nAnt using MOGA. Indeed, the blob design defect is known as one of the most difficult\ndesign defects to fix, and typically requires a large number of refactoring operations\nand code changes (several extract class, move method and move field refactorings).\nThis is also explained by the higher RP score, as it also complicated for developers to\napprove such refactorings.\n\n100 5 @ GA @ NSGA-II\n@ MOGA GO RS\n\n80 4\n60 +\n40\n\n20 4\not\n\nAntApache GanttProject. JFreeChart JHotDraw Rhino Xerces-J\n\n(a) Defect Correction Ratio (DCR).\n\nDefect Correction Ratio (DCR)\n\n100 4 @ GA @ NSGA-II\n@ MOGA O RS\n\n& 8074\n\u00a7\n2 604\n&\n2\n\u00a3 404\ng\n2 24\noJ\nAntApache GanttProject JFreeChart JHotDraw Rhino Xerces-J\n(b) Refactoring Precision (RP).\n8000 m GA @ NSGA-II\n@ MOGA \u00a9 RS\n5000\nSs\n8\n\u20182 4000\n8\n8, 3000\n\u00a9 2000\n3\n3\n8\n- TT\n0\n\nAntApache GanttProject JFreeChart JHotDraw Rhino Xerces-J\n\n(c) Code Change Score (CC).\n\nFig. 9: Refactoring results of different algorithms NSGA-II, MOGA, GA and RS in\nterms of (a) defects correction ratio, (b) refactoring precision and (c) code changes re-\nduction.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:32 A. Ouni et al.\n\nFor all experiments, we obtained a large difference between NSGA-II results and the\nmono-objective approaches (Harman et al., Kessentini et al., GA and random search)\nusing all the evaluation metrics. However, when comparing NSGA-II against MOGA,\nwe have found the following results: a) On small and medium-scale software systems\n(JFreeChart, Rhino and GanttProject) NSGA-II is better than MOGA on most systems\nwith a small and medium effect size; b) On large-scale software systems (Xerces-J,\nApache Ant and JDI-Ford), NSGA-II is better than MOGA on most systems with a\nhigh effect size.\n\nResults for RQ3: JDeodorant uses only structural information to detect and fix\ndesign defects, but does not handle all the six design defect types that we considered\nin our experiments. Thus, to make the comparison fair, we performed our comparison\nusing only two design defects that can be fixed by both tools: blob and feature envy.\nFigure summarizes our findings for the blob (figure and feature envy (fig-\nure t is clear that our proposal outperforms JDeodorant, on average, on all the\nsystems in terms of the number of fixed defects with a minimum number of changes\nand semantically coherent refactorings. The average number of fixed code smells is\ncomparable between both tools. However, our approach is clearly better in terms of\nsemantically coherent refactorings. This can be explained by the fact that JDeodor-\nant uses only structural metrics to evaluate the impact of suggested refactorings on\nthe detected code smells. In addition, our proposal supports more types of refactorings\nthan JDeodorant and this is also explains our outperformance. However, one of the\nadvantages of JDeodorant is that the suggested refactorings are easier to apply than\nthose proposed by our technique as it provides an Eclipse plugin to suggest and then\nautomatically apply a total of 4 types of refactorings, while the current version of our\ntool requires to apply refactorings by the developers using the Eclipse IDE with more\ncomplex types of refactorings.\n\nResults for RQ4: To evaluate the relevance of our suggested refactorings with our\nsubjects, we compared the refactoring strategies proposed by our technique and those\nproposed manually by groups G and H (6 subjects) to fix several defects on the six sys-\ntems. Figure|11}shows that most of the suggested refactorings by NSGA-II are similar\nto those applied by developers with an average of more than 73%. Some defects can be\nfixed by different refactoring strategies, and also the same solution can be expressed\nin different ways (complex and atomic refactorings). Thus we consider that the aver-\nage precision of more than 73% confirms the efficiency of our tool for real developers\nto automate the refactoring recommendation process. We discuss, in the next section,\nin more detail the relevance of our automated refactoring approach for software engi-\nneers.\n\n6. DISCUSSIONS\n\nThe obtained results from Section [5.3]suggest that our approach performs better than\ntwo existing approaches. We also compared different objective combinations and found\nthat the best compromise is obtained between the four objectives using NSGA-II when\ncompared to the use of only two or three objectives. Therefore, our four objectives are\nefficient for providing \"good\" refactoring suggestions. Moreover, we found that the re-\nsults achieved by NSGA-II outperforms the ones achieved by both multi-objective al-\ngorithms, MOGA and random search, and the mono-objective algorithm, GA.\n\nWe now provide more quantitative and qualitative analyses of our results and dis-\ncuss some observations drawn from our empirical evaluation of our refactoring ap-\nproach. We aim at answering the following research questions:\n\n\u2014 RQ5: What is the effect of suggested refactorings on the overall quality of systems?\n\u2014 RQ6: What is the effect of multiple executions on the refactoring results?\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering\n\n100 5\n\nao 4\n\neo 4\n\n44\n\na4\n\nDefect Correction Ratio (OCR)\n\nod\n\nRefactoring precision (RP)\n\n6000 5\n\n5000 4\n\n4000 |\n\n000 4\n\n2000\n\nCode change score (CC)\n\n1000 4\n\nB NS\n\n1 Deodorant\nGAS\n\nB NSGArI\n\nAmApache GantiProject JFreeChart JHotDraw Rhino \u2018Xerces-J\n\nDeodorant\nGach\n\nAntApache GanttProject JFreeChart JHotDraw Rhino \u2018Xerces~J\n\n'@ Deodorant\nB NSGATI\n\n\u2018AntApache GanttProject JFreeChart JHotDraw Rhino \u2014\u2014\u2018Xerces-J\n\n(a) Blob.\n\n100 5\n\nao 4\n\n60\n\n40\n\nDefect Correction Ratio (DCR)\n\n20\n\noJ\n\n100 5\n\nao 4\n\n\u00ab4\n\na4\n\nRefactoring precision (RP)\n\n20 4\n\nod\n\n6000\n\n4000\n\n3000\n\nCode change score (CC)\n\n+1000\n\n39:33\n\nm Deodorant\nNSAI\n\n\u2018AntApache GanitProject JFreeChart JHotDraw Rhino \u2014\u2014-Xerces-~J\n\n sDeodorant\nISGAI\n\non\n\nAmApache GanttProject JFreeChart JHotDraw Rhino Xerces-J\n\nDeodorant\n@ NSGA-I\n\nAntApache GanttProject JFreeChart JHolDraw Rhino \u2014-Xerves-J\n\n(b) Feature envy.\n\nFig. 10: Comparison results of our approach (NSGA-II) with JDeodorant in terms of\ndefects correction ratio (DCR), design coherence (RP) and code changes score (CC) for\neach system. For NSGA-II, we report the average DCR and CC scores and standard\n\ndeviation obtained through 31 independent runs. Note tha\n\nfor RP score, we did not\n\nreport the standard deviation as we directly conducted the qualitative evaluation with\nsubjects on the suggested refactoring solution that have the median DCR score.\n\n= NSGA-II\n100 4 \u2014\u2014 Harman et al\n= Kessentini et al.\n= 80 4 =e\nsc o\u2014_\u2014_\u2014_\u2014\u2014_e .\n5 so\nB\n8 40-\na\na4.\nA =\u2014 _ a a\n0 6g \"=e et\nT T T T T T\nXerces-J JFreeChart GanttProject. \u2014AntApache Rhino JHotDraw\n\nFig. 11: Comparison of our refactoring results with manual refactorings in terms of\nPrecision.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:34 A. Ouni et al.\n\n\u2014 RQ7: What is the distribution of the suggested refactoring types?\n\nIn the following subsections we answer each of these research questions.\n\n6.1. The refactoring impact (RQ5)\n\nAlthough our primary goal in this work is to demonstrate that design defects can be\nautomatically refactored, it is also important to assess the refactoring impact on design\nquality. The expected benefit from refactoring is to enhance the overall software design\nquality as well as fixing design defects [Fowler 1999]. We use the QMOOD (Quality\nModel for Object-Oriented Design) model [Bansiya and Davis 2002] to estimate the\neffect of the suggested refactoring solutions on quality attributes. We choose QMOOD,\nmainly because 1) it is widely used in the literature |Shatnawi and Li 2011} O'Keeffe\nand Cinn\u00e9ide 2008} Zibran and Roy 2011} to assess the effect of refactoring, and 2\n\nit has the advantage of defining six high level design quality attributes (reusability,\nflexibility, understandability, functionality, extendibility and effectiveness) that can be\ncalculated using 11 lower level design metrics (Bansiya and Davis 2003}. In our study\nwe consider the following quality attributes:\n\n\u2014Reusability: The degree to which a software module or other work product can be\nused in more than one computer program or software system.\n\n\u2014 Flexibility: The ease with which a system or component can be modified for use in\napplications or environments other than those for which it was specifically designed.\n\n\u2014Understandability: The properties of designs that enable it to be easily learned and\ncomprehended. This directly relates to the complexity of design structure.\n\n\u2014 Effectiveness: The degree to which a design is able to achieve the desired function-\nality and behavior using OO design concepts and techniques.\n\nWe did not assess the issue of functionality because we assume that, by definition,\nrefactoring does not change the behavior/functionality of systems; instead, it changes\nthe internal structure. We have also excluded the extendibility factor because it is, to\nsome extent, a subjective quality factor and using a model of merely static measures\n\nto evaluate extendibility is inadequate. Tables and summarize the QMOOD\nformulation of these quality attributes [Bansiya and Davis 2002].\nThe improvement in quality can be assessed by comparing the quality before and\n\nafter refactoring independently to the number of fixed design defects. Hence, the total\ngain in quality G for each of the considered QMOOD quality attributes qi before and\nafter refactoring can be easily estimated as:\n\nGa =%-\u2014% (17)\nwhere qj and q; denotes the value of the quality attribute i respectively after and\nbefore refactoring.\n\nIn Figure we show the obtained gain values (in terms of absolute value) that\nwe calculated for each QMOOD quality attribute before and after refactoring for each\nstudied system. We found that the systems quality increase across the four QMOOD\nquality factors much better than existing approaches. Understandability is the quality\nfactor that has the highest gain value; whereas the Effectiveness quality factor has\nthe lowest one. This mainly due to many reasons 1) the majority of fixed design defects\n(blob, spaghetti code) are known to increase the coupling (DCC) within classes, which\nheavily affect the quality index calculation of the Effectiveness factor; 2) the vast ma-\njority of suggested refactoring types were move method, move field, and extract class\n(Figure [12) that are known to have a high impact on coupling (DCC), cohesion (CAM)\nand the design size in classes (DSC) that serves to calculate the understandability\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:35\n\nTable XI: QMOOD metrics for design properties.\n\nDesign Property Metric Description\n\nDesign size DSC Design size in classes\nComplexity NOM Number of methods\n\nCoupling DCC Direct class coupling\nPolymorphism NOP Number of polymorphic methods\nHierarchies NOH Number of hierarchies\n\nCohesion CAM Cohesion among methods in class\nAbstraction ANA Average number of ancestors\nEncapsulation DAM Data access metric\n\nComposition MOA Measure of aggregation\nInheritance MFA Measure of functional abstraction\nMessaging CIS Class interface size\n\nTable XII: QMOOD quality factors.\n\nQuality attribute Quality Index Calculation\nReusability = -0.25 * DCC + 0.25 * CAM + 0.5 * CIS + 0.5 * DSC\n\nFlexibility = 0.25 * DAM - 0.25 * DCC + 0.5 * MOA +0.5 * NOP\n4, = 70.33 * ANA + 0.33 * DAM - 0.33 * DCC + 0.33 * = CAM -0.33 * NOP + 0.33 * NOM\nUnderstandability .\n- 0.33 * DSC\nEffectiveness = 0.2 *ANA + 0.2 *DAM + 0.2*MOA + 0.2 * MFA + 0.2 *NOP\n\nquality factor. Furthermore, we noticed that JHotDraw produced the lowest quality\nincrease for the four quality factors. This is justified by the fact that JHotDraw is\nknown to be of good design and implementation practices and\nit contains a small number of design defects compared to the five other studied sys-\ntems.\n\nTo sum up, we can conclude that our approach succeeded in improving the code\nquality not only by fixing the majority of detected design defects but also by improving\nthe user understandability, the reusability, the flexibility, as well as the effectiveness\nof the refactored program.\n\nFinally, it is worth to notice that since the application of refactorings to fix design\ndefects is a subjective process, it is normal that not all the programmers have the\nsame opinion. Thus it is important to study the level of agreement between subjects.\nTo address this issue, we evaluated the level of agreement using Cohen\u2019s Kappa coef-\nficient \u00ab (Cohen ef al. 1960), which measures to what extent the subjects agree when\nvoting for a recommended refactoring operation. The Kappa coefficient assessments\nwas 0.78, which is characterized as \u201csubstantial agreement\" by Landis and Koch [|\nidis and Koch 1977]. This obtained score makes us more confident that our suggested\nrefactorings are meaningful from software engineer\u2019s perspective.\n\n6.2. The effect of multiple executions (RQ6)\n\nIt is important to contrast the results of multiple executions with the execution time\nto evaluate the performance and the stability of our approach. The execution time for\nfinding the optimal refactoring solution with a number of iterations (stopping criteria)\nfixed to 6,000 was less than forty-eight minutes as shown in Figure Moreover, we\nevaluate the impact of the number of suggested refactorings on the DCR, RP, RR, and\ncode change scores in five different executions. Drawn for JFreeChart, the results of\nfigure[13]show that the number of suggested refactorings do not affect the refactoring\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:36 A. Ouni et al.\n\n04\n\no4\n: Kessentini eal @ Kessentini el al.\nm Harman eta Harman et al\n03 5 NSGA\u201cI\nos 4\nEy =\ng 8\n< 02 S a4\n$ 8\n2 os 204\n8 | mall 6 aul\n00 oo 4 a)\n~o1 -o1 J\nEffeciveness  Flexblly __Reusablly Understandabilty Effeciveness Flexibility Reusabilly  Understandabilty\n(a) Xerces-J. (b) JFreeChart.\no4 5 o4 5\n@ Kessentini el al. @ Kessentini el al.\n@ Harman et al. @ Harman et al\nos | 5 NSGA\u201cI os J 5 NSGA-II\n\u00a9 ow \u00a9 ow\n8 6\n2 os | 2 014\n6 6\noo 4 | _all]) oo | i ia | =\n-o1 J -o1 J\nEffectiveness Flexbilily _-Reusablly Understandablty Effectiveness Flexibilly \u2014-ReusabiltyUnderstandabilty\n(c) GanttProject. (d) AntApache.\n04 5\nos\n= Kessentin| ela & Kessentin lal\nB NOGA @ Harman et al.\n03 4 03 @ NSGA-II\nFoy =\ng 8\n\u00a9 OFF S 02\n\u00a7 5\n2 o41- 3\n2 go\noe i \u00b0 =H\n00 4 oo | y= \u2014== r\nor\n\n-04\n\nEffectiveness Flexibility Reusability_ Understandabilty Effectiveness Flexibility -Reusabilly  Understandabilty\n\n(e) Rhino. (f) JHotDraw.\n\nFig. 12: The impact of best refactoring solutions on QMOOD quality attributes.\n\nresults. Thus, a higher number of operations in a solution does not necessarily mean\nthat the results will be better. Consequently, we could conclude that our approach is\nscalable from the performance standpoint, especially that our technique is executed,\nin general, up front (at night) to find suitable refactorings. In addition, the results\u2019\naccuracy is not affected by the number of suggested refactorings.\n\nFurthermore, it is also important to assess the impact of the number of design de-\nfects on the size of the refactoring solution (number of refactorings). Figure[14|reports\nthe correlation between the number of design defects and the number of refactorings\nfor each system. Our findings confirm that the number of design defects does not affect\nthe number of refactorings due to the low value of correlation (0.04).\n\nIn addition, figure |15|reports the execution time for each of the search algorithms\nNSGA-II, Harman et al., Kessentini et al.. MOGA, GA and RS. As shown in the figure,\nthe execution time of our NSGA-II approach was very similar to MOGA with an aver-\nage of less than 48 minutes per system. However, the execution time of random search\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:37\n\nnumberof umber of refactorings umber of refacorings\n\n(a) Xerces-J. (b) JFreeChart. (c) GanttProject.\n\number of efactorings numberof\n\nof refactorngs\n\n(d) AntApache. (e) Rhino. (f) JHotDraw.\n\nFig. 13: Results of multiple executions on different project in terms of defect correction\nratio (DCR), code changes (CC), reused refactorings (RR), and execution time (Time).\n\n250 4 JFreeChart\ne\n200 4\no Xerces-J\nD e\n2\n5 150 4\ng\n\u00a9 100 precrew GantiBroject Rhino\n6\n5 AntApache\na e\n\u2014 50-4\n5\n2\no-4\nT T T T T T T\n20 30 40 50 60 70 80\n\nnumber of design defects\n\nFig. 14: Impact of the number of design defects on the size of the refactoring solution\n(number of refactorings).\n\nwas half of time spent by NSGA-II and MOGA, but the quality of the random search\nsolutions are much lower. The performance of NSGA-II is slightly better than MOGA\nbased on the different evaluation metrics. However, the adaptation of an NSGA-II al-\ngorithm to our refactoring problem is more complex than MOGA_It is expected that the\nexecution time of the remaining mono-objective approach is almost half the NSGA-II\none due to the following reasons: (1) they just considered one objective function, (2) the\ntime consuming for semantics and history functions of our approach are not consid-\nered by existing mono-objective approaches which require additional time processing,\nfiltering and comparing the identifiers within classes, and (3) existing mono-objective\napproaches are limited to few types of refactorings. Since our refactoring problem is\nnot a real time one, the execution time of NSGA-II is considered acceptable by all the\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:38 A. Ouni et al.\n\nprogrammers of our experiments. In fact, they mentioned that it is not required to use\nthe tool daily and they can execute it at the end of the day and check the results the\nnext day.\n\n@GA G MOGA\n60 + @ Harman etal. O NSGA-II\n@ Kessentini et al. # RS\n\n40 4\n\n30 4\n\nExecution Time (minutes)\n\nApache Ant GanttProject JFreeChart JHotDraw Rhino Xerces-J\n\nFig. 15: Comparison of the execution time for each of the search techniques, NSGA-II,\nHarman et al., Kessentini et al.. MOGA, GA and RS.\n\n6.3. The distribution of suggested refactoring types (RQ7)\n\nAnother important consideration is the refactoring operations\u2019 distribution. We con-\ntrast that the most suggested refactorings are move method, move field, and extract\nclass for the majority of studied systems except JHotDraw. For instance, in Xerces-J,\nwe had different distribution of different refactoring types as illustrated in Figure\nWe notice that the most suggested refactorings are related to moving code elements\n(fields, methods) and extract/inline class. This is mainly due to the type of defects\ndetected in Xerces-J (most of the defects are related to the blob defect) that need par-\nticular refactorings to move elements from blob class to other classes in order to reduce\nthe number of functionalities from them. On the other hand, we found for JHotDraw\nless move method, move field, and extract class refactorings. This is mainly because\nJHotDraw contains a small number of blobs (only three blobs were detected), and it is\nknown to be of good quality. Thus, our results in Figure{16] reveal an effect we found:\nrefactorings like move field, move method, and extract class are likely to be more use-\nful to correcting the blob defect. As part of future work, we plan to investigate the\nrelationship between defect types and refactoring types.\n\n7. INDUSTRIAL CASE STUDY\n\nThe goal of this study is to evaluate the efficiency of our refactoring tool in practice.\nWe conducted an evaluation with potential software engineers, who can use our tool,\nrelated to the relevance of our approach for software engineers. One of the advantages\nof this industrial validation is the participation of the original developers of a system\nin the evaluation of recommended refactorings.\n\nWe performed a small industrial case studybased on one industrial project JDI-Ford\nv5.8. JDI-Ford is a Java-based software system that implements 638 classes having\n247 KLOC. This system is used by our industrial partner, the Ford Motor Company,\nto analyze useful information from the past sales of dealerships data and to suggest\nwhich vehicles to order for their dealer inventories in the future. JDI-Ford is the main\nkey software application used by the Ford Motor Company to improve their vehicle\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:39\n\n\\ i Move method\nRhino alls 2 Move field\nJHotDraw | * Extract class\n4 is Inline class\nAntApache mr EN Es +4 Extract interface\n4 . # Move class\nGanttProject = 4 Extract method\need Pull up field\nJFreeChart | WINES = Pull up method\nXerces-J eee BC S8 ill! Push down method\ny = Push down field\n0 50 100 150 200 250 300 350 400\n\nNumber of refactorings % Other refactorings\n\nFig. 16: Suggested refactorings distribution.\n\nsales by selecting the right vehicle configuration to the expectations of customers. Sev-\neral versions of JDI were proposed by software engineers at Ford during the past 10\nyears. Due to the importance of the application and the high number of updates per-\nformed during a period of 10 years, it is critical to make sure that all the JDI releases\nare within a good quality to reduce the time required by developers to introduce new\nfeatures in the future.\n\nThe software engineers from Ford manually evaluated all the recommended refac-\ntorings for JDI by our tool using the RP metric, described in the previous section,\nbased on their knowledge of the system since they are some of the original developers.\nWe also evaluated the relevance of some of the suggested refactoring for the develop-\ners. In addition, we asked 4 out of the 10 software engineers from Ford to manually\nrefactor some code fragments with a poor quality then we compared their suggested\nrefactorings with the recommended ones by our approach. To decide about the quality\nof a code fragment, we used the domain knowledge of the 10 programmers from Ford\n(since they are part of the original developers of the systems), the quality metrics and\ndetected design defects (to guide developers to identify a list of refactoring opportuni-\nties). Thus, we defined a metric called ER that represents the ratio of the number of\ngood refactoring recommendation over the number of expected refactorings. The four\nselected software engineers are part of the original developers of the JDI system thus\nthey easily provided different refactoring suggestions.\n\nIn this section, we aim at answering to the following two questions:\n\n(1) To what extent can our approach propose correct refactoring recommendations?\n(2) To what extent the suggested refactorings are relevant and useful for software\nengineers?\n\nWe describe, first, in this section the subjects participated in our study. Second, we\ngive details about the questionnaire, instructions, and the conducted pilot study. Fi-\nnally, we describe and discuss the obtained results.\n\n7.1. Subjects\n\nOur study involved 10 software engineers from the Ford Motor Company. All the sub-\njects are familiar with Java development, software maintenance activities including\nrefactoring. The experience of these subjects on Java programming ranged from 4 to\n17 years. They were selected, as part of a project funded by Ford, based on having\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:40 A. Ouni et al.\n\nsimilar development skills, their motivations to participate in the project and their\navailability. They are part of the original developers\u2019 team of the JDI system.\n\n7.2. Pilot Study\n\nSubjects were first asked to fill out a pre-study questionnaire containing six questions.\nThe questionnaire helped to collect background information such as their role within\nthe company, their contribution in the development of JDI, their programming experi-\nence, their familiarity with quality assurance and software refactoring.\n\nWe divided the subjects into 5 groups (two developers per group) to evaluate the\ncorrectness and the relevance of the recommended refactorings according to the num-\nber of refactorings to evaluate, and the results of the pre-study questionnaire. All the\ngroups are similar, in average, in terms of programming experience, familiarity with\nthe system and used tools, and have almost the same refactoring and code smells back-\nground. The study consists of two parts:\n\n(1) The first part of the questionnaire includes questions to evaluate the correctness of\nthe recommended refactoring using the following options: 1. Not correct; 2. Maybe\nCorrect; and 3. Correct.\n\n(2) The second part of the questionnaire includes questions around the relevance of\nthe recommended refactorings using the following scale: 1. Not at all relevant; 2.\nSlightly relevant; 3. Moderately relevant; and 4. Extremely relevant.\n\nThe questionnaire is completed anonymously thus ensuring confidentiality and this\nstudy was approved by the IRB at the University of Michigan: \u201cResearch involving\nthe collection or study of existing data, documents, records, pathological specimens, or\ndiagnostic specimens, if these sources are publicly available or if the information is\nrecorded by the investigator in such a manner that participants cannot be identified,\ndirectly or through identifiers linked to the participants\u201d.\n\nThe different programmers from the Ford Motor Company were asked not only to\nevaluate the generated refactoring solutions by our tool but they also used the tool\nto generate the refactoring solutions for the industrial system to evaluate. Thus, they\nperformed all the required steps from the configuration of the multi-objective algo-\nrithm to the generation and analysis of the results. The programmers agreed that the\ntool was very easy to use due to the friendly graphical interface provided by the tool.\nAll the programmers successfully executed the tool without any help from the supervi-\nsors of the experiments. During the entire process, subjects were encouraged to think\naloud and to share their opinions, issues, detailed explanations, and ideas with the\norganizers of the study (one graduate student and one faculty from the University of\nMichigan) and not only answering the questions.\n\nA brief tutorial session was organized for every participant around refactoring to\nmake sure that all of them have a minimum background to participate in the study.\nAll the developers performed the experiments in a similar environment: similar con-\nfiguration of the computers, tools (Eclipse, Excel, etc.) and facilitators of the study.\nBecause some support was needed for the installation of our Eclipse plug-in and the\nother detection techniques considered in our experiments, we added a short descrip-\ntion of this instruction for the participants. These sessions were also recorded as audio\nand the average time required to finish all the questions was 3.5 hours. Thus, the max-\nimum time spent by the developers was 8.5 hours (including the refactoring execution\nand inspection) however the total average time was 4 hours.\n\nPrior to the actual experiment, we did a pilot run of the entire experiment with\none software engineer from Ford. We performed this pilot study to verify whether the\nassignments were clear and if our estimation of the required time to finalize the exper-\niments evaluation were realistic thus all the assignments could be completed in two\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:41\n\nsessions (one day) by the subjects. The pilot study pointed out that the assignments\nand the questions in the questionnaire form were clear and relevant, and that they\ncould be executed as offered by the subjects of the pilot study within a maximum of\n5 hours. The pilot study also pointed out that the description of refactorings and the\nexamples were clear and sufficient to understand the different types of refactorings\nconsidered in our experiments. Note that the engineer who participated in the pilot\nstudy was not involved for the rest of experiment reported in the paper, and was in-\nstructed not to share information about the experience prior to the study.\n\n7.3. Results of the Industrial Case Study\n\nIn this section, we evaluate the performance of our multi-objective refactoring tech-\nnique in an industrial setting.\n\nOur first experiment was to assess to correctness of the suggested refactorings. From\nthe set of suggested refactoring, 87 out of 104 refactoring was accepted by Ford devel-\nopers suggesting that our approach was correct with a precision higher of 84%. For\nmore details, figure [17] reports the different types of refactorings that was correctly\nsuggested by our approach and approved by the majority of software engineers.\n\nSimilar facts were found when analyzing the similarity between the refactorings\nrecommended by our approach and those manually proposed by developers for several\ncode fragments. Most of the fixed code fragments by the software engineers were re-\nlated to the most severe and urgent ones based on their knowledge of the system. A\nnumber of 34 out of the 42 refactorings suggested by the developers were also proposed\nby our technique resulting to a precision of more than 80%. Only 5% of recommended\nrefactorings were considered as not correct and 11% as maybe correct.\n\n100 5\n\n80 4\n\n40 4\n\nRefactoring precision (RP)\n\n204\n\neS\noF yo*\nye\n\nFig. 17: Correctness of the different types of suggested refactorings.\n\nIn fact, the incorrect refactorings are due to some generated conflicts related to the\nfact that we are combining both complex and atomic refactorings in our solution. Al-\nthough our repair operator eliminates the detected identical redundant refactorings\nwithin one solution, it is challenging to detect such issue when dealing with complex\nand atomic refactorings. For example, an extract class is composed by several atomic\nrefactorings such create new class, move methods, move attributes, redirect method\ncalls, etc. Thus, it is challenging to eliminate some conflicts between atomic and com-\nplex refactorings when it is a redundancy issue. A possible solution is to convert all\ncomplex refactorings to atomic ones then we can perform the comparison to detect the\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:42 A. Ouni et al.\n\n@ Recommended refactorings (104)\n1B Applied refactorings (87)\n04\n\n0.3\n\n0.2\n\nImo i\n|\n\nEffectiveness Flexibility Reusability  Understandability\n\nQuality Improvements\n\nFig. 18: Quality improvements on JDI-Ford after applying the recommended refactor-\nings.\n\nredundancy. However, the conversion process is not straightforward since one complex\nrefactoring can be translated in different ways in terms of atomic refactorings.\n\nTo better investigate the relevance of the recommended refactorings, we evaluated\ntheir impact on the quality of JDI-Ford based on QMOOD. Figure}18/depicts the quality\nattributes improvements of the JDI system after applying (i) all recommended refac-\ntorings (104 refactorings), and (ii) only the selected refactorings by the developers (87\nrefactorings). The obtained results shows that our approach succeeded in improving\ndifferent aspect of software quality including reusability (0.11 of improvement), flex-\nibility (0.13), understandability (0.34), and effectiveness (0.09). An interesting point\nhere is that the results achieved by the selected refactorings (87 out of 104) outperform\nthe ones achieved by all the recommended refactorings (104) in terms of understand-\nability and reusability. This finding provides evidence that although developers seek\nto improve the overall quality of their code, they are prioritizing the understandability\nand reusability than other quality aspects. Indeed, we expected that developers will\nmainly apply refactorings that improve the readability and understandability of their\ncode.\n\nMoreover, we asked the developers to evaluate the relevance of the recommended\nrefactorings for the JDI-Ford system. Only less than 5% of recommended refactor-\nings are considered not at all relevant by the software engineers, 7% are considered\nas slightly relevant, 19% are moderately relevant, while 69% are considered as ex-\ntremely relevant. Moreover, the assessment of the Cohen\u2019s Kappa coefficient \u00ab\n\net al. 1960], which measures to what extent the developers agree when voting for a\nrecommended refactoring, indicates a score of \u00ab = 0.79. This significant score indi-\n\ncates \u201csubstantial agreement\u201d as characterized by Landis and Koch\n[1977]. This confirms the importance of the recommended refactorings for developers\nthat they need to apply them for a better quality of their systems.\n\nTo get more insights about the 5% of refactorings that are voted as \u201cnot at all rele-\nvant\u201d, we asked the developers to comment on some particular cases. We noticed that\nmost of these rejected refactoring were related to utility classes in JDI, where move\nmethod refactorings are suggested to move some utility methods to the classes that are\ncalling them. Developers mentioned that this kind of refactorings tends to be mean-\ningless.\n\nTo better evaluate the relevance of the recommended refactorings, we investigated\nthe types of refactorings that developers may consider them more or less important\nthan others. Figure [19| shows that move method is considered as one of the most ex-\ntremely relevant refactorings. In addition, extract method is also considered as another\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:43\n\nvery important and useful refactoring. This can be explained by the fact that the de-\nvelopers are more interested to fix quality issues that are related to the size of classes\nor methods. Overall, the different types of refactorings are considered relevant. One\nreason can be that our approach provides a sequence of refactorings.\n\n= Extremely relevant Moderately relevant Slightly relevant = Not at all relevant\n\nPush dow eld SS Eee\nPush down method = \u2014\u2014\u2014\u2014\u2014\u2014\u2014\nlpr SS \u2014\u2014\u2014\u2014\u2014\u2014_\u2014_\u2014_\u2014_\u2014_\u2014_\u2014_\u2014_\u2014_\u2014\u2014>-[-\u2014-=\nPull Up field\nExtract Method \u2014\u2014\u2014eEe=~-~~'\u00bbeF\u00bb'__\u2014\u2014 DOS\noe ls _\u2014>E>E>E\u2014\u2014\u2014\u2014E\u2014_\u2014\u2014_\u2014_\u2014_\u2014\u2014_\u2014_-_\u2014-=-*\u201c*\u201c>~>~-=~=\natectinteree = \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\u201c\u201c_tbre_\nine ass EE \u2014\u2014\u2014\u2014\u2014ee\u2014e\u2014eee\u2014e\u2014e\ntls EE \u2014 ee \u2014__\u2014_\u2014_\u2014_\u2014_\u2014\u2014esss\u2014\u2014\nove fd EE \u2014$\u2014\u2014\u2014$\u2014\u2014\u2014\u2014-=se*r\u00b0xX  _,_:\nMove rethed eee\n\n0 10 20 30 40 50 60 70\n\nFig. 19: The relevance of different types of recommended refactorings on JDI-Ford.\n\nIt was clear for our participants that our tool can provide faster and similar results\nthat they can manually suggest. The refactoring of large scale system can be time\nconsuming to fix several quality issues. The participants provided some suggestions\nto make our refactoring better and more efficient. First, the tool does not provide any\nranking to prioritize the suggested refactorings. In fact, the developers do not have\nenough time to apply all the suggested refactorings but they prefer to fix the most se-\nvere quality issues. Second, our technique does not provide a support to fix refactoring\nsolutions when the developers did not approve part of the suggested refactorings. Fi-\nnally, the software engineers prefer that our tool provides a feature to automatically\napply some regression testing techniques to generate test cases for the modified code\nfragments after refactoring. Such a feature is very interesting to include in our tool to\nautomatically test the Java refactoring engine similarly to SafeRefactor\n.\n\n8. THREATS TO VALIDITY\n\nSome potential threats can affect the validity of our experiments. We now discuss these\npotential threats and how we deal with them.\n\nConstruct validity concerns the relation between the theory and the observation.\nIn our experiments, the design defect detection rules we use to\nmeasure DCR could be questionable. To mitigate this threat, we manually inspect and\nvalidate each detected defect. Moreover, our refactoring tool configuration is flexible\nand can support other state-of-the-art detection rules. In addition, different threshold\nvalues were used in our experiments based on trial-and-error, however these values\ncan be configured once then used independently from the system to evaluate. Another\nthreat concerns the data about the actual refactorings of the studied systems. In ad-\ndition to the documented refactorings, we are using Ref-Finder, which is known to\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:44 A. Ouni et al.\n\nbe efficient [Prete et al. 2010]. Indeed, Ref-Finder was able to detect refactoring op-\nerations with an average recall of 95% and an average precision of 79% [Prete et al.|\n\n2010]. To ensure the precision, we manually inspect the refactorings found by Ref-\n\u2018inder. We identify three threats to internal validity: selection, learning and fatigue,\nand diffusion.\n\nFor the selection threat, the subject diversity in terms of profile and experience could\naffect our study. First, all subjects were volunteers. We also mitigated the selection\nthreat by giving written guidelines and examples of refactorings already evaluated\nwith arguments and justification. Additionally, each group of subjects evaluated differ-\nent refactorings from different systems for different techniques/algorithms. Further-\nmore, the selection refactorings to be evaluated for each refactoring solution was com-\npletely random.\n\nRandomization also helps to prevent the learning and fatigue threats. For the fatigue\nthreat, specifically, we did not limit the time to fill the questionnaire for the open source\nsystems. Consequently, we sent the questionnaires to the subjects by email and gave\nthem enough time to complete the tasks. Finally, only ten refactorings per system was\nrandomly picked for the evaluation. However, all refactoring solutions were evaluated\nfor the industrial system.\n\nDiffusion threat is limited in our study because most of the subjects are geograph-\nically located in three different universities and a company, and the majority do not\nknow each other. For the few ones who are in the same location, they were instructed\nnot to share information about the experience prior to the study.\n\nConclusion validity deals with the relation between the treatment and the out-\ncome. Thus, to ensure the heterogeneity of subjects and their differences, we took spe-\ncial care to diversify them in terms of professional status, university/company affilia-\ntions, gender, and years of experience. In addition, we organized subjects into balanced\ngroups. Having said that, we plan to test our tool with Java development companies,\nto draw better conclusions. Moreover, the automatic evaluation is also a way to limit\nthe threats related to subjects as it helps to ensure that our approach is efficient and\nuseful in practice. Indeed, we compare our suggested refactorings with the expected\nones that are already applied to the next releases and detected using Ref-Finder.\n\nAnother potential threat can be related to parameters selection. We selected differ-\nent parameters of our NSGA-II algorithm, such as the population size, the maximum\nnumber of iterations, mutation and crossover probabilities, and the solution length,\nbased on the trial-and-error method and depending on the size of the evaluated sys-\ntems, the initial number of design defect instances detected, and the number of refac-\ntoring types implemented in our tool (11 types, table[II). However, as these parameters\nare independent each other, they can be easily configured according to the preferences\nof the developers, for example if they want to reduce the execution time (e.g., reduce\nthe number of iterations) and maybe sacrifice a bit on the quality of the solutions.\n\nAlso when comparing the different approaches, some of them are using less types of\nrefactorings. We believe that this is one of the limitations of these approaches thus it is\ninteresting to show that considering the 11 types of refactorings of our approach may\nimprove the results (even if programmers may apply them less frequently). Further-\nmore, when comparing the different approaches from the effort perspective, the code\nchanges score is relative to DCR level. Not all design defects require the same amount\nof code changes. The process prioritizes the correction of design defects that require\nless changes to have higher DCR score. In addition, our results were consistent on all\nthe different DCR levels for all the systems.\n\nExternal validity refers to the generalizability of our findings. In this study, we\nperformed our experiments on different open-source and industrial Java systems be-\nlonging to different application domains and with different sizes. However, we cannot\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:45\n\nassert that our results can be generalized to other programming languages, and to\nother practitioners.\n\nThe industrial validation section was checked by the Ford Motor Company. Our in-\ndustrial partner accepted to only include the results mentioned in the current valida-\ntion section for several reasons. Similar to most collaborations with industry, we are\nnot allowed to mention the name of code elements or providing any example from the\nsource code. One of our motivations to use open source systems in our validation is the\nhard constraint to not share the industrial data. Thus, the readers can at least check\ndifferent examples of suggested refactorings on the open source system in the website\nprovided with this paper.\n\n9. RELATED WORK\n\nSeveral studies have been focused on software refactoring in recent years. In this sec-\ntion, we survey those works that can be classified into three broad categories: (i) man-\nual and semi-automated approaches, (ii) search-based approaches, and (iii) semantics-\nbased approaches.\n\n9.1. Manual and semi-automated approaches\n\nThe first book in the literature was written by Fowler and provides a\nnon-exhaustive list of low-level design problems in source code have been defined. For\neach design problem (i.e., design defect), a particular list of possible refactorings are\nsuggested to be applied by software maintainers manually. After Fowler\u2019s book sev-\neral approaches have merged with the goal of taking advantage from refactoring to\nimprove quality metrics of software systems. In (Sahraout etal. 2000), Sahraoui et al.\nproposed an approach to detect opportunities of code transformations (i.e., refactor-\nings) based on the study of the correlation between certain quality metrics and refac-\ntoring changes. Consequently, different rules are manually defined as a combination\nof metrics/thresholds to be used as indicators for detecting refactoring opportunities.\nFor each code smell a pre-defined and standard list of transformations should be ap-\nplied. In contrast to our approach, we do not have a pre-defined list of refactorings to\napply, instead, our approach automatically recommends refactorings depending on the\ncontext.\n\nAnother similar work is proposed by Du Bois et al. who starts\nfrom the hypothesis that refactoring opportunities correspond of those which improves\ncohesion and coupling metrics to perform an optimal distribution of features over\nclasses. Anquetil et al. analyze how refactorings manipulate coupling and cohesion\nmetrics, and how to identify refactoring opportunities that improve these metrics (An\niquetil and Laval 2011}. The authors reported that refactorings manually performed by\ndevelopers do not necessarily improve the modularity in terms of cohesion/coupling.\nThis suggests that goal-oriented refactoring recommendation is useful to improve spe-\ncific aspects of the system, which is one of the motivations of our approach. However,\nthese two approaches are limited to only some possible refactoring operations with few\nnumber of quality metrics. In addition, the proposed refactoring strategies cannot be\napplied for the problem of correcting design defects. In our approach, we are taking as\ninput the set of code smells that could be detected using the above studies but we did\nnot address the problem of using quality metrics to identify design defects.\n\nMoha et al. proposed an approach that suggests refactorings using\nFormal Concept Analysis A) to fix god class design defect. This work combines the\nefficiency of cohesion/coupling metrics with FCA to suggest refactoring opportunities.\nHowever, the link between defect detection and correction is not obvious, which make\nthe inspection difficult for the maintainers. Similarly, Joshi et al.\nhave presented an approach based on concept analysis aimed at identifying less\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:46 A. Ouni et al.\n\ncohesive classes. It also helps identify less cohesive methods, attributes and classes\nat the same time. Furthermore, the approach guides refactoring opportunities iden-\ntification such as extract class, move method, localize attributes and remove unused\nattributes. In addition, Tahvildari et al. proposed\na framework of object-oriented metrics used to suggest refactoring opportunities to\nimprove the quality of object-oriented legacy systems. In contrast, our approach is not\nbased explicitly on quality metrics as indicator for quality improvements, instead, we\nare based on the number of fixed design defects. Indeed, improving quality does not\nnecessarily mean that actual design defects are fixed.\n\nAnother generation of semi-automated refactoring techniques have emerged.\n\nMurphy-Hill et al. [Murphy-Hill and Black 2008}|Murphy-Hill et al. 2012} Murphy-Hill\npropose several techniques and empirical studies to support refactor-\ning activities. Tn (Murphy-Hill and Black 2008) and (Murphy-Hill and Black 2012), the\nauthors propose new tools to assist software engineers in applying refactoring such as\n\nselection assistant, box view, and refactoring annotation based on structural informa-\ntion and program analysis techniques. Recently, Ge and Murphy-Hill have proposed\nnew refactoring tool called GhostFactor that allows the\ndeveloper to transform code manually, but check the correctness of the transforma-\ntions automatically. However, the correction is based mainly on the structure of the\ncode and does not consider the issue of design coherence as our proposal does. Other\ncontributions are based on rules that can be expressed as assertions (invariants, pre\nand post-condition). The use of invariants has been proposed to detect parts of pro-\ngram that require refactoring by (Kataoka et al. 2001), In addition, Opdyke\n[1992] proposed the definition and the use of pre- and post-condition with invariants\nto preserve the behavior of the software when applying refactoring. Hence, behavior\npreservation is based on the verification/satisfaction of a set of pre and post-condition.\nAll these conditions are expressed in terms of rules. However, unlike our approach,\nthese approaches focus only on behavior preservation and do not consider the design\ncoherence of the program.\n\nTsantalis et al. |Tsantalis and Chatzigeorgiou 2009] and Sales et al. [Sales et al.\n\nproposed techniques to identify move methods opportunities by studying the ex-\nisting dependencies between classes. A similar technique was suggested by Fokaefs\net al. to detect extract class possibilities by analyzing dependen-\ncies between methods and classes. However, such approaches are local, i.e., they focus\non a specific code fragment. In contrast to our approach, we are providing a generic\n\nrefactoring approach that consider the effect on the whole system being refactored.\nFurthermore, several empirical studies [Kim et al. 2014} Negara et al. 2013}\nFranklin et al. 2013} [Alves et al. 2014] was performed recently to understand the ben-\nefits and risk of refactoring. Thee studies show that the main risk that refactorings\ncould introduce is the creation of bugs after refactoring but several benefits could be\nobtained such as reducing the time that programmers spent to understand existing\n\nimplemented features.\nMore details about current literature related to manual or semi-automated software\n\nrefactoring can be found in the following two recent surveys [Bavota et al. 2014b\nAl Dallal 2015).\n\n9.2. Search-based approaches\n\nTo automate refactoring activities, new approaches have emerged where search-based\ntechniques have been used. These approaches cast the refactoring problem as an opti-\nmization problem, where the goal is to improve the design quality of a system based\nmainly on a set of software metrics. After formulating the refactoring as an optimiza-\ntion problem, several techniques can be applied for automating refactoring, e.g., ge-\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:47\n\nnetic algorithms, simulated annealing, and Pareto optimality, etc. Hence, we classify\nthose approaches into two main categories: (1) mono-objective and (2) multi-objective\noptimization approaches.\n\nIn the first category, the majority of existing work combines several metrics in a\nsingle fitness function to find the best sequence of refactorings. Seng et al.\n[2006] propose a single-objective search-based approach using genetic algorithm to sug-\ngest a list of refactorings to improve software quality. The search process uses a single\nfitness function to maximize a weighted sum of several quality metrics. The employed\nmetrics are mainly related to various class level properties such as coupling, cohesion,\ncomplexity and stability while satisfying a set of pre-conditions for each refactoring.\nThese conditions serve at preserving the program behavior (refactoring feasibility).\nHowever, in contrast to our approach, this approach does not consider the design co-\nherence of the refactored program and limited only to move method refactoring. An-\nother similar work of O\u2019Keeffe et al. |O\u2019Keeffe and Cinn\u00e9ide 2008] that uses different\nlocal search-based techniques such as hill climbing and simulated annealing to provide\nan automated refactoring support based on the QMOOD metrics suite. Interestingly,\nthey also found that the understandability function yielded the greatest quality gain,\n\nin keeping with our observation in Section\nQayum et al. |Qayum and Heckel 2009] considered the problem of refactoring\nscheduling as a graph transformation problem. They expressed refactorings as a\n\nsearch for an optimal path, using Ant Colony Optimization, in the graph where nodes\nand edges represent respectively refactoring candidates and dependencies between\nthem. However the use of graphs is limited only on structural and syntactical informa-\ntion and does not consider the design semantics neither its runtime behavior. Fatire-\ngun et al. [Fatiregun et al. 2004] show how search-based transformations could be\nused to reduce code size and construct amorphous program slices. However, they have\nused small atomic level transformations in their approach. However, their aim was to\nreduce program size rather than improving its structure/quality.\n\nOtero et al. [Otero et al. 2010] introduced an approach to explore the addition of a\nrefactoring step into the genetic programming iteration. It consists of an additional\nloop in which refactoring steps, drawn from a catalog, will be applied to individuals\nof the population. Jensen et al. (Jensen and Cheng 2010) and Cheng 2010] have proposed an approach\nthat supports composition of design changes and makes the introduction of design\npatterns a primary goal of the refactoring process. They used genetic programming\nand software metrics to identify the most suitable set of refactorings to apply to a\nsoftware design. Furthermore, Kilic et al. explore the use of a variety\nof population-based approaches to search-based parallel refactoring, finding that local\nbeam search could find the best solutions. However, still these approach focusing on\nspecific refctoring types and not not consider the design semantics.\n\nZibran et al. formulated the problem of scheduling of code\nclone refactoring activities as a constraint satisfaction optimization problem (CSOP)\nto fix known duplicate code code-smells. The proposed approach consists of applying\nconstraint programming (CP) technique that aims to maximize benefits while mini-\nmizing refactoring efforts. An effort model is used for estimating the effort required\nto refactor code clones in object-oriented codebase. Although there is a slight similar-\nity between the proposed effort model and our code changes score model\nd72a), the proposed approach does not ensure the design coherence of the refactored\nprogram.\n\nIn the second category, the first multi-objective approach was introduce by Harman\net al. [Harman and Tratt 2007] as described earlier. Recently, O Cinneide et al. [0 Cin,\nn\u00e9ide et al. 201 ave proposed a multi-objective search-based refactoring to conduct\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n\n",
                    "39:48 A. Ouni et al.\n\nan empirical investigation to assess some structural metrics and to explore relation-\nships between them. To this end, they have used a variety of search techniques (Pareto-\noptimal search, semi-random search) guided by a set of cohesion metrics. The main\nweakness in all of these approaches is that the design preservation have not been ad-\ndressed to obtain correct and meaningful refactorings, neither the effort required to\napply refactoring which addressed in our approach.\n\n9.3. Semantic coherence for refactoring\nThere exists few works focusing on refactorings that involves semantic coherence. In\n\ntheir approach, Fatiregun et al. have\napplied a number of simple atomic transformation rules called axioms. The authors\npresume that if each axiom preserves semantics then a whole sequence of axioms ought\nto preserve semantics equivalence. However, semantics equivalence depends on the\nprogram and the context and therefore it could not be always proved. Indeed, their\nproposed semantic equivalence is based only on structural rules related to the axioms,\nrather than a semantic analysis of the code.\n\nLater, Bavota et al. have proposed an approach for automating\nthe refactoring extract class based on graph theory that exploits structural and se-\nmantic relationships between methods. The proposed approach uses a weighted graph\nto represent a class to be refactored, where each node represents a method of that\nclass. The weight of an edge that connects two nodes (representing methods) is a mea-\nsure of the structural and semantic relationship between two methods that contribute\nto class cohesion. After that, they split the built graph in two sub-graphs, to be used\nlater to build two new classes having higher cohesion than the original class. Similarly,\nBavota et al. used cohesion metrics in order\nto identify opportunities of extract class.\n\nBy exploiting semantic information, Bavota et al. proposed a\ntechnique for the recommendation of move method using semantic information and\nrelational topic models. Other studies tried to formulate semantic information based\non cohesion for software clustering and remodularization ||Corazza et al. 2011}|Bavota\nSeanniello et al. 2010), Mkaouer et al. [Mkaouer et al. 2015] formulate:\nsoftware remodularization as many-objective problem however they used very basic\nsemantic measures and it was limited to few refactorings applied at the package level.\nMoreover, Bavota et al. suggested an approach to recommend\nappropriate refactoring operations to adjust the design according to the teams\u2019 activity\npatterns.\n\nIn (Baar and Markovi\u00e9 2007), Baar et al. have presented a simple criterion and a\nproof technique for the preservation of the design coherence of refactoring rules that\nare defined for UML class diagrams and OCL constraints. Their approach is based on\nformalization of the OCL semantics taking the form of graph transformation rules.\nHowever, their approach does not provide a concrete design preservation since there\nis no explicit differentiation between behaviour and design preservation. In fact, they\nconsider that the semantic coherence \u201cmeans that the observable behaviors of original\nand refactored programs coincide\u201d. In addition, in contrast to our approach, they par-\ntially address the design preservation in the model level with a high level of abstrac-\ntion without considering the code/implementation level. In addition, this approach\nuses only the refactoring move attribute and do not consider popular refactoringg\"|\n\nAnother semantics-based framework was introduced by Logozzo [Logozzo and|\nCortesi 2006] for the definition and manipulation of class hierarchies-based refactor-\nings. The framework is based on the notion of the observable part of a class, i.e., an\n\n1 Thttp //www.refactoring.com/catalog/,\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:49\n\nabstraction of its semantics when focusing on a behavioral property of interest. They\ndefine a semantic subclass relation, capturing the fact that a subclass preserves the\nbehavior of its superclass up to a given observed property.\n\nFurthermore, it is worth to note that most of the existing techniques are limited\nto a small number of refactorings (single refactoring based approaches). For instance,\n\nHarman et al. [Harman and Tratt 2007], Bavota et al. |[Bavota et al. 2014a], Tsantalis\n\nand Chatzigeorgiou [Tsantalis and Chatzigeorgiou 2009), and Sales et al. [Sales et al)\n2013] recommend only move method refactoring. Bavota et al. [Bavota et al. 2014a)\n\nBavota ot a. 2011} |Bavota et al. 2014b}/Bavota et al. 2010] and Fokaefs et al. [Fokaefs|\n[Fokaefs et al. 2012] address the recommendation of the extract class refac-\ntoring. On i e other and, ilva et al., [Silva et al. 2014], Tsantalis and Chatzige-\norgiou |Tsantalis and Chatzigeorgiou 2011] introduce an approach for recommending\nextract method refactorings, while} Krishnan and Tsantalis focus on code clone refactor-\nings [Krishnan and Tsantalis 2014]. To help devele pers with efficient refactoring rec-\nommendations, JDendorant [Fokaefs etal. 2011\nTsantalis and ae 009] unifies different techniques\nin one tool t. at oe ive refactorings (move method, extract class, extract method,\nreplace conditional with polymorphism, and replace type code with state/strategy) to\nfix four types of code smells (god class, feature envy, type checking, and long method).\n\nMoreover, Seng et al. implemented five refactorings (move method,\npull up attribute, push down attribute, pull up method, and push down method) but\nthey only focus on move method refactoring in their paper (Seng et al. 2006) In con-\ntrast, one of the strengths of our approach is that it addresses several refactoring types\n(11 refactorings) at the same time as listed in Table|II}\n\n10. CONCLUSIONS AND FUTURE WORK\n\nThis paper presented a novel search-based approach taking into consideration multiple\ncriteria to suggest \u201cgood\u201d refactoring solutions to improve software quality. The process\naims at finding the sequence of refactorings that (i) improves design quality, (ii) pre-\nserves the design coherence of the refactored program, (iii) minimizes code changes,\nand (iv) maximizes the consistency with development change history. We, thus, formu-\nlated our problem as a multi-objective search problem to find a trade-off between all\nthese objectives using NSGA-II. Moreover, we defined different measures to estimate\nthe design coherence of a code after refactoring and we also used the similarity with\nprevious code changes as an indicator of the design consistency.\n\nTo evaluate our approach, we conducted an empirical study from both quantitative\nand qualitative perspectives on open-source and industrial projects. The open-source\nevaluation involved six medium and large size open-source systems with a comparison\n\nagainst three existing approaches [Harman and Tratt 2007; Kessentini et al. 2011}\nFokaefs et al. 2011].\n\nOur empirical study shows the efficiency of our approach in im-\nproving the quality of the studied systems while successfully fixing an average of 84%\nof design defects with low code change score (an average of 2,937 of low level changes).\nThe qualitative evaluation shows that most of the suggested refactorings (an aver-\nage of 80%) are considered as relevant and meaningful from developer\u2019s point of view.\nMoreover, unlike existing approaches, the obtained results show that our approach\nis efficient in suggesting a significant number of expected refactorings that was per-\nformed in the next release of the systems being studied which provides evidence that\nour approach is more efficient and useful in practice.\n\nIn addition, we conducted an industrial validation of our approach on a large-scale\nproject and the results was manually evaluated by 10 active software engineers to as-\nsess the relevance and usefulness of our refactoring suggestions. The obtained results\nprovide evidence that our approach succeeded in improving different aspect of software\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:50 A. Ouni et al.\n\nquality including reusability (0.11 of improvement), flexibility (0.13), understandabil-\nity (0.34), and effectiveness (0.09). Moreover, 84% of the recommended (87 out of 104)\nwas meaningful and useful from developer\u2019s point of view.\n\nIn future work, we are planning to conduct an empirical study to understand the\ncorrelation between correcting design defects and introducing new ones or fixing\nother design defects implicitly. We also plan to adapt our multi-objective approach\nto fix other types of defects that can occur in new emergent service-based applica-\ntions (Ratem-Gal-Or et al. 2012) Future replications of our study with additional\nsystems and design defect types are necessary to confirm our findings. Another lim-\nitation of our current approach is the selection of the best solution from the Pareto\nfront. We used the technique of selecting the closest solution to the ideal point. How-\never, we plan in our future work to integrate the developers preferences to select the\nbest solution from the set of non-dominated solutions. Moreover, one limitation of our\napproach is that one input is a base of recorded/collected code changes on previous\nversions. We believe that this data is not always available, especially in the begin-\nning of the projects. As a future work, we plan to reuse refactorings recorded/col-\nlected for other similar contexts can be used instead. This can be done by calculat-\ning the similarity with not only the refactoring type but also between the contexts\n(code fragments). Furthermore, we are planning to include more criteria and con-\nstraints to improve the meaningfulness of the suggested refactorings, an interesting\none is to identify refactorings related to utility classes and prevent moving method-\ns/fields between utility and functional classes, as these refactoring are unlikely to be\nmeaningful. Finally, we are planning to include other fine-grained refactoring opera-\ntions such as Decompose Conditional, Replace Conditional with Polymorphism, and\nReplace Type Code with State/Strategy to improve the quality of the code.\n\nACKNOWLEDGMENTS\n\nThe authors would like to thank the anonymous reviewers for their relevant feedback and useful comments\nthat helped them to improve this work. Also, the authors are grateful to the subject students from the\nUniversity of Michigan-Dearborn and developers who participated in their empirical study. This work was\nsupported by the Ford-University of Michigan alliance Program, Japan Society for the Promotion of Science,\nGrant-in-Aid for Scientific Research (S) (No.25220003) and by Osaka University Program for Promoting\nInternational Joint Research.\n\nREFERENCES\n\nJehad Al Dallal. 2015. Identifying refactoring opportunities in object-oriented code: A systematic literature\nreview. Information and Software Technology 58 (2015), 231-249.\n\nEverton LG Alves, Myoungkyu Song, and Miryung Kim. 2014. RefDistiller: a refactoring aware code re-\nview tool for inspecting manual refactoring edits. In 22nd ACM SIGSOFT International Symposium on\nFoundations of Software Engineering (FSE). 751-754.\n\nRaquel Amaro, Rui Pedro Chaves, Palmira Marrafa, and Sara Mendes. 2006. Enriching wordnets with new\nrelations and with event and argument structures. In Computational Linguistics and Intelligent Text\nProcessing. Springer, 28-40.\n\nNicolas Anquetil and Jannik Laval. 2011. Legacy software restructuring: Analyzing a concrete case. In 15th\nEuropean Conference on Software Maintenance and Reengineering (CSMR). 279-286.\n\nThomas Baar and Slavisa Markovi\u00e9. 2007. A graphical approach to prove the semantic preservation of\nUML/OCL refactoring rules. In Perspectives of systems informatics. Springer, 70-83.\n\nJagdish Bansiya and Carl G Davis. 2002. A hierarchical model for object-oriented design quality assessment.\nIEEE Transactions on Software Engineering 28, 1 (2002), 4-17.\n\nGabriele Bavota, Andrea De Lucia, Andrian Marcus, and Rocco Oliveto. 2014a. Automating extract class\nrefactoring: an improved method and its evaluation. Empirical Software Engineering 19, 6 (2014), 1617-\n1664.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:51\n\nGabriele Bavota, Andrea De Lucia, Andrian Marcus, and Rocco Oliveto. 2014b. Recommending Refactoring\nOperations in Large Software Systems. In Recommendation Systems in Software Engineering. Springer,\n387-419.\n\nGabriele Bavota, Andrea De Lucia, Andrian Marcus, Rocco Oliveto, and Fabio Palomba. 2012. Supporting\nextract class refactoring in Eclipse: the ARIES project. In 34th International Conference on Software\nEngineering (ICSE). IEEE Press, 1419-1422.\n\nGabriele Bavota, Andrea De Lucia, and Rocco Oliveto. 2011. Identifying extract class refactoring opportu-\nnities using structural and semantic cohesion measures. Journal of Systems and Software 84, 3 (2011),\n397-414.\n\nGabriele Bavota, Malcom Gethers, Rocco Oliveto, Denys Poshyvanyk, and Andrea de Lucia. 2014. Improving\nsoftware modularization via automated analysis of latent topics and dependencies. ACM Transactions\non Software Engineering and Methodology 23, 1 (2014), 4.\n\nGabriele Bavota, Rocco Oliveto, Andrea De Lucia, Giuliano Antoniol, and Yann-Ga\u00e9l Gu\u00e9h\u00e9neuc. 2010. Play-\ning with refactoring: Identifying extract class opportunities through game theory. In 26th International\nConference on Software Maintenance (ICSM). 1-5.\n\nGabriele Bavota, Rocco Oliveto, Malcom Gethers, Denys Poshyvanyk, and Andrea De Lucia. 2014a. Method-\nbook: Recommending move method refactorings via relational topic models. IEEE Transactions on Soft-\nware Engineering 40, 7 (2014), 671-694.\n\nGabriele Bavota, Sebastiano Panichella, Nikolaos Tsantalis, Massimiliano Di Penta, Rocco Oliveto, and\nGerardo Canfora. 2014b. Recommending refactorings based on team co-maintenance patterns. In 29th\nInternational Conference on Automated software engineering (ASE). 337-342.\n\nWilliam H Brown, Raphael C Malveau, and Thomas J Mowbray. 1998. AntiPatterns: refactoring software,\narchitectures, and projects in crisis. (1998).\n\nMel O Cinn\u00e9ide. 2001. Automated application of design patterns: a refactoring approach. Ph.D. Dissertation.\nTrinity College Dublin.\n\nNorman Cliff. 1993. Dominance statistics: Ordinal analyses to answer ordinal questions. Psychological Bul-\nletin 114, 3 (1993), 494.\n\nJacob Cohen and others. 1960. A coefficient of agreement for nominal scales. Educational and psychological\nmeasurement 20, 1 (1960), 37-46.\n\nAnna Corazza, Sergio Di Martino, and Valerio Maggio. 2012. LINSEN: An efficient approach to split iden-\ntifiers and expand abbreviations. In 28th International Conference on Software Maintenance (ICSM).\n233-242.\n\nAnna Corazza, Sergio Di Martino, Valerio Maggio, and Giuseppe Scanniello. 2011. Investigating the use of\nlexical information for software system clustering. In 15th European Conference on Software Mainte-\nnance and Reengineering (CSMR). 35-44.\n\nMarco D\u2019Ambros, Alberto Bacchelli, and Michele Lanza. 2010. On the impact of design flaws on software\ndefects. In 10th International Conference on Quality Software (QSIC). 23-31.\n\nKalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. 2002. A fast and elitist multiobjec-\ntive genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation 6, 2 (2002), 182-197.\n\nIgnatios Deligiannis, Martin Shepperd, Manos Roumeliotis, and Ioannis Stamelos. 2003. An empirical in-\nvestigation of an object-oriented design heuristic for maintainability. Journal of Systems and Software\n65, 2 (2003), 127-139.\n\nK. Dhambri, H. Sahraoui, and P. Poulin. 2008. Visual Detection of Design Anomalies. In 12th European\nConference on Software Maintenance and Reengineering (CSMR). 279-283.\n\nBart Du Bois, Serge Demeyer, and Jan Verelst. 2004. Refactoring-improving coupling and cohesion of exist-\ning code. In 11th Working Conference on Reverse Engineering (WCRE). 144-151.\n\nLen Erlikh. 2000. Leveraging legacy system dollars for e-business. IT professional 2, 3 (2000), 17-23.\n\nDeji Fatiregun, Mark Harman, and Robert M Hierons. 2004. Evolving transformation sequences using ge-\nnetic algorithms. In 4th International Workshop on Source Code Analysis and Manipulation (SCAM).\n65-74.\n\nDeji Fatiregun, Mark Harman, and Robert M Hierons. 2005. Search-based amorphous slicing. In 12th Work-\ning Conference on Reverse Engineering (WCRE). IEEE, 10\u2014pp.\n\nNorman E. Fenton and Shari Lawrence Pfleeger. 1998. Software Metrics: A Rigorous and Practical Approach\n(2nd ed.). PWS Publishing Co., Boston, MA, USA.\n\nMarios Fokaefs, Nikolaos Tsantalis, Eleni Stroulia, and Alexander Chatzigeorgiou. 2011. JDeodorant: iden-\ntification and application of extract class refactorings. In 33rd International Conference on Software\nEngineering (ICSE). 1037-1039.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:52 A. Ouni et al.\n\nMarios Fokaefs, Nikolaos Tsantalis, Eleni Stroulia, and Alexander Chatzigeorgiou. 2012. Identification and\napplication of extract class refactorings in object-oriented systems. Journal of Systems and Software 85,\n10 (2012), 2241-2260.\n\nCarlos M Fonseca, Peter J Fleming, and others. 1993. Genetic Algorithms for Multiobjective Optimization:\nFormulation, Discussion and Generalization.. In 5th International Conference on Genetic Algorithms,\nVol. 93. 416-423.\n\nMartin Fowler. 1999. Refactoring: Improving the Design of Existing Code. Addison-Wesley Longman Pub-\nlishing Co., Inc., Boston, MA, USA.\n\nLyle Franklin, Alex Gyori, Jan Lahoda, and Danny Dig. 2013. LAMBDAFICATOR: from imperative to func-\ntional programming through automated refactoring. In 35th International Conference on Software En-\ngineering (ICSE). 1287-1290.\n\nXi Ge and Emerson Murphy-Hill. 2014. Manual refactoring changes with automated refactoring validation.\n36th International Conference on Software Engineering (ICSE) 36 (2014), 1095-1105.\n\nMohamed Salah Hamdi. 2011. SOMSE: A semantic map based meta-search engine for the purpose of web\ninformation customization. Applied Soft Computing 11, 1 (2011), 1310-1321.\n\nMark Harman, S Afshin Mansouri, and Yuanyuan Zhang. 2012. Search-based software engineering: Trends,\ntechniques and applications. ACM Computing Surveys (CSUR) 45, 1 (2012), 11.\n\nMark Harman and Laurence Tratt. 2007. Pareto optimal search based refactoring at the design level. In 9th\nannual conference on Genetic and evolutionary computation (GECCO). 1106-1113.\n\nPaul Jaccard. 1901. Etude comparative de la distribution florale dans une portion des Alpes et des Jura.\nBulletin del la Soci\u00e9t\u00e9 Vaudoise des Sciences Naturelles 37 (1901), 547-579.\n\nAdam C. Jensen and Betty H.C. Cheng. 2010. On the Use of Genetic Programming for Automated Refactor-\ning and the Introduction of Design Patterns. In 12th Annual Conference on Genetic and Evolutionary\nComputation (GECCO). 1341-1348.\n\nPadmaja Joshi and Rushikesh K. Joshi. 2009. Concept Analysis for Class Cohesion. In 13th European Con-\nference on Software Maintenance and Reengineering (CSMR). Washington, DC, USA, 237-240.\n\nYoshio Kataoka, David Notkin, Michael D Ernst, and William G Griswold. 2001. Automated support for\nprogram refactoring using invariants. In International Conference on Software Maintenance (ICSM).\nIEEE Computer Society, 736.\n\nMarouane Kessentini, Wael Kessentini, Houari Sahraoui, Mounir Boukadoum, and Ali Ouni. 2011. Design\ndefects detection and correction by example. In 19th International Conference on Program Comprehen-\nsion (ICPC). 81-90.\n\nMarouane Kessentini, St\u00e9phane Vaucher, and Houari Sahraoui. 2010. Deviance from perfection is a bet-\nter criterion than closeness to evil when identifying risky code. In 25th International Conference on\nAutomated Software engineering (ASE). 113-122.\n\nHurevren Kilic, Ekin Koc, and Ibrahim Cereci. 2011. Search-based parallel refactoring using population-\nbased direct approaches. In 3rd International Symposium on Search Based Software Engineering (SS-\nBSE). Springer, 271-272.\n\nMiryung Kim, Thomas Zimmermann, and Nachiappan Nagappan. 2014. An Empirical Study of Refactor-\ningChallenges and Benefits at Microsoft. IEEE Transactions on Software Engineering 40, 7 (2014),\n633-649.\n\nGiri Panamoottil Krishnan and Nikolaos Tsantalis. 2014. Unification and refactoring of clones. In Software\nEvolution Week - IEEE Conference on Software Maintenance, Reengineering and Reverse Engineering\n(CSMR-WCRE). 104-113.\n\nJ Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data.\nbiometrics (1977), 159-174.\n\nWei Li and Raed Shatnawi. 2007. An empirical study of the bad smells and class error probability in the\npost-release object-oriented system evolution. Journal of systems and software 80, 7 (2007), 1120-1128.\n\nRensis Likert. 1932. A technique for the measurement of attitudes. Archives of psychology (1932).\n\nFrancesco Logozzo and Agostino Cortesi. 2006. Semantic hierarchy refactoring by abstract interpretation.\nIn Verification, Model Checking, and Abstract Interpretation. Springer, 313-331.\n\nMika Mantyla, Jari Vanhanen, and Casper Lassenius. 2003. A taxonomy and an initial empirical study of\nbad smells in code. In International Conference on Software Maintenance (ICSM). IEEE, 381-384.\n\nRadu Marinescu. 2004. Detection strategies: metrics-based rules for detecting design flaws. In 20th Interna-\ntional Conference on Software Maintenance (ICSM). 350-359.\n\nTom Mens and Tom Tourw\u00e9. 2004. A survey of software refactoring. IEEE Transactions on Software Engi-\nneering 30, 2 (2004), 126-139.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "Multi-criteria Code Refactoring Using Search-Based Software Engineering 39:53\n\nWiem Mkaouer, Marouane Kessentini, Adnan Shaout, Patrice Koligheu, Slim Bechikh, Kalyanmoy Deb,\nand Ali Ouni. 2015. Many-Objective Software Remodularization Using NSGA-III. ACM Transactions\non Software Engineering and Methodology 24, 3 (2015), 17.\n\nNaouel Moha, Yann-Ga\u00e9l Gu\u00e9h\u00e9neuc, Laurence Duchien, and Anne-Francoise Le Meur. 2010. DECOR: A\nMethod for the Specification and Detection of Code and Design Smells. IEEE Transactions on Software\nEngineering 36, 1 (Jan 2010), 20-36.\n\nNaouel Moha, Amine Mohamed Rouane Hacene, Petko Valtchev, and Yann-Ga\u00e9l Gu\u00e9h\u00e9neuc. 2008. Refactor-\nings of Design Defects Using Relational Concept Analysis. In Formal Concept Analysis, Raoul Medina\nand Sergei Obiedkov (Eds.). Lecture Notes in Computer Science, Vol. 4933. Springer Berlin Heidelberg,\n289-304.\n\nEmerson Murphy-Hill and Andrew P Black. 2008. Breaking the barriers to successful refactoring. In 30th\nInternational Conference on Software Engineering (ICSE). 421-430.\n\nEmerson Murphy-Hill and Andrew P Black. 2010. An interactive ambient visualization for code smells. In\n5th international symposium on Software visualization (VISSOFT). ACM, 5-14.\n\nEmerson Murphy-Hill and Andrew P Black. 2012. Programmer-friendly refactoring errors. IEEE Transac-\ntions on Software Engineering 38, 6 (2012), 1417-1431.\n\nEmerson Murphy-Hill, Chris Parnin, and Andrew P Black. 2012. How we refactor, and how we know it.\nIEEE Transactions on Software Engineering 38, 1 (2012), 5-18.\n\nStas Negara, Nicholas Chen, Mohsen Vakilian, Ralph E. Johnson, and Danny Dig. 2013. A Comparative\nStudy of Manual and Automated Refactorings. In 27th European Conference on Object-Oriented Pro-\ngramming (ECOOP). 552-576.\n\nMel O Cinn\u00e9ide, Laurence Tratt, Mark Harman, Steve Counsell, and Iman Hemati Moghadam. 2012. Ex-\nperimental assessment of software metrics using automated refactoring. In International Symposium\non Empirical Software Engineering and Measurement (ESEM). 49-58.\n\nMark O\u2019Keeffe and Mel O Cinn\u00e9ide. 2008. Search-based refactoring for software maintenance. Journal of\nSystems and Software 81, 4 (2008), 502-516.\n\nWilliam F Opdyke. 1992. Refactoring: A program restructuring aid in designing object-oriented application\nframeworks. Ph.D. Dissertation. PhD thesis, University of Illinois at Urbana-Champaign.\n\nFernando EB Otero, Colin G Johnson, Alex A Freitas, and Simon J Thompson. 2010. Refactoring in auto-\nmatically generated programs. In 2nd International Symposium on Search Based Software Engineering\n(SSBSE), Massimiliano Di Penta, Simon Poulding, Lionel Briand, and John Clark (Eds.). Benevento,\nItaly.\n\nAli Ouni, Marouane Kessentini, and Houari Sahraoui. 2013. Search-Based Refactoring Using Recorded Code\nChanges. In 17th European Conference on Software Maintenance and Reengineering (CSMR). 221-230.\n\nAli Ouni, Marouane Kessentini, Houari Sahraoui, and Mounir Boukadoum. 2012a. Maintainability defects\ndetection and correction: a multi-objective approach. Automated Software Engineering 20, 1 (2012), 47\u2014\n79.\n\nAli Ouni, Marouane Kessentini, Houari Sahraoui, and Mohamed Salah Hamdi. 2012b. Search-based refac-\ntoring: Towards semantics preservation. In 28th International Conference on Software Maintenance\n(ICSM). 347-356.\n\nAli Ouni, Marouane Kessentini, Houari Sahraoui, and Mohamed Salah Hamdi. 2013. The use of develop-\nment history in software refactoring using a multi-objective evolutionary algorithm. In 15th annual\nconference on Genetic and Evolutionary Computation (GECCO). 1461-1468.\n\nKyle Prete, Napol Rachatasumrit, Nikita Sudan, and Miryung Kim. 2010. Template-based reconstruction of\ncomplex refactorings. In 26th International Conference on Software Maintenance (ICSM). 1-10.\n\nFawad Qayum and Reiko Heckel. 2009. Local Search-Based Refactoring as Graph Transformation. In Ist\nInternational Symposium on Search Based Software Engineering (SSBSE). 43-46.\n\nDonald Bradley Roberts and Ralph Johnson. 1999. Practical analysis for refactoring. Ph.D. Dissertation.\n\nArnon Rotem-Gal-Oz, Eric Bruno, and Udi Dahan. 2012. SOA patterns. Manning.\n\nHouari Sahraoui, Robert Godin, Thieny Miceli, and others. 2000. Can metrics help to bridge the gap between\nthe improvement of 00 design quality and its automation?. In International Conference on Software\nMaintenance (ICSM). 154-162.\n\nVicent Sales, Ricardo Terra, Luis Fernando Miranda, and Marco Tulio Valente. 2013. Recommending\nmove method refactorings using dependency sets. In 20th Working Conference on Reverse Engineering\n(WCRE). 232-241.\n\nGiuseppe Scanniello, Anna D\u2019Amico, Carmela D\u2019Amico, and Teodora D\u2019Amico. 2010. Using the kleinberg\nalgorithm and vector space model for software system clustering. In 18th International Conference on\nProgram Comprehension (ICPC). 180-189.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n",
                    "39:54 A. Ouni et al.\n\nOlaf Seng, Johannes Stammel, and David Burkhart. 2006. Search-based determination of refactorings for\nimproving the class structure of object-oriented systems. In 8th annual Conference on Genetic and Evo-\nlutionary Computation (GECCO). ACM, 1909-1916.\n\nRaed Shatnawi and Wei Li. 2011. An empirical assessment of refactoring impact on software quality using\na hierarchical quality model. International Journal of Software Engineering and Its Applications 5, 4\n(2011), 127-149.\n\nDanilo Silva, Ricardo Terra, and Marco Tulio Valente. 2014. Recommending automated extract method\nrefactorings. In 22nd International Conference on Program Comprehension (ICPC). 146-156.\n\nGustavo Soares, Rohit Gheyi, and Tiago Massoni. 2013. Automated behavioral testing of refactoring engines.\nIEEE Transactions on Software Engineering 39, 2 (2013), 147-162.\n\nLadan Tahvildari and Kostas Kontogiannis. 2003. A metric-based approach to enhance design quality\nthrough meta-pattern transformations. In 7th European Conference on Software Maintenance and\nReengineering (CSMR). 183-192.\n\nFrank Tip and Jens Palsberg. 2000. Scalable Propagation-based Call Graph Construction Algorithms. In\n15th Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA).\n281-293.\n\nNikolaos Tsantalis and Alexander Chatzigeorgiou. 2009. Identification of move method refactoring opportu-\nnities. IEEE Transactions on Software Engineering 35, 3 (2009), 347-367.\n\nNikolaos Tsantalis and Alexander Chatzigeorgiou. 2011. Identification of extract method refactoring oppor-\ntunities for the decomposition of methods. Journal of Systems and Software 84, 10 (2011), 1757-1782.\n\nRaja Vall\u00e9e-Rai, Etienne Gagnon, Laurie Hendren, Patrick Lam, Patrice Pominville, and Vijay Sundaresan.\n2000. Optimizing Java Bytecode Using the Soot Framework: Is It Feasible? In Compiler Construction,\nDavid A. Watt (Ed.). Lecture Notes in Computer Science, Vol. 1781. Springer Berlin Heidelberg, 18-34.\n\nAtsushi Yamashita and Leon Moonen. 2013. Do developers care about code smells? An exploratory survey.\nIn 20th Working Conference on Reverse Engineering (WCRE). IEEE, 242-251.\n\nMinhaz F Zibran and Chanchal K Roy. 2011. A constraint programming approach to conflict-aware optimal\nscheduling of prioritized code clone refactoring. In 11th International Working Conference on Source\nCode Analysis and Manipulation (SCAM). 105-114.\n\nEckart Zitzler and Lothar Thiele. 1998. Multiobjective optimization using evolutionary algorithms\u2014a com-\nparative case study. In Parallel problem solving from nature-PPSN V. Springer, 292-301.\n\nACM Transactions on Software Engineering and Methodology, Vol. 9, No. 4, Article 39, Pub. date: March 2015.\n"
                ]
            }
        }
    ]
}
```